
ibookroot october 20 2007 fourier analysis
ibookroot october 20 2007 princeton lectures analysis fourier analysis introduction ii complex analysis iii real analysis measure theory integration hilbert spaces
ibookroot october 20 2007 princeton lectures analysis fourier analysis introduction elias m. stein rami shakarchi princeton university press princeton oxford
copyright 2003 princeton university press published princeton university press 41 william street princeton new jersey 08540 united kingdom princeton university press 6 oxford street woodstock oxfordshire ox20 1tw rights reserved library congress control number 2003103688 isbn 9780691113845 british library cataloginginpublication data available publisher would like acknowledge authors volume providing cameraready copy book printed printed acidfree paper. press.princeton.edu printed united states america 5 7 9 10 8 6
ibookroot october 20 2007 grandchildren carolyn alison jason e.m.s. parents mohamed mireille brother karim r.s.

ibookroot october 20 2007 foreword beginning spring 2000 series four onesemester courses taught princeton university whose purpose present integrated manner core areas analysis. objective make plain organic unity exists various parts subject illustrate wide applicability ideas analysis ﬁelds mathematics science. present series books elaboration lectures given. number excellent texts dealing individual parts cover exposition aims diﬀerent goal pre senting various subareas analysis separate disciplines rather highly interconnected. view seeing relations resulting synergies motivate reader attain better understanding subject whole. outcome mind concentrated main ideas theorems shaped ﬁeld sometimes sacriﬁcing systematic approach sensitive historical order logic subject developed. organized exposition four volumes reﬂecting material covered semester. contents may broadly sum marized follows i. fourier series integrals. ii. complex analysis. iii. measure theory lebesgue integration hilbert spaces. iv. selection topics including functional analysis distri butions elements probability theory. however listing give complete picture many interconnections presented applications branches highlighted. give examples ele ments ﬁnite fourier series studied book i lead dirichlet characters inﬁnitude primes arithmetic progression xray radon transforms arise number
ibookroot october 20 2007 foreword problems book i reappear book iii play important role understanding besicovitchlike sets two three dimensions fatous theorem guarantees existence boundary values bounded holomorphic functions disc whose proof relies ideas devel oped ﬁrst three books theta function ﬁrst occurs book solution heat equation used book ii ﬁnd number ways integer represented sum two four squares analytic continuation zeta function. words books courses based. courses given rather intensive pace 48 lecturehours semester. weekly problem sets played indispens able part result exercises problems similarly im portant role books. chapter series exercises tied directly text easy others may require eﬀort. however substantial number hints given enable reader attack exercises. also involved challenging problems ones diﬃcult go beyond scope text marked asterisk. despite substantial connections exist diﬀerent volumes enough overlapping material provided ﬁrst three books requires minimal prerequisites acquaintance elementary topics analysis limits series diﬀerentiable functions riemann integration together exposure lin ear algebra. makes books accessible students interested diverse disciplines mathematics physics engineering ﬁnance undergraduate graduate level. great pleasure express appreciation aided enterprise. particularly grateful stu dents participated four courses. continuing interest enthusiasm dedication provided encouragement made project possible. also wish thank adrian banner jose luis rodrigo special help running courses eﬀorts see students got class. addition adrian banner also made valuable suggestions incorporated text. viii
ibookroot october 20 2007 ix foreword wish also record note special thanks following in dividuals charles feﬀerman taught ﬁrst week successfully launching whole project paul hagelstein addition read ing part manuscript taught several weeks one courses since taken teaching second round series daniel levine gave valuable help proofreading. last least thanks go gerree pecht consummate skill type setting time energy spent preparation aspects lectures transparencies notes manuscript. also happy acknowledge indebtedness support received 250th anniversary fund princeton university national science foundations vigre program. elias m. stein rami shakarchi princeton new jersey august 2002

ibookroot october 20 2007 preface book eﬀort present overall view analysis must start deal following questions one begin initial subjects treated order relevant concepts basic techniques developed answers questions guided view centrality fourier analysis role played development subject fact ideas permeate much present day analysis. reasons devoted ﬁrst volume exposition basic facts fourier series taken together study elements fourier transforms ﬁnite fourier analysis. starting way allows one see rather easily certain applications sciences together link topics partial diﬀerential equations number theory. later volumes several connec tions taken systematic point view ties exist complex analysis real analysis hilbert space theory areas explored further. spirit mindful overburden begin ning student diﬃculties inherent subject proper appreciation subtleties technical complications arise come one mastered initial ideas in volved. point view led us following choice material present volume . fourier series. early stage appropriate intro duce measure theory lebesgue integration. reason treatment fourier series ﬁrst four chapters carried context riemann integrable functions. even restriction substantial part theory developed de tailing convergence summability also variety connections problems mathematics illustrated. . fourier transform. reasons instead undertaking theory general setting conﬁne chapters 5 6 largely framework test functions. despite lim itations learn number basic interesting facts fourier analysis rd relation areas including wave equation radon transform.
ibookroot october 20 2007 preface book . finite fourier analysis. introductory subject par excel lence limits integrals explicitly present. nev ertheless subject several striking applications including proof inﬁnitude primes arithmetic progression. taking account introductory nature ﬁrst volume kept prerequisites minimum. although suppose acquaintance notion riemann integral provide appendix contains results integration needed text. hope approach facilitate goal set ourselves inspire interested reader learn fascinating subject discover fourier analysis aﬀects decisively parts mathematics science. xii
ibookroot october 20 2007 contents foreword preface chapter 1. genesis fourier analysis 1 1 vibrating string 2 1.1 derivation wave equation 6 1.2 solution wave equation 8 1.3 example plucked string 16 2 heat equation 18 2.1 derivation heat equation 18 2.2 steadystate heat equation disc 19 3 exercises 22 4 problem 27 chapter 2. basic properties fourier series 29 1 examples formulation problem 30 1.1 main deﬁnitions examples 34 2 uniqueness fourier series 39 3 convolutions 44 4 good kernels 48 5 ces aro abel summability applications fourier series 51 5.1 ces aro means summation 51 5.2 fej ers theorem 52 5.3 abel means summation 54 5.4 poisson kernel dirichlets problem unit disc 55 6 exercises 58 7 problems 65 chapter 3. convergence fourier series 69 1 meansquare convergence fourier series 70 1.1 vector spaces inner products 70 1.2 proof meansquare convergence 76 2 return pointwise convergence 81 2.1 local result 81 2.2 continuous function diverging fourier series 83 vii xi
ibookroot october 20 2007 contents 3 exercises 87 4 problems 95 chapter 4. applications fourier series 100 1 isoperimetric inequality 101 2 weyls equidistribution theorem 105 3 continuous nowhere diﬀerentiable function 113 4 heat equation circle 118 5 exercises 120 6 problems 125 chapter 5. fourier transform r 129 1 elementary theory fourier transform 131 1.1 integration functions real line 131 1.2 deﬁnition fourier transform 134 1.3 schwartz space 134 1.4 fourier transform 136 1.5 fourier inversion 140 1.6 plancherel formula 142 1.7 extension functions moderate decrease 144 1.8 weierstrass approximation theorem 144 2 applications partial diﬀerential equations 145 2.1 timedependent heat equation real line 145 2.2 steadystate heat equation upper half plane 149 3 poisson summation formula 153 3.1 theta zeta functions 155 3.2 heat kernels 156 3.3 poisson kernels 157 4 heisenberg uncertainty principle 158 5 exercises 161 6 problems 169 chapter 6. fourier transform rd 175 1 preliminaries 176 1.1 symmetries 176 1.2 integration rd 178 2 elementary theory fourier transform 180 3 wave equation rd r 184 3.1 solution terms fourier transforms 184 3.2 wave equation r3 r 189 xiv
ibookroot october 20 2007 contents xv 3.3 wave equation r2 r descent 194 4 radial symmetry bessel functions 196 5 radon transform applications 198 5.1 xray transform r2 199 5.2 radon transform r3 201 5.3 note plane waves 207 6 exercises 207 7 problems 212 chapter 7. finite fourier analysis 218 1 fourier analysis zn 219 1.1 group zn 219 1.2 fourier inversion theorem plancherel identity zn 221 1.3 fast fourier transform 224 2 fourier analysis ﬁnite abelian groups 226 2.1 abelian groups 226 2.2 characters 230 2.3 orthogonality relations 232 2.4 characters total family 233 2.5 fourier inversion plancherel formula 235 3 exercises 236 4 problems 239 chapter 8. dirichlets theorem 241 1 little elementary number theory 241 1.1 fundamental theorem arithmetic 241 1.2 inﬁnitude primes 244 2 dirichlets theorem 252 2.1 fourier analysis dirichlet characters reduc tion theorem 254 2.2 dirichlet lfunctions 255 3 proof theorem 258 3.1 logarithms 258 3.2 lfunctions 261 3.3 nonvanishing lfunction 265 4 exercises 275 5 problems 279 appendix integration 281 1 deﬁnition riemann integral 281
ibookroot october 20 2007 x contents 1.1 basic properties 282 1.2 sets measure zero discontinuities inte grable functions 286 2 multiple integrals 289 2.1 riemann integral rd 289 2.2 repeated integrals 291 2.3 change variables formula 292 2.4 spherical coordinates 293 3 improper integrals. integration rd 294 3.1 integration functions moderate decrease 294 3.2 repeated integrals 295 3.3 spherical coordinates 297 notes references 298 bibliography 300 symbol glossary 303 index 305 vi
ibookroot october 20 2007 1 genesis fourier analysis regarding researches dalembert euler could one add knew expansion made imperfect use it. persuaded arbitrary discontinuous func tion could never resolved series kind even seem anyone developed constant cosines multiple arcs ﬁrst problem solve theory heat. j. fourier 18089 beginning problem vibrating string later investigation heat ﬂow led development fourier analysis. laws governing distinct physical phenomena expressed two diﬀerent partial diﬀerential equations wave heat equations solved terms fourier series. want start describing detail development ideas. initially context problem vibrating string proceed three steps. first de scribe several physical empirical concepts motivate correspond ing mathematical ideas importance study. are role functions cos t sin t eit suggested simple harmonic mo tion use separation variables derived phenomenon standing waves related concept linearity connected superposition tones. next derive partial diﬀerential equation governs motion vibrating string. finally use learned physical nature problem expressed mathematically solve equation. last section use approach study problem heat diﬀusion. given introductory nature chapter subject matter covered presentation cannot based purely mathematical rea soning. rather proceeds plausibility arguments aims provide motivation rigorous analysis succeeding chap ters. impatient reader wishes begin immediately theorems subject may prefer pass directly next chapter.
ibookroot october 20 2007 2 chapter 1. genesis fourier analysis 1 vibrating string problem consists study motion string ﬁxed end points allowed vibrate freely. mind physical systems strings musical instrument. mentioned above begin brief description several observable physical phenomena study based. are . simple harmonic motion . standing traveling waves . harmonics superposition tones. understanding empirical facts behind phenomena moti vate mathematical approach vibrating strings. simple harmonic motion simple harmonic motion describes behavior basic oscil latory system called simple harmonic oscillator therefore natural place start study vibrations. consider mass m attached horizontal spring attached ﬁxed wall assume system lies frictionless surface. choose axis whose origin coincides center mass rest that is spring neither stretched compressed shown figure 1. mass displaced initial equilibrium 0 yt 0 figure 1. simple harmonic oscillator position released undergo simple harmonic motion. motion described mathematically found diﬀerential equation governs movement mass. let yt denote displacement mass time t. assume spring ideal sense satisﬁes hookes law restoring force f exerted spring mass given f kyt.
ibookroot october 20 2007 1. vibrating string 3 k 0 given physical quantity called spring constant. applying newtons law force mass acceleration obtain kyt myt use notation y denote second derivative respect t. c p km second order ordinary diﬀerential equation becomes 1 yt c2yt 0. general solution equation 1 given yt cos ct b sin ct b constants. clearly functions form solve equation 1 exercise 6 outlines proof twice diﬀerentiable solutions diﬀerential equation. expression yt quantity c given b real numbers. order determine particular solution equation must impose two initial conditions view two unknown constants b. example given y0 y0 initial position velocity mass solution physical problem unique given yt y0 cos ct y0 c sin ct . one easily verify exist constants 0 ϕ r cos ct b sin ct cosct ϕ. physical interpretation given above one calls a2 b2 amplitude motion c natural frequency ϕ phase uniquely determined integer multiple 2π 2πc period motion. typical graph function cosct ϕ illustrated figure 2 exhibits wavelike pattern obtained translating stretching or shrinking usual graph cos t. make two observations regarding examination simple har monic motion. ﬁrst mathematical description elementary oscillatory system namely simple harmonic motion involves
ibookroot october 20 2007 4 chapter 1. genesis fourier analysis figure 2. graph cosct ϕ basic trigonometric functions cos sin t. impor tant follows recall connection functions complex numbers given eulers identity eit cos sin t. second observation simple harmonic motion determined function time two initial conditions one determining position velocity speciﬁed example time 0. property shared general oscillatory systems shall see below. standing traveling waves turns out vibrating string viewed terms one dimensional wave motions. want describe two kinds mo tions lend simple graphic representations. . first consider standing waves. wavelike motions described graphs ux t developing time shown figure 3. words initial proﬁle ϕx representing wave time 0 amplifying factor ψt depending t ux t ux t ϕxψt. nature standing waves suggests mathematical idea separation variables return later. . second type wave motion often observed nature traveling wave. description particularly simple
ibookroot october 20 2007 1. vibrating string 5 ux 0 ϕx ux t0 x figure 3. standing wave diﬀerent moments time 0 t0 initial proﬁle fx ux t equals fx 0. evolves proﬁle displaced right ct units c positive constant namely ux t fx ct. graphically situation depicted figure 4. fx fx ct0 figure 4. traveling wave two diﬀerent moments time 0 t0 since movement rate c constant represents velocity wave. function fx ct onedimensional traveling wave moving right. similarly ux t fx ct onedimensional traveling wave moving left.
ibookroot october 20 2007 6 chapter 1. genesis fourier analysis harmonics superposition tones ﬁnal physical observation want mention without going details now one musicians aware since time immemorial. existence harmonics overtones. pure tones accompanied combinations overtones primar ily responsible timbre or tone color instrument. idea combination superposition tones implemented mathematically basic concept linearity shall see below. turn attention main problem describing motion vibrating string. first derive wave equation is partial diﬀerential equation governs motion string. 1.1 derivation wave equation imagine homogeneous string placed x yplane stretched along xaxis x 0 x l. set vibrate displacement ux t function x t goal derive diﬀerential equation governs function. purpose consider string subdivided large number n masses which think individual particles distributed uniformly along xaxis nth particle xcoordinate xn nln. shall therefore conceive vibrat ing string complex system n particles oscillating vertical direction only however unlike simple harmonic oscillator considered previously particle oscillation linked immediate neighbor tension string. yn1 yn yn1 xn1 xn1 xn h figure 5. vibrating string discrete system masses
ibookroot october 20 2007 1. vibrating string 7 set ynt uxn t note xn1 xn h h ln. assume string constant density ρ 0 reasonable assign mass equal ρh particle. newtons law ρhy nt equals force acting nth particle. make simple assumption force due eﬀect two nearby particles ones xcoordinates xn1 xn1 see figure 5. assume force or tension coming right nth particle proportional yn1 ynh h distance xn1 xn hence write tension ³τ h yn1 yn τ 0 constant equal coeﬃcient tension string. similar force coming left ³τ h yn1 yn. altogether adding forces gives us desired relation oscillators ynt namely 2 ρhy nt τ h yn1t yn1t 2ynt . one hand notation chosen above see yn1t yn1t 2ynt uxn h t uxn h t 2uxn t. hand reasonable function fx that is one continuous second derivatives fx h fx h 2fx h2 f x h 0. thus may conclude dividing h 2 letting h tend zero that is n goes inﬁnity ρ 2u t2 τ 2u x2 1 c2 2u t2 2u x2 c p τρ. relation known onedimensional wave equation simply wave equation. reasons apparent later coeﬃcient c 0 called velocity motion.
ibookroot october 20 2007 8 chapter 1. genesis fourier analysis connection partial diﬀerential equation make im portant simplifying mathematical remark. scaling language physics change units. is think coordinate x x ax appropriate positive constant. now terms new coordinate x interval 0 x l becomes 0 x la. similarly replace time coordinate bt b another positive constant. set ux t ux t u x au x 2u x2 a2 2u x2 similarly derivatives t. choose b appropri ately transform onedimensional wave equation 2u t 2 2u x2 eﬀect setting velocity c equal 1. moreover freedom transform interval 0 x l 0 x π. we shall see choice π convenient many circumstances. accomplished taking lπ b lcπ. solve new equation course return original equation making inverse change variables. hence sacriﬁce generality thinking wave equation given interval 0 π velocity c 1. 1.2 solution wave equation derived equation vibrating string explain two methods solve it . using traveling waves . using superposition standing waves. ﬁrst approach simple elegant directly give full insight problem second method accomplishes that moreover wide applicability. ﬁrst believed second method applied simple cases initial position velocity string given superposition standing waves. however consequence fouriers ideas became clear problem could worked either way initial conditions.
ibookroot october 20 2007 1. vibrating string 9 traveling waves simplify matters before assume c 1 l π equation wish solve becomes 2u t2 2u x2 0 x π. crucial observation following f twice diﬀerentiable function ux t fx t ux t fx t solve wave equation. veriﬁcation simple exercise diﬀerentiation. note graph ux t fx t time 0 simply graph f time 1 becomes graph f translated right 1. therefore recognize fx t traveling wave travels right speed 1. similarly ux t fx t wave traveling left speed 1. motions depicted figure 6. fx t fx fx t figure 6. waves traveling directions discussion tones combinations leads us observe wave equation linear. means ux t vx t particular solutions αux t βvx t α β constants. therefore may superpose two waves traveling opposite directions ﬁnd whenever f g twice diﬀerentiable functions ux t fx t gx t solution wave equation. fact show solutions take form. drop moment assumption 0 x π suppose u twice diﬀerentiable function solves wave equation
ibookroot october 20 2007 10 chapter 1. genesis fourier analysis real x t. consider following new set variables ξ x t η x t deﬁne vξ η ux t. change variables formula shows v satisﬁes 2v ξη 0. integrating relation twice gives vξ η fξ gη implies ux t fx t gx t functions f g. must connect result original problem is physical motion string. there imposed restrictions 0 x π initial shape string ux 0 fx also fact string ﬁxed end points namely u0 t uπ t 0 t. use simple observation above ﬁrst extend f r making odd1 π π periodic2 x period 2π similarly ux t solution problem. extension u solves wave equation r ux 0 fx x r. therefore ux t fx t gx t setting 0 ﬁnd fx gx fx. since many choices f g satisfy identity suggests imposing another initial condition u similar two initial condi tions case simple harmonic motion namely initial velocity string denote gx u t x 0 gx course g0 gπ 0. again extend g r ﬁrst mak ing odd π π periodic period 2π. two initial conditions position velocity translate following sys tem ½ fx gx fx f x gx gx . 1a function f deﬁned set u odd x u whenever x u fx fx even fx fx. 2a function f r periodic period ω fx ω fx x.
ibookroot october 20 2007 1. vibrating string 11 diﬀerentiating ﬁrst equation adding second obtain 2f x f x gx. similarly 2gx f x gx hence constants c1 c2 fx 1 2 fx z x 0 gy dy c1 gx 1 2 fx z x 0 gy dy c2. since fx gx fx conclude c1 c2 0 therefore ﬁnal solution wave equation given initial conditions takes form ux t 1 2 fx t fx t 1 2 z xt xt gy dy. form solution known dalemberts formula. observe extensions chose f g guarantee string always ﬁxed ends is u0 t uπ t 0 t. ﬁnal remark order. passage 0 r back 0 made above exhibits time reversal property wave equation. words solution u wave equation 0 leads solution udeﬁned negative time 0 simply setting ux t ux t fact follows invariance wave equation transformation 7t. situation quite diﬀerent case heat equation. superposition standing waves turn second method solving wave equation based two fundamental conclusions previous physical obser vations. considerations standing waves led look special solutions wave equation form ϕxψt. procedure works equally well contexts in case heat equation instance called separation variables constructs solutions called pure tones. linearity
ibookroot october 20 2007 12 chapter 1. genesis fourier analysis wave equation expect combine pure tones complex combination sound. pushing idea further hope ultimately express general solution wave equation terms sums particular solutions. note one side wave equation involves diﬀerentiation x other diﬀerentiation t. observation pro vides another reason look solutions equation form ux t ϕxψt that is separate variables hope reduce diﬃcult partial diﬀerential equation system simpler ordinary diﬀerential equations. case wave equation u form get ϕxψt ϕxψt therefore ψt ψt ϕx ϕx . key observation lefthand side depends t righthand side x. happen sides equal constant say λ. therefore wave equation reduces following 3 ½ ψt λψt 0 ϕx λϕx 0. focus attention ﬁrst equation system. point reader recognize equation obtained study simple harmonic motion. note need consider case λ 0 since λ 0 solution ψ oscillate time varies. therefore may write λ m2 solution equation given ψt cos mt b sin mt. similarly ﬁnd solution second equation 3 ϕx cos mx b sin mx. take account string attached x 0 x π. translates ϕ0 ϕπ 0 turn gives 0 b 0 must integer. 0 solution vanishes identically 1 may rename constants reduce
ibookroot october 20 2007 1. vibrating string 13 case 1 since function sin odd cos even. finally arrive guess 1 function umx t am cos mt bm sin mt sin mx recognize standing wave solution wave equa tion. note argument divided ϕ ψ sometimes vanish one must actually check hand standing wave um solves equation. straightforward calculation left exercise reader. proceeding analysis wave equation pause discuss standing waves detail. terminology comes looking graph umx t ﬁxed t. suppose ﬁrst 1 take ux t cos sin x. then figure 7 a gives graph u diﬀerent values t. b a 0 π 2π 0 π 2π π 2π π 2π figure 7. fundamental tone a overtones b diﬀerent moments time case 1 corresponds fundamental tone ﬁrst har monic vibrating string. take 2 look ux t cos 2t sin 2x. corre sponds ﬁrst overtone second harmonic motion described figure 7 b. note uπ2 t 0 t. points remain motionless time called nodes points whose motion maximum amplitude named antinodes. higher values get overtones higher harmonics. note increases frequency increases period 2πm
ibookroot october 20 2007 14 chapter 1. genesis fourier analysis decreases. therefore fundamental tone lower frequency overtones. return original problem. recall wave equation linear sense u v solve equation αu βv constants α β. allows us construct solutions taking linear combinations standing waves um. technique called superposition leads ﬁnal guess solution wave equation 4 ux t x m1 am cos mt bm sin mt sin mx. note sum inﬁnite questions convergence arise since arguments far formal worry point now. suppose expression gave solutions wave equa tion. require initial position string time 0 given shape graph function f 0 π course f0 fπ 0 would ux 0 fx hence x m1 sin mx fx. since initial shape string reasonable function f must ask following basic question given function f 0 π with f0 fπ 0 ﬁnd coeﬃcients 5 fx x m1 sin mx question stated loosely lot eﬀort next two chapters book formulate question precisely attempt answer it. basic problem initiated study fourier analysis. simple observation allows us guess formula giving expansion 5 hold. indeed multiply sides sin nx
ibookroot october 20 2007 1. vibrating string 15 integrate 0 π working formally obtain z π 0 fx sin nx dx z π 0 ã x m1 sin mx sin nx dx x m1 z π 0 sin mx sin nx dx π 2 used fact z π 0 sin mx sin nx dx ½ 0 n π2 n. therefore guess an called nth fourier sine coeﬃcient f 6 2 π z π 0 fx sin nx dx. shall return formula similar ones later. one transform question fourier sine series 0 π general question interval π π. could express f 0 π terms sine series expansion would also hold π π extend f interval making odd. similarly one ask even function gx π π expressed cosine series namely gx x m0 a cos mx. generally since arbitrary function f π π expressed f g f odd g even3 may ask f written fx x m1 sin mx x m0 a cos mx applying eulers identity eix cos x sin x could hope f takes form fx x m ameimx. 3take example fx fx fx2 gx fx fx2.
ibookroot october 20 2007 16 chapter 1. genesis fourier analysis analogy 6 use fact 1 2π z π π eimxeinx dx ½ 0 n 1 n m see one expects 1 2π z π π fxeinx dx. quantity called nth fourier coeﬃcient f. reformulate problem raised above question given reasonable function f π π fourier coeﬃcients deﬁned above true 7 fx x m ameimx formulation problem terms complex exponentials form shall use follows. joseph fourier 17681830 ﬁrst believe arbitrary function f could given series 7. words idea function linear combination possibly inﬁnite basic trigonometric functions sin mx cos mx ranges integers.4 although idea implicit earlier work fourier conviction predecessors lacked used study heat diﬀusion began subject fourier analysis. discipline ﬁrst developed solve certain physical problems proved many applications mathematics ﬁelds well shall see later. return wave equation. formulate problem correctly must impose two initial conditions experience simple harmonic motion traveling waves indicated. conditions assign initial position velocity string. is require u satisfy diﬀerential equation two conditions ux 0 fx u t x 0 gx 4the ﬁrst proof general class functions represented fourier series given later dirichlet see problem 6 chapter 4.
ibookroot october 20 2007 1. vibrating string 17 f g preassigned functions. note consistent 4 requires f g expressible fx x m1 sin mx gx x m1 mbm sin mx. 1.3 example plucked string apply reasoning particular problem plucked string. simplicity choose units string taken interval 0 π satisﬁes wave equation c 1. string assumed plucked height h point p 0 p π initial position. is take initial position triangular shape given fx xh p 0 x p hπ x π p p x π depicted figure 8. 0 h p π figure 8. initial position plucked string also choose initial velocity gx identically equal 0. then compute fourier coeﬃcients f exercise 9 assuming answer question raised 5 positive obtain fx x m1 sin mx 2h m2 sin mp pπ p.
ibookroot october 20 2007 18 chapter 1. genesis fourier analysis thus 8 ux t x m1 cos mt sin mx note series converges absolutely. solution also expressed terms traveling waves. fact 9 ux t fx t fx t 2 . fx deﬁned x follows ﬁrst f extended π π making odd f extended whole real line making periodic period 2π is fx 2πk fx integers k. observe 8 implies 9 view trigonometric identity cos v sin u 1 2 sinu v sinu v. ﬁnal remark note unsatisfactory aspect so lution problem however nature things. since initial data fx plucked string twice continuously dif ferentiable neither function u given 9. hence u truly solution wave equation ux t represent position plucked string satisfy partial diﬀerential equation set solve state aﬀairs may understood properly realize u solve equation appropriate generalized sense. better understanding phenomenon requires ideas relevant study weak solutions theory dis tributions. topics consider later books iii iv. 2 heat equation discuss problem heat diﬀusion following framework wave equation. first derive timedependent heat equation study steadystate heat equation disc leads us back basic question 7. 2.1 derivation heat equation consider inﬁnite metal plate model plane r2 suppose given initial heat distribution time 0. let temperature point x y time denoted ux y t.
ibookroot october 20 2007 2. heat equation 19 consider small square centered x0 y0 sides parallel axis side length h shown figure 9. amount heat energy time given ht σ z z ux y t dx dy σ 0 constant called speciﬁc heat material. there fore heat ﬂow h t σ z z u t dx dy approximately equal σh2 u t x0 y0 t since area h2. apply newtons law cooling states heat ﬂows higher lower temperature rate proportional diﬀerence is gradient. x0 h2 y0 x0 y0 h h figure 9. heat ﬂow small square heat ﬂow vertical side right therefore κh u xx0 h2 y0 t κ 0 conductivity material. similar argument sides shows total heat ﬂow square
ibookroot october 20 2007 20 chapter 1. genesis fourier analysis given κh u xx0 h2 y0 t u xx0 h2 y0 t u y x0 y0 h2 t u y x0 y0 h2 t . applying mean value theorem letting h tend zero ﬁnd σ κ u t 2u x2 2u y2 called timedependent heat equation often abbreviated heat equation. 2.2 steadystate heat equation disc long period time heat exchange system reaches thermal equilibrium ut 0. case timedependent heat equation reduces steadystate heat equation 10 2u x2 2u y2 0. operator 2x2 2y2 importance mathematics physics often abbreviated and given name laplace operator laplacian. steadystate heat equation written u 0 solutions equation called harmonic functions. consider unit disc plane x y r2 x2 y2 1 whose boundary unit circle c. polar coordinates r θ 0 r 0 θ 2π r θ 0 r 1 c r θ r 1. problem often called dirichlet problem for laplacian unit disc solve steadystate heat equation unit
ibookroot october 20 2007 2. heat equation 21 disc subject boundary condition u f c. corresponds ﬁxing predetermined temperature distribution circle waiting long time looking temperature distribution inside disc. u1 θ fθ x 0 u 0 figure 10. dirichlet problem disc method separation variables turn useful equation 10 diﬃculty comes fact boundary condition easily expressed terms rectangular coordinates. since boundary condition best described coordinates r θ namely u1 θ fθ rewrite laplacian polar coordinates. application chain rule gives exercise 10 u 2u r2 1 r u r 1 r2 2u θ2 . multiply sides r2 since u 0 get r2 2u r2 ru r 2u θ2 . separating variables looking solution form ur θ frgθ ﬁnd r2f r rf r fr gθ gθ .
ibookroot october 20 2007 22 chapter 1. genesis fourier analysis since two sides depend diﬀerent variables must constant say equal λ. therefore get following equations ½ gθ λgθ 0 r2f r rf r λfr 0. since g must periodic period 2π implies λ 0 as seen before λ m2 integer hence gθ cos mθ b sin mθ. application eulers identity eix cos x sin x allows one rewrite g terms complex exponentials gθ aeimθ beimθ. λ m2 0 two simple solutions equation f fr rm fr rm exercise 11 gives information solutions. 0 fr 1 fr log r two solu tions. 0 note rm grows unboundedly large r tends zero frgθ unbounded origin occurs 0 fr log r. reject solutions contrary intuition. therefore left following special functions umr θ rmeimθ z. make important observation 10 linear case vibrating string may superpose special solutions obtain presumed general solution ur θ x m amrmeimθ. expression gave solutions steadystate heat equation reasonable f u1 θ x m ameimθ fθ. therefore ask context given reasonable function f 0 2π f0 f2π ﬁnd coeﬃcients fθ x m ameimθ
ibookroot october 20 2007 3. exercises 23 historical note dalembert in 1747 ﬁrst solved equation vibrating string using method traveling waves. solution elaborated euler year later. 1753 d. bernoulli proposed solution intents purposes fourier series given 4 euler entirely convinced full generality since could hold arbitrary function could expanded fourier series. dalembert mathematicians also doubts. viewpoint changed fourier in 1807 study heat equation conviction work eventually led others complete proof general function could represented fourier series. 3 exercises 1. z x iy complex number x r deﬁne z x2 y212 call quantity modulus absolute value z. a geometric interpretation z b show z 0 z 0. c show λ r λz λz λ denotes standard absolute value real number. d z1 z2 two complex numbers prove z1z2 z1z2 z1 z2 z1 z2. e show z 0 1z 1z. 2. z x iy complex number x r deﬁne complex conjugate z z x iy. a geometric interpretation z b show z2 zz. c prove z belongs unit circle 1z z.
ibookroot october 20 2007 24 chapter 1. genesis fourier analysis 3. sequence complex numbers wn n1 said converge exists w c lim nwn w 0 say w limit sequence. a show converging sequence complex numbers unique limit. sequence wn n1 said cauchy sequence every ϵ 0 exists positive integer n wn wm ϵ whenever n n. b prove sequence complex numbers converges cauchy sequence. hint similar theorem exists convergence sequence real numbers. carry sequences complex numbers series p n1 zn complex numbers said converge sequence formed partial sums sn n x n1 zn converges. let an n1 sequence nonnegative real numbers series p n converges. c show zn n1 sequence complex numbers satisfying zn an n series p n zn converges. hint use cauchy criterion. 4. z c deﬁne complex exponential ez x n0 zn n . a prove deﬁnition makes sense showing series converges every complex number z. moreover show conver gence uniform5 every bounded subset c. b z1 z2 two complex numbers prove ez1ez2 ez1z2. hint use binomial theorem expand z1 z2n well formula binomial coeﬃcients. 5a sequence functions fnz n1 said uniformly convergent set exists function f every ϵ 0 integer n fnz fz ϵ whenever n n z s.
ibookroot october 20 2007 3. exercises 25 c show z purely imaginary is z iy r eiy cos sin y. eulers identity. hint use power series. d generally exiy excos sin y whenever x r show exiy ex. e prove ez 1 z 2πki integer k. f show every complex number z x iy written form z reiθ r unique range 0 r θ r unique integer multiple 2π. check r z θ arctanyx whenever formulas make sense. g particular eiπ2. geometric meaning multiplying complex number i eiθ θ r h given θ r show cos θ eiθ eiθ 2 sin θ eiθ eiθ 2i . also called eulers identities. i use complex exponential derive trigonometric identities cosθ ϑ cos θ cos ϑ sin θ sin ϑ show 2 sin θ sin ϕ cosθ ϕ cosθ ϕ 2 sin θ cos ϕ sinθ ϕ sinθ ϕ. calculation connects solution given dalembert terms traveling waves solution terms superposition standing waves.
ibookroot october 20 2007 26 chapter 1. genesis fourier analysis 5. verify fx einx periodic period 2π 1 2π z π π einx dx ½ 1 n 0 0 n 0. use fact prove n 1 1 π z π π cos nx cos mx dx ½ 0 n m 1 n m similarly 1 π z π π sin nx sin mx dx ½ 0 n m 1 n m. finally show z π π sin nx cos mx dx 0 n m. hint calculate einxeimx einxeimx einxeimx einxeimx. 6. prove f twice continuously diﬀerentiable function r solution equation f t c2ft 0 exist constants b ft cos ct b sin ct. done diﬀerentiating two functions gt ft cos ct c1f t sin ct ht ft sin ct c1f t cos ct. 7. show b real one write cos ct b sin ct cosct ϕ a2 b2 ϕ chosen cos ϕ a2 b2 sin ϕ b a2 b2 . 8. suppose f function a b two continuous derivatives. show whenever x x h belong a b one may write fx h fx hf x h2 2 f x h2ϕh
ibookroot october 20 2007 4. problem 27 ϕh 0 h 0. deduce fx h fx h 2fx h2 f x h 0. hint simply taylor expansion. may obtained noting fx h fx z xh x f y dy writing f y f x y xf x y xψy x ψh 0 h 0. 9. case plucked string use formula fourier sine coeﬃ cients show 2h m2 sin mp pπ p. position p second fourth . . . harmonics missing position p third sixth . . . harmonics missing 10. show expression laplacian 2 x2 2 y2 given polar coordinates formula 2 r2 1 r r 1 r2 2 θ2 . also prove u x 2 u y 2 u r 2 1 r2 u θ 2 . 11. show n z solutions diﬀerential equation r2f r rf r n2fr 0 twice diﬀerentiable r 0 given linear combinations rn rn n 0 1 log r n 0. hint f solves equation write fr grrn ﬁnd equation satisﬁed g conclude rgr 2ngr c c constant.
ibookroot october 20 2007 28 chapter 1. genesis fourier analysis u f1 u 0 u f0 u 0 0 1 π u 0 figure 11. dirichlet problem rectangle 4 problem 1. consider dirichlet problem illustrated figure 11. precisely look solution steadystate heat equation u 0 rectangle r x y 0 x π 0 y 1 vanishes vertical sides r ux 0 f0x ux 1 f1x f0 f1 initial data ﬁx temperature distribution horizontal sides rectangle. use separation variables show f0 f1 fourier expansions f0x x k1 ak sin kx f1x x k1 bk sin kx ux y x k1 µsinh k1 y sinh k ak sinh ky sinh k bk sin kx. recall deﬁnitions hyperbolic sine cosine functions sinh x ex ex 2 cosh x ex ex 2 . compare result solution dirichlet problem strip ob tained problem 3 chapter 5.
ibookroot october 20 2007 2 basic properties fourier series nearly ﬁfty years passed without progress question analytic representation arbitrary function assertion fourier threw new light subject. thus new era began de velopment part mathematics heralded stunning way major developments mathematical physics. b. riemann 1854 chapter begin rigorous study fourier series. set stage introducing main objects subject for mulate basic problems already touched upon earlier. ﬁrst result disposes question uniqueness two func tions fourier coeﬃcients necessarily equal indeed simple argument shows functions continuous fact must agree. next take closer look partial sums fourier series. using formula fourier coeﬃcients which involves integration make key observation sums written conveniently integrals 1 2π z dnx yfy dy dn family functions called dirichlet kernels. expression convolution f function dn. convo lutions play critical role analysis. general given family functions kn led investigate limiting properties n tends inﬁnity convolutions 1 2π z knx yfy dy. ﬁnd family kn satisﬁes three important properties good kernels convolutions tend fx n at least f continuous. sense family kn
ibookroot october 20 2007 30 chapter 2. basic properties fourier series approximation identity. unfortunately dirichlet kernels dn belong category good kernels indicates question convergence fourier series subtle. instead pursuing stage problem convergence con sider various methods summing fourier series function. ﬁrst method involves averages partial sums leads con volutions good kernels yields important theorem fej er. this deduce fact continuous function circle approximated uniformly trigonometric polynomials. second may also sum fourier series sense abel en counter family good kernels. case results convo lutions good kernels lead solution dirichlet problem steadystate heat equation disc considered end previous chapter. 1 examples formulation problem commence brief description types functions shall concerned. since fourier coeﬃcients f deﬁned 1 l z l 0 fxe2πinxl dx n z f complexvalued 0 l necessary place in tegrability conditions f. shall therefore assume remainder book functions least riemann integrable.1 some times illuminating focus attention functions regular is functions possess certain continuity diﬀerentiability properties. below list several classes functions increasing order generality. emphasize generally restrict attention realvalued functions contrary fol lowing pictures may suggest almost always allow functions take values complex numbers c. furthermore sometimes think functions deﬁned circle rather interval. elaborate upon below. 1limiting riemann integrable functions natural elementary stage study subject. advanced notion lebesgue integrability taken book iii.
ibookroot october 20 2007 1. examples formulation problem 31 everywhere continuous functions complexvalued functions f continuous every point segment 0 l. typical continuous function sketched figure 1 a. shall note later continuous functions circle satisfy additional condition f0 fl. piecewise continuous functions bounded functions 0 l ﬁnitely many discontinuities. example function simple discontinuities pictured figure 1 b. a b 0 x l 0 x l figure 1. functions 0 l continuous piecewise continuous class functions wide enough illustrate many the orems next chapters. however logical completeness consider also general class riemann integrable functions. extended setting natural since formula fourier coeﬃcients involves integration. riemann integrable functions general class functions concerned with. functions bounded may inﬁnitely many discontinu ities. recall deﬁnition integrability. realvalued function f deﬁned 0 l riemann integrable which abbreviate in tegrable2 bounded every ϵ 0 subdivision 0 x0 x1 xn1 xn l interval 0 l u 2starting book iii term integrable used broader sense lebesgue theory.
ibookroot october 20 2007 32 chapter 2. basic properties fourier series l are respectively upper lower sums f subdivi sion namely u n x j1 sup xj1xxj fxxj xj1 l n x j1 inf xj1xxj fxxj xj1 u l ϵ. finally say complexvalued function integrable real imaginary parts integrable. worthwhile remember point sum product two integrable functions integrable. simple example integrable function 0 1 inﬁnitely many discontinuities given fx 1 1n 1 x 1n n odd 0 1n 1 x 1n n even 0 x 0. example illustrated figure 2. note f discontinuous x 1n x 0. 1 3 1 2 1 5 1 4 0 1 1 figure 2. riemann integrable function elaborate examples integrable functions whose discontinuities dense interval 0 1 described problem 1. general integrable functions may inﬁnitely many discontinuities
ibookroot october 20 2007 1. examples formulation problem 33 functions actually characterized fact that precise sense discontinuities numerous negligible is set points integrable function discontinuous mea sure 0. reader ﬁnd details riemann integration appendix. on shall always assume functions integrable even state requirement explicitly. functions circle natural connection 2πperiodic functions r like exponentials einθ functions interval length 2π functions unit circle. connection arises follows. point unit circle takes form eiθ θ real number unique integer multiples 2π. f function circle may deﬁne real number θ fθ feiθ observe deﬁnition function f periodic r period 2π is fθ 2π fθ θ. integrability continu ity smoothness properties f determined f. instance say f integrable circle f integrable every interval length 2π. also f continuous circle f continuous r saying f continuous interval length 2π. moreover f continuously diﬀerentiable f continuous derivative forth. since f period 2π may restrict interval length 2π say 0 2π π π still capture initial function f circle. note f must take value endpoints interval since correspond point circle. conversely function 0 2π f0 f2π extended periodic function r identiﬁed function circle. particular continuous function f interval 0 2π gives rise continuous function circle f0 f2π. conclusion functions r 2πperiodic functions interval length 2π take value endpoints two equivalent descriptions mathematical objects namely functions circle. connection mention item notational usage. functions deﬁned interval line often use x independent variable however consider functions
ibookroot october 20 2007 34 chapter 2. basic properties fourier series circle usually replace variable x θ. reader note strictly bound rule since practice mostly matter convenience. 1.1 main deﬁnitions examples begin study fourier analysis precise deﬁnition fourier series function. here important pin function originally deﬁned. f integrable function given interval a b length l that is b a l nth fourier coeﬃcient f deﬁned ˆ fn 1 l z b fxe2πinxl dx n z. fourier series f given formally3 x n ˆ fne2πinxl. shall sometimes write fourier coeﬃcients f use notation fx x n ane2πinxl indicate series righthand side fourier series f. instance f integrable function interval π π nth fourier coeﬃcient f ˆ fn 1 2π z π π fθeinθ dθ n z fourier series f fθ x n aneinθ. use θ variable since think angle ranging π π. 3at point say anything convergence series.
ibookroot october 20 2007 1. examples formulation problem 35 also f deﬁned 0 2π formulas above except integrate 0 2π deﬁnition fourier coeﬃcients. may also consider fourier coeﬃcients fourier series function deﬁned circle. previous discussion may think function circle function f r 2πperiodic. may restrict function f interval length 2π instance 0 2π π π compute fourier coeﬃcients. fortunately f periodic exercise 1 shows resulting integrals independent chosen interval. thus fourier coeﬃcients function circle well deﬁned. finally shall sometimes consider function g given 0 1. ˆ gn z 1 0 gxe2πinx dx gx x n ane2πinx. use x variable ranging 0 1. course f initially given 0 2π gx f2πx deﬁned 0 1 change variables shows nth fourier coeﬃcient f equals nth fourier coeﬃcient g. fourier series part larger family called trigonometric se ries which deﬁnition expressions form p ncne2πinxl cn c. trigonometric series involves ﬁnitely many non zero terms is cn 0 large n called trigonometric polynomial degree largest value n cn 0. n th partial sum fourier series f n positive integer particular example trigonometric polynomial. given snfx n x nn ˆ fne2πinxl. note deﬁnition sum symmetric since n ranges n n choice natural resulting decomposition fourier series sine cosine series. consequence convergence fourier series understood in book limit n tends inﬁnity symmetric sums. fact using partial sums fourier series reformulate basic question raised chapter 1 follows problem sense snf converge f n
ibookroot october 20 2007 36 chapter 2. basic properties fourier series proceeding question turn simple examples fourier series. example 1. let fθ θ π θ π. calculation fourier coeﬃcients requires simple integration parts. first n 0 ˆ fn 1 2π z π π θeinθ dθ 1 2π θ ineinθ π π 1 2πin z π π einθ dθ 1n1 n 0 clearly ˆ f0 1 2π z π π θ dθ 0. hence fourier series f given fθ x n0 1n1 einθ 2 x n1 1n1 sin nθ n . ﬁrst sum nonzero integers second obtained application eulers identities. possible prove elementary means series converges every θ obvious converges fθ. proved later exercises 8 9 deal similar situation. example 2. deﬁne fθ π θ24 0 θ 2π. successive integration parts similar performed previous example yield fθ π2 12 x n1 cos nθ n2 . example 3. fourier series function fθ π sin παeiπθα 0 2π fθ x n einθ n α
ibookroot october 20 2007 1. examples formulation problem 37 whenever α integer. example 4. trigonometric polynomial deﬁned x π π dnx n x nn einx called n th dirichlet kernel fundamental importance theory as shall see later. notice fourier coeﬃcients property 1 n n 0 otherwise. closed form formula dirichlet kernel dnx sinn 1 2x sinx2 . seen summing geometric progressions n x n0 ωn 1 x nn ωn ω eix. sums are respectively equal 1 ωn1 1 ω ωn 1 1 ω . sum ωn ωn1 1 ω ωn12 ωn12 ω12 ω12 sinn 1 2x sinx2 giving desired result. example 5. function prθ called poisson kernel deﬁned θ π π 0 r 1 absolutely uniformly convergent series prθ x n rneinθ. function arose implicitly solution steadystate heat equation unit disc discussed chapter 1. note calcu lating fourier coeﬃcients prθ interchange order integration summation since sum converges uniformly θ
ibookroot october 20 2007 38 chapter 2. basic properties fourier series ﬁxed r obtain nth fourier coeﬃcient equals rn. one also sum series prθ see prθ 1 r2 1 2r cos θ r2 . fact prθ x n0 ωn x n1 ωn ω reiθ series converge absolutely. ﬁrst sum an inﬁnite geomet ric progression equals 11 ω likewise second ω1 ω. together combine give 1 ω 1 ωω 1 ω1 ω 1 ω2 1 ω2 1 r2 1 2r cos θ r2 claimed. poisson kernel reappear later context abel summability fourier series function. let us return problem formulated earlier. deﬁnition fourier series f purely formal obvious whether converges f. fact solution problem hard relatively easy depending sense expect series converge additional restrictions place f. let us precise. suppose sake discussion function f which always assumed riemann integrable deﬁned π π. ﬁrst question one might ask whether partial sums fourier series f converge f pointwise. is 1 lim nsnfθ fθ every θ see quite easily general cannot expect result true every θ since always change integrable function one point without changing fourier coeﬃcients. result might ask question assuming f continuous periodic. long time believed additional assumptions answer would yes. surprise du boisreymond showed exists continuous function whose fourier series diverges point. give example next chapter. despite negative result might ask happens add smoothness conditions f example might assume f continuously
ibookroot october 20 2007 2. uniqueness fourier series 39 diﬀerentiable twice continuously diﬀerentiable. see fourier series f converges f uniformly. also interpret limit 1 showing fourier series sums sense ces aro abel function f points continuity. approach involves appropriate averages partial sums fourier series f. finally also deﬁne limit 1 mean square sense. next chapter show f merely integrable 1 2π z π π snfθ fθ2 dθ 0 n . interest know problem pointwise convergence fourier series settled 1966 l. carleson showed among things f integrable sense4 fourier series f converges f except possibly set measure 0. proof theorem diﬃcult beyond scope book. 2 uniqueness fourier series assume fourier series functions f converge f appropriate sense could infer function uniquely determined fourier coeﬃcients. would lead following statement f g fourier coeﬃcients f g necessarily equal. taking diﬀerence f g proposition reformulated as ˆ fn 0 n z f 0. stated assertion cannot correct without reservation since calculating fourier coeﬃcients requires integration see that example two functions diﬀer ﬁnitely many points fourier series. however following positive result. theorem 2.1 suppose f integrable function circle ˆ fn 0 n z. fθ0 0 whenever f continuous point θ0. thus terms know set discontinuities in tegrable functions5 conclude f vanishes most values θ. proof. suppose ﬁrst f realvalued argue con tradiction. assume without loss generality f deﬁned 4carlesons proof actually holds wider class functions square inte grable lebesgue sense. 5see appendix.
ibookroot october 20 2007 40 chapter 2. basic properties fourier series π π θ0 0 f0 0. idea construct fam ily trigonometric polynomials pk peak 0 r pkθfθ dθ as k . desired contradiction since integrals equal zero assumption. since f continuous 0 choose 0 δ π2 fθ f02 whenever θ δ. let pθ ϵ cos θ ϵ 0 chosen small pθ 1 ϵ2 whenever δ θ π. then choose positive η η δ pθ 1 ϵ2 θ η. finally let pkθ pθk select b fθ b θ. possible since f integrable hence bounded. figure 3 illustrates family pk. p p6 p15 figure 3. functions p p6 p15 ϵ 0.1 construction pk trigonometric polynomial since ˆ fn 0 n must z π π fθpkθ dθ 0 k. however estimate z δθ fθpkθ dθ 2πb1 ϵ2k.
ibookroot october 20 2007 2. uniqueness fourier series 41 also choice δ guarantees pθ fθ nonnegative whenever θ δ thus z ηθδ fθpkθ dθ 0. finally z θη fθpkθ dθ 2ηf0 2 1 ϵ2k. therefore r pkθfθ dθ as k concludes proof f realvalued. general write fθ uθ ivθ u v realvalued. deﬁne fθ fθ uθ fθ fθ 2 vθ fθ fθ 2i since ˆ fn ˆ fn conclude fourier coeﬃcients u v vanish hence f 0 points continuity. idea constructing family functions trigonometric polynomials case peak origin together nice properties play important role book. families functions taken later section 4 connection notion convolution. now note theorem implies following. corollary 2.2 f continuous circle ˆ fn 0 n z f 0. next corollary shows problem 1 formulated earlier simple positive answer assumption series fourier coeﬃcients converges absolutely. corollary 2.3 suppose f continuous function circle fourier series f absolutely convergent p n ˆ fn . then fourier series converges uniformly f is lim nsnfθ fθ uniformly θ. proof. recall sequence continuous functions converges uniformly limit also continuous. observe assumption p ˆ fn implies partial sums fourier
ibookroot october 20 2007 42 chapter 2. basic properties fourier series series f converge absolutely uniformly therefore function g deﬁned gθ x n ˆ fneinθ lim n n x nn ˆ fneinθ continuous circle. moreover fourier coeﬃcients g precisely ˆ fn since interchange inﬁnite sum integral a consequence uniform convergence series. therefore previous corollary applied function f g yields f g desired. conditions f would guarantee absolute convergence fourier series turns out smoothness f directly related decay fourier coeﬃcients general smoother function faster decay. result expect relatively smooth functions equal fourier series. fact case show. order state result concisely introduce standard o notation use freely rest book. exam ple statement ˆ fn o1n2 n means left hand side bounded constant multiple righthand side is exists c 0 ˆ fn cn2 large n. generally fx ogx x a means constant c fx cgx x approaches a. particular fx o1 means f bounded. corollary 2.4 suppose f twice continuously diﬀerentiable func tion circle. ˆ fn o1n2 n fourier series f converges absolutely uniformly f.
ibookroot october 20 2007 2. uniqueness fourier series 43 proof. estimate fourier coeﬃcients proved integrating parts twice n 0. obtain 2π ˆ fn z 2π 0 fθeinθ dθ fθ einθ 2π 0 1 z 2π 0 f θeinθ dθ 1 z 2π 0 f θeinθ dθ 1 f θ einθ 2π 0 1 in2 z 2π 0 f θeinθ dθ 1 n2 z 2π 0 f θeinθ dθ. quantities brackets vanish since f f periodic. therefore 2πn2 ˆ fn z 2π 0 f θeinθ dθ z 2π 0 f θ dθ c constant c independent n. we take c 2πb b bound f . since p 1n2 converges proof corollary complete. incidentally also established following important identity b f n ˆ fn n z. n 0 proof given above n 0 left exercise reader. f diﬀerentiable f p aneinθ f p anineinθ. also f twice continuously diﬀerentiable f p anin2einθ on. smoothness conditions f imply even better decay fourier coeﬃcients exercise 10. also stronger versions corollary 2.4. shown example fourier series f converges absolutely assuming f one continuous derivative. even generally fourier series f converges absolutely and hence uniformly f f satisﬁes h older condition order α α 12 is sup θ fθ t fθ atα t. matters see exercises end chapter 3.
ibookroot october 20 2007 44 chapter 2. basic properties fourier series point worthwhile introduce common notation say f belongs class ck f k times continuously diﬀerentiable. belonging class ck satisfying h older condition two possible ways describe smoothness function. 3 convolutions notion convolution two functions plays fundamental role fourier analysis appears naturally context fourier series also serves generally analysis functions settings. given two 2πperiodic integrable functions f g r deﬁne convolution f g π π 2 f gx 1 2π z π π fygx y dy. integral makes sense x since product two integrable functions integrable. also since functions periodic change variables see f gx 1 2π z π π fx ygy dy. loosely speaking convolutions correspond weighted averages. instance g 1 2 f g constant equal 1 2π r π π fy dy may interpret average value f circle. also convolution f gx plays role similar to sense replaces pointwise product fxgx two functions f g. context chapter interest convolutions originates fact partial sums fourier series f expressed follows snfx n x nn ˆ fneinx n x nn µ 1 2π z π π fyeiny dy einx 1 2π z π π fy ã n x nn einxy dy f dnx
ibookroot october 20 2007 3. convolutions 45 dn n th dirichlet kernel see example 4 given dnx n x nn einx. observe problem understanding snf reduces understanding convolution f dn. begin gathering main properties convolutions. proposition 3.1 suppose f g h 2πperiodic integrable functions. then i f g h f g f h. ii cf g cf g f cg c c. iii f g g f. iv f g h f g h. v f g continuous. vi f gn ˆ fnˆ gn. ﬁrst four points describe algebraic properties convolutions linearity commutativity associativity. property v exhibits im portant principle convolution f g more regular f g. here f g continuous f g merely riemann integrable. finally vi key study fourier series. general fourier coeﬃcients product fg product fourier coeﬃ cients f g. however vi says relation holds replace product two functions f g convolution f g. proof. properties i ii follow linearity integral. properties easily deduced assume also f g continuous. case may freely interchange order
ibookroot october 20 2007 46 chapter 2. basic properties fourier series integration. instance establish vi write f gn 1 2π z π π f gxeinx dx 1 2π z π π 1 2π µz π π fygx y dy einx dx 1 2π z π π fyeiny µ 1 2π z π π gx yeinxy dx dy 1 2π z π π fyeiny µ 1 2π z π π gxeinx dx dy ˆ fnˆ gn. prove iii one ﬁrst notes f continuous 2πperiodic z π π fy dy z π π fx y dy x r. veriﬁcation identity consists change variables 7y followed translation 7y x. then one takes fy fygx y. also iv follows interchanging two integral signs appro priate change variables. finally show f g continuous f g continu ous. first may write f gx1 f gx2 1 2π z π π fy gx1 y gx2 y dy. since g continuous must uniformly continuous closed bounded interval. g also periodic must uniformly continuous r given ϵ 0 exists δ 0 gs gt ϵ whenever s t δ. then x1 x2 δ implies x1 y x2 y δ y hence f gx1 f gx2 1 2π z π π fy gx1 y gx2 y dy 1 2π z π π fy gx1 y gx2 y dy ϵ 2π z π π fy dy ϵ 2π 2π b
ibookroot october 20 2007 3. convolutions 47 b chosen fx b x. result conclude f g continuous proposition proved least f g continuous. general f g merely integrable may use re sults established far when f g continuous together following approximation lemma whose proof may found appendix. lemma 3.2 suppose f integrable circle bounded b. exists sequence fk k1 continuous functions circle sup xππ fkx b k 1 2 . . . z π π fx fkx dx 0 k . using result may complete proof proposition follows. apply lemma 3.2 f g obtain sequences fk gk approximating continuous functions. f g fk gk f fk g fk g gk. properties sequence fk f fk gx 1 2π z π π fx y fkx y gy dy 1 2π sup gy z π π fy fky dy 0 k . hence f fk g 0 uniformly x. similarly fk g gk 0 uni formly therefore fk gk tends uniformly f g. since fk gk continuous follows f g also continuous v. next establish vi. ﬁxed integer n must fk gkn f gn k tends inﬁnity since fk gk converges uni formly f g. however found earlier c fknc gkn fk gkn fk gk continuous. hence ˆ fn ˆ fkn 1 2π z π π fx fkxeinx dx 1 2π z π π fx fkx dx
ibookroot october 20 2007 48 chapter 2. basic properties fourier series result ﬁnd c fkn ˆ fn k goes inﬁnity. similarly c gkn ˆ gn desired property established let k tend inﬁnity. finally properties iii iv follow kind arguments. 4 good kernels proof theorem 2.1 constructed sequence trigonometric polynomials pk property functions pk peaked origin. result could isolate behavior f origin. section return families functions time general setting. first deﬁne notion good kernel discuss characteristic properties functions. then use convolutions show kernels used recover given function. family kernels knx n1 circle said family good kernels satisﬁes following properties a n 1 1 2π z π π knx dx 1. b exists 0 n 1 z π π knx dx m. c every δ 0 z δxπ knx dx 0 n . practice shall encounter families knx 0 case b consequence a. may interpret kernels knx weight distributions circle property a says kn assigns unit mass whole circle π π c mass concentrates near origin n becomes large.6 figure 4 a illustrates typical character family good kernels. importance good kernels highlighted use connec tion convolutions. 6in limit family good kernels represents dirac delta function. terminology comes physics.
ibookroot october 20 2007 4. good kernels 49 a b kny 0 fx y fx figure 4. good kernels theorem 4.1 let kn n1 family good kernels f inte grable function circle. lim nf knx fx whenever f continuous x. f continuous everywhere limit uniform. result family kn sometimes referred approximation identity. previously interpreted convolutions weighted averages. context convolution f knx 1 2π z π π fx ykny dy average fx y weights given kny. how ever weight distribution kn concentrates mass 0 n becomes large. hence integral value fx assigned full mass n . figure 4 b illustrates point. proof theorem 4.1. ϵ 0 f continuous x choose δ y δ implies fx y fx ϵ. then ﬁrst property good kernels write f knx fx 1 2π z π π knyfx y dy fx 1 2π z π π knyfx y fx dy.
ibookroot october 20 2007 50 chapter 2. basic properties fourier series hence f knx fx 1 2π z π π knyfx y fx dy 1 2π z yδ kny fx y fx dy 1 2π z δyπ kny fx y fx dy ϵ 2π z π π kny dy 2b 2π z δyπ kny dy b bound f. ﬁrst term bounded ϵm2π second property good kernels. third property see large n second term less ϵ. therefore constant c 0 large n f knx fx cϵ thereby proving ﬁrst assertion theorem. f continuous everywhere uniformly continuous δ chosen in dependent x. provides desired conclusion f kn f uniformly. recall beginning section 3 snfx f dnx dnx pn nn einx dirichlet kernel. natural us ask whether dn good kernel since true theorem 4.1 would imply fourier series f converges fx whenever f continuous x. unfortunately case. indeed estimate shows dn violates second property precisely one see problem 2 z π π dnx dx c log n n . however note formula dn sum exponen tials immediately gives 1 2π z π π dnx dx 1 ﬁrst property good kernels actually veriﬁed. fact mean value dn 1 integral absolute value large
ibookroot october 20 2007 5. ces aro abel summability applications fourier series 51 result cancellations. indeed figure 5 shows function dnx takes positive negative values oscillates rapidly n gets large. figure 5. dirichlet kernel large n observation suggests pointwise convergence fourier series intricate may even fail points continuity. indeed case see next chapter. 5 ces aro abel summability applications fourier series since fourier series may fail converge individual points led try overcome failure interpreting limit lim nsnf f diﬀerent sense. 5.1 ces aro means summation begin taking ordinary averages partial sums technique describe detail.
ibookroot october 20 2007 52 chapter 2. basic properties fourier series suppose given series complex numbers c0 c1 c2 x k0 ck. deﬁne nth partial sum sn sn n x k0 ck say series converges limnsn s. natural commonly used type summability. consider however example series 3 1 1 1 1 x k0 1k. partial sums form sequence 1 0 1 0 . . . limit. partial sums alternate evenly 1 0 one might therefore suggest 12 limit sequence hence 12 equals sum particular series. give precise meaning deﬁning average ﬁrst n partial sums σn s0 s1 sn1 n . quantity σn called n th ces aro mean7 sequence sk n th ces aro sum series p k0 ck. σn converges limit σ n tends inﬁnity say series p cn ces aro summable σ. case series functions shall understand limit sense either pointwise uniform convergence depending situation. reader diﬃculty checking exam ple 3 series ces aro summable 12. moreover one show ces aro summation inclusive process convergence. fact series convergent s also ces aro summable limit exercise 12. 5.2 fej ers theorem interesting application ces aro summability appears context fourier series. 7note series p k1 ck begins term k 1 common prac tice deﬁne σn s1 snn. change notation little eﬀect follows.
ibookroot october 20 2007 5. ces aro abel summability applications fourier series 53 mentioned earlier dirichlet kernels fail belong family good kernels. quite surprisingly averages well behaved functions sense form family good ker nels. see this form n th ces aro mean fourier series deﬁnition σnfx s0fx sn1fx n . since snf f dn ﬁnd σnfx f fnx fnx nth fej er kernel given fnx d0x dn1x n . lemma 5.1 fnx 1 n sin2nx2 sin2x2 fej er kernel good kernel. proof formula fn a simple application trigonometric identities outlined exercise 15. prove rest lemma note fn positive 1 2π r π π fnx dx 1 view fact similar identity holds dirichlet kernels dn. however sin2x2 cδ 0 δ x π hence fnx 1ncδ follows z δxπ fnx dx 0 n . applying theorem 4.1 new family good kernels yields following important result. theorem 5.2 f integrable circle fourier series f ces aro summable f every point continuity f. moreover f continuous circle fourier series f uniformly ces aro summable f. may state two corollaries. ﬁrst result already established. second new fundamental importance.
ibookroot october 20 2007 54 chapter 2. basic properties fourier series corollary 5.3 f integrable circle ˆ fn 0 n f 0 points continuity f. proof immediate since partial sums 0 hence ces aro means 0. corollary 5.4 continuous functions circle uniformly ap proximated trigonometric polynomials. means f continuous π π fπ fπ ϵ 0 exists trigonometric polynomial p fx px ϵ π x π. follows immediately theorem since partial sums hence ces aro means trigonometric polynomials. corollary 5.4 periodic analogue weierstrass approximation theorem polyno mials found exercise 16. 5.3 abel means summation another method summation ﬁrst considered abel actually predates ces aro method. series complex numbers p k0 ck said abel summable every 0 r 1 series ar x k0 ckrk converges lim r1 ar s. quantities ar called abel means series. one prove series converges s abel summable s. moreover method abel summability even powerful ces aro method series ces aro summable always abel summable sum. however consider series 1 2 3 4 5 x k0 1kk 1 one show abel summable 14 since ar x k0 1kk 1rk 1 1 r2 series ces aro summable see exercise 13.
ibookroot october 20 2007 5. ces aro abel summability applications fourier series 55 5.4 poisson kernel dirichlets problem unit disc adapt abel summability context fourier series deﬁne abel means function fθ p naneinθ arfθ x n rnaneinθ. since index n takes positive negative values natural write c0 a0 cn aneinθ aneinθ n 0 abel means fourier series correspond deﬁnition given previous section numerical series. note since f integrable an uniformly bounded n arf converges absolutely uniformly 0 r 1. case ces aro means key fact abel means written convolutions arfθ f prθ prθ poisson kernel given 4 prθ x n rneinθ. fact arfθ x n rnaneinθ x n rn µ 1 2π z π π fϕeinϕ dϕ einθ 1 2π z π π fϕ ã x n rneinϕθ dϕ interchange integral inﬁnite sum justiﬁed uniform convergence series. lemma 5.5 0 r 1 prθ 1 r2 1 2r cos θ r2 .
ibookroot october 20 2007 56 chapter 2. basic properties fourier series poisson kernel good kernel8 r tends 1 below. proof. identity prθ 1r2 12r cos θr2 already derived section 1.1. note 1 2r cos θ r2 1 r2 2r1 cos θ. hence 12 r 1 δ θ π 1 2r cos θ r2 cδ 0. thus prθ 1 r2cδ δ θ π third property good kernels veriﬁed. clearly prθ 0 integrating expres sion 4 term term which justiﬁed absolute convergence series yields 1 2π z π π prθ dθ 1 thereby concluding proof pr good kernel. combining lemma theorem 4.1 obtain next result. theorem 5.6 fourier series integrable function circle abel summable f every point continuity. moreover f continuous circle fourier series f uniformly abel summable f. return problem discussed chapter 1 sketched solution steadystate heat equation u 0 unit disc boundary condition u f circle. expressed laplacian terms polar coordinates separated variables expected solution given 5 ur θ x m amrmeimθ mth fourier coeﬃcient f. words led take ur θ arfθ 1 2π z π π fϕprθ ϕ dϕ. position show indeed case. 8in case family kernels indexed continuous parameter 0 r 1 rather discrete n considered previously. deﬁnition good kernels simply replace n r take limit property c appropriately example r 1 case.
ibookroot october 20 2007 5. ces aro abel summability applications fourier series 57 theorem 5.7 let f integrable function deﬁned unit circle. function u deﬁned unit disc poisson integral 6 ur θ f prθ following properties i u two continuous derivatives unit disc satisﬁes u 0. ii θ point continuity f lim r1 ur θ fθ. f continuous everywhere limit uniform. iii f continuous ur θ unique solution steady state heat equation disc satisﬁes conditions i ii. proof. i recall function u given series 5. fix ρ 1 inside disc radius r ρ 1 centered origin series u diﬀerentiated term term diﬀerentiated se ries uniformly absolutely convergent. thus u diﬀerentiated twice in fact inﬁnitely many times since holds ρ 1 conclude u twice diﬀerentiable inside unit disc. moreover polar coordinates u 2u r2 1 r u r 1 r2 2u θ2 term term diﬀerentiation shows u 0. proof ii simple application previous theorem. prove iii argue follows. suppose v solves steadystate heat equation disc converges f uniformly r tends 1 below. ﬁxed r 0 r 1 function vr θ fourier series x n anreinθ anr 1 2π z π π vr θeinθ dθ. taking account vr θ solves equation 7 2v r2 1 r v r 1 r2 2v θ2 0
ibookroot october 20 2007 58 chapter 2. basic properties fourier series ﬁnd 8 a nr 1 r a nr n2 r2 anr 0. indeed may ﬁrst multiply 7 einθ integrate θ. then since v periodic two integrations parts give 1 2π z π π 2v θ2 r θeinθ dθ n2anr. finally may interchange order diﬀerentiation integra tion permissible since v two continuous derivatives yields 8. therefore must anr anrn bnrn constants bn n 0 see exercise 11 chapter 1. evaluate constants ﬁrst observe term anr bounded v bounded therefore bn 0. ﬁnd let r 1. since v converges uniformly f r 1 ﬁnd 1 2π z π π fθeinθ dθ. similar argument formula also holds n 0. con clusion 0 r 1 fourier series v given series ur θ uniqueness fourier series continuous functions must u v. remark. part iii theorem may conclude u solves u 0 disc converges 0 uniformly r 1 u must identically 0. however uniform convergence replaced pointwise convergence conclusion may fail see exercise 18. 6 exercises 1. suppose f 2πperiodic integrable ﬁnite interval. prove a b r z b fx dx z b2π a2π fx dx z b2π a2π fx dx. also prove z π π fx a dx z π π fx dx z πa πa fx dx.
ibookroot october 20 2007 6. exercises 59 2. exercise show symmetries function imply certain properties fourier coeﬃcients. let f 2πperiodic riemann integrable function deﬁned r. a show fourier series function f written fθ ˆ f0 x n1 ˆ fn ˆ fn cos nθ i ˆ fn ˆ fn sin nθ. b prove f even ˆ fn ˆ fn get cosine series. c prove f odd ˆ fn ˆ fn get sine series. d suppose fθ π fθ θ r. show ˆ fn 0 odd n. e show f realvalued ˆ fn ˆ fn n. 3. return problem plucked string discussed chapter 1. show initial condition f equal fourier sine series fx x m1 sin mx 2h m2 sin mp pπ p. hint note am cm2. 4. consider 2πperiodic odd function deﬁned 0 π fθ θπ θ. a draw graph f. b compute fourier coeﬃcients f show fθ 8 π x k odd 1 sin kθ k3 . 5. interval π π consider function fθ 0 θ δ 1 θδ θ δ. thus graph f shape triangular tent. show fθ δ 2π 2 x n1 1 cos nδ n2πδ cos nθ. 6. let f function deﬁned π π fθ θ.
ibookroot october 20 2007 60 chapter 2. basic properties fourier series a draw graph f. b calculate fourier coeﬃcients f show ˆ fn π 2 n 0 1 1n πn2 n 0. c fourier series f terms sines cosines d taking θ 0 prove x n odd 1 1 n2 π2 8 x n1 1 n2 π2 6 . see also example 2 section 1.1. 7. suppose ann n1 bnn n1 two ﬁnite sequences complex numbers. let bk pk n1 bn denote partial sums series p bn convention b0 0. a prove summation parts formula n x nm anbn anbn ambm1 n1 x nm an1 anbn. b deduce formula dirichlets test convergence series partial sums series p bn bounded an sequence real numbers decreases monotonically 0 p anbn converges. 8. verify 1 2i p n0 einx n fourier series 2πperiodic sawtooth function illustrated figure 6 deﬁned f0 0 fx π 2 x 2 π x 0 π 2 x 2 0 x π. note function continuous. show nevertheless series converges every x by mean usual symmetric partial sums series converge. particular value series origin namely 0 average values fx x approaches origin left right.
ibookroot october 20 2007 6. exercises 61 0 π 2 π π π 2 figure 6. sawtooth function hint use dirichlets test convergence series p anbn. 9. let fx χabx characteristic function interval a b π π is χabx ½ 1 x a b 0 otherwise. a show fourier series f given fx b a 2π x n0 eina einb 2πin einx. sum extends positive negative integers excluding 0. b show π b π b fourier series converge absolutely x. hint suﬃces prove many values n one sin nθ0 c 0 θ0 b a2. c however prove fourier series converges every point x. happens π b π 10. suppose f periodic function period 2π belongs class ck. show ˆ fn o1nk n . notation means exists constant c ˆ fn cnk. could also write nk ˆ fn o1 o1 means bounded. hint integrate parts. 11. suppose fk k1 sequence riemann integrable functions interval 0 1 z 1 0 fkx fx dx 0 k .
ibookroot october 20 2007 62 chapter 2. basic properties fourier series show ˆ fkn ˆ fn uniformly n k . 12. prove series complex numbers p cn converges s p cn ces aro summable s. hint assume sn 0 n . 13. purpose exercise prove abel summability stronger standard ces aro methods summation. a show series p n1 cn complex numbers converges ﬁnite limit s series abel summable s. hint enough prove theorem 0 assuming 0 show sn c1 cn pn n1 cnrn 1 r pn n1 snrn snrn1. let n show x cnrn 1 r x snrn. finally prove righthand side converges 0 r 1. b however show exist series abel summable converge. hint try cn 1n. abel limit p cn c argue similarly prove series p n1 cn ces aro summable σ abel summable σ. hint note x n1 cnrn 1 r2 x n1 nσnrn assume σ 0. d give example series abel summable ces aro summable. hint try cn 1n1n. note p cn ces aro summable cnn tends 0. results summarized following implications series convergent ces aro summable abel summable fact none arrows reversed. 14. exercise deals theorem tauber says additional condition coeﬃcients cn arrows reversed. a p cn ces aro summable σ cn o1n that is ncn 0 p cn converges σ. hint sn σn n 1cn c2n. b statement holds replace ces aro summable abel summable. hint estimate diﬀerence pn n1 cn pn n1 cnrn r 1 1n.
ibookroot october 20 2007 6. exercises 63 15. prove fej er kernel given fnx 1 n sin2nx2 sin2x2 . hint remember nfnx d0x dn1x dnx dirichlet kernel. therefore ω eix nfnx n1 x n0 ωn ωn1 1 ω . 16. weierstrass approximation theorem states let f continuous function closed bounded interval a b r. then ϵ 0 exists polynomial p sup xab fx px ϵ. prove applying corollary 5.4 fej ers theorem using fact exponential function eix approximated polynomials uniformly interval. 17. section 5.4 proved abel means f converge f points continuity is lim r1 arfθ lim r1pr fθ fθ 0 r 1 whenever f continuous θ. exercise study behavior arfθ certain points discontinuity. integrable function said jump discontinuity θ two limits lim h 0 h 0 fθ h fθ lim h 0 h 0 fθ h fθ exist. a prove f jump discontinuity θ lim r1 arfθ fθ fθ 2 0 r 1. hint explain 1 2π r 0 π prθ dθ 1 2π r π 0 prθ dθ 1 2 modify proof given text.
ibookroot october 20 2007 64 chapter 2. basic properties fourier series b using similar argument show f jump discontinuity θ fourier series f θ ces aro summable fθfθ 2 . 18. prθ denotes poisson kernel show function ur θ pr θ deﬁned 0 r 1 θ r satisﬁes i u 0 disc. ii limr1 ur θ 0 θ. however u identically zero. 19. solve laplaces equation u 0 semi inﬁnite strip x y 0 x 1 0 y subject following boundary conditions u0 y 0 0 y u1 y 0 0 y ux 0 fx 0 x 1 f given function course f0 f1 0. write fx x n1 sinnπx expand general solution terms special solutions given unx y enπy sinnπx. express u integral involving f analogous poisson integral for mula 6. 20. consider dirichlet problem annulus deﬁned r θ ρ r 1 0 ρ 1 inner radius. problem solve 2u r2 1 r u r 1 r2 2u θ2 0 subject boundary conditions ½ u1 θ fθ uρ θ gθ
ibookroot october 20 2007 7. problems 65 f g given continuous functions. arguing previously dirichlet problem disc hope write ur θ x cnreinθ cnr anrn bnrn n 0. set fθ x aneinθ gθ x bneinθ. want cn1 cnρ bn. leads solution ur θ x n0 µ 1 ρn ρn ρrn rρn rn rnbn einθ a0 b0 a0log r log ρ. show result ur θ pr fθ 0 r 1 uniformly θ ur θ pρr gθ 0 r ρ uniformly θ. 7 problems 1. one construct riemann integrable functions 0 1 dense set discontinuities follows. a let fx 0 x 0 fx 1 x 0. choose countable dense sequence rn 0 1. then show function fx x n1 1 n2 fx rn integrable discontinuities points sequence rn. hint f monotonic bounded. b consider next fx x n1 3ngx rn gx sin 1x x 0 g0 0. f integrable discontinuous x rn fails monotonic subinterval 0 1. hint use fact 3k p nk 3n.
ibookroot october 20 2007 66 chapter 2. basic properties fourier series c original example riemann function fx x n1 nx n2 x x x 12 12 x continued r periodicity is x 1 x. shown f discontinuous whenever x m2n m n z odd n 0. 2. let dn denote dirichlet kernel dnθ n x kn eikθ sinn 12θ sinθ2 deﬁne ln 1 2π z π π dnθ dθ. a prove ln c log n constant c 0. hint show dnθ c sinn12θ θ change variables prove ln c z nπ π sin θ θ dθ o1. write integral sum pn1 k1 r k1π kπ . conclude use fact pn k1 1k c log n. careful estimate gives ln 4 π2 log n o1. b prove following consequence n 1 exists contin uous function fn fn 1 snfn0 c log n. hint function gn equal 1 dn positive 1 dn negative desired property continuous. approximate gn integral norm in sense lemma 3.2 continuous functions hk satisfying hk 1. 3.littlewood provided reﬁnement taubers theorem
ibookroot october 20 2007 7. problems 67 a p cn abel summable cn o1n p cn converges s. b consequence p cn ces aro summable cn o1n p cn converges s. results may applied fourier series. exercise 17 imply f integrable function satisﬁes ˆ fν o1ν then i f continuous θ snfθ fθ n . ii f jump discontinuity θ snfθ fθ fθ 2 n . iii f continuous π π snf f uniformly. simpler assertion b hence proof i ii iii see problem 5 chapter 4.
ibookroot october 20 2007
ibookroot october 20 2007 3 convergence fourier series sine cosine series one repre sent arbitrary function given interval enjoy among remarkable properties con vergent. property escape great ge ometer fourier began introduc tion representation functions mentioned new career applications analysis stated memoir contains ﬁrst research heat. one far knowledge gave general proof . . . g. dirichlet 1829 chapter continue study problem convergence fourier series. approach problem two diﬀerent points view. ﬁrst global concerns overall behavior function f entire interval 0 2π. result mind mean square convergence f integrable circle 1 2π z 2π 0 fθ snfθ2 dθ 0 n . heart result fundamental notion orthogonal ity idea expressed terms vector spaces inner products related inﬁnite dimensional variants hilbert spaces. connected result parseval identity equates meansquare norm function corresponding norm fourier coeﬃ cients. orthogonality fundamental mathematical notion many applications analysis. second viewpoint local concerns behavior f near given point. main question consider problem pointwise convergence fourier series f converge value fθ given θ ﬁrst show convergence indeed hold whenever f diﬀerentiable θ. corollary obtain riemann localization principle states question whether snfθ fθ completely determined behavior f
ibookroot october 20 2007 70 chapter 3. convergence fourier series arbitrarily small interval θ. remarkable result since fourier coeﬃcients hence fourier series f depend values f whole interval 0 2π. even though convergence fourier series holds points f diﬀerentiable may fail f merely continuous. chapter concludes presentation continuous function whose fourier series converge given point promised earlier. 1 meansquare convergence fourier series aim section proof following theorem. theorem 1.1 suppose f integrable circle. 1 2π z 2π 0 fθ snfθ2 dθ 0 n . remarked earlier key concept involved orthogonal ity. correct setting orthogonality vector space equipped inner product. 1.1 vector spaces inner products review deﬁnitions vector space r c inner product associated norm. addition familiar ﬁnite dimensional vector spaces rd cd also examine two inﬁnite dimensional examples play central role proof theo rem 1.1. preliminaries vector spaces vector space v real numbers r set whose elements may added together multiplied scalars. precisely may associate pair x v element v called sum denoted x . require addition respects usual laws arithmetic commutativity x x associativity x y z x z etc. also given x v real num ber λ assign element λx v called product x λ. scalar multiplication must satisfy standard properties instance λ1λ2x λ1λ2x λx λx λy . may instead allow scalar multiplication numbers c say v vector space complex numbers.
ibookroot october 20 2007 1. meansquare convergence fourier series 71 example set rd dtuples real numbers x1 x2 . . . xd vector space reals. addition deﬁned componentwise x1 . . . xd y1 . . . yd x1 y1 . . . xd yd multiplication scalar λ r λx1 . . . xd λx1 . . . λxd. similarly space cd the complex version previous example set dtuples complex numbers z1 z2 . . . zd. vector space c addition deﬁned componentwise z1 . . . zd w1 . . . wd z1 w1 . . . zd wd. multiplication scalars λ c given λz1 . . . zd λz1 . . . λzd. inner product vector space v r associates pair x elements v real number denote x . particular inner product must symmetric x y x linear variables is αx βy z αx z βy z whenever α β r x y z v . also require inner prod uct positivedeﬁnite is x x 0 x v . particular given inner product may deﬁne norm x x x x12. addition x 0 implies x 0 say inner product strictly positivedeﬁnite. example space rd equipped strictly positivedeﬁnite inner product deﬁned x x1y1 xdyd x x1 . . . xd y1 . . . yd. x x x12 q x2 1 x2 d
ibookroot october 20 2007 72 chapter 3. convergence fourier series usual euclidean distance. one also uses notation x instead x. vector spaces complex numbers inner product two elements complex number. moreover inner products called hermitian instead symmetric since must satisfy x y x. hence inner product linear ﬁrst variable conjugatelinear second αx βy z αx z βy z x αy βz αx βx z. also must x x 0 norm x deﬁned x x x12 before. again inner product strictly positive deﬁnite x 0 implies x 0. example inner product two vectors z z1 . . . zd w w1 . . . wd cd deﬁned z w z1w1 zdwd. norm vector z given z z z12 q z12 zd2. presence inner product vector space allows one deﬁne geometric notion orthogonality. let v vector space over r c inner product associated norm . two elements x orthogonal x 0 write x y . three important results derived notion orthogonality i pythagorean theorem x orthogonal x 2 x2 y 2. ii cauchyschwarz inequality x v x xy . iii triangle inequality x v x x y .
ibookroot october 20 2007 1. meansquare convergence fourier series 73 proofs facts simple. i suﬃces expand x y x use assumption x 0. ii ﬁrst dispose case y 0 showing implies x 0 x. indeed real 0 x ty 2 x2 2t rex rex 0 contradicts inequality take large positive or negative. similarly considering x ity 2 ﬁnd imx 0. y 0 may set c x y x cy orthogonal therefore also cy . write x x cy cy apply pythagorean theorem get x2 x cy 2 cy 2 c2y 2. taking square roots sides gives result. note equality precisely x cy . finally iii ﬁrst note x 2 x x x y x y . x x x2 y y 2 cauchyschwarz inequal ity x y x 2 xy therefore x 2 x2 2 xy y 2 x y 2. two important examples vector spaces rd cd ﬁnite dimensional. context fourier series need work two inﬁnitedimensional vector spaces describe. example 1. vector space ℓ2z c set twosided inﬁnite sequences complex numbers . . . an . . . a1 a0 a1 . . . an . . . x nz an2
ibookroot october 20 2007 74 chapter 3. convergence fourier series is series converges. addition deﬁned componentwise scalar multiplication. inner product two vectors . . . a1 a0 a1 . . . b . . . b1 b0 b1 . . . deﬁned absolutely convergent series a b x nz anbn. norm given a a a12 ãx nz an2 12 . must ﬁrst check ℓ2z vector space. requires b two elements ℓ2z vector b. see this integer n 0 let denote truncated element . . . 0 0 an . . . a1 a0 a1 . . . an 0 0 . . . set 0 whenever n n. deﬁne truncated element bn similarly. then triangle inequality holds ﬁnite dimensional euclidean space an bnan bna b. thus x nn an bn2 a b2 letting n tend inﬁnity gives p nz an bn2 . also fol lows a ba b triangle inequality. cauchyschwarz inequality states sum p nz anbn con verges absolutely a b ab deduced way ﬁnite analogue. three examples rd cd ℓ2z vector spaces inner products norms satisfy two important properties i inner product strictly positivedeﬁnite is x 0 implies x 0. ii vector space complete deﬁnition means every cauchy sequence norm converges limit vector space.
ibookroot october 20 2007 1. meansquare convergence fourier series 75 inner product space two properties called hilbert space. see rd cd examples ﬁnitedimensional hilbert spaces ℓ2z example inﬁnitedimensional hilbert space see exercises 1 2. either conditions fail space called prehilbert space. give important example prehilbert space conditions i ii fail. example 2. let r denote set complexvalued riemann integrable functions 0 2π or equivalently integrable functions circle. vector space c. addition deﬁned pointwise f gθ fθ gθ. naturally multiplication scalar λ c given λfθ λ fθ. inner product deﬁned vector space 1 f g 1 2π z 2π 0 fθgθ dθ. norm f f µ 1 2π z 2π 0 fθ2 dθ 12 . one needs check analogue cauchyschwarz tri angle inequalities hold example is f g fgand f gf g. facts obtained consequences corresponding inequalities previous examples argument little elaborate prefer proceed diﬀerently. ﬁrst observe 2ab a2 b2 two real numbers b. set λ12fθ b λ12gθ λ 0 get fθgθ 1 2λfθ2 λ1gθ2. integrate θ obtain f g 1 2π z 2π 0 fθ gθ dθ 1 2λf2 λ1g2. then put λ gfto get cauchyschwarz inequality. tri angle inequality simple consequence seen above.
ibookroot october 20 2007 76 chapter 3. convergence fourier series course choice λ must assume f 0 g 0 leads us following observation. r condition i hilbert space fails since f 0 implies f vanishes points continuity. serious problem since appendix show integrable function continuous except negligible set f 0 implies f vanishes except set measure zero. one get around diﬃculty f identically zero adopting convention functions actually zero function since purpose integration f behaves precisely like zero function. essential diﬃculty space r complete. one way see start function fθ 0 θ 0 log1θ 0 θ 2π. since f bounded belong space r. moreover sequence truncations fn deﬁned fnθ 0 0 θ 1n fθ 1n θ 2π easily seen form cauchy sequence r see exercise 5. how ever sequence cannot converge element r since limit existed would f another example see exercise 7. complicated examples motivate search com pletion r class riemann integrable functions 0 2π. construction identiﬁcation completion lebesgue class l20 2π represents important turning point development analysis somewhat akin much earlier completion rationals is passage q r. discussion fun damental ideas postponed book iii take lebesgue theory integration. turn proof theorem 1.1. 1.2 proof meansquare convergence consider space r integrable functions circle inner product f g 1 2π z 2π 0 fθgθ dθ
ibookroot october 20 2007 1. meansquare convergence fourier series 77 norm fdeﬁned f2 f f 1 2π z 2π 0 fθ2 dθ. notation must prove f snf0 n tends inﬁnity. integer n let enθ einθ observe family ennz orthonormal is en em 1 n 0 n m. let f integrable function circle let denote fourier coeﬃcients. important observation fourier coeﬃcients represented inner products f elements orthonor mal set ennz f en 1 2π z 2π 0 fθeinθ dθ an. particular snf p nn anen. orthonormal property family en fact f en imply diﬀerence f p nn anen orthogonal en n n. therefore must 2 f x nn anen x nn bnen complex numbers bn. draw two conclusions fact. first apply pythagorean theorem decomposition f f x nn anen x nn anen choose bn an obtain f2 f x nn anen2 x nn anen2. since orthonormal property family ennz implies x nn anen2 x nn an2
ibookroot october 20 2007 78 chapter 3. convergence fourier series deduce 3 f2 f snf2 x nn an2. second conclusion may draw 2 following simple lemma. lemma 1.2 best approximation f integrable circle fourier coeﬃcients an f snff x nn cnen complex numbers cn. moreover equality holds precisely cn n n. proof. follows immediately applying pythagorean theo rem f x nn cnen f snf x nn bnen bn cn. lemma clear geometric interpretation. says trigonometric polynomial degree n closest f norm is partial sum snf. geometric property partial sums depicted figure 1 orthogonal projection f plane spanned en . . . e0 . . . en simply snf. f snf snf f e0 en e1 en figure 1. best approximation lemma give proof snf f0 using best ap proximation lemma well important fact trigonometric polynomials dense space continuous functions circle.
ibookroot october 20 2007 1. meansquare convergence fourier series 79 suppose f continuous circle. then given ϵ 0 exists by corollary 5.4 chapter 2 trigonometric polynomial p say degree m fθ pθ ϵ θ. particular taking squares integrating inequality yields f p ϵ best approximation lemma conclude f snf ϵ whenever n m. proves theorem 1.1 f continuous. f merely integrable longer approximate f uniformly trigonometric polynomials. instead apply approximation lemma 3.2 chapter 2 choose continuous function g circle satisﬁes sup θ02π gθ sup θ02π fθ b z 2π 0 fθ gθ dθ ϵ2. get f g2 1 2π z 2π 0 fθ gθ2 dθ 1 2π z 2π 0 fθ gθ fθ gθ dθ 2b 2π z 2π 0 fθ gθ dθ cϵ2. may approximate g trigonometric polynomial p g p ϵ. f p cϵ may conclude ap plying best approximation lemma. completes proof partial sums fourier series f converge f mean square norm . note result relation 3 imply nth fourier coeﬃcient integrable function f series p nan2 converges fact parsevals identity x n an2 f2.
ibookroot october 20 2007 80 chapter 3. convergence fourier series identity provides important connection norms two vector spaces ℓ2z r. summarize results section. theorem 1.3 let f integrable function circle f p naneinθ. have i meansquare convergence fourier series 1 2π z 2π 0 fθ snfθ2 dθ 0 n . ii parsevals identity x n an2 1 2π z 2π 0 fθ2 dθ. remark 1. en orthonormal family functions circle f en may deduce relation 3 x n an2 f2. known bessels inequality. equality holds as parsevals identity precisely family en also basis sense p nn anen f0 n . remark 2. may associate every integrable function se quence an formed fourier coeﬃcients. parsevals identity guar antees an ℓ2z. since ℓ2z hilbert space failure r complete discussed earlier may understood follows exist sequences annz p nz an2 yet riemann in tegrable function f nth fourier coeﬃcient equal n. example given exercise 6. since terms converging series tend 0 deduce par sevals identity bessels inequality following result. theorem 1.4 riemannlebesgue lemma f integrable circle ˆ fn 0 n . equivalent reformulation proposition f integrable 0 2π z 2π 0 fθ sinnθ dθ 0 n
ibookroot october 20 2007 2. return pointwise convergence 81 z 2π 0 fθ cosnθ dθ 0 n . conclude section give general version parseval identity use next chapter. lemma 1.5 suppose f g integrable circle f x aneinθ g x bneinθ. 1 2π z 2π 0 fθgθ dθ x n anbn. recall discussion example 1 series p nanbn converges absolutely. proof. proof follows parsevals identity fact f g 1 4 f g2 f g2 f ig2 f ig2 holds every hermitian inner product space. veriﬁcation fact left reader. 2 return pointwise convergence meansquare convergence theorem provide insight problem pointwise convergence. indeed theorem 1.1 guarantee fourier series converges θ. exercise 3 helps explain statement. however function diﬀerentiable point θ0 fourier series converges θ0. proving result give example continuous function diverging fourier series one point. phenomena indicative intricate nature problem pointwise convergence theory fourier series. 2.1 local result theorem 2.1 let f integrable function circle dif ferentiable point θ0. snfθ0 fθ0 n tends inﬁnity.
ibookroot october 20 2007 82 chapter 3. convergence fourier series proof. deﬁne ft fθ0 t fθ0 0 t π f θ0 0. first f bounded near 0 since f diﬀerentiable there. second small δ function f integrable π δ δ π f property t δ there. consequence proposition 1.4 appendix function f integrable π π. know snfθ0 f dnθ0 dn dirichlet kernel. since 1 2π r dn 1 ﬁnd snfθ0 fθ0 1 2π z π π fθ0 tdnt dt fθ0 1 2π z π π fθ0 t fθ0dnt dt 1 2π z π π fttdnt dt. recall tdnt sint2 sinn 12t quotient sint2 continuous interval π π. since write sinn 12t sinnt cost2 cosnt sint2 apply riemannlebesgue lemma riemann integrable functions ftt cost2 sint2 ftt ﬁnish proof the orem. observe conclusion theorem still holds assume f satisﬁes lipschitz condition θ0 is fθ fθ0 mθ θ0 0 θ. saying f satisﬁes h older condition order α 1. striking consequence theorem localization principle riemann. result states convergence snfθ0 depends behavior f near θ0. clear ﬁrst since forming fourier series requires integrating f whole circle.
ibookroot october 20 2007 2. return pointwise convergence 83 theorem 2.2 suppose f g two integrable functions deﬁned circle θ0 exists open interval containing θ0 fθ gθ θ i. snfθ0 sngθ0 0 n tends inﬁnity. proof. function f g 0 i diﬀerentiable θ0 may apply previous theorem conclude proof. 2.2 continuous function diverging fourier series turn attention example continuous periodic func tion whose fourier series diverges point. thus theorem 2.1 fails diﬀerentiability assumption replaced weaker assumption continuity. counterexample shows hypothesis appeared plausible fact false moreover construction also illuminates important principle theory. principle involved referred symmetry breaking.1 symmetry mind symmetry be tween frequencies einθ einθ appear fourier expan sion function. example partial sum operator sn deﬁned way reﬂects symmetry. also dirichlet fej er poisson kernels symmetric sense. break symme try is split fourier series p naneinθ two pieces p n0 aneinθ p n0 aneinθ introduce new farreaching phenomena. give simple example. start sawtooth function f odd θ equals iπ θ 0 θ π. then exercise 8 chapter 2 know 4 fθ x n0 einθ n . consider result breaking symmetry resulting series n1 x n einθ n . then unlike 4 longer fourier series riemann integrable function. indeed suppose fourier series 1we borrowed terminology physics used diﬀerent context.
ibookroot october 20 2007 84 chapter 3. convergence fourier series integrable function say f particular f bounded. using abel means ar f0 x n1 rn n tends inﬁnity r tends 1 p 1n diverges. gives desired contradiction since ar f0 1 2π z π π fθprθ dθ sup θ fθ prθ denotes poisson kernel discussed previous chapter. sawtooth function object fashion counterexample. proceed follows. n 1 deﬁne following two functions π π fnθ x 1nn einθ n fnθ x nn1 einθ n . contend that i fn0 c log n. ii fnθ uniformly bounded n θ. ﬁrst statement consequence fact pn n1 1n log n easily established see also figure 2 n x n1 1 n n1 x n1 z n1 n dx x z n 1 dx x log n. prove ii argue spirit proof taubers theorem says series p cn abel summable cn o1n p cn actually converges see exercise 14 chap ter 2. fact proof taubers theorem quite similar lemma below. lemma 2.3 suppose abel means ar p n1 rncn series p n1 cn bounded r tends 1 with r 1. cn o1n partial sums sn pn n1 cn bounded.
ibookroot october 20 2007 2. return pointwise convergence 85 1 x n n 1 1 n figure 2. comparing sum integral proof. let r 1 1n choose ncn m. esti mate diﬀerence sn ar n x n1 cn rncn x nn1 rncn follows sn ar n x n1 cn1 rn x nn1 rncn m n x n1 1 r n x nn1 rn mn1 r n 1 1 r 2m used simple observation 1 rn 1 r1 r rn1 n1 r. see satisﬁes ar m ncn m sn 3m. apply lemma series x n0 einθ n
ibookroot october 20 2007 86 chapter 3. convergence fourier series fourier series sawtooth function f used above. cn einθn einθn n 0 clearly cn o1n. finally abel means series arfθ f prθ. f bounded pr good kernel snfθ uniformly bounded n θ shown. come heart matter. notice fn fn trigonometric polynomials degree n that is nonzero fourier coeﬃcients n n. these form trigono metric polynomials pn pn degrees 3n 2n 1 displacing frequencies fn fn 2n units. words deﬁne pnθ ei2nθfnθ pnθ ei2nθ fnθ. fn nonvanishing fourier coeﬃcients 0 n n coef ﬁcients pn nonvanishing n n 3n n 2n. moreover n 0 center symmetry fn n 2n center symmetry pn. next consider partial sums sm. lemma 2.4 smpn pn 3n pn 2n 0 n. clear said figure 3. s2nei2nθfnθ ei2nθ fnθ n n 0 2n 3n 0 n fnθ ei2nθfnθ pnθ 2n 3n 0 n figure 3. breaking symmetry lemma 2.4 eﬀect 2n operator sm breaks symme try pn cases covered lemma action sm
ibookroot october 20 2007 3. exercises 87 relatively benign since outcome either pn 0. finally need ﬁnd convergent series positive terms p αk sequence integers nk increases rapidly enough that i nk1 3nk ii αk log nk as k . choose for example αk 1k2 nk 32k easily seen satisfy criteria. finally write desired function. fθ x k1 αkpnkθ. due uniform boundedness pn recall pnθ fnθ series converges uniformly continuous periodic function. however lemma get s2nmf0 cαm log nm o1 . 3nk1 nk1 3nk1 nk1 3nk nk 2nk figure 4. symmetry broken middle interval nk 3nk indeed terms correspond nk k k con tribute o1 0 respectively because pns uniformly bounded term corresponds nm absolute value greater cαm log nm pnθ fnθ c log n. partial sums fourier series f 0 bounded done since proves divergence fourier series f θ 0. produce function whose series diverges preassigned θ θ0 suﬃces consider function fθ θ0. 3 exercises 1. show ﬁrst two examples inner product spaces namely rd cd complete.
ibookroot october 20 2007 88 chapter 3. convergence fourier series hint every cauchy sequence r limit. 2. prove vector space ℓ2z complete. hint suppose ak aknnz k 1 2 . . . cauchy sequence. show n akn k1 cauchy sequence complex numbers therefore converges limit say bn. taking partial sums ak akand letting k show ak b0 k b . . . b1 b0 b1 . . . finally prove b ℓ2z. 3. construct sequence integrable functions fk 0 2π lim k 1 2π z 2π 0 fkθ2 dθ 0 limkfkθ fails exist θ. hint choose sequence intervals ik 0 2π whose lengths tend 0 point belongs inﬁnitely many them let fk χik. 4. recall vector space r integrable functions inner product norm f µ 1 2π z 2π 0 fx2 dx 12 . a show exist nonzero integrable functions f f 0. b however show f r f 0 fx 0 whenever f continuous x. c conversely show f r vanishes points continuity f 0. 5. let fθ 0 θ 0 log1θ 0 θ 2π deﬁne sequence functions r fnθ 0 0 θ 1n fθ 1n θ 2π. prove fn n1 cauchy sequence r. however f belong r.
ibookroot october 20 2007 3. exercises 89 hint show r b log θ2 dθ 0 0 b b 0 using fact derivative θlog θ2 2θ log θ 2θ equal log θ2. 6. consider sequence ak kdeﬁned ak ½ 1k k 1 0 k 0. note ak ℓ2z riemann integrable function kth fourier coeﬃcient equal ak k. 7. show trigonometric series x n2 1 log n sin nx converges every x yet fourier series riemann integrable function. true p sin nx nα 0 α 1 case 12 α 1 diﬃcult. see problem 1. 8. exercise 6 chapter 2 dealt sums x n odd 1 1 n2 x n1 1 n2 . similar sums derived using methods chapter. a let f function deﬁned π π fθ θ. use parsevals identity ﬁnd sums following two series x n0 1 2n 14 x n1 1 n4 . fact π496 π490 respectively. b consider 2πperiodic odd function deﬁned 0 π fθ θπ θ. show x n0 1 2n 16 π6 960 x n1 1 n6 π6 945. remark. general expression k even p n1 1nk terms πk given problem 4. however ﬁnding formula sum p n1 1n3 generally p n1 1nk k odd famous unresolved question.
ibookroot october 20 2007 90 chapter 3. convergence fourier series 9. show α integer fourier series π sin παeiπxα 0 2π given x n einx n α. apply parsevals formula show x n 1 n α2 π2 sin πα2 . 10. consider example vibrating string analyzed chapter 1. displacement ux t string time satisﬁes wave equation 1 c2 2u t2 2u x2 c2 τρ. string subject initial conditions ux 0 fx u t x 0 gx assume f c1 g continuous. deﬁne total energy string et 1 2ρ z l 0 µu t 2 dx 1 2τ z l 0 µu x 2 dx. ﬁrst term corresponds kinetic energy string in analogy 12mv2 kinetic energy particle mass velocity v second term corresponds potential energy. show total energy string conserved sense et constant. therefore et e0 1 2ρ z l 0 gx2 dx 1 2τ z l 0 f x2 dx. 11. inequalities wirtinger poincar e establish relationship norm function derivative.
ibookroot october 20 2007 3. exercises 91 a f tperiodic continuous piecewise c1 r 0 ft dt 0 show z 0 ft2 dt t 2 4π2 z 0 f t2 dt equality ft sin2πtt b cos2πtt. hint apply parsevals identity. b f g c1 tperiodic prove z 0 ftgt dt 2 t 2 4π2 z 0 ft2 dt z 0 gt2 dt. c compact interval a b continuously diﬀerentiable function f fa fb 0 show z b ft2 dt b a2 π2 z b f t2 dt. discuss case equality prove constant b a2π2 can improved. hint extend f odd respect periodic period 2b a integral interval length 0. apply part a get inequality conclude equality holds ft sinπ ta ba. 12. prove z 0 sin x x dx π 2 . hint start fact integral dnθ equals 2π note diﬀerence 1 sinθ2 2θ continuous π π. apply riemann lebesgue lemma. 13. suppose f periodic class ck. show ˆ fn o1nk is nk ˆ fn goes 0 n . improvement exercise 10 chapter 2. hint use riemannlebesgue lemma. 14. prove fourier series continuously diﬀerentiable function f circle absolutely convergent. hint use cauchyschwarz inequality parsevals identity f . 15. let f 2πperiodic riemann integrable π π.
ibookroot october 20 2007 92 chapter 3. convergence fourier series a show ˆ fn 1 2π z π π fx πneinx dx hence ˆ fn 1 4π z π π fx fx πneinx dx. b assume f satisﬁes h older condition order α namely fx h fx chα 0 α 1 c 0 x h. use part a show ˆ fn o1nα. c prove result cannot improved showing func tion fx x k0 2kαei2kx 0 α 1 satisﬁes fx h fx chα ˆ fn 1n α whenever n 2k. hint c break sum follows fx h fx p 2k1h p 2k1h. estimate ﬁrst sum use fact 1 eiθ θ whenever θ small. estimate second sum use obvious inequality eix eiy 2. 16. let f 2πperiodic function satisﬁes lipschitz condition constant k is fx fy kx y x y. simply h older condition α 1 previous exercise see ˆ fn o1n. since harmonic series p 1n diverges cannot say anything yet absolute convergence fourier series f. outline actually proves fourier series f converges absolutely uniformly.
ibookroot october 20 2007 3. exercises 93 a every positive h deﬁne ghx fx h fx h. prove 1 2π z 2π 0 ghx2 dx x n 4 sin nh2 ˆ fn2 show x n sin nh2 ˆ fn2 k2h2. b let p positive integer. choosing h π2p1 show x 2p1n2p ˆ fn2 k2π2 22p1 . c estimate p 2p1n2p ˆ fn conclude fourier series f converges absolutely hence uniformly. hint use cauchyschwarz inequality estimate sum. d fact modify argument slightly prove bernsteins theorem f satisﬁes h older condition order α 12 fourier series f converges absolutely. 17. f bounded monotonic function π π ˆ fn o1n. hint one may assume f increasing say f m. first check fourier coeﬃcients characteristic function a b satisfy o1n. show sum form n x k1 αkχakak1x π a1 a2 an1 π m α1 αn m fourier coeﬃcients o1n uniformly n. summing parts one gets telescopic sum pαk1 αk bounded 2m. approximate f functions type. 18. things learned decay fourier coeﬃcients a f class ck ˆ fn o1nk b f lipschitz ˆ fn o1n
ibookroot october 20 2007 94 chapter 3. convergence fourier series c f monotonic ˆ fn o1n d f satisﬁes h older condition exponent α 0 α 1 ˆ fn o1nα e f merely riemann integrable p ˆ fn2 and therefore ˆ fn o1. nevertheless show fourier coeﬃcients continuous function tend 0 arbitrarily slowly proving every sequence nonnegative real numbers ϵn converging 0 exists continuous function f ˆ fn ϵn inﬁnitely many values n. hint choose subsequence ϵnk p k ϵnk . 19. give another proof sum p 0nn einxn uniformly bounded n x π π using fact 1 2i x 0nn einx n n x n1 sin nx n 1 2 z x 0 dnt 1 dt dn dirichlet kernel. use fact r 0 sin dt which proved exercise 12. 20. let fx denote sawtooth function deﬁned fx π x2 interval 0 2π f0 0 extended periodicity r. fourier series f fx 1 2i x n0 einx n x n1 sin nx n f jump discontinuity origin f0 π 2 f0 π 2 hence f0 f0 π. show max 0xπn snfx π 2 z π 0 sin dt π 2 roughly 9 jump π. result manifestation gibbss phenomenon states near jump discontinuity fourier series function overshoots or undershoots approximately 9 jump. hint use expression snf given exercise 19.
ibookroot october 20 2007 4. problems 95 4 problems 1. 0 α 1 series x n1 sin nx nα converges every x fourier series riemann integrable func tion. a conjugate dirichlet kernel deﬁned dnx x nn signx einx signx 1 n 0 0 n 0 1 n 0 show dnx cosx2 cosn 12x sinx2 z π π dnx dx c log n. b result f riemann integrable f dn0 olog n. c present case leads n x n1 1 nα olog n contradiction. 2. important fact proved family einxnz orthonormal r also complete sense fourier series f converges f norm. exercise consider another family possessing properties. 1 1 deﬁne lnx dn dxn x2 1n n 0 1 2 . . . ln polynomial degree n called nth legendre poly nomial.
ibookroot october 20 2007 96 chapter 3. convergence fourier series a show f indeﬁnitely diﬀerentiable 1 1 z 1 1 lnxfx dx 1n z 1 1 x2 1nf nx dx. particular show ln orthogonal xm whenever n. hence ln n0 orthogonal family. b show ln2 z 1 1 lnx2 dx n222n1 2n 1 . hint first note ln2 1n2n r 1 1x2 1n dx. write x2 1n x 1nx 1n integrate parts n times calculate last integral. c prove polynomial degree n orthogonal 1 x x2 . . . xn1 constant multiple ln. d let ln lnln normalized legendre polynomials. prove ln family obtained applying gramschmidt process 1 x . . . xn . . . conclude every riemann integrable function f 1 1 legendre expansion x n0 f lnln converges f meansquare sense. 3. let α complex number equal integer. a calculate fourier series 2πperiodic function deﬁned π π fx cosαx. b prove following formulas due euler x n1 1 n2 α2 1 2α2 π 2α tanαπ. u c πz cot u 1 u 2 x n1 u u2 n2π2 .
ibookroot october 20 2007 4. problems 97 c show α c z απ sinαπ 1 2α2 x n1 1n1 n2 α2 . d 0 α 1 show z 0 tα1 1 dt π sinαπ. hint split integral r 1 0 r 1 change variables 1u second integral. integrals form z 1 0 tγ1 1 dt 0 γ 1 one show equal p k0 1k kγ . use part c conclude proof. 4. problem ﬁnd formula sum series x n1 1 nk k even integer. sums expressed terms bernoulli numbers related bernoulli polynomials discussed next problem. deﬁne bernoulli numbers bn formula z ez 1 x n0 bn n zn. a show b0 1 b1 12 b2 16 b3 0 b4 130 b5 0. b show n 1 bn 1 n 1 n1 x k0 µn 1 k bk. c writing z ez 1 1 z 2 x n2 bn n zn
ibookroot october 20 2007 98 chapter 3. convergence fourier series show bn 0 n odd 1. also prove z cot z 1 x n1 1n 22nb2n 2n z2n. d zeta function deﬁned ζs x n1 1 ns 1. deduce result c expression cotangent func tion obtained previous problem x cot x 1 2 x m1 ζ2m π2m x2m. e conclude 2ζ2m 1m1 2π2m 2m b2m. 5. deﬁne bernoulli polynomials bnx formula zexz ez 1 x n0 bnx n zn. a functions bnx polynomials x bnx n x k0 µn k bkxnk. show b0x 1 b1x x 12 b2x x2 x 16 b3x x3 3 2x2 1 2x. b n 1 bnx 1 bnx nxn1 n 2 bn0 bn1 bn. c deﬁne smn 1m 2m n 1m. show m 1smn bm1n bm1.
ibookroot october 20 2007 4. problems 99 d prove bernoulli polynomials polynomials satisfy i b0x 1 ii b nx nbn1x n 1 iii r 1 0 bnx dx 0 n 1 show b one obtains z x1 x bnt dt xn. e calculate fourier series b1x conclude 0 x 1 b1x x 12 1 π x k1 sin2πkx k . integrate conclude b2nx 1n1 22n 2π2n x k1 cos2πkx k2n b2n1x 1n1 22n 1 2π2n1 x k1 sin2πkx k2n1 . finally show 0 x 1 bnx n 2πin x k0 e2πikx kn . observe bernoulli polynomials are normalization successive integrals sawtooth function.
ibookroot october 20 2007 4 applications fourier series fourier series analogous expansions intervene naturally general theory curves surfaces. eﬀect theory conceived point view analysis deals obviously study arbitrary functions. thus led use fourier series sev eral questions geometry obtained direction number results presented work. one notes considerations form beginning principal series researches would without doubt give many new results. a. hurwitz 1902 previous chapters introduced basic facts fourier analysis motivated problems arose physics. motion string diﬀusion heat two instances led naturally expansion function terms fourier series. propose next give reader ﬂavor broader impact fourier analysis illustrate ideas reach areas mathematics. particular consider following three problems i. among simple closed curves length ℓin plane r2 one encloses largest area ii. given irrational number γ said distri bution fractional parts sequence numbers nγ n 1 2 3 . . . iii. exist continuous function nowhere diﬀeren tiable ﬁrst problem clearly geometric nature ﬁrst sight would seem little fourier series. second question lies border number theory study dynamical systems gives us simplest example idea ergodicity. third problem analytic nature resisted many attempts
ibookroot october 20 2007 1. isoperimetric inequality 101 solution ﬁnally discovered. remarkable three questions resolved quite simply directly use fourier series. last section chapter return problem provided initial motivation. consider timedependent heat equation circle. investigation lead us important enigmatic heat kernel circle. however mysteries surrounding basic properties fully understood apply poisson summation formula next chapter. 1 isoperimetric inequality let γ denote closed curve plane intersect itself. also let ℓdenote length γ area bounded region r2 enclosed γ. problem determine given ℓthe curve γ maximizes if curve exists. γ small γ large figure 1. isoperimetric problem little experimentation reﬂection suggests solution circle. conclusion reached following heuristic con siderations. curve thought closed piece string lying ﬂat table. region enclosed string convex for ex ample one deform part string increase area enclosed it. also playing simple examples one convince oneself ﬂatter curve portion less eﬃcient enclosing area. therefore want maximize roundness curve point. although circle correct guess making ideas precise diﬃcult matter. key idea solution give isoperimetric problem con sists application parsevals identity fourier series. however attempt solution problem must deﬁne
ibookroot october 20 2007 102 chapter 4. applications fourier series notion simple closed curve length mean area region enclosed it. curves length area parametrized curve γ mapping γ a b r2. image γ set points plane call curve denote γ. curve γ simple intersect itself closed two endpoints coincide. terms parametrization above two conditions translate γs1 γs2 unless s1 s2 b case γa γb. may extend γ periodic function r period b a think γ function circle. also always impose smoothness curves assuming γ class c1 derivative γ satisﬁes γs 0. altogether conditions guarantee γ welldeﬁned tangent point varies continuously point curve varies. more over parametrization γ induces orientation γ parameter travels b. c1 bijective mapping c d a b gives rise another parametrization γ formula ηt γst. clearly conditions γ closed simple independent chosen parametrization. also say two parametrizations γ η equivalent st 0 t means η γ induce orientation curve γ. if however st 0 η reverses orientation. γ parametrized γs xs ys length curve γ deﬁned ℓ z b γs ds z b xs2 ys212 ds. length γ notion intrinsic curve depend parametrization. see indeed case suppose γst ηt. then change variables formula chain rule imply z b γs ds z c γst st dt z c ηt dt
ibookroot october 20 2007 1. isoperimetric inequality 103 desired. proof theorem below shall use special type parametrization γ. say γ parametrization arc length γs 1 s. means γs travels constant speed consequence length γ precisely b a. therefore possible additional translation parametrization arclength deﬁned 0 ℓ. curve admits parametrization arc length exercise 1. turn isoperimetric problem. attempt give precise formulation area region enclosed simple closed curve γ raises number tricky questions. variety simple situations evident area given following familiar formula calculus 1 2 z γ x dy y dx 1 1 2 z b xsys ysxs ds see example exercise 3. thus formulating result shall adopt easy expedient taking 1 deﬁnition area. device allows us give quick neat proof isoperimetric in equality. listing issues simpliﬁcation leaves unresolved found proof theorem. statement proof isoperimetric inequality theorem 1.1 suppose γ simple closed curve r2 length ℓ let denote area region enclosed curve. ℓ2 4π equality γ circle. ﬁrst observation rescale problem. means change units measurement factor δ 0 follows. consider mapping plane r2 itself sends point x y δx δy. look formula deﬁning length curve shows γ length ℓ image mapping length δℓ. operation magniﬁes contracts lengths factor δ depending whether δ 1 δ 1. similarly see
ibookroot october 20 2007 104 chapter 4. applications fourier series mapping magniﬁes or contracts areas factor δ2. taking δ 2πℓ see suﬃces prove ℓ 2π π equality γ circle. let γ 0 2π r2 γs xs ys parametrization arclength curve γ is xs2 ys2 1 0 2π. implies 2 1 2π z 2π 0 xs2 ys2 ds 1. since curve closed functions xs ys 2πperiodic may consider fourier series xs x aneins ys x bneins. then remarked later part section 2 chapter 2 xs x anineins ys x bnineins. parsevals identity applied 2 gives 3 x n n2 an2 bn2 1. apply bilinear form parsevals identity lemma 1.5 chap ter 3 integral deﬁning a. since xs ys realvalued an bn bn ﬁnd 1 2 z 2π 0 xsys ysxs ds π x n n ³ anbn bnan . observe next 4 anbn bnan 2 an bn an2 bn2 since n n2 may use 3 get π x n n2 an2 bn2 π
ibookroot october 20 2007 2. weyls equidistribution theorem 105 desired. π see argument xs a1eis a0 a1eis ys b1eis b0 b1eis n n2 soon n 2. know xs ys realvalued a1 a1 b1 b1. identity 3 implies 2 a12 b12 1 since equality 4 must a1 b1 12. write a1 1 2 eiα b1 1 2 eiβ. fact 1 2a1b1 a1b1 implies sinα β 1 hence α β kπ2 k odd integer. ﬁnd xs a0 cosα s ys b0 sinα s sign ys depends parity k 12. case see γ circle case equality obviously holds proof theorem complete. solution given due hurwitz 1901 indeed ele gant clearly leaves important issues unanswered. list follows. suppose γ simple closed curve. i region enclosed γ deﬁned ii geometric deﬁnition area region deﬁnition accord 1 iii results extended general class sim ple closed curves relevant problemthose curves rectiﬁablethat is ascribe ﬁnite length turns clariﬁcations problems raised connected number signiﬁcant ideas analysis. shall return questions succeeding books series. 2 weyls equidistribution theorem apply ideas coming fourier series problem dealing properties irrational numbers. begin brief discussion congruences concept needed understand main theorem.
ibookroot october 20 2007 106 chapter 4. applications fourier series reals modulo integers x real number let x denote greatest integer less equal x call quantity x integer part x. fractional part x deﬁned x x x. particular x0 1 every x r. example integer fractional parts 2.7 2 0.7 respectively integer fractional parts 3.4 4 0.6 respectively. may deﬁne relation r saying two numbers x equivalent congruent x y z. write x mod z x mod 1. means identify two real numbers diﬀer integer. observe real number x congruent unique number 0 1 precisely x fractional part x. eﬀect reducing real number modulo z means looking fractional part disregarding integer part. start real number γ 0 look sequence γ 2γ 3γ . . . . intriguing question ask happens sequence reduce modulo z is look sequence fractional parts γ 2γ 3γ . . . . simple observations i γ rational ﬁnitely many numbers appearing nγ distinct. ii γ irrational numbers nγare distinct. indeed part i note γ pq ﬁrst q terms sequence pq 2pq . . . q 1pq qpq 0. sequence begins repeat itself since q 1pq 1 pq pq on. however see exercise 6 reﬁned result. also part ii assume numbers distinct. there fore n1γ n2γfor n1 n2 n1γ n2γ z hence γ rational contradiction.
ibookroot october 20 2007 2. weyls equidistribution theorem 107 fact shown γ irrational nγis dense interval 0 1 result originally proved kronecker. words sequence nγhits every subinterval 0 1 and hence inﬁnitely many times. obtain fact corollary deeper theorem dealing uniform distribution sequence nγ. sequence numbers ξ1 ξ2 . . . ξn . . . 0 1 said equidis tributed every interval a b 0 1 lim n 1 n n ξn a b n b a a denotes cardinality ﬁnite set a. means large n proportion numbers ξn a b n n equal ratio length interval a b length interval 0 1. words sequence ξn sweeps whole interval evenly every subinterval gets fair share. clearly ordering sequence important next two examples illustrate. example 1. sequence 0 1 2 0 1 3 2 3 0 1 4 2 4 3 4 0 1 5 2 5 appears equidistributed since passes interval 0 1 evenly. course proof reader invited give one. somewhat related example see exercise 8 σ 12. example 2. let rn n1 enumeration rationals 0 1. sequence deﬁned ξn ½ rn2 n even 0 n odd equidistributed since half sequence 0. nevertheless sequence obviously dense. arrive main theorem section. theorem 2.1 γ irrational sequence fractional parts γ 2γ 3γ . . . equidistributed 0 1. particular nγis dense 0 1 get kroneckers theo rem corollary. figure 2 illustrate set points γ 2γ 3γ . . . nγfor three diﬀerent values n γ 2.
ibookroot october 20 2007 108 chapter 4. applications fourier series n 10 n 30 n 80 1 0 1 1 0 0 figure 2. sequence γ 2γ 3γ . . . nγwhen γ 2 fix a b 0 1 let χabx denote characteristic function interval a b is function equal 1 a b 0 0 1 a b. may extend function r periodicity pe riod 1 still denote extension χabx. then conse quence deﬁnitions ﬁnd 1 n n nγa b n x n1 χabnγ theorem reformulated statement 1 n n x n1 χabnγ z 1 0 χabx dx n . step removes diﬃculty working fractional parts reduces number theory analysis. heart matter lies following result. lemma 2.2 f continuous periodic period 1 γ irra tional 1 n n x n1 fnγ z 1 0 fx dx n . proof lemma divided three steps. step 1. ﬁrst check validity limit case f one exponentials 1 e2πix . . . e2πikx . . . . f 1 limit
ibookroot october 20 2007 2. weyls equidistribution theorem 109 surely holds. f e2πikx k 0 integral 0. since γ irrational e2πikγ 1 therefore 1 n n x n1 fnγ e2πikγ n 1 e2πiknγ 1 e2πikγ goes 0 n . step 2. clear f g satisfy lemma af bg a b c. therefore ﬁrst step implies lemma true trigonometric polynomials. step 3. let ϵ 0. f continuous periodic function period 1 choose trigonometric polynomial p supxr fx px ϵ3 this possible corollary 5.4 chapter 2. then step 1 large n 1 n n x n1 pnγ z 1 0 px dx ϵ3. therefore 1 n n x n1 fnγ z 1 0 fx dx 1 n n x n1 fnγ pnγ 1 n n x n1 pnγ z 1 0 px dx z 1 0 px fx dx ϵ lemma proved. ﬁnish proof theorem. choose two continuous periodic functions f ϵ f ϵ period 1 approximate χabx 0 1 below f ϵ f ϵ bounded 1 agree χabx except intervals total length 2ϵ see figure 3. particular f ϵ x χabx f ϵ x b a 2ϵ z 1 0 f ϵ x dx z 1 0 f ϵ x dx b a 2ϵ. sn 1 n pn n1 χabnγ get 1 n n x n1 f ϵ nγ sn 1 n n x n1 f ϵ nγ.
ibookroot october 20 2007 110 chapter 4. applications fourier series f ϵ f ϵ 0 b 1 b ϵ b ϵ ϵ ϵ figure 3. approximations χabx therefore b a 2ϵ lim inf nsn lim sup n sn b a 2ϵ. since true every ϵ 0 limit limnsn exists must equal b a. completes proof equidistribution theorem. theorem following consequence. corollary 2.3 conclusion lemma 2.2 holds every function f riemann integrable 0 1 periodic period 1. proof. assume f realvalued consider partition interval 0 1 say 0 x0 x1 xn 1. next deﬁne fux supxj1yxj fy x xj1 xj flx infxj1yxj fy x xj1 xj. clearly fl f fu z 1 0 flx dx z 1 0 fx dx z 1 0 fux dx. moreover making partition suﬃciently ﬁne guarantee given ϵ 0 z 1 0 fux dx z 1 0 flx dx ϵ. however 1 n n x n1 flnγ z 1 0 flx dx
ibookroot october 20 2007 2. weyls equidistribution theorem 111 theorem fl ﬁnite linear combination charac teristic functions intervals similarly 1 n n x n1 funγ z 1 0 fux dx. two assertions conclude proof corollary using previous approximation argument. interesting interpretation lemma corollary terms simple dynamical system. example underlying space circle parametrized angle θ. also consider mapping space itself here choose rotation ρ circle angle 2πγ is transformation ρ θ 7θ 2πγ. want next consider space underlying action ρ evolves time. words wish consider iterates ρ namely ρ ρ2 ρ3 . . . ρn ρn ρ ρ ρ θ 7θ 2πnγ think action ρn taking place time n. riemann integrable function f circle also asso ciate corresponding eﬀects rotation ρ obtain sequence functions fθ fρθ fρ2θ . . . fρnθ . . . fρnθ fθ 2πnγ. special context ergodicity system statement time average lim n 1 n n x n1 fρnθ exists θ equals space average 1 2π z 2π 0 fθ dθ whenever γ irrational. fact assertion merely rephrasing corollary 2.3 make change variables θ 2πx. returning problem equidistributed sequences observe proof theorem 2.1 gives following characterization.
ibookroot october 20 2007 112 chapter 4. applications fourier series weyls criterion. sequence real numbers ξ1 ξ2 . . . 0 1 equidistributed integers k 0 one 1 n n x n1 e2πikξn 0 n . one direction theorem eﬀect proved above con verse found exercise 7. particular ﬁnd understand equidistributive properties sequence ξn suﬃces estimate size corresponding exponential sum pn n1 e2πikξn. ex ample shown using weyls criterion sequence n2γ equidistributed whenever γ irrational. examples found exercises 8 9 also problems 2 3. last remark mention nice geometric interpretation distribution properties nγ. suppose sides square reﬂecting mirrors ray light leaves point inside square. kind path light trace out p figure 4. reﬂection ray light square solve problem main idea consider grid plane formed successively reﬂecting initial square across sides. appropriate choice axis path traced light square corresponds straight line p t γt plane. result reader may observe path either closed periodic dense square. ﬁrst situations
ibookroot october 20 2007 3. continuous nowhere differentiable function 113 happen slope γ initial direction light determined respect one sides square rational. second situation γ irrational density follows kroneckers theorem. stronger conclusion one get equidistribution theorem 3 continuous nowhere diﬀerentiable function many obvious examples continuous functions diﬀerentiable one point say fx x. almost easy con struct continuous function diﬀerentiable given ﬁnite set points even appropriate sets containing countably many points. subtle problem whether exists continuous function nowhere diﬀerentiable. 1861 riemann guessed function deﬁned 5 rx x n1 sinn2x n2 nowhere diﬀerentiable. led consider function close connection theta function introduced chapter 5. riemann never gave proof mentioned example one lectures. triggered interest weierstrass who attempt ﬁnd proof came across ﬁrst example continuous nowhere diﬀerentiable function. say 0 b 1 integer 1. 1872 proved ab 1 3π2 function wx x n1 bn cosanx nowhere diﬀerentiable. story complete without ﬁnal word riemanns original function. 1916 hardy showed r diﬀerentiable irrational multiples π also certain rational multiples π. however much later 1969 gerver completely settled problem ﬁrst proving function r actually diﬀerentiable rational multiples π form πpq p q odd integers showing r diﬀerentiable remaining cases. section prove following theorem.
ibookroot october 20 2007 114 chapter 4. applications fourier series theorem 3.1 0 α 1 function fαx fx x n0 2nαei2nx continuous nowhere diﬀerentiable. continuity clear absolute convergence se ries. crucial property f need many van ishing fourier coeﬃcients. fourier series skips many terms like one given above like wx called lacunary fourier series. proof theorem really story three methods sum ming fourier series. first ordinary convergence terms partial sums sng g dn. next ces aro summabil ity σng g fn fn fej er kernel. third method clearly connected second involves delayed means deﬁned ng 2σ2ng σng. hence ng g 2f2n fn. methods best visualized figure 5. suppose gx p aneinx. then . sn arises multiplying term aneinx 1 n n 0 n n. . σn arises multiplying aneinx 1 nn n n 0 n n. . n arises multiplying aneinx 1 n n 21 n2n n n 2n 0 n 2n. example note σngx s0gx s1gx sn1gx n 1 n n1 x ℓ0 x kℓ akeikx 1 n x nn n naneinx x nn µ 1 n n aneinx.
ibookroot october 20 2007 3. continuous nowhere differentiable function 115 partial sums n 1 n 1 n 0 n 1 2n n n 2n 0 0 ngx 2σ2ngx σngx delayed means ces aro means σngx p nn ³ 1 n n aneinx sngx p nn aneinx figure 5. three summation methods
ibookroot october 20 2007 116 chapter 4. applications fourier series proof assertion similar. delayed means two important features. one hand properties closely related good features ces aro means. hand series lacunary properties like f delayed means essentially equal partial sums. particular note function f fα 6 snf n f n largest integer form 2k n n. clear examining figure 5 deﬁnition f. turn proof theorem proper argue contradiction is assume f x0 exists x0. lemma 3.2 let g continuous function diﬀerentiable x0. then ces aro means satisfy σngx0 olog n therefore ngx0 olog n. proof. first σngx0 z π π f nx0 tgt dt z π π f ntgx0 t dt fn fej er kernel. since fn periodic r π πf ntdt0 implies σngx0 z π π f ntgx0 t gx0 dt. assumption g diﬀerentiable x0 get σngx0 c z π π f nt t dt. observe f n satisﬁes two estimates f nt an 2 f nt a t2 . ﬁrst inequality recall fn trigonometric polynomial degree n whose coeﬃcients bounded 1. therefore f n trigonometric polynomial degree n whose coeﬃcients bigger n. hence f t 2n 1n an 2.
ibookroot october 20 2007 3. continuous nowhere differentiable function 117 second inequality recall fnt 1 n sin2nt2 sin2t2 . diﬀerentiating expression get two terms sinnt2 cosnt2 sin2t2 1 n cost2 sin2nt2 sin3t2 . use facts sinnt2 cnt sint2 ct for t π get desired estimates f nt. using estimates ﬁnd σngx0 c z t1n f nt t dt c z t1n f nt t dt ca z t1n dt t z t1n dt olog n o1 olog n. proof lemma complete invoke deﬁnition ng. lemma 3.3 2n 2n 2nf nf 2nαei2nx. follows previous observation 6 2nf s2nf nf snf. now ﬁrst lemma 2nfx0 nfx0 olog n second lemma also implies 2nfx0 nfx0 2n1α cn 1α. desired contradiction since n 1α grows faster log n. additional remarks function fαx p n0 2nαei2nx order.
ibookroot october 20 2007 118 chapter 4. applications fourier series function complexvalued opposed examples r w above nowhere diﬀerentiability fα imply property real imaginary parts. however small modiﬁcation proof shows that fact real part fα x n0 2nα cos 2nx well imaginary part nowhere diﬀerentiable. see this observe ﬁrst proof lemma 3.2 following generalization g continuous function diﬀerentiable x0 ngx0 h olog n whenever h cn. proceed fx p n0 2nα cos 2nx noting 2nf nf 2nα cos 2nx result assuming f diﬀer entiable x0 get 2n1α sin2nx0 h olog n 2n 2n h cn. get contradiction need choose h sin2nx0 h 1 accomplished setting δ equal distance 2nx0 nearest number form k 12π k z so δ π2 taking h δ2n. clearly α 1 function fα continuously diﬀerentiable since series diﬀerentiated term term. finally nowhere dif ferentiability proved α 1 actually extends α 1 suitable reﬁnement argument see problem 8 chapter 5. fact using elaborate methods one also show weierstrass function w nowhere diﬀerentiable ab 1. 4 heat equation circle ﬁnal illustration return original problem heat diﬀusion considered fourier. suppose given initial temperature distribution 0 ring asked describe temperature points ring times 0. ring modeled unit circle. point circle de scribed angle θ 2πx variable x lies 0 1. ux t denotes temperature time point described
ibookroot october 20 2007 4. heat equation circle 119 angle θ considerations similar ones given chapter 1 show u satisﬁes diﬀerential equation 7 u t c 2u x2 . constant c positive physical constant depends material ring made see section 2.1 chapter 1. rescaling time variable may assume c 1. f initial data impose condition ux 0 fx. solve problem separate variables look special solutions form ux t axbt. inserting expression u heat equation get bt bt ax ax . sides therefore constant say equal λ. since must periodic period 1 see possibility λ 4π2n2 n z. linear combination exponentials e2πinx e2πinx bt multiple e4π2n2t. superposing solutions led 8 ux t x n ane4π2n2te2πinx where setting 0 see an fourier coeﬃcients f. note f riemann integrable coeﬃcients bounded since factor e4π2n2t tends zero extremely fast series deﬁning u converges. fact case u twice diﬀerentiable solves equation 7. natural question regard boundary condition following ux t fx tends 0 sense simple application parseval identity shows limit holds mean square sense exercise 11. better understanding properties solution 8 write ux t f htx
ibookroot october 20 2007 120 chapter 4. applications fourier series ht heat kernel circle given 9 htx x n e4π2n2te2πinx convolution functions period 1 deﬁned f gx z 1 0 fx ygy dy. analogy heat kernel poisson kernel of chapter 2 given exercise 12. however unlike case poisson kernel elementary formula heat kernel. nevertheless turns good kernel in sense chapter 2. proof obvious requires use celebrated poisson summation formula taken chapter 5. corollary also ﬁnd ht everywhere positive fact also obvious deﬁning expression 9. can however give following heuristic argument positivity ht. suppose begin initial temperature distribution f everywhere 0. physically reasonable expect ux t 0 since heat travels hot cold. ux t z 1 0 fx yhty dy. ht negative x0 may choose f 0 supported near x0 would imply ux0 t 0 contradiction. 5 exercises 1. let γ a b r2 parametrization closed curve γ. a prove γ parametrization arclength length curve γa γs precisely a is z γt dt a. b prove curve γ admits parametrization arclength. hint η parametrization let hs r ηt dt consider γ η h1. 2. suppose γ a b r2 parametrization closed curve γ γt xt yt.
ibookroot october 20 2007 5. exercises 121 a show 1 2 z b xsys ysxs ds z b xsys ds z b ysxs ds. b deﬁne reverse parametrization γ γ a b r2 γt γb t. image γis precisely γ except points γt γt travel opposite directions. thus γreverses orientation curve. prove z γ x dy y dx z γx dy y dx. particular may assume after possible change orientation 1 2 z b xsys ysxs ds z b xsys ds. 3. suppose γ curve plane exists set coordinates x xaxis divides curve union graph two continuous functions fx gx 0 x 1 fx gx see figure 6. let ωdenote region graphs two functions ω x y 0 x 1 gx y fx. 0 1 gx fx ω figure 6. simple version area formula familiar interpretation integral r hx dx gives area graph function h see area ωis r 1 0 fx dx
ibookroot october 20 2007 122 chapter 4. applications fourier series r 1 0 gx dx. show deﬁnition coincides area formula given text is z 1 0 fx dx z 1 0 gx dx z γ dx a. also note orientation curve chosen ωlies left γ formula holds without absolute value signs. formula generalizes set written ﬁnite union domains like ωabove. 4. observe deﬁnition ℓand given text isoperimetric inequality continues hold with proof even γ simple. show stronger version isoperimetric inequality equivalent wirtingers inequality says f 2πperiodic class c1 satisﬁes r 2π 0 ft dt 0 z 2π 0 ft2 dt z 2π 0 f t2 dt equality ft sin b cos exercise 11 chapter 3. hint one direction note length curve 2π γ appropriate arclength parametrization 2π a z 2π 0 xs ys2 ds z 2π 0 ys2 ys2 ds. change coordinates guarantee r 2π 0 ys ds 0. direction start realvalued f satisfying hypotheses wirtingers inequality construct g 2πperiodic term brackets vanishes. 5. prove sequence γn n1 γn fractional part µ1 5 2 n equidistributed 0 1. hint show un ³ 1 5 2 n ³ 1 5 2 n solution diﬀerence equation ur1 ur ur1 u0 2 u1 1. un satisfy diﬀerence equation fibonacci numbers. 6. let θ pq rational number p q relatively prime inte gers that is θ lowest form. assume without loss generality q 0. deﬁne sequence numbers 0 1 ξn nθwhere denotes
ibookroot october 20 2007 5. exercises 123 fractional part. show sequence ξ1 ξ2 . . . equidistributed points form 0 1q 2q . . . q 1q. fact prove 0 a q one n 1 n n nθ aq n 1 q µ 1 n . hint integer k 0 exists unique integer n kq n k 1q nθ aq. one assume k 0 prove existence n using fact1 p q relatively prime exist integers x xp yq 1. next divide n q remainder is write n ℓq r 0 ℓand 0 r q. establish inequalities ℓn 1 n n nθ aq ℓ 1. 7. prove second part weyls criterion sequence numbers ξ1 ξ2 . . . 0 1 equidistributed k z 0 1 n n x n1 e2πikξn 0 n . hint suﬃces show 1 n pn n1 fξn r 1 0 fx dx continuous f. prove ﬁrst f characteristic function interval. 8. show 0 σ 0 σ 1 sequence anσis equidis tributed 0 1. hint prove pn n1 e2πibnσ on σ on 1σ b 0. fact note following n x n1 e2πibnσ z n 1 e2πibxσ dx ã n x n1 n1σ . 9. contrast result exercise 8 prove a log nis equidis tributed a. hint compare sum pn n1 e2πib log n corresponding integral. 10. suppose f periodic function r period 1 ξn sequence equidistributed 0 1. prove that 1the elementary results arithmetic used exercise found begin ning chapter 8.
ibookroot october 20 2007 124 chapter 4. applications fourier series a f continuous satisﬁes r 1 0 fx dx 0 lim n 1 n n x n1 fx ξn 0 uniformly x. hint establish result ﬁrst trigonometric polynomials. b f merely integrable 0 1 satisﬁes r 1 0 fx dx 0 lim n z 1 0 1 n n x n1 fx ξn 2 dx 0. 11. show ux t f htx ht heat kernel f riemann integrable z 1 0 ux t fx2 dx 0 0. 12. change variables 8 leads solution uθ τ x anen2τeinθ f hτθ equation u τ 2u θ2 0 θ 2π τ 0 boundary condition uθ 0 fθ p aneinθ. hτθ p nen2τeinθ. version heat kernel 0 2π analogue poisson kernel written prθ p nenτeinθ r eτ and 0 r 1 corresponds τ 0. 13. fact kernel htx good kernel hence ux t fx point continuity f easy prove. shown next chapter. however one prove directly htx peaked x 0 0 following sense a show r 12 12 htx2 dx order magnitude t12 0. precisely prove t12 r 12 12 htx2 dx converges nonzero limit 0. b prove r 12 12 x2htx2 dx ot12 0.
ibookroot october 20 2007 6. problems 125 hint a compare sum p ecn2t integral r ecx2t dx c 0. b use x2 csin πx2 12 x 12 apply mean value theorem ecx2t. 6 problems 1.this problem explores another relationship geometry curve fourier series. diameter closed curve γ parametrized γt xt yt π π deﬁned sup p qγ p q sup t1 t2ππ γt1 γt2. nth fourier coeﬃcient γt xt iyt ℓdenotes length γ a 2an d n 0. b ℓπd whenever γ convex. property a follows fact 2an 1 2π r π πγt γt πneint dt. equality ℓ πd satisﬁed γ circle surprisingly case. fact one ﬁnds ℓ πd equivalent 2a1 d. reparametrize γ π π tangent curve makes angle yaxis. then a1 1 γt ieit1 rt r realvalued function satisﬁes rt rt π 0 rt 1. figure 7 a shows curve obtained setting rt cos 5t. also figure 7 b consists curve rt h3t hs 1 π 0 hs 1 0 π. curve which piecewise class c1 known reuleaux triangle classical example convex curve constant width circle. 2.here present estimate weyl leads interesting results. a let sn pn n1 e2πifn. show h n one sn2 c n h h x h0 nh x n1 e2πifnhfn constant c 0 independent n h f. b use estimate show sequence n2γis equidistributed 0 1 whenever γ irrational.
ibookroot october 20 2007 126 chapter 4. applications fourier series a b figure 7. curves maximal length given diameter c generally show ξn sequence real numbers positive integers h diﬀerence ξnh ξnis equidistributed 0 1 ξnis also equidistributed 0 1. d suppose px cnxn c0 polynomial real coeﬃcients least one c1 . . . cn irrational. sequence pnis equidistributed 0 1. hint a let e2πifn 1 n n 0 otherwise. write h p n ph k1 p n ank apply cauchyschwarz inequality. b note n h2γ n2γ 2nhγ h2γ use fact integer h sequence 2nhγis equidistributed. finally prove d assume ﬁrst px qx c1x c0 c1 irrational estimate exponential sum pn n1 e2πikp n. then argue induction highest degree term irrational coeﬃcient use part c. 3.if σ 0 integer 0 anσis equidistributed 0 1. see also exercise 8. 4. elementary construction continuous nowhere diﬀerentiable func tion obtained piling singularities follows. 1 1 consider function ϕx x extend ϕ r requiring periodic period 2. clearly ϕ continuous r ϕx 1 x function f deﬁned fx x n0 µ3 4 n ϕ4nx continuous r.
ibookroot october 20 2007 6. problems 127 a fix x0 r. every positive integer m let δm 1 24m sign chosen integer lies 4mx0 4mx0 δm. consider quotient γn ϕ4nx0 δm ϕ4nx0 δm . prove n m γn 0 0 n m one γn 4n γm 4m. b observations prove estimate fx0 δm fx0 δm 1 23m 1 conclude f diﬀerentiable x0. 5. let f riemann integrable function interval π π. deﬁne generalized delayed means fourier series f σnk sn snk1 k . note particular σ0n σn σn1 sn σnn n n speciﬁc delayed means used section 3. a show σnk 1 k n kσnk nσn σnk sn x n1νnk1 µ 1 ν n k ˆ fνeiνθ. last expression σnk conclude σnk sm x n1νnk1 ˆ fν n m n k.
ibookroot october 20 2007 128 chapter 4. applications fourier series b use one formulas fej ers theorem show n kn k n σknnfθ fθ n whenever f continuous θ also σknnfθ fθ fθ 2 n jump discontinuity refer preceding chapters exer cises appropriate deﬁnitions results. case f continuous π π show σknnf f uniformly n . c using part a show ˆ fν o1ν kn m k 1n get σknn sm c k constant c 0. d suppose ˆ fν o1ν. prove f continuous θ snfθ fθ n f jump discontinuity θ snfθ fθ fθ 2 n . also show f continuous π π snf f uniformly. e arguments show p cn ces aro summable cn o1n p cn converges s. weak version littlewoods theorem problem 3 chapter 2. 6. dirichlets theorem states fourier series real continuous peri odic function f ﬁnite number relative maxima minima converges everywhere f and uniformly. prove theorem showing function satisﬁes ˆ fn o1n. hint argue exercise 17 chapter 3 use conclusion d problem 5 above.
ibookroot october 20 2007 5 fourier transform r theory fourier series integrals always major diﬃculties necessitated large math ematical apparatus dealing questions con vergence. engendered development methods summation although lead com pletely satisfactory solution problem. . . fourier transform introduction distributions hence space s inevitable either explicit hidden form. . . result one may obtain desired point view continuity inversion fourier transform. l. schwartz 1950 theory fourier series applies functions circle equiv alently periodic functions r. chapter develop analogous theory functions entire real line nonperiodic. functions consider suitably small inﬁnity. sev eral ways deﬁning appropriate notion smallness nevertheless vital assume sort vanishing inﬁnity. one hand recall fourier series periodic function associates sequence numbers namely fourier coeﬃcients function hand given suitable function f r analogous object associated f fact another function ˆ f r called fourier transform f. since fourier transform function r function r one observe symmetry function fourier transform whose analogue apparent setting fourier series. roughly speaking fourier transform continuous version fourier coeﬃcients. recall fourier coeﬃcients function f deﬁned circle given 1 z 1 0 fxe2πinx dx
ibookroot october 20 2007 130 chapter 5. fourier transform r appropriate sense 2 fx x n ane2πinx. replaced θ 2πx frequently done previously. now consider following analogy replace discrete symbols such integers sums continuous counterparts such real numbers integrals. words given function f r deﬁne fourier transform changing domain integration circle r replacing n z ξ r 1 is setting 3 ˆ fξ z fxe2πixξ dx. push analogy further consider following continuous ver sion 2 replacing sum integral ˆ fξ leads fourier inversion formula 4 fx z ˆ fξe2πixξ dξ. suitable hypotheses f identity 4 actually holds much theory chapter aims proving exploiting relation. validity fourier inversion formula also suggested following simple observation. suppose f supported ﬁnite interval contained l2 l2 expand f fourier series i. then letting l tend inﬁnity led 4 see exercise 1. special properties fourier transform make important tool study partial diﬀerential equations. instance shall see fourier inversion formula allows us analyze equations modeled real line. particular following ideas developed circle solve timedependent heat equation inﬁnite rod steadystate heat equation upper halfplane. last part chapter discuss topics related poisson summation formula x nz fn x nz ˆ fn gives another remarkable connection periodic functions and fourier series nonperiodic functions line and
ibookroot october 20 2007 1. elementary theory fourier transform 131 fourier transforms. identity allows us prove assertion made previous chapter namely heat kernel htx satisﬁes properties good kernel. addition poisson summation formula arises many settings particular parts number theory shall see book ii. make ﬁnal comment approach chosen. study fourier series found useful consider riemann integrable functions circle. particular generality assured us even functions certain discontinuities could treated theory. contrast exposition elementary properties fourier transform stated terms schwartz space testing functions. functions indeﬁnitely diﬀerentiable that together derivatives rapidly decreasing inﬁnity. reliance space functions device allows us come quickly main conclusions formulated direct transparent fashion. carried out point easy extensions somewhat wider setting. general theory fourier transforms which must necessarily based lebesgue integration treated book iii. 1 elementary theory fourier transform begin extending notion integration functions deﬁned whole real line. 1.1 integration functions real line given notion integral function closed bounded interval natural extension deﬁnition continuous func tions r z fx dx lim n z n n fx dx. course limit may exist. example clear fx 1 even fx 11 x limit inﬁnite. moments reﬂection suggests limit exist impose f enough decay x tends inﬁnity. useful condition follows. function f deﬁned r said moderate decrease f continuous exists constant 0 fx 1 x2 x r.
ibookroot october 20 2007 132 chapter 5. fourier transform r inequality says f bounded by instance also decays inﬁnity least fast 1x2 since a1 x2 ax2. example function fx 11 xn moderate decrease long n 2. another example given function eax 0. shall denote mr set functions moderate decrease r. exercise reader check usual addition functions multiplication scalars mr forms vector space c. next see whenever f belongs mr may deﬁne 5 z fx dx lim n z n n fx dx limit exists. indeed n integral r n n fx dx well deﬁned f continuous. suﬃces show in cauchy sequence follows n im in z nxm fx dx a z nxm dx x2 2a n 0 n . notice also proved r xn fx dx 0 n . point remark may replace exponent 2 deﬁnition moderate decrease 1 ϵ ϵ 0 is fx 1 x1ϵ x r. deﬁnition would work well purpose theory developed chapter. chose ϵ 1 merely matter conve nience. summarize elementary properties integration r proposition. proposition 1.1 integral function moderate decrease deﬁned 5 satisﬁes following properties
ibookroot october 20 2007 1. elementary theory fourier transform 133 i linearity f g mr a b c z afx bgx dx z fx dx b z gx dx. ii translation invariance every h r z fx h dx z fx dx. iii scaling dilations δ 0 δ z fδx dx z fx dx. iv continuity f mr z fx h fx dx 0 h 0. say words proof. property i immediate. verify property ii suﬃces see z n n fx h dx z n n fx dx 0 n . since r n n fx h dx r nh nh fx dx diﬀerence majorized z n nh fx dx z n nh fx dx a 1 n 2 large n tends 0 n tends inﬁnity. proof property iii similar observe δ r n n fδx dx r δn δn fx dx. prove property iv suﬃces take h 1. preassigned ϵ 0 ﬁrst choose n large z xn fx dx ϵ4 z xn fx h dx ϵ4. n ﬁxed use fact since f continuous uni formly continuous interval n 1 n 1. hence
ibookroot october 20 2007 134 chapter 5. fourier transform r supxn fx h fx 0 h tends 0. take h small supremum less ϵ4n. altogether then z fx h fx dx z n n fx h fx dx z xn fx h dx z xn fx dx ϵ2 ϵ4 ϵ4 ϵ thus conclusion iv follows. 1.2 deﬁnition fourier transform f mr deﬁne fourier transform ξ r ˆ fξ z fxe2πixξ dx. course e2πixξ 1 integrand moderate decrease integral makes sense. fact last observation implies ˆ f bounded moreover simple argument shows ˆ f continuous tends 0 ξ exercise 5. however nothing deﬁnition guarantees ˆ f moderate decrease speciﬁc decay. particular clear context make sense integral r ˆ fξe2πixξ dξ resulting fourier inversion formula. remedy this introduce reﬁned space functions considered schwartz useful establishing initial properties fourier transform. choice schwartz space motivated important prin ciple ties decay ˆ f continuity diﬀerentiability properties f and vice versa faster ˆ fξ decreases ξ smoother f must be. example reﬂects principle given exercise 3. also note relationship f ˆ f reminiscent similar one smoothness function circle decay fourier coeﬃcients see discussion corollary 2.4 chapter 2. 1.3 schwartz space schwartz space r consists set indeﬁnitely diﬀer entiable functions f f derivatives f f . . . f ℓ . . .
ibookroot october 20 2007 1. elementary theory fourier transform 135 rapidly decreasing sense sup xr xkf ℓx every k ℓ0. denote space sr again reader verify sr vector space c. moreover f sr f x f dx sr xfx sr. expresses important fact schwartz space closed diﬀerentiation multiplication polynomials. simple example function sr gaussian deﬁned fx ex2 plays central role theory fourier transform well ﬁelds for example probability theory physics. reader check derivatives f form pxex2 p polynomial immediately shows f sr. fact eax2 belongs sr whenever 0. later normalize gaussian choosing π. 1 1 1 0 figure 1. gaussian eπx2 important class examples sr bump func tions vanish outside bounded intervals exercise 4. ﬁnal remark note although ex decreases rapidly inﬁnity diﬀerentiable 0 therefore belong sr.
ibookroot october 20 2007 136 chapter 5. fourier transform r 1.4 fourier transform fourier transform function f sr deﬁned ˆ fξ z fxe2πixξ dx. simple properties fourier transform gathered fol lowing proposition. use notation fx ˆ fξ mean ˆ f denotes fourier transform f. proposition 1.2 f sr then i fx h ˆ fξe2πihξ whenever h r. ii fxe2πixh ˆ fξ h whenever h r. iii fδx δ1 ˆ fδ1ξ whenever δ 0. iv f x 2πiξ ˆ fξ. v 2πixfx d dξ ˆ fξ. particular except factors 2πi fourier transform inter changes diﬀerentiation multiplication x. key property makes fourier transform central object theory diﬀer ential equations. shall return point later. proof. property i immediate consequence translation invariance integral property ii follows deﬁnition. also third property proposition 1.1 establishes iii. integrating parts gives z n n f xe2πixξ dx h fxe2πixξin n 2πiξ z n n fxe2πixξ dx letting n tend inﬁnity gives iv. finally prove property v must show ˆ f diﬀerentiable ﬁnd derivative. let ϵ 0 consider ˆ fξ h ˆ fξ h 2πixfξ z fxe2πixξ e2πixh 1 h 2πix dx.
ibookroot october 20 2007 1. elementary theory fourier transform 137 since fx xfx rapid decrease exists integer n r xn fx dx ϵ r xn x fx dx ϵ. moreover x n exists h0 h h0 implies e2πixh 1 h 2πix ϵ n . hence h h0 ˆ fξ h ˆ fξ h 2πixfξ z n n fxe2πixξ e2πixh 1 h 2πix dx cϵ cϵ. theorem 1.3 f sr ˆ f sr. proof easy application fact fourier transform interchanges diﬀerentiation multiplication. fact note f sr fourier transform ˆ f bounded also pair nonnegative integers ℓand k expression ξk µ dξ ℓ ˆ fξ bounded since last proposition fourier transform 1 2πik µ dx k 2πixℓfx. proof inversion formula fx z ˆ fξe2πixξ dξ f sr give next section based careful study function eax2 which already observed sr 0.
ibookroot october 20 2007 138 chapter 5. fourier transform r gaussians good kernels begin considering case π normalization 6 z eπx2 dx 1. see 6 true use multiplicative property expo nential reduce calculation twodimensional integral. precisely argue follows µz eπx2 dx 2 z z eπx2y2 dx dy z 2π 0 z 0 eπr2r dr dθ z 0 2πreπr2 dr h eπr2i 0 1 evaluated twodimensional integral using polar coor dinates. fundamental property gaussian interest us actually follows 6 eπx2 equals fourier transform isolate important result theorem. theorem 1.4 fx eπx2 ˆ fξ fξ. proof. deﬁne fξ ˆ fξ z eπx2e2πixξ dx observe f0 1 previous calculation. property v proposition 1.2 fact f x 2πxfx obtain f ξ z fx2πixe2πixξ dx z f xe2πixξ dx. iv proposition ﬁnd f ξ i2πiξ ˆ fξ 2πξfξ.
ibookroot october 20 2007 1. elementary theory fourier transform 139 deﬁne gξ fξeπξ2 seen above follows gξ 0 hence g constant. since f0 1 conclude g identically equal 1 therefore fξ eπξ2 shown. scaling properties fourier transform dilations yield following important transformation law follows iii proposition 1.2 with δ replaced δ12. corollary 1.5 δ 0 kδx δ12eπx2δ c kδξ eπδξ2. pause make important observation. δ tends 0 function kδ peaks origin fourier transform c kδ gets ﬂatter. particular example see kδ c kδ cannot localized that is concentrated origin. example general phenomenon called heisenberg uncertainty principle discuss end chapter. constructed family good kernels real line analogous circle considered chapter 2. indeed kδx δ12eπx2δ have i r kδx dx 1. ii r kδx dx m. iii every η 0 r xη kδx dx 0 δ 0. prove i may change variables use 6 note integral equals c kδ0 1 corollary 1.5. since kδ 0 clear property ii also true. finally change variables get z xη kδx dx z yηδ12 eπy2 dy 0 δ tends 0. thus proved following result. theorem 1.6 collection kδδ0 family good kernels δ 0. next apply good kernels via operation convolution given follows. f g sr convolution deﬁned 7 f gx z fx tgt dt.
ibookroot october 20 2007 140 chapter 5. fourier transform r ﬁxed value x function fx tgt rapid decrease t hence integral converges. argument section 4 chapter 2 with slight modiﬁcation get following corollary. corollary 1.7 f sr f kδx fx uniformly x δ 0. proof. first claim f uniformly continuous r. indeed given ϵ 0 exists r 0 fx ϵ4 whenever x r. moreover f continuous hence uniformly continuous compact interval r r together previous observation ﬁnd η 0 fx fy ϵ whenever x y η. argue usual. using ﬁrst property good kernels write f kδx fx z kδt fx t fx dt since kδ 0 ﬁnd f kδx fx z tη z tη kδt fx t fx dt. ﬁrst integral small third property good kernels fact f bounded second integral also small since f uniformly continuous r kδ 1. concludes proof corollary. 1.5 fourier inversion next result identity sometimes called multiplication for mula. proposition 1.8 f g sr z fxˆ gx dx z ˆ fygy dy. prove proposition need digress brieﬂy discuss inter change order integration double integrals. suppose fx y continuous function plane x y r2. assume following decay condition f fx y a1 x21 y2.
ibookroot october 20 2007 1. elementary theory fourier transform 141 then state x function fx y moderate decrease y similarly ﬁxed function fx y moderate decrease x. moreover function f1x r fx y dy continuous moderate decrease similarly function f2y r fx y dx. finally z f1x dx z f2y dy. proof facts may found appendix. apply fx y fxgye2πixy. f1x fxˆ gx f2y ˆ fygy z fxˆ gx dx z ˆ fygy dy assertion proposition. multiplication formula fact gaussian fourier transform lead proof ﬁrst major theorem. theorem 1.9 fourier inversion f sr fx z ˆ fξe2πixξ dξ. proof. ﬁrst claim f0 z ˆ fξ dξ. let gδx eπδx2 c gδξ kδξ. multiplication for mula get z fxkδx dx z ˆ fξgδξ dξ. since kδ good kernel ﬁrst integral goes f0 δ tends 0. since second integral clearly converges r ˆ fξ dξ δ tends 0 claim proved. general let fy fy x fx f0 z ˆ fξ dξ z ˆ fξe2πixξ dξ. name theorem 1.9 suggests provides formula inverts fourier transform fact see fourier transform
ibookroot october 20 2007 142 chapter 5. fourier transform r inverse except change x x. precisely may deﬁne two mappings f sr sr f sr sr ffξ z fxe2πixξ dx fgx z gξe2πixξ dξ. thus f fourier transform theorem 1.9 guarantees ff sr identity mapping. moreover since deﬁnitions f fdiﬀer sign exponential see ffy ffy also f f i. conse quence conclude fis inverse fourier transform sr get following result. corollary 1.10 fourier transform bijective mapping schwartz space. 1.6 plancherel formula need results convolutions schwartz functions. key fact fourier transform interchanges convolutions pointwise products result analogous situation fourier series. proposition 1.11 f g sr then i f g sr. ii f g g f. iii f gξ ˆ fξˆ gξ. proof. prove f g rapidly decreasing observe ﬁrst ℓ0 supx xℓgx y aℓ1 yℓ g rapidly decreasing to check assertion consider separately two cases x 2y x 2y. this see sup x xℓf gx aℓ z fy1 yℓdy xℓf gx bounded function every ℓ0. esti mates carry derivatives f g thereby proving f g sr µ dx k f gx f µ dx k gx k 1 2 . . .
ibookroot october 20 2007 1. elementary theory fourier transform 143 identity proved ﬁrst k 1 diﬀerentiating inte gral deﬁning f g. interchange diﬀerentiation integration justiﬁed case rapid decrease dgdx. identity follows every k iteration. ﬁxed x change variables x y u shows f gx z fx ugu du g fx. change variables composition two changes 7y 7y h with h x. ﬁrst one use observation r fx dx r fx dx schwartz function f second apply ii proposition 1.1 finally consider fx y fygx ye2πixξ. since f g rapidly decreasing considering separately two cases x 2y x 2y see discussion change order integration proposition 1.8 applies f. case f1x f gxe2πixξ f2y fye2πiyξˆ gξ. thus r f1x dx r f2y dy implies iii. proposition therefore proved. use properties convolutions schwartz functions prove main result section. result mind analogue functions r parsevals identity fourier series. schwartz space equipped hermitian inner product f g z fxgx dx whose associated norm f µz fx2 dx 12 . second major theorem theory states fourier transform unitary transformation sr. theorem 1.12 plancherel f sr ˆ f f. proof. f sr deﬁne f x fx. c f ξ ˆ fξ. let h f f . clearly ˆ hξ ˆ fξ2 h0 z fx2 dx.
ibookroot october 20 2007 144 chapter 5. fourier transform r theorem follows inversion formula applied x 0 is z ˆ hξ dξ h0. 1.7 extension functions moderate decrease previous sections limited assertion fourier inversion plancherel formulas case function involved belonged schwartz space. really involve ideas extend results functions moderate decrease make additional assumption fourier transform function consideration also moderate decrease. indeed key observation easy prove convolution f g two functions f g moderate decrease function moderate decrease exer cise 7 also f g ˆ fˆ g. moreover multiplication formula continues hold deduce fourier inversion plancherel formulas f ˆ f moderate decrease. generalization although modest scope nevertheless useful circumstances. 1.8 weierstrass approximation theorem digress brieﬂy exploiting good kernels prove weierstrass approximation theorem. result already alluded chapter 2. theorem 1.13 let f continuous function closed bounded interval a b r. then ϵ 0 exists polynomial p sup xab fx px ϵ. words f uniformly approximated polynomials. proof. let m m denote interval contains a b interior let g continuous function r equals 0 outside m m equals f a b. example extend f follows b deﬁne g straight line segment going fb 0 m straight line segment fa also 0. let b
ibookroot october 20 2007 2. applications partial differential equations 145 bound g is gx b x. then since kδ family good kernels g continuous compact support may argue proof corollary 1.7 see g kδ converges uniformly g δ tends 0. fact choose δ0 gx g kδ0x ϵ2 x r. now recall ex given power series expansion ex p n0 xnn converges uniformly every compact interval r. therefore exists integer n kδ0x rx ϵ 4mb x 2m 2m rx δ12 0 pn n0 πx2δ0n n . then recalling g vanishes outside interval m m x m m g kδ0x g rx z m gt kδ0x t rx t dt z m gt kδ0x t rx t dt 2mb sup z2m2m kδ0z rz ϵ2. therefore triangle inequality implies gx g rx ϵ whenever x m m hence fx g rx ϵ x a b. finally note g r polynomial x variable. indeed deﬁnition g rx r m gtrx t dt rx t polynomial x since expressed several expansions rx t p n antxn sum ﬁnite. concludes proof theorem. 2 applications partial diﬀerential equations mentioned earlier crucial property fourier transform interchanges diﬀerentiation multiplication polynomials. use crucial fact together fourier inversion theorem solve speciﬁc partial diﬀerential equations. 2.1 timedependent heat equation real line chapter 4 considered heat equation circle. study analogous problem real line.
ibookroot october 20 2007 146 chapter 5. fourier transform r consider inﬁnite rod model real line suppose given initial temperature distribution fx rod time 0. wish determine temperature ux t point x time 0. considerations similar ones given chapter 1 show u appropriately normalized solves following partial diﬀerential equation 8 u t 2u x2 called heat equation. initial condition impose ux 0 fx. case circle solution given terms convolution. indeed deﬁne heat kernel line htx kδx δ 4πt htx 1 4πt12 ex24t ˆ htξ e4π2tξ2. taking fourier transform equation 8 x variable for mally leads ˆ u t ξ t 4π2ξ2ˆ uξ t. fixing ξ ordinary diﬀerential equation variable with unknown ˆ uξ exists constant aξ ˆ uξ t aξe4π2ξ2t. may also take fourier transform initial condition obtain ˆ uξ 0 ˆ fξ hence aξ ˆ fξ. leads following theorem. theorem 2.1 given f sr let ux t f htx 0 ht heat kernel. then i function u c2 x r 0 u solves heat equation.
ibookroot october 20 2007 2. applications partial differential equations 147 ii ux t fx uniformly x 0. hence set ux 0 fx u continuous closure upper halfplane r2 x t x r 0. iii r ux t fx2 dx 0 0. proof. u f ht taking fourier transform x variable gives ˆ u ˆ f ˆ ht ˆ uξ t ˆ fξe4π2ξ2t. fourier inver sion formula gives ux t z ˆ fξe4π2tξ2e2πiξx dξ. diﬀerentiating integral sign one veriﬁes i. fact one observes u indeﬁnitely diﬀerentiable. note ii imme diate consequence corollary 1.7. finally plancherels formula z ux t fx2 dx z ˆ uξ t ˆ fξ2 dξ z ˆ fξ2 e4π2tξ2 1 dξ. see last integral goes 0 0 argue follows since e4π2tξ2 1 2 f sr ﬁnd n z ξn ˆ fξ2e4π2tξ2 1 dξ ϵ small supξn ˆ fξ2e4π2tξ2 1 ϵ2n since ˆ f bounded. thus z ξn ˆ fξ2 e4π2tξ2 1 dξ ϵ small t. completes proof theorem. theorem guarantees existence solution heat equation initial data f. solution also unique uniqueness formulated appropriately. regard note u f ht f sr satisﬁes following additional property. corollary 2.2 u t belongs sr uniformly t sense 0 9 sup x r 0 xk ℓ xℓux t k ℓ0.
ibookroot october 20 2007 148 chapter 5. fourier transform r proof. result consequence following estimate ux t z yx2 fx yhty dy z yx2 fx yhty dy cn 1 xn c tecx2t. indeed since f rapidly decreasing fx y cn1 xn y x2. also y x2 hty ct12ecx2t obtain inequality. consequently see ux t rapidly decreasing uniformly 0 t. argument applied derivatives u x variable since may diﬀerentiate integral sign apply estimate f replaced f on. leads following uniqueness theorem. theorem 2.3 suppose ux t satisﬁes following conditions i u continuous closure upper halfplane. ii u satisﬁes heat equation 0. iii u satisﬁes boundary condition ux 0 0. iv u t sr uniformly t 9. then conclude u 0. use abbreviations ℓ xu tu denote ℓuxℓand ut respectively. proof. deﬁne energy time solution ux t et z r ux t2 dx. clearly et 0. since e0 0 suﬃces show e de creasing function achieved proving dedt 0. assumptions u allow us diﬀerentiate et integral sign de dt z r tux tux t ux ttux t dx. u satisﬁes heat equation therefore tu 2 xu tu 2 xu integration parts use fact u
ibookroot october 20 2007 2. applications partial differential equations 149 x derivatives decrease rapidly x ﬁnd de dt z r 2 xux tux t ux t2 xux t dx z r xux txux t xux txux t dx 2 z r xux t2 dx 0 claimed. thus et 0 t hence u 0. another uniqueness theorem heat equation less restric tive assumption 9 found problem 6. examples uniqueness fails given exercise 12 problem 4. 2.2 steadystate heat equation upper halfplane equation concerned 10 u 2u x2 2u y2 0 upper halfplane r2 x y x r 0. boundary con dition require ux 0 fx. operator is laplacian partial diﬀerential equation describes steadystate heat dis tribution r2 subject u f boundary. kernel solves problem called poisson kernel upper halfplane given pyx 1 π x2 y2 x r 0. analogue poisson kernel disc discussed sec tion 5.4 chapter 2. note ﬁxed kernel py moderate decrease function x use theory fourier transform appropriate types functions see section 1.7. proceed case timedependent heat equation taking fourier transform equation 10 formally x variable thereby obtaining 4π2ξ2ˆ uξ y 2ˆ u y2 ξ y 0
ibookroot october 20 2007 150 chapter 5. fourier transform r boundary condition ˆ uξ 0 ˆ fξ. general solution ordinary diﬀerential equation with ξ ﬁxed takes form ˆ uξ y aξe2πξy bξe2πξy. disregard second term rapid exponential increase ﬁnd setting 0 ˆ uξ y ˆ fξe2πξy. therefore u given terms convolution f kernel whose fourier transform e2πξy. precisely poisson kernel given above prove next. lemma 2.4 following two identities hold z e2πξye2πiξx dξ pyx z pyxe2πixξ dx e2πξy. proof. ﬁrst formula fairly straightforward since split integral to 0 0 . then since 0 z 0 e2πξye2πiξx dξ z 0 e2πixiyξ dξ e2πixiyξ 2πix iy 0 1 2πix iy similarly z 0 e2πξye2πiξx dξ 1 2πix iy. therefore z e2πξye2πiξx dξ 1 2πix iy 1 2πix iy πx2 y2. second formula consequence fourier inversion theorem applied case f ˆ f moderate decrease. lemma 2.5 poisson kernel good kernel r 0.
ibookroot october 20 2007 2. applications partial differential equations 151 proof. setting ξ 0 second formula lemma shows r pyx dx 1 clearly pyx 0 remains check last property good kernels. given ﬁxed δ 0 may change variables u xy z δ x2 y2 dx z δy du 1 u2 arctan u δy π2 arctanδy quantity goes 0 0. since pyx even function proof complete. following theorem establishes existence solution problem. theorem 2.6 given f sr let ux y f pyx. then i ux y c2 r2 u 0. ii ux y fx uniformly 0. iii r ux y fx2 dx 0 0. iv ux 0 fx u continuous closure r2 upper halfplane vanishes inﬁnity sense ux y 0 x . proof. proofs parts i ii iii similar case heat equation left reader. part iv consequence two easy estimates whenever f moderate decrease. first f pyx c µ 1 1 x2 x2 y2 proved as case heat equation splitting integral r fx tpyt dt part t x2 part t x2. also f pyx cy since supx pyx cy. using ﬁrst estimate x y second x y gives desired decrease inﬁnity. next show solution essentially unique. theorem 2.7 suppose u continuous closure upper half plane r2 satisﬁes u 0 x y r2 ux 0 0 ux y van ishes inﬁnity. u 0.
ibookroot october 20 2007 152 chapter 5. fourier transform r simple example shows condition concerning decay u inﬁnity needed take ux y y. clearly u satisﬁes steadystate heat equation vanishes real line yet u identically zero. proof theorem relies basic fact harmonic func tions functions satisfying u 0. fact value harmonic function point equals average value around circle centered point. lemma 2.8 meanvalue property suppose ωis open set r2 let u function class c2 u 0 ω. closure disc centered x y radius r contained ω ux y 1 2π z 2π 0 ux r cos θ r sin θ dθ 0 r r. proof. let ur θ ux r cos θ r sin θ. expressing lapla cian polar coordinates equation u 0 implies 0 2u θ2 r r µ ru r . deﬁne fr 1 2π r 2π 0 ur θ dθ gives r r µ rf r 1 2π z 2π 0 2u θ2 r θ dθ. integral 2uθ2 circle vanishes since uθ peri odic hence r r r f r 0 consequently rfr must constant. evaluating expression r 0 ﬁnd fr 0. thus f constant since f0 ux y ﬁnally ﬁnd fr ux y 0 r r meanvalue property. finally note argument implicit proof the orem 5.7 chapter 2. prove theorem 2.7 argue contradiction. considering sepa rately real imaginary parts u may suppose u realvalued somewhere strictly positive say ux0 y0 0 x0 r y0 0. shall see leads contradiction. first since u vanishes inﬁnity ﬁnd large semidisc ra dius r d r x y x2 y2 r 0 outside ux y 1 2ux0 y0. next since u continuous d r attains maximum there exists point x1 y1 d r ux1 y1 m
ibookroot october 20 2007 3. poisson summation formula 153 ux y m semidisc also since ux y 1 2ux0 y0 m2 out side semidisc ux y m throughout entire upper halfplane. meanvalue property harmonic functions implies ux1 y1 1 2π z 2π 0 ux1 ρ cos θ y1 ρ sin θ dθ whenever circle integration lies upper halfplane. par ticular equation holds 0 ρ y1. since ux1 y1 equals maximum value m ux1 ρ cos θ y1 ρ sin θ m follows continuity ux1 ρ cos θ y1 ρ sin θ whole circle. otherwise ux y m ϵ arc length δ 0 circle would give 1 2π z 2π 0 ux1 ρ cos θ y1 ρ sin θ dθ m ϵδ 2π m contradicting fact ux1 y1 m. letting ρ y1 using continuity u again see implies ux1 0 0 contradicts fact ux 0 0 x. 3 poisson summation formula deﬁnition fourier transform motivated desire continuous version fourier series applicable functions deﬁned real line. show exists remarkable connection analysis functions circle related functions r. given function f sr real line construct new function circle recipe f1x x n fx n. since f rapidly decreasing series converges absolutely uni formly every compact subset r f1 continuous. note f1x 1 f1x passage n n 1 sum merely shifts terms series deﬁning f1x. hence f1 periodic period 1. function f1 called periodization f. another way arrive periodic version f time fourier analysis. start identity fx z ˆ fξe2πiξx dξ
ibookroot october 20 2007 154 chapter 5. fourier transform r consider discrete analogue integral replaced sum f2x x n ˆ fne2πinx. again sum converges absolutely uniformly since ˆ f belongs schwartz space hence f2 continuous. moreover f2 also periodic period 1 since case one exponentials e2πinx. fundamental fact two approaches produce f1 f2 actually lead function. theorem 3.1 poisson summation formula f sr x n fx n x n ˆ fne2πinx. particular setting x 0 x n fn x n ˆ fn. words fourier coeﬃcients periodization f given precisely values fourier transform f integers. proof. check ﬁrst formula suﬃces theorem 2.1 chapter 2 show sides which continuous fourier coeﬃcients viewed functions circle. clearly mth fourier coeﬃcient righthand side ˆ fm. lefthand side z 1 0 ã x n fx n e2πimx dx x n z 1 0 fx ne2πimx dx x n z n1 n fye2πimy dy z fye2πimy dy ˆ fm interchange sum integral permissible since f rapidly decreasing. completes proof theorem.
ibookroot october 20 2007 3. poisson summation formula 155 observe theorem extends case merely assume f ˆ f moderate decrease proof fact unchanged. turns operation periodization important number questions even poisson summation formula apply. give example considering elementary function fx 1x x 0. result p n1x n summed symmetrically gives partial fraction decomposition cotangent function. fact sum equals π cot πx x integer. similarly fx 1x2 get p n1x n2 π2sin πx2 whenever x z see exercise 15. 3.1 theta zeta functions deﬁne theta function ϑs 0 ϑs x n eπn2s. condition ensures absolute convergence series. crucial fact special function satisﬁes following functional equation. theorem 3.2 s12ϑ1s ϑs whenever 0. proof identity consists simple application poisson summation formula pair fx eπsx2 ˆ fξ s12eπξ2s. theta function ϑs also extends complex values res 0 functional equation still valid then. theta function intimately connected important function number theory zeta function ζs deﬁned res 1 ζs x n1 1 ns . later see function carries essential information prime numbers see chapter 8. also turns ζ ϑ another important function γ related following identity πs2γs2ζs 1 2 z 0 ts21ϑt 1 dt
ibookroot october 20 2007 156 chapter 5. fourier transform r valid 1 exercises 17 18. returning function ϑ deﬁne generalization θzτ given θzτ x n eiπn2τe2πinz whenever imτ 0 z c. taking z 0 τ get θzτ ϑs. 3.2 heat kernels another application related poisson summation formula theta function timedependent heat equation circle. solution equation u t 2u x2 subject ux 0 fx f periodic period 1 given previous chapter ux t f htx htx heat kernel circle is htx x n e4π2n2te2πinx. note particular deﬁnition generalized theta func tion previous section θx4πit htx. also recall heat equation r gave rise heat kernel htx 1 4πt12 ex24t ˆ htξ e4π2ξ2t. fundamental relation two objects immediate consequence poisson summation formula theorem 3.3 heat kernel circle periodization heat kernel real line htx x n htx n.
ibookroot october 20 2007 3. poisson summation formula 157 although proof ht good kernel r fairly straightfor ward left open harder problem ht good kernel circle. results allow us resolve matter. corollary 3.4 kernel htx good kernel 0. proof. already observed r x12 htx dx 1. note ht 0 immediate formula since ht 0. finally claim x 12 htx htx etx error satisﬁes etx c1ec2t c1 c2 0 0 1. see this note formula theorem gives htx htx x n1 htx n therefore since x 12 etx 1 4πt x n1 exn24t ct12 x n1 ecn2t. note n2t n2 n2t 1t whenever 0 1 ecn2t ec 2 n2ec 2 1 . hence etx ct12ec 2 1 x n1 ec 2 n2 c1ec2t. proof claim complete result r x12 etx dx 0 0. clear ht satisﬁes z ηx12 htx dx 0 0 ht does. 3.3 poisson kernels similar manner discussion heat kernels state relation poisson kernels disc upper halfplane prθ 1 r2 1 2r cos θ r2 pyx 1 π y2 x2 .
ibookroot october 20 2007 158 chapter 5. fourier transform r theorem 3.5 pr2πx p nz pyx n r e2πy. immediate corollary poisson summation formula applied fx pyx ˆ fξ e2πξy. course use poisson summation formula assumptions f ˆ f moderate decrease. 4 heisenberg uncertainty principle mathematical thrust principle formulated terms relation function fourier transform. basic under lying law formulated vaguest general form states function fourier transform cannot essentially localized. somewhat precisely preponderance mass func tion concentrated interval length l preponderance mass fourier transform cannot lie interval length essentially smaller l1. exact statement follows. theorem 4.1 suppose ψ function sr satisﬁes nor malizing condition r ψx2 dx 1. µz x2ψx2 dx µz ξ2 ˆ ψξ2 dξ 1 16π2 equality holds ψx aebx2 b 0 a2 p 2bπ. fact µz x x02ψx2 dx µz ξ ξ02 ˆ ψξ2 dξ 1 16π2 every x0 ξ0 r. proof. second inequality actually follows ﬁrst re placing ψx e2πixξ0ψx x0 changing variables. prove ﬁrst inequality argue follows. beginning normalizing as sumption r ψ2 1 recalling ψ ψ rapidly decreasing integration parts gives 1 z ψx2 dx z x dxψx2 dx z ³ xψxψx xψxψx dx.
ibookroot october 20 2007 4. heisenberg uncertainty principle 159 last identity follows ψ2 ψψ. therefore 1 2 z x ψx ψx dx 2 µz x2ψx2 dx 12 µz ψx2 dx 12 used cauchyschwarz inequality. identity z ψx2 dx 4π2 z ξ2 ˆ ψξ2 dξ holds properties fourier transform plancherel formula concludes proof inequality theorem. equality holds must also equality applied cauchyschwarz inequality result ﬁnd ψx βxψx constant β. solutions equation ψx aeβx22 constant. since want ψ schwartz function must take β 2b 0 since impose condition r ψx2 dx 1 ﬁnd a2 p 2bπ shown. precise assertion contained theorem 4.1 ﬁrst came light study quantum mechanics. arose one considered extent one could simultaneously locate position momentum particle. assuming dealing say electron travels along real line according laws physics matters governed state function ψ assume sr normalized according requirement 11 z ψx2 dx 1. position particle determined deﬁnite point x instead probable location given rules quantum mechanics follows . probability particle located interval a b r b ψx2 dx. according law calculate probable location particle aid ψ fact may small probability particle located given interval a b nevertheless somewhere real line since r ψx2 dx 1.
ibookroot october 20 2007 160 chapter 5. fourier transform r addition probability density ψx2dx ex pectation particle might be. expectation best guess position particle given probability distribution determined ψx2dx quantity deﬁned 12 x z xψx2 dx. best guess consider simpler idealized situation given particle found ﬁnitely many diﬀerent points x1 x2 . . . xn real axis pi probability particle xi p1 p2 pn 1. then knew nothing else forced make one choice position particle would naturally take x pn i1 xipi appropriate weighted average possible positions. quantity 12 clearly general integral version this. next come notion variance terminology uncertainty attached expectation. determined expected position particle x given 12 resulting uncertainty quantity 13 z x x2ψx2 dx. notice ψ highly concentrated near x means high probability x near x 13 small contribution integral takes place values x near x. small uncertainty. hand ψx rather ﬂat that is probability distribution ψx2dx concentrated integral 13 rather big large values x x2 come play result uncertainty relatively large. also worthwhile observe expectation x choice uncertainty r x x2ψx2 dx smallest. indeed try minimize quantity equating 0 derivative respect x ﬁnd 2 r x xψx2 dx 0 gives 12. far discussed expectation uncertainty related position particle. equal relevance corresponding notions regarding momentum. corresponding rule quantum mechanics is . probability momentum ξ particle belongs interval a b r b ˆ ψξ2 dξ ˆ ψ fourier transform ψ.
ibookroot october 20 2007 5. exercises 161 combining two laws theorem 4.1 gives 116π2 lower bound product uncertainty position uncer tainty momentum particle. certain location particle less certain mo mentum vice versa. however simpliﬁed statement two laws rescaling change units measurement. actually enters fundamental small physical number ℏcalled plancks constant. properly taken account physical conclusion uncertainty positionuncertainty momentum ℏ16π2. 5 exercises 1. corollary 2.3 chapter 2 leads following simpliﬁed version fourier inversion formula. suppose f continuous function supported interval m m whose fourier transform ˆ f moderate decrease. a fix l l2 m show fx p anle2πinxl anl 1 l z l2 l2 fxe2πinxl dx 1 l ˆ fnl. alternatively may write fx δ p nˆ fnδe2πinδx δ 1l. b prove f continuous moderate decrease z fξ dξ lim δ 0 δ 0 δ x n fδn. c conclude fx z ˆ fξe2πixξ dξ. hint a note fourier series f l2 l2 converges ab solutely. b ﬁrst approximate integral r n n f sum δ p nnδ fnδ. approximate second integral riemann sums. 2. let f g functions deﬁned fx χ11x ½ 1 x 1 0 otherwise gx ½ 1 x x 1 0 otherwise. although f continuous integral deﬁning fourier transform still makes sense. show ˆ fξ sin 2πξ πξ ˆ gξ µsin πξ πξ 2
ibookroot october 20 2007 162 chapter 5. fourier transform r understanding ˆ f0 2 ˆ g0 1. 3. following exercise illustrates principle decay ˆ f related continuity properties f. a suppose f function moderate decrease r whose fourier transform ˆ f continuous satisﬁes ˆ fξ µ 1 ξ1α ξ 0 α 1. prove f satisﬁes h older condition order α is fx h fx mhα 0 x h r. b let f continuous function r vanishes x 1 f0 0 equal 1 log1x x neighborhood origin. prove ˆ f moderate decrease. fact ϵ 0 ˆ fξ o1ξ1ϵ ξ . hint part a use fourier inversion formula express fx h fx integral involving ˆ f estimate integral separately ξ two ranges ξ 1h ξ 1h. 4. bump functions. examples compactly supported functions sr handy many applications analysis. examples are a suppose b f function fx 0 x a x b fx e1xae1bx x b. show f indeﬁnitely diﬀerentiable r. b prove exists indeﬁnitely diﬀerentiable function f r fx 0 x a fx 1 x b f strictly increasing a b. c let δ 0 small δ b δ. show exists indef initely diﬀerentiable function g g 0 x a x b g 1 a δ b δ g strictly monotonic a δ b δ b. hint b consider fx c r x ft dt c appropriate constant. 5. suppose f continuous moderate decrease. a prove ˆ f continuous ˆ fξ 0 ξ .
ibookroot october 20 2007 5. exercises 163 b show ˆ fξ 0 ξ f identically 0. hint part a show ˆ fξ 1 2 r fx fx 12ξe2πixξ dx. part b verify multiplication formula r fxˆ gx dx r ˆ fygy dy still holds whenever g sr. 6. function eπx2 fourier transform. generate functions up constant multiple fourier transforms. must constant multiples be decide this prove f4 i. ff ˆ f fourier transform f4 f f f f identity operator ifx fx see also problem 7. 7. prove convolution two functions moderate decrease function moderate decrease. hint write z fx ygy dy z yx2 z yx2 . ﬁrst integral fx y o11 x2 second integral gy o11 x2. 8. prove f continuous moderate decrease r fyey2e2xydy 0 x r f 0. hint consider f ex2. 9. f moderate decrease 14 z r r µ 1 ξ r ˆ fξe2πixξ dξ f frx fej er kernel real line deﬁned frt r µsin πtr πtr 2 0 r 0. show fr family good kernels r therefore 14 tends uniformly fx r . analogue fej ers theorem fourier series context fourier transform. 10. outline diﬀerent proof weierstrass approximation theorem.
ibookroot october 20 2007 164 chapter 5. fourier transform r deﬁne landau kernels lnx 1 x2n cn 1 x 1 0 x 1 cn chosen r lnx dx 1. prove lnn0 family good kernels n . result show f continuous func tion supported 12 12 f lnx sequence polynomials 12 12 converges uniformly f. hint first show cn 2n 1. 11. suppose u solution heat equation given u f ht f sr. also set ux 0 fx prove u continuous closure upper halfplane vanishes inﬁnity is ux t 0 x . hint prove u vanishes inﬁnity show i ux t c ii ux t c1 x2 ct12ecx2t. use i x t ii other wise. 12. show function deﬁned ux t x htx satisﬁes heat equation 0 limt0 ux t 0 every x u continuous origin. hint approach origin x t parabola x24t c c constant. 13. prove following uniqueness theorem harmonic functions strip x y 0 1 x u harmonic strip continuous closure ux 0 ux 1 0 x r u vanishes inﬁnity u 0. 14. prove periodization fej er kernel fn real line exer cise 9 equal fej er kernel periodic functions period 1. words x n fnx n fnx n 1 integer fnx n x nn µ 1 n n e2πinx 1 n sin2nπx sin2πx .
ibookroot october 20 2007 5. exercises 165 15. exercise provides another example periodization. a apply poisson summation formula function g exercise 2 obtain x n 1 n α2 π2 sin πα2 whenever α real equal integer. b prove consequence 15 x n 1 n α π tan πα whenever α real equal integer. hint first prove 0 α 1. so integrate formula b. precise meaning series lefthand side 15 evaluate α 12. 16. dirichlet kernel real line deﬁned z r r ˆ fξe2πixξ dξ f drx drx χrrx sin2πrx πx . also modiﬁed dirichlet kernel periodic functions period 1 deﬁned d nx x nn1 e2πinx 1 2e2πinx e2πinx. show result exercise 15 gives x n dnx n d nx n 1 integer inﬁnite series must summed symmetrically. words periodization dn modiﬁed dirichlet kernel d n. 17. gamma function deﬁned 0 γs z 0 exxs1 dx. a show 0 integral makes sense is following two limits exist lim δ 0 δ 0 z 1 δ exxs1 dx lim a z 1 exxs1 dx.
ibookroot october 20 2007 166 chapter 5. fourier transform r b prove γs 1 sγs whenever 0 conclude every integer n 1 γn 1 n. c show γ µ1 2 π γ µ3 2 π 2 . hint c use r eπx2 dx 1. 18. zeta function deﬁned 1 ζs p n1 1ns. verify identity πs2γs2ζs 1 2 z 0 2 1ϑt 1 dt whenever 1 γ ϑ gamma theta functions respectively γs z 0 etts1 dt ϑs x n eπn2s. zeta function relation prime number theorem found book ii. 19. following variant calculation ζ2m p n1 1n2m found problem 4 chapter 3. a apply poisson summation formula fx tπx2 t2 ˆ fξ e2πtξ 0 order get 1 π x n t2 n2 x n e2πtn. b prove following identity valid 0 1 1 π x n t2 n2 1 πt 2 π x m1 1m1ζ2mt2m1 well x n e2πtn 2 1 e2πt 1.
ibookroot october 20 2007 5. exercises 167 c use fact z ez 1 1 z 2 x m1 b2m 2mz2m bk bernoulli numbers deduce formula 2ζ2m 1m1 2π2m 2m b2m. 20. following results relevant information theory one tries recover signal samples. suppose f moderate decrease fourier transform ˆ f sup ported 12 12. then f entirely determined restriction z. means g another function moderate decrease whose fourier transform supported fn gn n z f g. precisely a prove following reconstruction formula holds fx x n fnkx n ky sin πy πy . note ky o1y y . b λ 1 fx x n 1 λf ³n λ kλ ³ x n λ kλy cos πy cos πλy π2λ 1y2 . thus one samples f more often series reconstruction formula converges faster since kλy o1y2 y . note kλy ky λ 1. c prove z fx2 dx x n fn2. hint part a show χ characteristic function i ˆ fξ χξ p nfne2πinξ. b use function figure 2 instead χξ. 21. suppose f continuous r. show f ˆ f cannot compactly supported unless f 0. viewed spirit uncertainty principle.
ibookroot october 20 2007 168 chapter 5. fourier transform r 1 2 1 2 λ 2 1 λ 2 figure 2. function exercise 20 hint assume f supported 0 12. expand f fourier series interval 0 1 note result f trigonometric polynomial. 22. heuristic assertion stated theorem 4.1 made precise follows. f function r say preponderance mass contained interval centered origin 16 z x2fx2 dx 1 2 z r x2fx2 dx. suppose f s 16 holds f f i1 also f ˆ f i2. lj denotes length ij l1l2 1 2π . similar conclusion holds intervals necessarily centered origin. 23. heisenberg uncertainty principle formulated terms operator l d2 dx2 x2 acts schwartz functions formula lf d2f dx2 x2f. operator sometimes called hermite operator quantum ana logue harmonic oscillator. consider usual inner product given f g z fxgx dx whenever f g s.
ibookroot october 20 2007 6. problems 169 a prove heisenberg uncertainty principle implies lf f f f f s. usually denoted l i. hint integrate parts. b consider operators adeﬁned af f dx xf af d f dx xf. operators aare sometimes called annihilation cre ation operators respectively. prove f g s i af g f ag ii af af aaf f 0 iii aa l i. particular shows l i. c r let atf f dx txf a f d f dx txf. use fact a atf f 0 give another proof heisenberg uncertainty principle says whenever r fx2 dx 1 µz x2fx2 dx ãz f dx 2 dx 14. hint think a atf f quadratic polynomial t. 6 problems 1. equation 17 x2 2u x2 axu x u t ux 0 fx 0 x and 0 variant heat equation occurs number applications. solve 17 make change vari ables x ey . set uy t uey t fy fey. problem reduces equation 2u y2 1 au y u t
ibookroot october 20 2007 170 chapter 5. fourier transform r uy 0 fy. solved like usual heat equation the case 1 taking fourier transform variable. one must compute integral r e4π2ξ21a2πiξte2πiξv dξ. show solution original problem given ux t 1 4πt12 z 0 elogvx1at24tfv dv v . 2. blackscholes equation ﬁnance theory 18 v t rsv s σ2s2 2 2v s2 rv 0 0 t subject ﬁnal boundary condition v s t fs. appropriate change variables reduces equation problem 1. alternatively substi tution v s t eaxbτux τ x log s τ σ2 2 t t 1 2 r σ2 b 1 2 r σ2 2 reduces 18 onedimensional heat equation ini tial condition ux 0 eaxfex. thus solution blackscholes equa tion v s t ert t p 2πσ2t t z 0 e logssrσ22t t2 2σ2t t fs ds. 3. the dirichlet problem strip. consider equation u 0 horizontal strip x y 0 1 x boundary conditions ux 0 f0x ux 1 f1x f0 f1 schwartz space. a show formally u solution problem ˆ uξ y aξe2πξy bξe2πξy. express b terms b f0 b f1 show ˆ uξ y sinh2π1 yξ sinh2πξ b f0ξ sinh2πyξ sinh2πξ b f0ξ. b prove result z ux y f0x2 dx 0 0 z ux y f1x2 dx 0 1.
ibookroot october 20 2007 6. problems 171 c φξ sinh 2πaξsinh 2πξ 0 a 1 φ fourier transform ϕ ϕx sin πa 2 1 cosh πx cos πa. shown instance using contour integration residue formula complex analysis see book ii chapter 3. d use result express u terms poissonlike integrals involving f0 f1 follows ux y sin πy 2 µz f0x t cosh πt cos πy dt z f1x t cosh πt cos πy dt . e finally one check function ux y deﬁned ex pression harmonic strip converges uniformly f0x 0 f1x 1. moreover one sees ux y vanishes inﬁnity is limxux y 0 uniformly y. exercise 12 gave example function satisﬁes heat equation upper halfplane boundary value 0 identically 0. observed case u fact continuous boundary. problem 4 exhibit examples illustrating nonuniqueness time continuity boundary 0. examples satisfy growth condition inﬁnity namely ux t cecx2ϵ ϵ 0. problems 5 6 show restrictive growth condition ux t cecx2 uniqueness hold. 4.if g smooth function r deﬁne formal power series 19 ux t x n0 gnt x2n 2n. a check formally u solves heat equation. b 0 consider function deﬁned gt ½ eta 0 0 0. one show exists 0 θ 1 depending gkt k θtk e1 2 ta 0. c result x series 19 converges u solves heat equation u vanishes 0 u satisﬁes estimate ux t cecx2aa1 constants c c 0.
ibookroot october 20 2007 172 chapter 5. fourier transform r d conclude every ϵ 0 exists nonzero solution heat equation continuous x r 0 satisﬁes ux 0 0 ux t cecx2ϵ. 5.the following maximum principle solutions heat equation used next problem. theorem. suppose ux t realvalued solution heat equation upper halfplane continuous closure. let r denote rectangle r x y r2 x b 0 t c r part boundary r consists two vertical sides base line 0 see figure 3. min xtr ux t min xtr ux t max xtr ux t max xtr ux t. b c r x r figure 3. rectangle r part boundary r steps leading proof result outlined below. a show suﬃces prove u 0 r u 0 r. b ϵ 0 let vx t ux t ϵt. then v minimum r say x1 t1. show x1 b else t1 0. so suppose contrary x1 b 0 t1 c prove vxxx1 t1 vtx1 t1 ϵ. however show also lefthand side must nonnegative. c deduce b ux t ϵt1 t x t r let ϵ 0. 6.the examples problem 4 optimal sense following unique ness theorem due tychonoﬀ.
ibookroot october 20 2007 6. problems 173 theorem. suppose ux t satisﬁes following conditions i ux t solves heat equation x r 0. ii ux t continuous x r 0 t c. iii ux 0 0. iv ux t meax2 m a x r 0 t c. u identically equal 0. 7.the hermite functions hkx deﬁned generating identity x k0 hkxtk k ex222txt2. a show alternate deﬁnition hermite functions given formula hkx 1kex22 µ dx k ex2. hint write ex222txt2 ex22ext2 use taylors formula. conclude expression hkx form pkxex22 pk polynomial degree k. particular her mite functions belong schwartz space h0x ex22 h1x 2xex22. b prove family hk k0 complete sense f schwartz function f hk z fxhkx dx 0 k 0 f 0. hint use exercise 8. c deﬁne h kx hk2π12x. c h kξ ikh kξ. therefore h k eigenfunction fourier transform. d show hk eigenfunction operator deﬁned exercise 23 fact prove lhk 2k 1hk. particular conclude functions hk mutually orthogonal l2 inner product schwartz space. e finally show r hkx2 dx π122kk. hint square generat ing relation.
ibookroot october 20 2007 174 chapter 5. fourier transform r 8.to reﬁne results chapter 4 prove fαx x n0 2nαe2πi2nx nowhere diﬀerentiable even case α 1 need consider variant delayed means n turn analyzed poisson summation formula. a fix indeﬁnitely diﬀerentiable function φ satisfying φξ ½ 1 ξ 1 0 ξ 2. fourier inversion formula exists ϕ s ˆ ϕξ φξ. let ϕnx nϕnx c ϕnξ φξn. finally set nx x n ϕnx n. observe poisson summation formula nx p nφnne2πinx thus n trigonometric polynomial degree 2n terms whose coeﬃcients 1 n n. let nf f n. note snfα nfα n largest integer form 2k n n. b set nx ϕnx enx enx x n1 ϕnx n one sees that i supx12 e nx 0 n . ii nx cn 2. iii nx cnx3 x 12. moreover r x12 nx dx 0 r x12 x nx dx 1 n . c estimates imply f x0 exists f nx0 hn f x0 n whenever hn cn. then conclude real imaginary parts f1 nowhere diﬀerentiable proof given section 3 chapter 4.
ibookroot october 20 2007 6 fourier transform rd occurred order improve treatment planning one know distribution at tenuation coeﬃcient tissues body. in formation would useful diagnostic purposes would constitute tomogram series tomograms. immediately evident problem mathematical one. ﬁne beam gamma rays intensity i0 incident body emerg ing density i measurable quantity g equals logi0i r l f ds f variable absorption coeﬃcient along line l. hence f function two dimensions g known lines intersect ing body question is f determined g known fourteen years would elapse learned radon solved problem 1917. a. m. cormack 1979 previous chapter introduced theory fourier transform r illustrated applications partial diﬀerential equa tions. here aim present analogous theory functions several variables. brief review relevant notions rd begin general facts fourier transform schwartz space srd. fortunately main ideas techniques already considered onedimensional case. fact appropriate notation statements and proofs key theorems fourier inversion plancherel formulas remain unchanged. next highlight connection higher dimensional prob lems mathematical physics particular investigate wave equation dimensions detailed analysis cases 3 2. stage discover rich interplay fourier transform rotational symmetry arises rd 2. finally chapter ends discussion radon transform. topic substantial interest right addition signiﬁcant relevance application use xray scans well
ibookroot october 20 2007 176 chapter 6. fourier transform rd parts mathematics. 1 preliminaries setting chapter rd vector space1 dtuples real numbers x1 . . . xd xi r. addition vectors component wise multiplication real scalars. given x x1 . . . xd rd deﬁne x x2 1 x2 d12 x simply length vector x usual euclidean norm. fact equip rd standard inner product deﬁned x x1y1 xdyd x2 x x. use notation x place x y chap ter 3. given dtuple α α1 . . . αd nonnegative integers sometimes called multiindex monomial xα deﬁned xα xα1 1 xα2 2 xαd . similarly deﬁne diﬀerential operator xα µ x α µ x1 α1 µ x2 α2 µ xd αd α xα1 1 xαd α α1 αd order multiindex α. 1.1 symmetries analysis rd particular theory fourier transform shaped three important groups symmetries underlying space i translations ii dilations iii rotations 1see chapter 3 brief review vector spaces inner products. ﬁnd convenient use lower case letters x as opposed x designate points rd. also use instead to denote euclidean norm.
ibookroot october 20 2007 1. preliminaries 177 seen translations x 7x h h rd ﬁxed dila tions x 7δx δ 0 play important role onedimensional theory. r two rotations identity multiplica tion 1. however rd 2 rotations understanding interaction fourier transform rotations leads fruitful insights regarding spherical symmetries. rotation rd linear transformation r rd rd pre serves inner product. words . rax by arx bry x rd a b r. . rx ry x x rd. equivalently last condition replaced rx x x rd rt r1 rt r1 denote transpose inverse r respectively.2 particular detr 1 detr determinant r. detr 1 say r proper rotation otherwise say r improper rotation. example 1. real line r two rotations identity proper multiplication 1 improper. example 2. rotations plane r2 described terms complex numbers. identify r2 c assigning point x y complex number z x iy. identiﬁcation proper rotations form z 7zeiϕ ϕ r improper rota tions form z 7zeiϕ ϕ r here z x iy denotes complex conjugate z. see exercise 1 argument leading result. example 3. euler gave following simple geometric description rotations r3. given proper rotation r exists unit vector γ that i r ﬁxes γ is rγ γ. ii p denotes plane passing origin perpendicular γ r p p restriction r p rotation r2. 2recall transpose linear operator rd rd linear operator b rd rd satisﬁes ax x by x rd. write b at. inverse when exists linear operator c rd rd c c a where identity write c a1.
ibookroot october 20 2007 178 chapter 6. fourier transform rd geometrically vector γ gives direction axis rotation. proof fact given exercise 2. finally r improper r proper since r3 detr detr r composition proper rotation symmetry respect origin. example 4. given two orthonormal bases e1 . . . ed e 1 . . . e d rd deﬁne rotation r letting rei e 1 . . . d. conversely r rotation e1 . . . ed orthonormal basis e 1 . . . e d e j rej another orthonormal basis. 1.2 integration rd since shall dealing functions rd need discuss aspects integration functions. detailed review integration rd given appendix. continuous complexvalued function f rd said rapidly decreasing every multiindex α function xαfx bounded. equivalently continuous function rapid decrease sup xrd xk fx every k 0 1 2 . . . given function rapid decrease deﬁne z rd fx dx lim n z qn fx dx qn denotes closed cube centered origin sides length n parallel coordinate axis is qn x rd xi n2 1 . . . d. integral qn multiple integral usual sense riemann integration. limit exists follows fact integrals r qn fx dx form cauchy sequence n tends inﬁnity. two observations order. first may replace square qn ball bn x rd x n without changing deﬁnition. second need full force rapid decrease show limit exists. fact suﬃces assume f continuous sup xrd xdϵ fx ϵ 0.
ibookroot october 20 2007 1. preliminaries 179 example functions moderate decrease r correspond ϵ 1. keeping deﬁne functions moderate decrease rd continuous satisfy inequality ϵ 1. interaction integration three important groups sym metries follows f moderate decrease i z rd fx h dx z rd fx dx h rd ii δd z rd fδx dx z rd fx dx δ 0 iii z rd frx dx z rd fx dx every rotation r. polar coordinates convenient introduce polar coordinates rd ﬁnd corresponding integration formula. begin two examples correspond case 2 3. a elaborate discussion applying contained appendix. example 1. r2 polar coordinates given r θ r 0 0 θ 2π. jacobian change variables equal r z r2 fx dx z 2π 0 z 0 fr cos θ r sin θ r dr dθ. may write point unit circle s1 γ cos θ sin θ given function g circle deﬁne integral s1 z s1 gγ dσγ z 2π 0 gcos θ sin θ dθ. notation z r2 fx dx z s1 z 0 frγ r dr dσγ. example 2. r3 one uses spherical coordinates given x1 r sin θ cos ϕ x2 r sin θ sin ϕ x3 r cos θ
ibookroot october 20 2007 180 chapter 6. fourier transform rd 0 r 0 θ π 0 ϕ 2π. jacobian change variables r2 sin θ z r3 fx dx z 2π 0 z π 0 z 0 fr sin θ cos ϕ r sin θ sin ϕ r cos θr2dr sin θ dθ dϕ. g function unit sphere s2 x r3 x 1 γ sin θ cos ϕ sin θ sin ϕ cos θ deﬁne surface element dσγ z s2 gγ dσγ z 2π 0 z π 0 gγ sin θ dθ dϕ. result z r3 fx dx z s2 z 0 frγ r2 dr dσγ. general possible write point rd 0 uniquely x rγ γ lies unit sphere sd1 rd r 0. indeed take r x γ xx. thus one may proceed cases 2 3 deﬁne spherical coordinates. formula shall use z rd fx dx z sd1 z 0 frγ rd1 dr dσγ whenever f moderate decrease. dσγ denotes surface element sphere sd1 obtained spherical coordinates. 2 elementary theory fourier transform schwartz space srd sometimes abbreviated s consists indeﬁnitely diﬀerentiable functions f rd sup xrd xα µ x β fx every multiindex α β. words f derivatives required rapidly decreasing.
ibookroot october 20 2007 2. elementary theory fourier transform 181 example 1. example function srd ddimensional gaussian given eπx2. theory chapter 5 already made clear central role played function case 1. fourier transform schwartz function f deﬁned ˆ fξ z rd fxe2πixξ dx ξ rd. note resemblance formula onedimension except integrating rd product x ξ replaced inner product two vectors. list simple properties fourier transform. next proposition arrow indicates taken fourier trans form fx gξ means gξ ˆ fξ. proposition 2.1 let f srd. i fx h ˆ fξe2πiξh whenever h rd. ii fxe2πixh ˆ fξ h whenever h rd. iii fδx δd ˆ fδ1ξ whenever δ 0. iv µ x α fx 2πiξα ˆ fξ. v 2πixαfx µ ξ α ˆ fξ. vi frx ˆ frξ whenever r rotation. ﬁrst ﬁve properties proved way one dimensional case. verify last property simply change variables rx integral. then recall detr 1 r1y ξ rξ r rotation. properties iv v proposition show that factors 2πi fourier transform interchanges diﬀerentiation multiplication monomials. motivates deﬁnition schwartz space leads next corollary. corollary 2.2 fourier transform maps srd itself. point disgress observe simple fact concerning in terplay fourier transform rotations. say
ibookroot october 20 2007 182 chapter 6. fourier transform rd function f radial depends x words f radial function f0u deﬁned u 0 fx f0x. note f radial frx fx every rotation r. one direction obvious since rx x. conversely suppose frx fx rotations r. deﬁne f0 f0u ½ f0 u 0 fx x u. note f0 well deﬁned since x x points x x always rotation r x rx. corollary 2.3 fourier transform radial function radial. follows property vi last proposition. indeed condition frx fx r implies ˆ frξ ˆ fξ r thus ˆ f radial whenever f is. example radial function rd gaussian eπx2. also observe 1 radial functions precisely even functions is fx fx. preliminaries retrace steps taken previous chapter obtain fourier inversion formula plancherel theorem rd. theorem 2.4 suppose f srd. fx z rd ˆ fξe2πixξ dξ. moreover z rd ˆ fξ2 dξ z rd fx2 dx. proof proceeds following stages. step 1. fourier transform eπx2 eπξ2. prove this notice properties exponential functions imply eπx2 eπx2 1 eπx2 e2πixξ e2πix1ξ1 e2πixdξd integrand fourier transform product functions depending variable xj 1 j d only. thus assertion
ibookroot october 20 2007 2. elementary theory fourier transform 183 follows writing integral rd series repeated integrals taken r. example 2 z r2 eπx2e2πixξ dx z r eπx2 2e2πix2ξ2 µz r eπx2 1e2πix1ξ1dx1 dx2 z r eπx2 2e2πix2ξ2eπξ2 1 dx2 eπξ2 1eπξ2 2 eπξ2. consequence proposition 2.1 applied δ12 instead δ ﬁnd eπδx2 δd2eπξ2δ. step 2. family kδx δd2eπx2δ family good kernels rd. mean i z rd kδx dx 1 ii z rd kδx dx m in fact kδx 0 iii every η 0 z xη kδx dx 0 δ 0. proofs assertions almost identical case 1. result z rd kδxfx dx f0 δ 0 f schwartz function generally f bounded continuous origin. step 3. multiplication formula z rd fxˆ gx dx z rd ˆ fygy dy holds whenever f g s. proof requires evaluation integral fxgye2πixy x y r2d rd rd repeated integral separate integration taken rd. justiﬁcation similar proof proposition 1.8 previous chapter. see appendix. fourier inversion simple consequence multiplica tion formula family good kernels kδ chapter 5. also
ibookroot october 20 2007 184 chapter 6. fourier transform rd follows fourier transform f bijective map srd itself whose inverse fgx z rd gξe2πixξ dξ. step 4. next turn convolution deﬁned f gx z rd fygx y dy f g s. f g srd f g g f f gξ ˆ fξˆ gξ. argument similar onedimension. calculation fourier transform f g involves integration fygx ye2πixξ over r2d rd rd expressed repeated integral. then following argument previous chapter obtain ddimensional plancherel formula thereby concluding proof theorem 2.4. 3 wave equation rd r next goal apply learned fourier trans form study wave equation. here simplify matters restricting functions schwartz class s. note analysis wave equation important allow functions much general behavior particular may discontinuous. however lose generality considering schwartz functions gain transparency. study restricted context allow us explain certain basic ideas simplest form. 3.1 solution terms fourier transforms motion vibrating string satisﬁes equation 2u x2 1 c2 2u t2 referred onedimensional wave equation. natural generalization equation space variables 1 2u x2 1 2u x2 1 c2 2u t2 . fact known case 3 equation determines behavior electromagnetic waves vacuum with c speed light.
ibookroot october 20 2007 3. wave equation rd r 185 also equation describes propagation sound waves. thus 1 called ddimensional wave equation. ﬁrst observation may assume c 1 since rescale variable necessary. also deﬁne laplacian dimen sions 2 x2 1 2 x2 wave equation rewritten 2 u 2u t2 . goal section ﬁnd solution equation subject initial conditions ux 0 fx u t x 0 gx f g srd. called cauchy problem wave equation. solving problem note think variable time restrict 0. see solution obtain makes sense r. manifestation fact wave equation reversed time unlike heat equation. formula solution problem given next theorem. heuristic argument leads formula important since already seen applies boundary value problems well. suppose u solves cauchy problem wave equation. technique employed consists taking fourier transform equa tion initial conditions respect space variables x1 . . . xd. reduces problem ordinary diﬀerential equation time variable. indeed recalling diﬀerentiation respect xj becomes multiplication 2πiξj diﬀerentiation respect commutes fourier transform space variables ﬁnd 2 becomes 4π2ξ2ˆ uξ t 2ˆ u t2 ξ t. ﬁxed ξ rd ordinary diﬀerential equation whose solution given ˆ uξ t aξ cos2πξt bξ sin2πξt
ibookroot october 20 2007 186 chapter 6. fourier transform rd ξ aξ bξ unknown constants determined initial conditions. fact taking fourier transform in x initial conditions yields ˆ uξ 0 ˆ fξ ˆ u t ξ 0 ˆ gξ. may solve aξ bξ obtain aξ ˆ fξ 2πξbξ ˆ gξ. therefore ﬁnd ˆ uξ t ˆ fξ cos2πξt ˆ gξsin2πξt 2πξ solution u given taking inverse fourier transform ξ variables. formal derivation leads precise existence theorem problem. theorem 3.1 solution cauchy problem wave equation 3 ux t z rd ˆ fξ cos2πξt ˆ gξsin2πξt 2πξ e2πixξ dξ. proof. ﬁrst verify u solves wave equation. straightforward note diﬀerentiate x un der integral sign because f g schwartz functions therefore u least c2. one hand diﬀerentiate expo nential respect x variables get ux t z rd ˆ fξ cos2πξt ˆ gξsin2πξt 2πξ 4π2ξ2e2πixξ dξ hand diﬀerentiate terms brackets re spect twice get 2u t2 x t z rd 4π2ξ2 ˆ fξ cos2πξt 4π2ξ2ˆ gξsin2πξt 2πξ e2πixξ dξ. shows u solves equation 2. setting 0 get ux 0 z rd ˆ fξe2πixξ dξ fx
ibookroot october 20 2007 3. wave equation rd r 187 fourier inversion theorem. finally diﬀerentiating re spect t setting 0 using fourier inversion shows u t x 0 gx. thus u also veriﬁes initial conditions proof theorem complete. reader note ˆ fξ cos2πξt ˆ gξ sin2πξt 2πξ functions s assuming f g s. be cause cos u sin uu even functions indeﬁnitely diﬀerentiable. proved existence solution cauchy problem wave equation raise question uniqueness. solutions problem u 2u t2 subject ux 0 fx u t x 0 gx one given formula theorem fact answer is expected no. proof fact given but see problem 3 based conservation energy argument. local counterpart global conservation energy statement present. observed exercise 10 chapter 3 onedimensional case total energy vibrating string conserved time. analogue fact holds higher dimensions well. deﬁne energy solution et z rd u t 2 u x1 2 u xd 2 dx. theorem 3.2 u solution wave equation given for mula 3 et conserved is et e0 r. proof requires following lemma. lemma 3.3 suppose b complex numbers α real. a cos α b sin α2 a sin α b cos α2 a2 b2.
ibookroot october 20 2007 188 chapter 6. fourier transform rd follows directly e1 cos α sin α e2 sin α cos α pair orthonormal vectors hence z a b c2 z2 z e12 z e22 represents inner product c2. plancherels theorem z rd u t 2 dx z rd 2πξ ˆ fξ sin2πξt ˆ gξ cos2πξt 2 dξ. similarly z rd x j1 u xj 2 dx z rd 2πξ ˆ fξ cos2πξt ˆ gξ sin2πξt 2 dξ. apply lemma 2πξ ˆ fξ b ˆ gξ α 2πξt. result et z rd u t 2 u x1 2 u xd 2 dx z rd4π2ξ2 ˆ fξ2 ˆ gξ2 dξ clearly independent t. thus theorem 3.2 proved. drawback formula 3 give solution wave equation quite indirect involving calculation fourier transforms f g inverse fourier trans form. however every dimension explicit formula. formula simple 1 little less 3. generally formula elementary whenever odd complicated even see problems 4 5. follows consider cases 1 3 2 together give picture general situation. recall chapter 1 discussing wave equation interval 0 l found solution given dalemberts formula 4 ux t fx t fx t 2 1 2 z xt xt gy dy.
ibookroot october 20 2007 3. wave equation rd r 189 interpretation f g extended outside 0 l making odd l l periodic real line period 2l. formula 4 holds solution wave equation 1 initial data functions sr. fact follows directly 3 note cos2πξt 1 2e2πiξt e2πiξt sin2πξt 2πξ 1 4πiξe2πiξt e2πiξt. finally note two terms appear dalemberts for mula 4 consist appropriate averages. indeed ﬁrst term pre cisely average f two points boundary interval x t x t second term is factor t mean value g interval is 12t r xt xt gy dy. suggests generalization higher dimensions might expect write solution problem averages initial data. fact case treat detail particular situation 3. 3.2 wave equation r3 r s2 denotes unit sphere r3 deﬁne spherical mean function f sphere radius centered x 5 mtfx 1 4π z s2 fx tγ dσγ dσγ element surface area s2. since 4π area unit sphere interpret mtf average value f sphere centered x radius t. lemma 3.4 f sr3 ﬁxed mtf sr3. moreover mtf indeﬁnitely diﬀerentiable t tderivative also belongs sr3. proof. let fx mtfx. show f rapidly decreasing start inequality fx an1 xn holds every ﬁxed n 0. simple consequence whenever ﬁxed fx γt a n1 xn γ s2.
ibookroot october 20 2007 190 chapter 6. fourier transform rd see consider separately cases x 2t x 2t. therefore integration fx a n1 xn since holds every n function f rapidly decreasing. one next observes f indeﬁnitely diﬀerentiable 6 µ x α fx mtf αx f αx xαf. suﬃces prove xα xk proceed induction get general case. further more enough take k 1. fx1 h x2 x3 fx1 x2 x3 h 1 4π z s2 ghγ dσγ ghγ fx e1h γt fx γt h e1 1 0 0. now suﬃces observe gh x1 fx γt h 0 uniformly γ. result ﬁnd 6 holds ﬁrst argument follows x α fx also rapidly decreasing hence f s. argument applies tderivative mtf. basic fact integration spheres shall need following fourier transform formula. lemma 3.5 1 4π z s2 e2πiξγ dσγ sin2πξ 2πξ . formula shall see following section connected fact fourier transform radial function radial. proof. note integral left radial ξ. indeed r rotation z s2 e2πirξγ dσγ z s2 e2πiξr1γ dσγ z s2 e2πiξγ dσγ may change variables γ r1γ. for this see formula 4 appendix. ξ ρ suﬃces prove lemma
ibookroot october 20 2007 3. wave equation rd r 191 ξ 0 0 ρ. ρ 0 lemma obvious. ρ 0 choose spherical coordinates ﬁnd lefthand side equal 1 4π z 2π 0 z π 0 e2πiρ cos θ sin θ dθ dϕ. change variables u cos θ gives 1 4π z 2π 0 z π 0 e2πiρ cos θ sin θ dθ dϕ 1 2 z π 0 e2πiρ cos θ sin θ dθ 1 2 z 1 1 e2πiρu du 1 4πiρ e2πiρu1 1 sin2πρ 2πρ formula proved. deﬁning formula 5 may interpret mtf convolution function f element dσ since fourier transform interchanges convolutions products led believe mtf product corresponding fourier transforms. indeed identity 7 mtfξ ˆ fξsin2πξt 2πξt . see this write mtfξ z r3 e2πixξ µ 1 4π z s2 fx γt dσγ dx note may interchange order integration make simple change variables achieve desired identity. result ﬁnd solution problem may expressed using spherical means initial data. theorem 3.6 solution 3 cauchy problem wave equation u 2u t2 subject ux 0 fx u t x 0 gx given ux t ttmtfx tmtgx.
ibookroot october 20 2007 192 chapter 6. fourier transform rd proof. consider ﬁrst problem u 2u t2 subject ux 0 0 u t x 0 gx. theorem 3.1 know solution u1 given u1x t z r3 ˆ gξsin2πξt 2πξ e2πixξ dξ z r3 ˆ gξsin2πξt 2πξt e2πixξ dξ tmtgx used 7 applied g fourier inversion formula. according theorem 3.1 again solution problem u 2u t2 subject ux 0 fx u t x 0 0 given u2x t z r3 h ˆ fξ cos2πξt e2πixξ dξ t µ z r3 ˆ fξsin2πξt 2πξt e2πixξ dξ ttmtfx. may superpose two solutions obtain u u1 u2 solution original problem. huygens principle solutions wave equation one three dimensions given respectively ux t fx t fx t 2 1 2 z xt xt gy dy ux t ttmtfx tmtgx.
ibookroot october 20 2007 3. wave equation rd r 193 x x t x t 0 x x figure 1. huygens principle 1 observe onedimensional problem value solution x t depends values f g interval centered x length 2t shown figure 1. addition g 0 solution depends data two boundary points interval. three dimensions boundary dependence always holds. precisely solution ux t depends values f g immediate neighborhood sphere centered x radius t. situation depicted figure 2 drawn cone originating x t base ball centered x radius t. cone called backward light cone originating x t. xspace x t figure 2. backward light cone originating x t alternatively data point x0 plane 0 inﬂuences solution boundary cone originating x0 called forward light cone depicted figure 3. phenomenon known huygens principle immediate formulas u given above. another important aspect wave equation connected
ibookroot october 20 2007 194 chapter 6. fourier transform rd x0 figure 3. forward light cone originating x0 considerations ﬁnite speed propagation. in case c 1 speed 1. means initial disturbance localized x x0 ﬁnite time t eﬀects propagated inside ball centered x0 radius t. state precisely suppose initial conditions f g supported ball radius δ centered x0 think δ small. ux t supported ball radius t δ centered x0. assertion clear discussion. 3.3 wave equation r2 r descent remarkable fact solution wave equation three dimensions leads solution wave equation two dimensions. deﬁne corresponding means f mtfx 1 2π z y1 fx ty1 y212 dy. theorem 3.7 solution cauchy problem wave equation two dimensions initial data f g sr2 given 8 ux t tt f mtfx f mtgx. notice diﬀerence case case 3. here u x t depends f g whole disc of radius t centered x values initial data near boundary disc. formally identity theorem arises follows. start initial pair functions f g sr2 may consider corresponding functions f g r3 merely extensions f g constant x3 variable is fx1 x2 x3 fx1 x2 gx1 x2 x3 gx1 x2.
ibookroot october 20 2007 3. wave equation rd r 195 now u solution given previous section 3dimensional wave equation initial data f g one expect u also constant x3 u satisﬁes 2dimensional wave equation. diﬃculty argument f g rapidly decreasing since constant x3 previous methods apply. however easy modify argument obtain proof theorem 3.7. ﬁx 0 consider function ηx3 sr ηx3 1 x3 3t. trick truncate f g x3variable consider instead f x1 x2 x3 fx1 x2ηx3 gx1 x2 x3 gx1 x2ηx3. f and gare sr3 theorem 3.6 provides solution uof wave equation initial data f and g. easy see formula ux t independent x3 whenever x3 t t t. particular deﬁne ux1 x2 t ux1 x2 0 t u satisﬁes 2dimensional wave equation t t. since arbitrary u solution problem remains see u desired form. deﬁnition spherical coordinates recall integral function h sphere s2 given 1 4π z s2 hγ dσγ 1 4π z 2π 0 z π 0 hsin θ cos ϕ sin θ sin ϕ cos θ sin θ dθ dϕ. h depend last variable is hx1 x2 x3 hx1 x2 function h two variables mthx1 x2 0 1 4π z 2π 0 z π 0 hx1 t sin θ cos ϕ x2 t sin θ sin ϕ sin θ dθ dϕ. calculate last integral split θintegral 0 π2 π2 π. making change variables r sin θ ﬁnd ﬁnal change polar coordinates mthx1 x2 0 1 2π z y1 hx ty1 y212 dy f mthx1 x2.
ibookroot october 20 2007 196 chapter 6. fourier transform rd applying h f h f h g h g ﬁnd u given formula 8 proof theorem 3.7 complete. remark. case general d solution wave equation shares many properties discussed special cases 1 2 3. . given time t initial data point x aﬀects solu tion u speciﬁc region. 1 odd data inﬂuences points boundary forward light cone origi nating x 1 even aﬀects points forward light cone. alternatively solution point x t depends data base backward light cone originating x t. fact 1 odd data immediate neighborhood boundary base inﬂuence ux t. . waves propagate ﬁnite speed initial data supported bounded set support solution u spreads velocity 1 or generally c wave equation normal ized. illustrate facts following observation diﬀerent behavior propagation waves three two dimen sions. since propagation light governed threedimensional wave equation 0 light ﬂashes origin following hap pens observer see ﬂash after ﬁnite amount time instant. contrast consider happens two dimensions. drop stone lake point surface begin after time undulate although amplitude oscillations decrease time undulations continue in principle indeﬁnitely. diﬀerence character formulas solutions wave equation 1 3 one hand 2 hand illustrates general principle ddimensional fourier analysis signiﬁcant number formulas arise simpler case odd dimensions compared corresponding situations even dimensions. see several examples below. 4 radial symmetry bessel functions observed earlier fourier transform radial function rd also radial. words fx f0x f0
ibookroot october 20 2007 4. radial symmetry bessel functions 197 ˆ fξ f0ξ f0. natural problem determine relation f0 f0. problem simple answer dimensions one three. 1 relation seek 9 f0ρ 2 z 0 cos2πρrf0r dr. recall r two rotations identity multiplication 1 ﬁnd function radial precisely even. made observation easy see f radial ξ ρ f0ρ ˆ fξ z fxe2πixξ dx z 0 f0re2πirξ e2πirξ dr 2 z 0 cos2πρrf0r dr. case 3 relation f0 f0 also quite simple given formula 10 f0ρ 2ρ1 z 0 sin2πρrf0rr dr. proof identity based formula fourier trans form surface element dσ given lemma 3.5 f0ρ ˆ fξ z r3 fxe2πixξ dx z 0 f0r z s2 e2πirγξdσγr2 dr z 0 f0r2 sin2πρr ρr r2 dr 2ρ1 z 0 sin2πρrf0rr dr. generally relation f0 f0 nice description terms family special functions arise naturally problems exhibit radial symmetry. bessel function order n z denoted jnρ deﬁned nth fourier coeﬃcient function eiρ sin θ. jnρ 1 2π z 2π 0 eiρ sin θeinθ dθ
ibookroot october 20 2007 198 chapter 6. fourier transform rd therefore eiρ sin θ x n jnρeinθ. result deﬁnition ﬁnd 2 relation be tween f0 f0 11 f0ρ 2π z 0 j02πrρf0rr dr. indeed since ˆ fξ radial take ξ 0 ρ ˆ fξ z r2 fxe2πix0ρ dx z 2π 0 z 0 f0re2πirρ sin θr dr dθ 2π z 0 j02πrρf0rr dr desired. general corresponding formulas relating f0 f0 rd terms bessel functions order d2 1 see problem 2. even dimensions bessel functions deﬁned above. odd dimensions need general deﬁnition bessel functions encompass halfintegral orders. note formulas fourier transform radial functions give another illustration diﬀerences odd even dimensions. 1 3 as well 3 odd formulas terms elementary functions case even. 5 radon transform applications invented johann radon 1917 integral transform discuss next many applications mathematics sciences includ ing signiﬁcant achievement medicine. motivate deﬁnitions central problem reconstruction ﬁrst present close con nection radon transform development xray scans or cat scans theory medical imaging. solution reconstruction problem introduction new algorithms faster computers contributed rapid development computerized tomography. practice xray scans provide picture internal organ one helps detect locate many types abnormalities.
ibookroot october 20 2007 5. radon transform applications 199 brief description xray scans two dimensions deﬁne xray transform formulate basic problem inverting mapping. although problem explicit solution r2 complicated analogous problem three dimensions hence give complete solution reconstruction problem r3. another example results simpler odd dimensional case evendimensional situation. 5.1 xray transform r2 consider twodimensional object lying plane r2 may think planar cross section human organ. first assume homogeneous suppose narrow beam xray photons traverses object. i0 figure 4. attenuation xray beam i0 denote intensity beam passing o respectively following relation holds i0edρ. distance traveled beam object ρ denotes attenuation coeﬃcient or absorption coeﬃcient depends density physical characteristics o. object homogeneous consists two materials attenuation coeﬃcients ρ1 ρ2 observed decrease intensity beam
ibookroot october 20 2007 200 chapter 6. fourier transform rd given i0ed1ρ1d2ρ2 d1 d2 denote distances traveled beam ma terial. case arbitrary object whose density physical characteristics vary point point attenuation factor func tion ρ r2 relations become i0e r l ρ. l line r2 traced beam r l ρ denotes line integral ρ l. since observe i0 data gather sending xray beam object along line l quantity z l ρ. since may initially send beam given direction may calculate integral every line r2. deﬁne xray transform or radon transform r2 ρ xρl z l ρ. note transform assigns appropriate function ρ r2 for example ρ sr2 another function xρ whose domain set lines l r2. unknown ρ since original interest lies precisely composition object problem becomes reconstruct function ρ collected data is xray transform. therefore pose following reconstruction problem find formula ρ terms xρ. mathematically problem asks formula giving inverse x. inverse even exist ﬁrst step pose following simpler uniqueness question xρ xρ conclude ρ ρ reasonable priori expectation xρ actually deter mines ρ one see counting dimensionality or degrees freedom involved. function ρ r2 depends two parameters the x1 x2 coordinates example. similarly function xρ function lines l also determined two parameters for ex ample slope l x2intercept. sense ρ xρ
ibookroot october 20 2007 5. radon transform applications 201 convey equivalent amount information unreasonable suppose xρ determines ρ. satisfactory answer reconstruction problem positive answer uniqueness question r2 shall forego giving here. however see exercise 13 problem 8. instead shall deal analogous simpler situation r3. let us ﬁnally remark fact one sample xray trans form determine xρl ﬁnitely many lines. therefore reconstruction method implemented practice based general theory also sampling procedures numerical approx imations computer algorithms. turns method used developing eﬀective relevant algorithms fast fourier transform incidentally take next chapter. 5.2 radon transform r3 experiment described previous section applies three dimen sions well. object r3 determined function ρ describes density physical characteristics object sending xray beam determines quantity z l ρ every line r3. r2 knowledge enough uniquely de termine ρ r3 need much data. fact using heuristic argument counting number degrees freedom see functions ρ r3 number three number parameters determining line l r3 four for example two intercept x1 x2 plane two direction line. thus sense problem overdetermined. turn instead natural mathematical generalization two dimensional problem. wish determine function r3 knowing integral planes3 r3. precise speak plane mean plane necessarily passing origin. p plane deﬁne radon transform rf rfp z p f. simplify presentation shall follow practice assuming dealing functions class sr3. however many 3note dimensionality associated points r3 planes r3 equals three cases.
ibookroot october 20 2007 202 chapter 6. fourier transform rd results obtained shown valid much larger classes functions. first explain mean integral f plane. description use planes r3 following given unit vector γ s2 number r deﬁne plane ptγ ptγ x r3 x γ t. parametrize plane unit vector γ orthogonal it distance origin see figure 5. note ptγ ptγ allow take negative values. γ ptγ 0 figure 5. description plane r3 given function f srd need make sense integral ptγ. proceed follows. choose unit vectors e1 e2 e1 e2 γ orthonormal basis r3. x ptγ written uniquely x tγ u u u1e1 u2e2 u1 u2 r. f sr3 deﬁne 12 z ptγ f z r2 ftγ u1e1 u2e2 du1 du2. consistent must check deﬁnition independent choice vectors e1 e2.
ibookroot october 20 2007 5. radon transform applications 203 proposition 5.1 f sr3 γ deﬁnition r ptγ f independent choice e1 e2. moreover z ãz ptγ f dt z r3 fx dx. proof. e 1 e 2 another choice basis vectors γ e 1 e 2 orthonormal consider rotation r r2 takes e1 e 1 e2 e 2. changing variables u ru integral proves deﬁnition 12 independent choice basis. prove formula let r denote rotation takes stan dard basis unit vectors4 r3 γ e1 e2. z r3 fx dx z r3 frx dx z r3 fx1γ x2e1 x3e2 dx1 dx2 dx3 z ãz ptγ f dt. remark. digress point xray transform deter mines radon transform since twodimensional integrals ex pressed iterated onedimensional integrals. words knowl edge integral function lines determines integral function plane. disposed preliminary matters turn study original problem. radon transform function f sr3 deﬁned rft γ z ptγ f. particular see radon transform function set planes r3. parametrization given plane may equivalently think rf function product r s2 t γ r γ s2 s2 denotes unit sphere r3. relevant class functions r s2 consists satisfy schwartz condition uniformly γ. words deﬁne sr s2 space continuous functions ft γ indeﬁnitely 4here referring vectors 1 0 0 0 1 0 0 0 1.
ibookroot october 20 2007 204 chapter 6. fourier transform rd diﬀerentiable t satisfy sup tr γs2 tk dℓf tℓt γ integers k ℓ0. goal solve following problems. uniqueness problem rf rg f g. reconstruction problem express f terms rf. solutions obtained using fourier transform. fact key point elegant essential relation radon fourier transforms. lemma 5.2 f sr3 rft γ sr ﬁxed γ. more over b rfs γ ˆ fsγ. precise ˆ f denotes threedimensional fourier transform f b rfs γ denotes onedimensional fourier transform rft γ function t γ ﬁxed. proof. since f sr3 every positive integer n con stant so 1 tn1 unftγ u an recall x tγ u γ orthogonal u. therefore soon n 3 ﬁnd 1 tnrft γ an z r2 du 1 un . similar argument derivatives shows rft γ sr ﬁxed γ. establish identity ﬁrst note b rfs γ z ãz ptγ f e2πist dt z z r2 ftγ u1e1 u2e2 du1 du2e2πist dt.
ibookroot october 20 2007 5. radon transform applications 205 however since γ u 0 γ 1 may write e2πist e2πisγtγu. result ﬁnd b rfs γ z z r2 ftγ u1e1 u2e2e2πisγtγu du1 du2 dt z z r2 ftγ ue2πisγtγu du dt. ﬁnal rotation γ e1 e2 standard basis r3 proves b rfs γ ˆ fsγ desired. consequence identity answer uniqueness ques tion radon transform r3 aﬃrmative. corollary 5.3 f g sr3 rf rg f g. proof corollary follows application lemma diﬀerence f g use fourier inversion theorem. ﬁnal task give formula allows us recover f radon transform. since rf function set planes r3 f function space variables x r3 recover f introduce dual radon transform passes functions deﬁned planes functions r3. given function f r s2 deﬁne dual radon transform 13 rfx z s2 fx γ γ dσγ. observe point x belongs ptγ x γ t idea given x r3 obtain rfx integrating f subset planes passing x is rfx z ptγ xptγ f integral right given precise meaning 13. use terminology dual following observation. v1 sr3 usual hermitian inner product f g1 z r3 fxgx dx
ibookroot october 20 2007 206 chapter 6. fourier transform rd v2 sr s2 hermitian inner product f g2 z r z s2 ft γgt γ dσγ dt r v1 v2 r v2 v1 14 rf f2 f rf1. validity identity needed argument below veriﬁcation left exercise reader. state reconstruction theorem. theorem 5.4 f sr3 rrf 8π2f. recall 2 x2 1 2 x2 2 2 x2 3 laplacian. proof. previous lemma rft γ z ˆ fsγe2πits ds. therefore rrfx z s2 z ˆ fsγe2πixγs ds dσγ hence rrfx z s2 z ˆ fsγ4π2s2e2πixγs ds dσγ 4π2 z s2 z ˆ fsγe2πixγss2 ds dσγ 4π2 z s2 z 0 ˆ fsγe2πixγss2 ds dσγ 4π2 z s2 z 0 ˆ fsγe2πixγss2 ds dσγ 8π2 z s2 z 0 ˆ fsγe2πixγss2 ds dσγ 8π2fx.
ibookroot october 20 2007 6. exercises 207 ﬁrst line diﬀerentiated integral sign used fact e2πixγs 4π2s2e2πixγs since γ 1. last step fol lows formula polar coordinates r3 fourier inver sion theorem. 5.3 note plane waves conclude chapter brieﬂy mentioning nice connection radon transform solutions wave equation. comes following way. recall 1 solution wave equation expressed sum traveling waves see chapter 1 natural ask analogue traveling waves exists higher dimensions. answer follows. let f function one variable assume suﬃciently smooth say c2 consider ux t deﬁned ux t fx γ t x rd γ unit vector rd. easy verify directly u solution wave equation rd with c 1. solution called plane wave indeed notice u constant every plane perpendicular direction γ time increases wave travels γ direction. it remarked plane waves never functions srd 1 constant directions perpendicular γ.5 basic fact 1 solution wave equation written integral as opposed sum 1 plane waves fact done via radon transform initial data f g. relevant formulas 3 see problem 6. 6 exercises 1. suppose r rotation plane r2 let r µ b c denote matrix respect standard basis vectors e1 1 0 e2 0 1. a write conditions rt r1 detr 1 terms equations a b c d. 5incidentally observation indication fuller treatment wave equation requires lifting restriction functions belong srd.
ibookroot october 20 2007 208 chapter 6. fourier transform rd b show exists ϕ r ib eiϕ. c conclude r proper expressed z 7zeiϕ r improper takes form z 7zeiϕ z x iy. 2. suppose r r3 r3 proper rotation. a show pt detr ti polynomial degree 3 prove exists γ s2 where s2 denotes unit sphere r3 rγ γ. hint use fact p0 0 see λ 0 pλ 0. r λi singular kernel nontrivial. b p denotes plane perpendicular γ passing origin show r p p linear map rotation. 3. recall formula z rd fx dx z sd1 z 0 frγrd1 dr dσγ. apply special case fx grfγ x rγ prove rotation r one z sd1 frγ dσγ z sd1 fγ dσγ whenever f continuous function sphere sd1. 4. let ad vd denote area volume unit sphere unit ball rd respectively. a prove formula ad 2πd2 γd2 a2 2π a3 4π a4 2π2 . . . γx r 0 ettx1 dt gamma function. hint use polar coordinates fact r rd eπx2 dx 1.
ibookroot october 20 2007 6. exercises 209 b show vd ad hence vd πd2 γd2 1. particular v2 π v3 4π3 . . . 5. let positive deﬁnite symmetric matrix real coeﬃcients. show z rd eπxax dx deta12. generalizes fact r rd eπx2 dx 1 corresponds case identity. hint apply spectral theorem write rdr1 r rotation and diagonal entries λ1 . . . λd λi eigenvalues a. 6. suppose ψ srd satisﬁes r ψx2 dx 1. show µz rd x2ψx2 dx µz rd ξ2 ˆ ψξ2 dξ d2 16π2 . statement heisenberg uncertainty principle dimensions. 7. consider timedependent heat equation rd 15 u t 2u x2 1 2u x2 0 boundary values ux 0 fx srd. hd x 1 4πtd2 ex24t z rd e4π2tξ2e2πixξ dξ ddimensional heat kernel show convolution ux t f hd x indeﬁnitely diﬀerentiable x rd 0. moreover u solves 15 continuous boundary 0 ux 0 fx. reader may also wish formulate ddimensional analogues theo rem 2.1 2.3 chapter 5. 8. chapter 5 found solution steadystate heat equation upper halfplane boundary values f given convolution u f py poisson kernel pyx 1 π x2 y2 x r 0.
ibookroot october 20 2007 210 chapter 6. fourier transform rd generally one calculate ddimensional poisson kernel using fourier transform follows. a subordination principle allows one write expressions involv ing function ex terms corresponding expressions involving function ex2. one form identity eβ z 0 eu πueβ24u du β 0. prove identity β 2πx taking fourier transform sides. b consider steadystate heat equation upper halfspace x y x rd 0 x j1 2u x2 j 2u y2 0 dirichlet boundary condition ux 0 fx. solution problem given convolution ux y f p d x p d x ddimensional poisson kernel p d x z rd e2πixξe2πξy dξ. compute p d x using subordination principle ddimensional heat kernel. see exercise 7. show p d x γd 12 πd12 x2 y2d12 . 9. spherical wave solution ux t cauchy problem wave equation rd function x radial. prove u spherical wave initial data f g s radial. 10. let ux t solution wave equation let et denote energy wave et z rd u t x t 2 x j1 z rd u xj x t 2 dx. seen et constant using plancherels formula. give alternate proof fact diﬀerentiating integral respect showing de dt 0.
ibookroot october 20 2007 6. exercises 211 hint integrate parts. 11. show solution wave equation 2u t2 2u x2 1 2u x2 2 2u x2 3 subject ux 0 fx u t x 0 gx f g sr3 given ux t 1 sx t z sxt tgy fy fy y x dσy sx t denotes sphere center x radius t sx t area. alternate expression solution wave equation given theorem 3.6. sometimes called kirchhoﬀs formula. 12. establish identity 14 dual transform given text. words prove 16 z r z s2 rft γft γdσγ dt z r3 fxrfx dx f sr3 f sr s2 rf z ptγ f rfx z s2 fx γ γ dσγ. hint consider integral z z z ftγ u1e2 u2e2ft γ dt dσγ du1 du2. integrating ﬁrst u gives lefthand side 16 integrating u setting x tγ u1e2 u2e2 gives righthand side. 13. t θ r θ π let l ltθ denote line x yplane given x cos θ sin θ t. line perpendicular direction cos θ sin θ distance origin we allow negative t. f sr2 xray transform two dimensional radon transform f deﬁned xft θ z ltθ f z ft cos θ u sin θ sin θ u cos θ du.
ibookroot october 20 2007 212 chapter 6. fourier transform rd calculate xray transform function fx y eπx2y2. 14. let x xray transform. show f s xf 0 f 0 taking fourier transform one variable. 15. f sr s1 deﬁne dual xray transform xf integrat ing f lines pass point x y that is lines ltθ x cos θ sin θ t xfx y z fx cos θ sin θ θ dθ. check case f sr2 f sr s1 z z xft θft θ dt dθ z z fx yxfx y dx dy. 7 problems 1. let jn denote nth order bessel function n z. prove a jnρ real real ρ. b jnρ 1njnρ. c 2j nρ jn1ρ jn1ρ. d ³ 2n ρ jnρ jn1ρ jn1ρ. e ρnjnρ ρnjn1ρ. f ρnjnρ ρnjn1ρ. g jnρ satisﬁes second order diﬀerential equation j nρ ρ1j nρ 1 n2ρ2jnρ 0. h show jnρ ³ρ 2 n x m0 1m ρ2m 22mmn m. i show integers n real numbers b jna b x ℓz jℓajnℓb.
ibookroot october 20 2007 7. problems 213 2. another formula jnρ allows one deﬁne bessel functions nonintegral values n n 12 jnρ ρ2n γn 12π z 1 1 eiρt1 t2n12 dt. a check formula agrees deﬁnition jnρ in tegral n 0. hint verify n 0 check sides satisfy recursion formula e problem 1. b note j12ρ q 2 π ρ12 sin ρ. c prove lim n12 jnρ r 2 π ρ12 cos ρ. d observe formulas proved text giving f0 terms f0 when describing fourier transform radial function take form 17 f0ρ 2πρd21 z 0 jd212πρrf0rrd2 dr 1 2 3 one uses formulas understanding j12ρ limn12 jnρ. turns relation f0 f0 given 17 valid dimensions d. 3. observed solution ux t cauchy problem wave equation given formula 3 depends initial data base backward light cone. natural ask property shared solution wave equation. aﬃrmative answer would imply uniqueness solution. let bx0 r0 denote closed ball hyperplane 0 centered x0 radius r0. backward light cone base bx0 r0 deﬁned lbx0r0 x t rd r x x0 r0 t 0 t r0. theorem suppose ux t c2 function closed upper halfplane x t x rd 0 solves wave equation 2u t2 u. ux 0 u t x 0 0 x bx0 r0 ux t 0 x t lbx0r0.
ibookroot october 20 2007 214 chapter 6. fourier transform rd words initial data cauchy problem wave equation vanishes ball b solution u problem vanishes backward light cone base b. following steps outline proof theorem. a assume u real. 0 t r0 let btx0 r0 x x x0 r0 t also deﬁne ux t µ u x1 . . . u xd u t . consider energy integral et 1 2 z btx0r0 u2 dx 1 2 z btx0r0 µu t 2 x j1 µ u xj 2 dx. observe et 0 e0 0. prove et z btx0r0 u t 2u t2 x j1 u xj 2u xjt dx 1 2 z btx0r0 u2 dσγ. b show xj u xj u t u xj 2u xjt 2u x2 j u t . c use last identity divergence theorem fact u solves wave equation prove et z btx0r0 x j1 u xj u t νj dσγ 1 2 z btx0r0 u2 dσγ νj denotes jth coordinate outward normal btx0 r0. d use cauchyschwarz inequality conclude x j1 u xj u t νj 1 2u2 result et 0. deduce et 0 u 0.
ibookroot october 20 2007 7. problems 215 4.there exist formulas solution cauchy problem wave equation 2u t2 2u x2 1 2u x2 ux 0 fx u t x 0 gx rd r terms spherical means generalize formula given text 3. fact solution even dimensions deduced odd dimensions discuss case ﬁrst. suppose 1 odd let h srd. spherical mean h ball centered x radius deﬁned mrhx mhx r 1 ad z sd1 hx rγ dσγ ad denotes area unit sphere sd1 rd. a show xmhx r 2 r 1 r mhx r x denotes laplacian space variables x r r. b show twice diﬀerentiable function ux t satisﬁes wave equation 2 r 1 r mux r t 2 mux r t mux r t denote spherical means function ux t. c 2k 1 deﬁne tϕr r1rk1r2k1ϕr let u tmu. function solves onedimensional wave equation ﬁxed x 2 ux r t 2 r ux r t. one use dalemberts formula ﬁnd solution ux r t problem expressed terms initial data. d show ux t mux 0 t lim r0 ux r t αr α 1 3 d 2.
ibookroot october 20 2007 216 chapter 6. fourier transform rd e conclude solution cauchy problem ddimensional wave equation 1 odd ux t 1 1 3 d 2 tt1td32 td2mtfx t1td32 td2mtgx . 5.the method descent used prove solution cauchy problem wave equation case even given formula ux t 1 1 3 d 2 h tt1td32 ³ td2f mtfx t1td32 ³ td2f mtgx i f mt denotes modiﬁed spherical means deﬁned f mthx 2 ad1 z bd fx ty p 1 y2 dy. 6.given initial data f g form fx fx γ gx gx γ check plane wave given ux t fx γ t fx γ t 2 1 2 z xγt xγt gs ds solution cauchy problem ddimensional wave equation. general solution given superposition plane waves. case 3 expressed terms radon transform follows. let rft γ 1 8π2 µ dt 2 rft γ. ux t 1 2 z s2 rfx γ t γ rfx γ t γ z xγt xγt rgs γ ds dσγ.
ibookroot october 20 2007 7. problems 217 7. every real number 0 deﬁne operator a formula afx z rd2πξ2a ˆ fξe2πiξx dξ whenever f srd. a check a agrees usual deﬁnition ath power that is compositions minus laplacian positive integer. b verify af indeﬁnitely diﬀerentiable. c prove integer general af rapidly decreasing. d let ux y solution steadystate heat equation 2u y2 x j1 2u x2 j 0 ux 0 fx given convolving f poisson kernel see exercise 8. check 12fx lim y0 u y x y generally k2fx 1k lim y0 ku yk x y positive integer k. 8.the reconstruction formula radon transform rd follows a 2 12 4π rrf f 12 deﬁned problem 7. b radon transform dual deﬁned analogy cases 2 3 general d 2π1d 2 d12rrf f.
ibookroot october 20 2007 7 finite fourier analysis past year seen birth rather re birth exciting revolution computing fourier transforms. class algorithms known fast fourier transform fft forcing complete re assessment many computational paths frequency analysis ﬁelds problems reduced fourier transforms andor convolu tions. c. bingham j. w. tukey 1966 previous chapters studied fourier series functions circle fourier transform functions deﬁned euclidean space rd. goal introduce another version fourier analy sis functions deﬁned ﬁnite sets precisely ﬁnite abelian groups. theory particularly elegant simple since inﬁ nite sums integrals replaced ﬁnite sums thus questions convergence disappear. turning attention ﬁnite fourier analysis begin simplest example zn underlying space multiplica tive group n th roots unity circle. group also realized additive form znz equivalence classes integers modulo n. group zn arises natural approximation circle as n tends inﬁnity since ﬁrst picture points zn correspond n points circle uniformly distributed. reason practical applications group zn becomes natural candidate storage information function circle resulting numerical computations involving fourier series. situation particularly nice n large form n 2n. computations fourier coeﬃcients lead fast fourier transform exploits fact induction n requires log n steps go n 1 n 2n. yields substantial saving time practical applications. second part chapter undertake general the ory fourier analysis ﬁnite abelian groups. fundamental example multiplicative group zq. fourier inversion formula
ibookroot october 20 2007 1. fourier analysis zn 219 zq seen key step proof dirichlets theorem primes arithmetic progression take next chapter. 1 fourier analysis zn turn group n th roots unity. group arises naturally simplest ﬁnite abelian group. also gives uniform partition circle therefore good choice one wishes sample appropriate functions circle. moreover partition gets ﬁner n tends inﬁnity one might expect discrete fourier theory discuss tends continuous theory fourier series circle. broad sense case although aspect problem one develop. 1.1 group zn let n positive integer. complex number z n th root unity zn 1. set n th roots unity precisely n 1 e2πin e2πi2n . . . e2πin1no . indeed suppose zn 1 z reiθ. must rneinθ 1 taking absolute values yields r 1. therefore einθ 1 means nθ 2πk k z. ζ e2πin ﬁnd ζk exhausts n th roots unity. however notice ζn 1 n diﬀer integer multiple n ζn ζm. fact clear ζn ζm n m divisible n. denote set n th roots unity zn. fact set gives uniform partition circle clear deﬁnition. note set zn satisﬁes following properties i z w zn zw zn zw wz. ii 1 zn. iii z zn z1 1z zn course zz1 1. result conclude zn abelian group complex multiplication. appropriate deﬁnitions set detail later section 2.1.
ibookroot october 20 2007 220 chapter 7. finite fourier analysis ζ ζ8 ζ6 ζ4 ζ3 ζ2 1 zn n 26 z9 ζ e2πi9 ζ5 ζ7 figure 1. group n th roots unity n 9 n 26 64 another way visualize group zn. consists choosing integer power ζ determines root unity. observed integer unique since ζn ζm whenever n diﬀer integer multiple n. naturally might select integer satisﬁes 0 n n 1. although choice perfectly reasonable terms sets ask happens multiply roots unity. clearly must add corresponding integers since ζnζm ζnm nothing guarantees 0 n n 1. fact ζnζm ζk 0 k n 1 n k diﬀer integer multiple n. so ﬁnd integer 0 n 1 corresponding root unity ζnζm see adding integers n must reduce modulo n is ﬁnd unique integer 0 k n 1 n m k integer multiple n. equivalent approach associate root unity ω class integers n ζn ω. root unity obtain partition integers n disjoint inﬁnite classes. add two classes choose integer one them say n m respectively deﬁne sum classes class contains integer n m. formalize notions. two integers x congru ent modulo n diﬀerence x y divisible n write x y mod n. words means x diﬀer integer multiple n. easy exercise check following three properties
ibookroot october 20 2007 1. fourier analysis zn 221 . x x mod n integers x. . x y mod n x mod n. . x y mod n z mod n x z mod n. deﬁnes equivalence relation z. let rx denote equivalence class residue class integer x. integer form x kn k z element or representative rx. fact precisely n equivalence classes class unique representative 0 n 1. may add equiva lence classes deﬁning rx ry rx y. deﬁnition course independent representatives x x rx y ry one checks easily x y rx y. turns set equivalence classes abelian group called group integers modulo n sometimes denoted znz. association rk e2πikn gives correspondence two abelian groups znz zn. since operations respected sense addition inte gers modulo n becomes multiplication complex numbers shall also denote group integers modulo n zn. observe 0 znz corresponds 1 unit circle. let v w denote vector spaces complexvalued functions group integers modulo n n th roots unity respectively. then identiﬁcation given carries v w follows fk fe2πikn f function integers modulo n f function n th roots unity. on write zn think either group integers modulo n group n th roots unity. 1.2 fourier inversion theorem plancherel identity zn ﬁrst crucial step developing fourier analysis zn ﬁnd functions correspond exponentials enx e2πinx case circle. important properties exponentials are
ibookroot october 20 2007 222 chapter 7. finite fourier analysis i ennz orthonormal set inner product 1 in chap ter 3 space riemann integrable functions circle. ii finite linear combinations ens the trigonometric polyno mials dense space continuous functions circle. iii enx y enxeny. zn appropriate analogues n functions e0 . . . en1 deﬁned eℓk ζℓk e2πiℓkn ℓ 0 . . . n 1 k 0 . . . n 1 ζ e2πin. understand parallel i ii think complexvalued functions zn vector space v endowed hermitian inner product f g n1 x k0 fkgk associated norm f2 n1 x k0 fk2. lemma 1.1 family e0 . . . en1 orthogonal. fact em eℓ ½ n ℓ 0 ℓ. proof. em eℓ n1 x k0 ζmkζℓk n1 x k0 ζmℓk. ℓ term sum equal 1 sum equals n. ℓ q ζmℓis equal 1 usual formula 1 q q2 qn1 1 qn 1 q shows em eℓ 0 qn 1. since n functions e0 . . . en1 orthogonal must lin early independent since vector space v ndimensional
ibookroot october 20 2007 1. fourier analysis zn 223 conclude e0 . . . en1 orthogonal basis v . clearly prop erty iii also holds is eℓk m eℓkeℓm ℓ k zn. lemma vector eℓhas norm n deﬁne e ℓ 1 n eℓ e 0 . . . e n1 orthonormal basis v . hence f v 1 f n1 x n0 f e ne n well f2 n1 x n0 f e n2. deﬁne nth fourier coeﬃcient f 1 n n1 x k0 fke2πiknn observations give following fundamental theorem zn version fourier inversion parsevalplancherel formulas. theorem 1.2 f function zn fk n1 x n0 ane2πinkn. moreover n1 x n0 an2 1 n n1 x k0 fk2. proof follows directly 1 observe 1 n f en 1 n f e n. remark. possible recover fourier inversion circle suﬃciently smooth functions say c2 letting n in ﬁnite model zn see exercise 3.
ibookroot october 20 2007 224 chapter 7. finite fourier analysis 1.3 fast fourier transform fast fourier transform method developed means calculating eﬃciently fourier coeﬃcients function f zn. problem arises naturally numerical analysis deter mine algorithm minimizes amount time takes computer calculate fourier coeﬃcients given function zn. since amount time roughly proportional number operations computer must perform problem becomes minimizing number operations necessary obtain fourier coeﬃcients an given values f zn. operations mean either addition multiplication complex numbers. begin naive approach problem. fix n suppose given f0 . . . fn 1 ωn e2πin. denote k f kth fourier coeﬃcient f zn deﬁnition k f 1 n n1 x r0 frωkr n crude estimates show number operations needed cal culate fourier coeﬃcients 2n 2 n. indeed takes n 2 multiplications determine ω2 n . . . ωn1 n coeﬃcient k requires n 1 multiplications n 1 additions. present fast fourier transform algorithm im proves bound on 2 obtained above. improvement possi ble if example restrict case partition circle dyadic is n 2n. see also exercise 9. theorem 1.3 given ωn e2πin n 2n possible calcu late fourier coeﬃcients function zn 4 2nn 4n log2n on log n operations. proof theorem consists using calculations division points obtain fourier coeﬃcients 2m division points. since choose n 2n obtain desired formula consequence recurrence involves n olog n steps. let m denote minimum number operations needed cal culate fourier coeﬃcients function zm. key proof theorem contained following recursion step.
ibookroot october 20 2007 1. fourier analysis zn 225 lemma 1.4 given ω2m e2πi2m 2m 2m 8m. proof. calculation ω2m . . . ω2m 2m requires 2m operations. note particular get ωm e2πim ω2 2m. main idea given function f z2m consider two functions f0 f1 zm deﬁned f0r f2r f1r f2r 1. assume possible calculate fourier coeﬃcients f0 f1 m operations each. denote fourier coeﬃcients corresponding groups z2m zm a2m k k respectively a2m k f 1 2 ³ k f0 k f1ωk 2m . prove this sum odd even integers deﬁnition fourier coeﬃcient a2m k f ﬁnd a2m k f 1 2m 2m1 x r0 frωkr 2m 1 2 ã 1 m1 x ℓ0 f2ℓωk2ℓ 2m 1 m1 x m0 f2m 1ωk2m1 2m 1 2 ã 1 m1 x ℓ0 f0ℓωkℓ 1 m1 x m0 f1mωkm ωk 2m establishes assertion. result knowing k f0 k f1 ωk 2m see a2m k f computed using three operations one ad dition two multiplications. 2m 2m 2m 3 2m 2m 8m proof lemma complete. induction n n 2n conclude proof the orem. initial step n 1 easy since n 2 two fourier coeﬃcients 0 f 1 2 f1 f1 1 f 1 2 f1 1f1 .
ibookroot october 20 2007 226 chapter 7. finite fourier analysis calculating fourier coeﬃcients requires ﬁve opera tions less 4 2 8. suppose theorem true n 2n1 n 4 2n1n 1. lemma must 2n 2 4 2n1n 1 8 2n1 4 2nn concludes inductive step proof theorem. 2 fourier analysis ﬁnite abelian groups main goal rest chapter generalize results fourier series expansions obtained special case zn. brief introduction notions related ﬁnite abelian groups turn important concept character. set ting ﬁnd characters play role exponentials e0 . . . en1 group zn thus provide key ingredient development theory arbitrary ﬁnite abelian groups. fact suﬃces prove ﬁnite abelian group enough charac ters leads automatically desired fourier theory. 2.1 abelian groups abelian group or commutative group set g together binary operation pairs elements g a b 7a b satisﬁes following conditions i associativity b c a b c a b c g. ii identity exists element u g often written either 1 0 u u g. iii inverses every g exists element a1 g a1 a1 u. iv commutativity a b g b b a. leave simple veriﬁcations facts identity element inverses unique. warning. deﬁnition abelian group used multi plicative notation operation g. sometimes one uses ad ditive notation b a instead b a1. times one notation may appropriate other examples illustrate point. group may diﬀerent interpretations one multiplicative notation suggestive another natural view group addition operation.
ibookroot october 20 2007 2. fourier analysis finite abelian groups 227 examples abelian groups . set real numbers r usual addition. identity 0 inverse x x. also r 0 r x r x 0 equipped stan dard multiplication abelian groups. cases unit 1 inverse x 1x. . usual addition set integers z abelian group. however z 0 abelian group standard mul tiplication since example 2 multiplicative inverse z. contrast q 0 abelian group standard multiplication. . unit circle s1 complex plane. view circle set points eiθ θ r group operation standard multiplication complex numbers. however identify points s1 angle θ s1 becomes r modulo 2π operation addition modulo 2π. . zn abelian group. viewed n th roots unity circle zn group multiplication complex numbers. however zn interpreted znz integers modulo n abelian group operation addition modulo n. . last example consists zq. group deﬁned set integers modulo q multiplicative inverses group operation multiplication modulo q. important example discussed detail below. homomorphism two abelian groups g h map f g h satisﬁes property fa b fa fb dot lefthand side operation g dot righthand side operation h. say two groups g h isomorphic write g h bijective homomorphism g h. equivalently g h isomorphic exists another homomorphism f h g g b h f fa f fb b.
ibookroot october 20 2007 228 chapter 7. finite fourier analysis roughly speaking isomorphic groups describe same object underlying group structure which really matters however particular notational representations might diﬀerent. example 1. pair isomorphic abelian groups arose already considered group zn. one representation given multiplicative group n th roots unity c. second repre sentation additive group znz residue classes integers modulo n. mapping n 7rn associates root unity z e2πinn ζn residue class znz determined n provides isomorphism two diﬀerent representations. example 2. parallel previous example see circle with multiplication isomorphic real numbers modulo 2π with addition. example 3. properties exponential logarithm guarantee exp r r log r r two homomorphisms inverses other. thus r with addition r with multiplication isomorphic. follows primarily interested abelian groups ﬁnite. case denote g number elements g call g order group. example order zn n. additional remarks order . g1 g2 two ﬁnite abelian groups direct product g1 g2 group whose elements pairs g1 g2 g1 g1 g2 g2. operation g1 g2 deﬁned g1 g2 g 1 g 2 g1 g 1 g2 g 2. clearly g1 g2 ﬁnite abelian groups g1 g2. deﬁnition direct product generalizes immediately case ﬁnitely many factors g1 g2 gn. . structure theorem ﬁnite abelian groups states group isomorphic direct product groups type zn see problem 2. nice result gives us overview class ﬁnite abelian groups. however since shall use theorem below omit proof.
ibookroot october 20 2007 2. fourier analysis finite abelian groups 229 discuss brieﬂy examples abelian groups play central role proof dirichlets theorem next chapter. group zq let q positive integer. see multiplication zq unambiguously deﬁned n congruent n congruent m both modulo q nm congruent nm modulo q. integer n zq unit exists integer zq nm 1 mod q. set units zq denoted zq clear deﬁnition zq abelian group multiplication modulo q. thus within additive group zq lies set zq group multiplication. alternative characterization zq given next chapter elements zq relatively prime q. example 4. group units z4 0 1 2 3 z4 1 3. reﬂects fact odd integers divided two classes de pending whether form 4k 1 4k 3. fact z4 isomorphic z2. indeed make following association z4 z2 1 0 3 1 notice multiplication z4 corresponds addition z2. example 5. units z5 z5 1 2 3 4. moreover z5 isomorphic z4 following identiﬁcation z5 z4 1 0 2 1 3 3 4 2
ibookroot october 20 2007 230 chapter 7. finite fourier analysis example 6. units z8 0 1 2 3 4 5 6 7 given z8 1 3 5 7. fact z8 isomorphic direct product z2 z2. case isomorphism groups given identiﬁcation z8 z2 z2 1 0 0 3 1 0 5 0 1 7 1 1 2.2 characters let g ﬁnite abelian group with multiplicative notation s1 unit circle complex plane. character g complex valued function e g s1 satisﬁes following condition 2 ea b eaeb a b g. words character homomorphism g circle group. trivial unit character deﬁned ea 1 g. characters play important role context ﬁnite fourier se ries primarily multiplicative property 2 generalizes analogous identity exponential functions circle law eℓk m eℓkeℓm held exponentials e0 . . . en1 used fourier theory zn. eℓk ζℓk e2πiℓkn 0 ℓn 1 k zn fact functions e0 . . . en1 precisely characters group zn. g ﬁnite abelian group denote ˆ g set characters g observe next set inherits structure abelian group.
ibookroot october 20 2007 2. fourier analysis finite abelian groups 231 lemma 2.1 set ˆ g abelian group multiplication deﬁned e1 e2a e1ae2a g. proof assertion straightforward one observes trivial character plays role unit. call ˆ g dual group g. light analogy characters general abelian group exponentials zn gather several examples groups duals. provides evidence central role played characters. see exercises 4 5 6. example 1. g zn characters g take form eℓk ζℓk e2πiℓkn 0 ℓn 1 easy check eℓ7ℓgives isomorphism zn zn. example 2. dual group circle1 precisely ennz where enx e2πinx. moreover en 7n gives isomorphism c s1 integers z. example 3. characters r described eξx e2πiξx ξ r. thus eξ 7ξ isomorphism b r r. example 4. since exp r r isomorphism deduce previous example characters r given eξx x2πiξ e2πiξ log x ξ r r isomorphic r or r. following lemma says nowhere vanishing multiplicative function character result useful later. lemma 2.2 let g ﬁnite abelian group e g c 0 mul tiplicative function namely ea b eaeb a b g. e character. 1in addition 2 deﬁnition character inﬁnite abelian group requires continuity. g circle r r meaning continuous refers standard notion limit.
ibookroot october 20 2007 232 chapter 7. finite fourier analysis proof. group g ﬁnite absolute value ea bounded ranges g. since ebn ebn conclude eb 1 b g. next step verify characters form orthonormal basis vector space v functions group g. fact obtained directly special case g zn explicit description characters e0 . . . en1. general case begin orthogonality relations prove enough characters showing many order group. 2.3 orthogonality relations let v denote vector space complexvalued functions deﬁned ﬁnite abelian group g. note dimension v g order g. deﬁne hermitian inner product v 3 f g 1 g x ag faga whenever f g v . sum taken group therefore ﬁnite. theorem 2.3 characters g form orthonormal family respect inner product deﬁned above. since ea 1 character ﬁnd e e 1 g x ag eaea 1 g x ag ea2 1. e e characters must prove e e 0 isolate key step lemma. lemma 2.4 e nontrivial character group g p ag ea 0. proof. choose b g eb 1. eb x ag ea x ag ebea x ag eab x ag ea. last equality follows ranges group ab ranges g well. therefore p ag ea 0.
ibookroot october 20 2007 2. fourier analysis finite abelian groups 233 conclude proof theorem. suppose e char acter distinct e. ee1 nontrivial lemma implies x ag eaea1 0. since ea1 ea theorem proved. consequence theorem see distinct characters linearly independent. since dimension v c g conclude order ˆ g ﬁnite g. main result turn that fact ˆ g g. 2.4 characters total family following completes analogy characters complex exponentials. theorem 2.5 characters ﬁnite abelian group g form basis vector space functions g. several proofs theorem. one consists using structure theorem ﬁnite abelian groups mentioned earlier states group direct product cyclic groups is groups type zn. since cyclic groups selfdual using fact would conclude ˆ g g therefore characters form basis g. see problem 3. shall prove theorem directly without considerations. suppose v vector space dimension inner product . linear transformation v v unitary preserves inner product tv tw v w v w v . spectral theorem linear algebra asserts unitary transformation diagonalizable. words exists basis v1 . . . vd eigenvectors v tvi λivi λi c eigenvalue attached vi. proof theorem 2.5 based following extension spectral theorem. lemma 2.6 suppose t1 . . . tk commuting family unitary trans formations ﬁnitedimensional inner product space v is titj tjti i j. t1 . . . tk simultaneously diagonalizable. words exists basis v consists eigenvectors every ti 1 . . . k.
ibookroot october 20 2007 234 chapter 7. finite fourier analysis proof. use induction k. case k 1 simply spec tral theorem. suppose lemma true family k 1 commuting unitary transformations. spectral theorem applied tk says v direct sum eigenspaces v vλ1 vλs vλi denotes subspace eigenvectors eigenvalue λi. claim one t1 . . . tk1 maps eigenspace vλi itself. indeed v vλi 1 j k 1 tktjv tjtkv tjλiv λitjv tjv vλi claim proved. since restrictions vλi t1 . . . tk1 form family commut ing unitary linear transformations induction hypothesis guarantees simultaneously diagonalizable subspace vλi. diagonalization provides us desired basis vλi thus v . prove theorem 2.5. recall vector space v complexvalued functions deﬁned g dimension g. g deﬁne linear transformation ta v v tafx fa x x g. since g abelian clear tatb tbta a b g one checks easily ta unitary hermitian inner product 3 de ﬁned v . lemma 2.6 family taag simultaneously di agonalizable. means basis vbxbg v vbx eigenfunction ta every a. let v one basis elements 1 unit element g. must v1 0 otherwise va va 1 tav1 λav1 0 λa eigenvalue v ta. hence v 0 contra diction. claim function deﬁned wx λx vxv1 character g. arguing ﬁnd wx 0 every x wa b va b v1 λavb v1 λaλb v1 v1 λaλb wawb. invoke lemma 2.2 conclude proof.
ibookroot october 20 2007 2. fourier analysis finite abelian groups 235 2.5 fourier inversion plancherel formula put together results obtained previous sections discuss fourier expansion function ﬁnite abelian group g. given function f g character e g deﬁne fourier coeﬃcient f respect e ˆ fe f e 1 g x ag faea fourier series f f x eˆ g ˆ fee. since characters form basis know f x eˆ g cee set constants ce. orthogonality relations satisﬁed characters ﬁnd f e ce f indeed equal fourier series namely f x eˆ g ˆ fee. summarize results. theorem 2.7 let g ﬁnite abelian group. characters g form orthonormal basis vector space v functions g equipped inner product f g 1 g x ag faga. particular function f g equal fourier series f x eˆ g ˆ fee. finally parsevalplancherel formula ﬁnite abelian groups.
ibookroot october 20 2007 236 chapter 7. finite fourier analysis theorem 2.8 f function g f2 x eˆ g ˆ fe2. proof. since characters g form orthonormal basis vector space v f e ˆ fe f2 f f x eˆ g f e ˆ fe x eˆ g ˆ fe2. apparent diﬀerence statement theorem 1.2 due diﬀerent normalizations fourier coeﬃcients used. 3 exercises 1. let f function circle. n 1 discrete fourier coeﬃcients f deﬁned ann 1 n n x k1 fe2πikne2πiknn n z. also let an z 1 0 fe2πixe2πinx dx denote ordinary fourier coeﬃcients f. a show ann ann n. b prove f continuous ann an n . 2. f c1 function circle prove ann cn whenever 0 n n2. hint write ann1 e2πiℓnn 1 n n x k1 fe2πikn fe2πikℓne2πiknn choose ℓso ℓnn nearly 12. 3. similar method show f c2 function circle ann cn2 whenever 0 n n2.
ibookroot october 20 2007 3. exercises 237 result prove inversion formula f c2 fe2πix x n ane2πinx ﬁnite version. hint ﬁrst part use second symmetric diﬀerence fe2πikℓn fe2πikℓn 2fe2πikn. second part n odd say write inversion formula fe2πikn x nn2 anne2πiknn. 4. let e character g zn additive group integers modulo n. show exists unique 0 ℓn 1 ek eℓk e2πiℓkn k zn. conversely every function type character zn. deduce eℓ7ℓdeﬁnes isomorphism ˆ g g. hint show e1 n th root unity. 5. show characters s1 given enx e2πinx n z check en 7n deﬁnes isomorphism c s1 z. hint f continuous fx y fxfy f diﬀerentiable. see this note f0 0 appropriate δ c r δ 0 fy dy 0 cfx r δx x fy dy. diﬀerentiate conclude fx eax a. 6. prove characters r take form eξx e2πiξx ξ r eξ 7ξ deﬁnes isomorphism b r r. argument exercise 5 applies well. 7. let ζ e2πin. deﬁne n n matrix ajk1jkn ajk n 12ζjk. a show unitary.
ibookroot october 20 2007 238 chapter 7. finite fourier analysis b interpret identity mu mv u v fact 1 terms fourier series zn. 8. suppose px n x n1 ane2πinx. a show using parseval identities circle zn z 1 0 px2 dx 1 n n x j1 pjn2. b prove reconstruction formula px n x j1 pjnkx jn kx e2πix n 1 e2πinx 1 e2πix 1 n e2πix e2πi2x e2πinx. observe p completely determined values pjn 1 j n. note also k0 1 kjn 0 whenever j congruent 0 modulo n. 9. prove following assertions modify argument given text. a show one compute fourier coeﬃcients function zn n 3n 6n log3 n operations. b generalize n αn α integer 1. 10. group g cyclic exists g g generates g is element g written gn n z. prove ﬁnite abelian group cyclic isomorphic zn n. 11. write multiplicative tables groups z3 z4 z5 z6 z8 z9. groups cyclic 12. suppose g ﬁnite abelian group e g c function satisﬁes ex y exey x g. prove either e identically 0 e never vanishes. second case show x ex e2πir r q form r pq q g.
ibookroot october 20 2007 4. problems 239 13. analogy ordinary fourier series one may interpret ﬁnite fourier expansions using convolutions follows. suppose g ﬁnite abelian group 1g unit v vector space complexvalued functions g. a convolution two functions f g v deﬁned g f ga 1 g x bg fbga b1. show e ˆ g one f ge ˆ feˆ ge. b use theorem 2.5 show e character g x eˆ g ec 0 whenever c g c 1g. c result b show fourier series sfa p eˆ g ˆ feea function f v takes form sf f d deﬁned 4 dc x eˆ g ec ½ g c 1g 0 otherwise. since f d f recover fact sf f. loosely speaking corresponds dirac delta function unit mass 1 g x cg dc 1 4 says mass concentrated unit element g. thus interpretation limit family good kernels. see section 4 chapter 2. note. function reappears next chapter δ1n. 4 problems 1. prove n two positive integers relatively prime znm zn zm.
ibookroot october 20 2007 240 chapter 7. finite fourier analysis hint consider map znm zn zm given k 7k mod n k mod m use fact exist integers x xn ym 1. 2.every ﬁnite abelian group g isomorphic direct product cyclic groups. two precise formulations theorem. . p1 . . . ps distinct primes appearing factorization order g g gp1 gps gp form gp zpr1 zprℓ 0 r1 rℓthis sequence integers depends p course. decomposition unique. . exist unique integers d1 . . . dk d1d2 d2d3 dk1dk g zd1 zdk. deduce second formulation ﬁrst. 3. let ˆ g denote collection distinct characters ﬁnite abelian group g. a note g zn ˆ g isomorphic g. b prove g1 g2 ˆ g1 ˆ g2. c prove using problem 2 g ﬁnite abelian group ˆ g iso morphic g. 4.when p prime group zp cyclic zp zp 1.
ibookroot october 20 2007 8 dirichlets theorem dirichlet gustav lejeune d uren 1805g ottingen 1859 german mathematician. number theorist heart. but studying paris like able person befriended fourier likeminded mathematicians learned analysis them. thus equipped able lay foundation application fourier analysis analytic theory numbers. s. bochner 1966 striking application theory ﬁnite fourier series prove dirichlets theorem primes arithmetic progression. the orem states q ℓare positive integers common factor progression ℓ ℓ q ℓ 2q ℓ 3q . . . ℓ kq . . . contains inﬁnitely many prime numbers. change subject matter undertake illustrates wide applicability ideas fourier analysis various areas outside seemingly narrower conﬁnes. particular case theory fourier series ﬁnite abelian group zq plays key role solution problem. 1 little elementary number theory begin introducing requisite background. involves elemen tary ideas divisibility integers particular properties regarding prime numbers. basic fact called fundamental theorem arithmetic every integer product primes essentially unique way. 1.1 fundamental theorem arithmetic following theorem mathematical formulation long division.
ibookroot october 20 2007 242 chapter 8. dirichlets theorem theorem 1.1 euclids algorithm integers b b 0 exist unique integers q r 0 r b qb r. q denotes quotient b r remainder smaller b. proof. first prove existence q r. let denote set nonnegative integers form qb q z. set nonempty fact contains arbitrarily large positive integers since b 0. let r denote smallest element s r qb integer q. construction 0 r claim r b. not may write r b 0 s r b qb implies q 1b. hence s r contradicts minimality r. r b hence q r satisfy conditions theorem. prove uniqueness suppose also q1b r1 0 r1 b. subtraction ﬁnd q q1b r1 r. lefthand side absolute value 0 b righthand side absolute value b. hence sides equation must 0 gives q q1 r r1. integer divides b exists another integer c ac b write ab say divisor b. note particular 1 divides every integer aa integers a. prime number positive integer greater 1 positive divisors besides 1 itself. main theorem section says positive integer written uniquely product prime numbers. greatest common divisor two positive integers b largest integer divides b. usually denote greatest common divisor gcda b. two positive integers relatively prime greatest common divisor 1. words 1 positive divisor common b.
ibookroot october 20 2007 1. little elementary number theory 243 theorem 1.2 gcda b d exist integers x ax d. proof. consider set positive integers form ax x z let smallest element s. claim d. construction exist integers x ax s. clearly divisor b divides s must s. proof complete show sa sb. euclids algorithm write qs r 0 r s. multiplying q ﬁnd qax qby qs therefore qax qby r. hence r a1 qx bqy. since minimal 0 r s conclude r 0 therefore divides a. similar argument shows divides b hence desired. particular record following three consequences theo rem. corollary 1.3 two positive integers b relatively prime exist integers x ax 1. proof. b relatively prime two integers x desired property exist theorem 1.2. conversely ax 1 holds positive divides b divides 1 hence 1. corollary 1.4 c relatively prime c divides ab c divides b. particular p prime divide p divides ab p divides b. proof. write 1 ax cy multiplying b ﬁnd b abx cby. hence cb. corollary 1.5 p prime p divides product a1 ar p divides ai i. proof. previous corollary p divide a1 p divides a2 ar eventually pai. prove main result section.
ibookroot october 20 2007 244 chapter 8. dirichlets theorem theorem 1.6 every positive integer greater 1 factored uniquely product primes. proof. first show factorization possible. proving set positive integers 1 factorization primes empty. arguing contradiction assume . let n smallest element s. since n cannot prime exist integers 1 b 1 ab n. n b n s well b s. hence b prime factorizations product n. implies n s therefore empty desired. turn attention uniqueness factorization. sup pose n two factorizations primes n p1p2 pr q1q2 qs. p1 divides q1q2 qs apply corollary 1.5 conclude p1qi i. since qi prime must p1 qi. continuing argument ﬁnd two factorizations n equal permutation factors. brieﬂy digress give alternate deﬁnition group zq appeared previous chapter. according initial deﬁni tion zq multiplicative group units zq n zq exists integer 1 nm 1 mod q. equivalently zq group multiplication integers zq relatively prime q. indeed notice 1 satisﬁed automatically n q relatively prime. conversely suppose assume n q relatively prime. then put n b q corollary 1.3 ﬁnd nx qy 1. hence nx 1 mod q take x establish equiva lence. 1.2 inﬁnitude primes study prime numbers always central topic arithmetic ﬁrst fundamental problem arose determine whether
ibookroot october 20 2007 1. little elementary number theory 245 inﬁnitely many primes not. problem solved euclids elements simple elegant argument. theorem 1.7 inﬁnitely many primes. proof. suppose not denote p1 . . . pn complete set primes. deﬁne n p1p2 pn 1. since n larger pi integer n cannot prime. therefore n divisible prime belongs list. also absurdity since every prime divides product yet prime divides 1. contradiction concludes proof. euclids argument actually modiﬁed deduce ﬁner results inﬁnitude primes. see this consider following prob lem. prime numbers except 2 divided two classes de pending whether form 4k 1 4k 3 theorem says least one classes inﬁnite. natu ral question ask whether classes inﬁnite not one is case primes form 4k 3 fact class inﬁnite proof similar euclids twist. ﬁnitely many primes enumerate increasing order omitting 3 p1 7 p2 11 . . . pn let n 4p1p2 pn 3. clearly n form 4k 3 cannot prime since n pn. since product two numbers form 4m 1 form 4m 1 one prime divisors n say p must form 4k 3. must p 3 since 3 divide product deﬁnition n. also p cannot one primes form 4k 3 is p pi 1 . . . n p divides product p1 pn divide 3. remains determine class primes form 4k 1 inﬁnite. simpleminded modiﬁcation argument work since product two numbers form 4m 3 never form 4m 3. generally attempt prove law quadratic reciprocity legendre formulated following statement
ibookroot october 20 2007 246 chapter 8. dirichlets theorem q ℓare relatively prime sequence ℓ kq k z contains inﬁnitely many primes hence least one prime. course condition q ℓbe relatively prime necessary otherwise ℓ kq never prime. words hypothesis says arithmetic progression could contain primes necessarily contains inﬁnitely many them. legendres assertion proved dirichlet. key idea proof eulers analytical approach prime numbers involving product formula gives strengthened version theorem 1.7. insight euler led deep connection theory primes analysis. zeta function euler product begin rapid review inﬁnite products. an n1 se quence real numbers deﬁne n1 lim n n n1 limit exists case say product converges. natural approach take logarithms transform products sums. gather lemma properties shall need function log x deﬁned positive real numbers. lemma 1.8 exponential logarithm functions satisfy follow ing properties i elog x x. ii log1 x x ex ex x2 x 12. iii log1 x x 12 y 2x. terms notation property ii recorded log1 x x ox2. proof. property i standard. prove property ii use power series expansion log1 x x 1 is 2 log1 x x n1 1n1 n xn.
ibookroot october 20 2007 1. little elementary number theory 247 ex log1 x x x2 2 x3 3 x4 4 triangle inequality implies ex x2 2 1 x x2 . therefore x 12 sum geometric series righthand side ﬁnd ex x2 2 µ 1 1 2 1 22 x2 2 µ 1 1 12 x2. proof property iii immediate x 0 x 12 log1 x x 1 ex x 1 x 2 x 0 iii clearly also true. prove main result inﬁnite products real numbers. proposition 1.9 1 p an converges prod uct q n converges product vanishes one factors vanishes. also 1 n q n 11 an converges. proof. p an converges large n must an 12. disregarding ﬁnitely many terms necessary may assume inequality holds n. may write partial products follows n n1 n n1 elog1an ebn bn pn n1 bn bn log1 an. lemma know bn 2an bn converges real number say b. since
ibookroot october 20 2007 248 chapter 8. dirichlets theorem exponential function continuous conclude ebn converges eb n goes inﬁnity proving ﬁrst assertion proposition. observe also 1 0 n product converges nonzero limit since expressed eb. finally observe partial products q n 11 an 1 qn n11 an argument proves product denominator converges nonzero limit. preliminaries behind us return heart matter. real number strictly greater 1 deﬁne zeta function ζs x n1 1 ns . see series deﬁning ζ converges use principle whenever f decreasing function one compare p fn r fx dx suggested figure 1. note also similar tech nique used chapter 3 time bounding sum integral. fx n 1 n fn figure 1. comparing sums integrals take fx 1xs see x n1 1 ns 1 x n2 z n n1 dx xs 1 z 1 dx xs
ibookroot october 20 2007 1. little elementary number theory 249 therefore 3 ζs 1 1 1. clearly series deﬁning ζ converges uniformly halfline s0 1 hence ζ continuous 1. zeta function already mentioned earlier discussion poisson summation formula theta function. key result eulers product formula. theorem 1.10 every 1 ζs p 1 1 1ps product taken primes. important remark identity analytic expression fundamental theorem arithmetic. fact factor product 11 ps written convergent geometric series 1 1 ps 1 p2s 1 pms . consider pj ã 1 1 ps j 1 p2s j 1 pms j product taken primes order increasing order p1 p2 . proceeding formally these manipulations justiﬁed below calculate product sum terms term originating picking term 1pks j in sum corresponding pj k course depend j k 0 j suﬃciently large. product obtained way 1 pk1 1 pk2 2 pkm s 1 ns integer n written product primes n pk1 1 pk2 2 pkm . fundamental theorem arithmetic integer 1 occurs way uniquely hence product equals x n1 1 ns .
ibookroot october 20 2007 250 chapter 8. dirichlets theorem justify heuristic argument. proof. suppose n positive integers n. observe positive integer n n written uniquely product primes prime must less equal n repeated less times. therefore n x n1 1 ns pn µ 1 1 ps 1 p2s 1 pms pn µ 1 1 ps p µ 1 1 ps . letting n tend inﬁnity yields x n1 1 ns p µ 1 1 ps . reverse inequality argue follows. again fundamen tal theorem arithmetic ﬁnd pn µ 1 1 ps 1 p2s 1 pms x n1 1 ns . letting tend inﬁnity gives pn µ 1 1 ps x n1 1 ns . hence p µ 1 1 ps x n1 1 ns proof product formula complete. come eulers version theorem 1.7 inspired dirich lets approach general problem primes arithmetic progression. point following proposition.
ibookroot october 20 2007 1. little elementary number theory 251 proposition 1.11 series x p 1p diverges sum taken primes p. course ﬁnitely many primes series would converge automatically. proof. take logarithms sides euler formula. since log x continuous may write logarithm inﬁnite product sum logarithms. therefore obtain 1 x p log1 1ps log ζs. since log1 x x ox2 whenever x 12 get x p 1ps o1p2s log ζs gives x p 1ps o1 log ζs. term o1 appears p p 1p2s p n1 1n2. let tend 1 above namely 1 note ζs since p n1 1ns pm n1 1ns therefore lim inf s1 x n1 1ns x n1 1n every m. conclude p p 1ps as 1 since 1p 1ps 1 ﬁnally x p 1p . rest chapter see dirichlet adapted eulers insight.
ibookroot october 20 2007 252 chapter 8. dirichlets theorem 2 dirichlets theorem remind reader goal theorem 2.1 q ℓare relatively prime positive integers inﬁnitely many primes form ℓ kq k z. following eulers argument dirichlet proved theorem showing series x pℓ mod q 1 p diverges sum primes congruent ℓmodulo q. q ﬁxed confusion possible write p ℓto denote prime congruent ℓmodulo q. proof consists several steps one requires fourier analysis group zq. proceeding theorem complete generality outline solution particular problem raised earlier inﬁnitely many primes form 4k 1 example consists special case q 4 ℓ 1 illustrates important steps proof dirichlets theorem. begin character z4 deﬁned χ1 1 χ3 1. extend character z follows χn 0 n even 1 n 4k 1 1 n 4k 3. note function multiplicative is χnm χnχm z. let ls χ p n1 χnns ls χ 1 1 3s 1 5s 1 7s . l1 χ convergent series given 1 1 3 1 5 1 7 . since terms series alternating absolute values decrease zero l1 χ 0. χ multiplicative euler product generalizes as prove later give x n1 χn ns p 1 1 χpps .
ibookroot october 20 2007 2. dirichlets theorem 253 taking logarithm sides ﬁnd log ls χ x p χp ps o1. letting 1 observation l1 χ 0 shows p p χpps remains bounded. hence x p1 1 ps x p3 1 ps bounded 1. however know proposition 1.11 x p 1 ps unbounded 1 putting two facts together ﬁnd 2 x p1 1 ps unbounded 1. hence p p1 1p diverges consequence inﬁnitely many primes form 4k 1. digress brieﬂy show fact l1 χ π4. see this integrate identity 1 1 x2 1 x2 x4 x6 get z 0 dx 1 x2 y3 3 y5 5 0 1. let tend 1. integral calculated z 1 0 dx 1 x2 arctan u1 0 π 4 proves series 1 13 15 abel summable π4. since know series converges limit abel limit hence 1 13 15 π4. rest chapter gives full proof dirichlets theorem. begin fourier analysis which actually last step example given above reduce theorem nonvanishing lfunctions.
ibookroot october 20 2007 254 chapter 8. dirichlets theorem 2.1 fourier analysis dirichlet characters reduction theorem follows take abelian group g zq. formulas involve order g number integers 0 n q relatively prime q number deﬁnes euler phi function ϕq g ϕq. consider function δℓon g think characteristic function ℓ n zq δℓn ½ 1 n ℓ mod q 0 otherwise. expand function fourier series follows δℓn x eˆ g b δℓeen b δℓe 1 g x mg δℓmem 1 g eℓ. hence δℓn 1 g x eˆ g eℓen. extend function δℓto z setting δℓm 0 whenever q relatively prime. similarly extensions characters e ˆ g z given recipe χm ½ em q relatively prime 0 otherwise called dirichlet characters modulo q. shall denote extension z trivial character g χ0 χ0m 1 q relatively prime 0 otherwise. note dirichlet characters modulo q multiplicative z sense χnm χnχm n z. since integer q ﬁxed may without fear confusion speak dirichlet characters omitting reference q.1 g ϕq may restate results follows 1we use notation χ instead e distinguish dirichlet characters deﬁned z characters e deﬁned zq.
ibookroot october 20 2007 2. dirichlets theorem 255 lemma 2.2 dirichlet characters multiplicative. moreover δℓm 1 ϕq x χ χℓχm sum dirichlet characters. lemma taken ﬁrst step towards proof theorem since lemma shows x pℓ 1 ps x p δℓp ps 1 ϕq x χ χℓ x p χp ps . thus suﬃces understand behavior p p χpps 1. fact divide sum two parts depending whether χ trivial. x pℓ 1 ps 1 ϕq x p χ0p ps 1 ϕq x χχ0 χℓ x p χp ps 1 ϕq x p dividing q 1 ps 1 ϕq x χχ0 χℓ x p χp ps . 4 since ﬁnitely many primes dividing q eulers theorem proposition 1.11 implies ﬁrst sum righthand side di verges tends 1. observations show dirichlets the orem consequence following assertion. theorem 2.3 χ nontrivial dirichlet character sum x p χp ps remains bounded 1. proof theorem 2.3 requires introduction lfunctions turn. 2.2 dirichlet lfunctions proved earlier zeta function ζs p n 1ns could ex pressed product namely x n1 1 ns p 1 1 ps.
ibookroot october 20 2007 256 chapter 8. dirichlets theorem dirichlet observed analogue formula socalled lfunctions deﬁned 1 ls χ x n1 χn ns χ dirichlet character. theorem 2.4 1 x n1 χn ns p 1 1 χpps product primes. assuming theorem now follow eulers argument for mally taking logarithm product using fact log1 x x ox2 whenever x small would get log ls χ x p log1 χpps x p χp ps µ 1 p2s x p χp ps o1. l1 χ ﬁnite nonzero log ls χ bounded 1 conclude sum x p χp ps bounded 1. make several observations formal argument. first must prove product formula theorem 2.4. since dirichlet characters χ complexvalued extend loga rithm complex numbers w form w 11 z z 1. this done terms power series. show deﬁnition logarithm proof eulers product formula given earlier carries lfunctions. second must make sense taking logarithm sides product formula. dirichlet characters real argument works
ibookroot october 20 2007 2. dirichlets theorem 257 precisely one given example corresponding primes form 4k 1. general diﬃculty lies fact χp complex number complex logarithm single valued particular logarithm product sum logarithms. third remains prove whenever χ χ0 log ls χ bounded 1. as shall see ls χ continuous 1 suﬃces show l1 χ 0. nonvanishing mentioned earlier corresponds alternating series nonzero previous example. fact l1 χ 0 diﬃcult part argument. focus three points 1. complex logarithms inﬁnite products. 2. study ls χ. 3. proof l1 χ 0 χ nontrivial. however enter details pause brieﬂy discuss historical facts surrounding dirichlets theorem. historical digression following list gathered names mathematicians whose work dealt closely series achievements related dirichlets theorem. give better perspective attach years reached age 35 euler 1742 legendre 1787 gauss 1812 dirichlet 1840 riemann 1861 mentioned earlier eulers discovery product formula zeta function starting point dirichlets argument. legendre eﬀect conjectured theorem needed proof law quadratic reciprocity. however goal ﬁrst accomplished gauss who knowing establish theorem primes arithmetic progression nevertheless found number diﬀerent proofs quadratic reciprocity. later riemann extended study zeta function complex plane indicated properties
ibookroot october 20 2007 258 chapter 8. dirichlets theorem related nonvanishing function central understanding distribution prime numbers. dirichlet proved theorem 1837. noted fourier befriended dirichlet latter young mathematician visiting paris died several years before. besides great activity mathematics period also fertile time arts particular music. era beethoven ended ten years earlier schumann reaching heights creativity. musician whose career closest dirichlet felix mendelssohn four years junior. happens latter began composing famous violin concerto year dirichlet succeeded proving theorem. 3 proof theorem return proof dirichlets theorem three diﬃculties mentioned above. 3.1 logarithms device deal ﬁrst point deﬁne two logarithms one complex numbers form 11 z z 1 denote log1 one function ls χ denote log2. ﬁrst logarithm deﬁne log1 µ 1 1 z x k1 zk k z 1. note log1 w deﬁned rew 12 equa tion 2 log1 w gives extension usual log x x real number 12. proposition 3.1 logarithm function log1 satisﬁes following prop erties i z 1 elog1 1 1z 1 1 z . ii z 1 log1 µ 1 1 z z e1z error e1 satisﬁes e1z z2 z 12.
ibookroot october 20 2007 3. proof theorem 259 iii z 12 log1 µ 1 1 z 2z. proof. establish ﬁrst property let z reiθ 0 r 1 observe suﬃces show 5 1 reiθ e p k1reiθkk 1. so diﬀerentiate lefthand side respect r gives eiθ 1 reiθ ã x k1 reiθkk e p k1reiθkk. term brackets equals eiθ 1 reiθeiθ ã x k1 reiθk1 eiθ 1 reiθeiθ 1 1 reiθ 0. found lefthand side equation 5 constant set r 0 get desired result. proofs second third properties real counterparts given lemma 1.8. using results state suﬃcient condition guaranteeing convergence inﬁnite products complex numbers. proof real case except use logarithm log1. proposition 3.2 p an converges 1 n n1 µ 1 1 an converges. moreover product nonzero. proof. n large enough an 12 may assume without loss generality inequality holds n 1. n n1 µ 1 1 an n n1 elog1 1 1an e pn n1 log1 1 1an .
ibookroot october 20 2007 260 chapter 8. dirichlets theorem know previous proposition log1 µ 1 1 z 2z fact series p an converges immediately implies limit lim n n x n1 log1 µ 1 1 an exists. since exponential function continuous conclude product converges ea clearly nonzero. may prove promised dirichlet product formula x n χn ns p 1 1 χpps. simplicity notation let l denote lefthand side equation. deﬁne sn x nn χnns πn pn µ 1 1 χpps . inﬁnite product π limnπn q p ³ 1 1χpps converges previous proposition. indeed set χpnps n pn nth prime note 1 p an . also deﬁne πnm pn µ 1 χp ps χpm pms . ﬁx ϵ 0 choose n large sn l ϵ πn π ϵ. next select large enough sn πnm ϵ πnm πn ϵ. see ﬁrst inequality one uses fundamental theorem arith metic fact dirichlet characters multiplicative.
ibookroot october 20 2007 3. proof theorem 261 second inequality follows merely series p n1 χpn pns con verges. therefore l π l sn sn πnm πnm πn πn π 4ϵ shown. 3.2 lfunctions next step better understanding lfunctions. behav ior functions especially near 1 depends whether χ trivial. ﬁrst case ls χ0 simple factors zeta function. proposition 3.3 suppose χ0 trivial dirichlet character χ0n ½ 1 n q relatively prime 0 otherwise q pa1 1 pan n prime factorization q. ls χ0 1 ps 1 1 ps 2 1 ps n ζs. therefore ls χ0 as 1. proof. identity follows comparing dirichlet euler product formulas. ﬁnal statement holds ζs as 1. behavior remaining lfunctions χ χ0 subtle. remarkable property functions deﬁned continuous 0. fact true. proposition 3.4 χ nontrivial dirichlet character series x n1 χnns converges 0 denote sum ls χ. moreover i function ls χ continuously diﬀerentiable 0 . ii exists constants c c 0 ls χ 1 oecs ls χ oecs .
ibookroot october 20 2007 262 chapter 8. dirichlets theorem ﬁrst isolate key cancellation property nontrivial dirichlet characters possess accounts behavior lfunction described proposition. lemma 3.5 χ nontrivial dirichlet character k x n1 χn q k. proof. first recall q x n1 χn 0. fact denotes sum zq multiplicative property dirichlet character χ gives χas x χaχn x χan x χn s. since χ nontrivial χa 1 a hence 0. write k aq b 0 b q note k x n1 χn aq x n1 χn x aqnaqb χn x aqnaqb χn q terms last sum. proof complete recall χn 1. prove proposition. let sk pk n1 χn s0 0. know ls χ deﬁned 1 series x n1 χn ns converges absolutely uniformly δ 1. moreover diﬀerentiated series also converges absolutely uniformly δ 1 shows ls χ continuously diﬀerentiable 1.
ibookroot october 20 2007 3. proof theorem 263 sum parts2 extend result 0. indeed n x k1 χk ks n x k1 sk sk1 ks n1 x k1 sk 1 ks 1 k 1s sn n n1 x k1 fks sn n fks sk ks k 1s. gx xs gx sxs1 applying meanvalue theorem x k x k 1 fact sk q ﬁnd fks qsks1. therefore series p fks converges absolutely uniformly δ 0 proves ls χ continuous 0. prove also continuously diﬀerentiable diﬀerentiate series term term obtaining x log n χn ns . again rewrite series using summation parts x sk ks log k k 1s logk 1 application meanvalue theorem function gx xs log x shows terms okδ21 thus proving diﬀerentiated series converges uniformly δ 0. hence ls χ continuously diﬀerentiable 0. now observe large ls χ 1 2q x n2 ns 2so1 take c log 2 see ls χ 1 oecs . similar argument also shows ls χ oecs with fact c c proof proposition complete. 2for formula summation parts see exercise 7 chapter 2.
ibookroot october 20 2007 264 chapter 8. dirichlets theorem facts gathered far ls χ position deﬁne logarithm lfunctions. done integrating logarithmic derivative. words χ nontrivial dirichlet character 1 deﬁne3 log2 ls χ z lt χ lt χ dt. know lt χ 0 every 1 since given product proposition 3.2 integral convergent lt χ lt χ oect follows behavior inﬁnity lt χ lt χ recorded earlier. following links two logarithms. proposition 3.6 1 elog2 lsχ ls χ. moreover log2 ls χ x p log1 µ 1 1 χpps . proof. diﬀerentiating elog2 lsχls χ respect gives ls χ ls χ elog2 lsχls χ elog2 lsχls χ 0. elog2 lsχls χ constant constant seen 1 letting tend inﬁnity. proves ﬁrst conclusion. prove equality logarithms ﬁx take ex ponential sides. lefthand side becomes elog2 lsχ ls χ righthand side becomes e p p log1 1 1χpps p elog1 1 1χpps p µ 1 1 χpps ls χ 3the notation log2 used context confused logarithm base 2.
ibookroot october 20 2007 3. proof theorem 265 i proposition 3.1 dirichlet product formula. therefore exists integer ms log2 ls χ x p log1 µ 1 1 χpps 2πims. reader may verify lefthand side continuous s implies continuity function ms. ms integervalued conclude ms constant constant seen 0 letting go inﬁnity. putting together work done far gives rigorous meaning formal argument presented earlier. indeed properties log1 show x p log1 µ 1 1 χpps x p χp ps ãx p 1 p2s x p χp ps o1. l1 χ 0 nontrivial dirichlet character in tegral representation log2 ls χ remains bounded 1. thus identity logarithms implies p p χpps remains bounded 1 desired result. therefore ﬁnish proof dirichlets theorem need see l1 χ 0 χ nontrivial. 3.3 nonvanishing lfunction turn proof following deep result theorem 3.7 χ χ0 l1 χ 0. several proofs fact involving algebraic number theory among dirichlets original argument others involving complex analysis. opt elementary argument requires special knowledge either areas. proof splits two cases depending whether χ complex real. dirich let character said real takes real values that is 1 1 0 complex otherwise. words χ real χn χn integers n.
ibookroot october 20 2007 266 chapter 8. dirichlets theorem case i complex dirichlet characters easier two cases. proof contradiction use two lemmas. lemma 3.8 1 χ ls χ 1 product taken dirichlet characters. particular product realvalued. proof. shown earlier 1 ls χ exp ãx p log1 µ 1 1 χpps . hence χ ls χ exp ãx χ x p log1 µ 1 1 χpps exp ãx χ x p x k1 1 k χpk pks exp ãx p x k1 x χ 1 k χpk pks . lemma 2.2 with ℓ 1 p χ χpk ϕqδ1pk hence χ ls χ exp ã ϕq x p x k1 1 k δ1pk pks 1 since term exponential nonnegative. lemma 3.9 following three properties hold i l1 χ 0 l1 χ 0. ii χ nontrivial l1 χ 0 ls χ cs 1 1 s 2.
ibookroot october 20 2007 3. proof theorem 267 iii trivial dirichlet character χ0 ls χ0 c s 1 1 2. proof. ﬁrst statement immediate l1 χ l1 χ. second statement follows meanvalue theorem since ls χ continuously diﬀerentiable 0 χ nontrivial. finally last statement follows proposition 3.3 ls χ0 1 ps 1 1 ps 2 1 ps n ζs ζ satisﬁes similar estimate 3. conclude proof l1 χ 0 χ nontrivial complex dirichlet character. not say l1 χ 0 also l1 χ 0. since χ χ least two terms product χ ls χ vanish like s 1 1. since trivial character con tributes term grows growth worse o1s 1 ﬁnd product goes 0 1 contradicting fact 1 lemma 3.8. case ii real dirichlet characters proof l1 χ 0 χ nontrivial real dirichlet character diﬀerent earlier complex case. method shall exploit involves summation along hyperbolas. curious fact method introduced dirichlet himself twelve years proof theorem arithmetic progressions establish another famous result his average order divisor function. however made connection proofs two theorems. instead proceed proving ﬁrst dirichlets divisor theorem simple example method summation along hyperbolas. then shall adapt ideas prove fact l1 χ 0. preliminary matter need deal simple sums corresponding integral analogues. sums vs. integrals use idea comparing sum corresponding integral already occurred estimate 3 zeta function.
ibookroot october 20 2007 268 chapter 8. dirichlets theorem proposition 3.10 n positive integer then i x 1nn 1 n z n 1 dx x o1 log n o1. ii precisely exists real number γ called eulers constant x 1nn 1 n log n γ o1n. proof. suﬃces establish reﬁned estimate given part ii. let γn 1 n z n1 n dx x . since 1x decreasing clearly 0 γn 1 n 1 n 1 1 n2 series p n1 γn converges limit denote γ. more over estimate p fn r fx dx fx 1x2 ﬁnd x nn1 γn x nn1 1 n2 z n dx x2 o1n. therefore n x n1 1 n z n 1 dx x γ x nn1 γn z n1 n dx x last integral o1n n . proposition 3.11 n positive integer x 1nn 1 n12 z n 1 dx x12 c o1n 12 2n 12 c o1n 12. proof essentially repetition proof previous proposi tion time using fact 1 n12 1 n 112 c n32 .
ibookroot october 20 2007 3. proof theorem 269 last inequality follows meanvalue theorem applied fx x12 x n x n 1. hyperbolic sums f function deﬁned pairs positive integers three ways calculate sn x x fm n sum taken pairs positive integers m n satisfy mn n. may carry summation one following three ways. see figure 2. a along hyperbolas sn x 1kn ã x nmk fm n b vertically sn x 1mn x 1nnm fm n c horizontally sn x 1nn x 1mnn fm n remarkable fact one obtain interesting conclusions obvious fact three methods summation give sum. apply idea ﬁrst study divisor problem. intermezzo divisor problem positive integer k let dk denote number positive divisors k. example k 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 dk 1 2 2 3 2 4 2 4 3 4 2 6 2 4 4 5 2
ibookroot october 20 2007 270 chapter 8. dirichlets theorem c a b figure 2. three methods summation one observes behavior dk k tends inﬁnity rather irregular fact seem possible approximate dk simple analytic expression k. however natural inquire average size dk. words one might ask behavior 1 n n x k1 dk n answer provided dirichlet made use hyperbolic sums. indeed observe dk x nmk 1nm 1. theorem 3.12 k positive integer 1 n n x k1 dk log n o1. precisely 1 n n x k1 dk log n 2γ 1 o1n 12 γ eulers constant. proof. let sn pn k1 dk. observed summing f 1 along hyperbolas gives sn. summing vertically ﬁnd sn x 1mn x 1nnm 1.
ibookroot october 20 2007 3. proof theorem 271 p 1nnm 1 nm nm o1 x denote greatest integer x. therefore sn x 1mn nm o1 n x 1mn 1m on. hence part i proposition 3.10 sn n log n o1 gives ﬁrst conclusion. reﬁned estimate proceed follows. consider three regions i ii iii shown figure 3. deﬁned 1 m n 12 n 12 n nm ii 1 m n 12 1 n n 12 iii n 12 nn 1 n n 12. n iii n 12 nm n n n ii n 12 figure 3. three regions i ii iii si sii siii denote sums taken regions i ii iii respectively sn si sii siii 2si sii sii
ibookroot october 20 2007 272 chapter 8. dirichlets theorem since symmetry si siii. sum vertically use ii proposition 3.10 obtain si sii x 1mn12 x 1nnm 1 x 1mn12 nm x 1mn12 nm o1 n x 1mn 12 1m on 12 n log n 12 nγ on 12. finally sii corresponds square sii x 1mn 12 x 1nn 12 1 n 122 n on 12. putting estimates together dividing n yields re ﬁned statement theorem. nonvanishing lfunction essential application summation along hyperbolas main point section namely l1 χ 0 nontrivial real dirich let character χ. given character let fm n χn nm12 deﬁne sn x x fm n sum integers m n 1 satisfy mn n. proposition 3.13 following statements true i sn c log n constant c 0. ii sn 2n 12l1 χ o1.
ibookroot october 20 2007 3. proof theorem 273 suﬃces prove proposition since assumption l1 χ 0 would give immediate contradiction. ﬁrst sum along hyperbolas. observe x nmk χn nm12 1 k12 x nk χn. conclusion i enough show following lemma. lemma 3.14 x nk χn ½ 0 k 1 k ℓ2 ℓz. lemma get sn x kℓ2 ℓn 12 1 k12 c log n last inequality follows i proposition 3.10. proof lemma simple. k power prime say k pa divisors k 1 p p2 . . . pa x nk χn χ1 χp χp2 χpa 1 χp χp2 χpa. sum equal 1 χp 1 1 χp 1 even 0 χp 1 odd 1 χp 0 pq. general k pa1 1 pan n divisor k form pb1 1 pbn n 0 bj aj j. therefore multiplicative property χ gives x nk χn n j1 ³ χ1 χpj χp2 j χpaj j proof complete. prove second statement proposition write sn si sii siii
ibookroot october 20 2007 274 chapter 8. dirichlets theorem sums si sii siii deﬁned earlier see also figure 3. evaluate si summing vertically sii siii summing hor izontally. order carry need following simple results. lemma 3.15 integers 0 b i b x na χn n12 oa12 ii b x na χn n oa1. proof. argument similar proof proposition 3.4 use summation parts. let sn p 1kn χk remember sn q n. b x na χn n12 b1 x na sn h n12 n 112i oa12 ã x na n32 oa12. comparing sum p na n32 integral fx x32 ﬁnd former also oa12. similar argument establishes ii. may ﬁnish proof proposition. summing vertically ﬁnd si x mn 12 1 m12 x n 12nnm χnn12 . lemma together proposition 3.11 shows si o1. finally
ibookroot october 20 2007 4. exercises 275 sum horizontally get sii siii x 1nn 12 χn n12 x mnn 1m12 x 1nn 12 χn n12 n 2nn12 c onn12 2n 12 x 1nn 12 χn n c x 1nn 12 χn n12 1 n 12 x 1nn 12 1 b c. observe lemma together deﬁnition ls χ implies 2n 12l1 χ on 12n 12. moreover part i lemma gives b o1 also clearly c o1. thus sn 2n 12l1 χ o1 part ii proposition 3.13. completes proof l1 χ 0 thus proof dirich lets theorem. 4 exercises 1. prove inﬁnitely many primes observing ﬁnitely many p1 . . . pn n j1 1 1 1pj x n1 1 n. 2. text showed inﬁnitely many primes form 4k 3 modiﬁcation euclids original argument. adapt technique prove similar result primes form 3k 2 form 6k 5. 3. prove p q relatively prime zp zq isomorphic zpq.
ibookroot october 20 2007 276 chapter 8. dirichlets theorem 4. let ϕn denote number positive integers n relatively prime n. use previous exercise show n relatively prime ϕnm ϕnϕm. one give formula euler phifunction follows a calculate ϕp p prime counting number elements zp. b give formula ϕpk p prime k 1 counting number elements zpk. c show ϕn n µ 1 1 pi pi primes divide n. 5. n positive integer show n x dn ϕd ϕ euler phifunction. hint precisely ϕnd integers 1 m n gcdm n d. 6. write characters groups z3 z4 z5 z6 z8. a ones real complex b ones even odd a character even χ1 1 odd otherwise. 7. recall z 1 log1 µ 1 1 z x k1 zk k . seen elog1 1 1z 1 1 z .
ibookroot october 20 2007 4. exercises 277 a show w 11 z z 1 rew 12. b show rew 12 w ρeiϕ ρ 0 ϕ π log1 w log ρ iϕ. hint eζ w real part ζ uniquely determined imaginary part determined modulo 2π. 8. let ζ denote zeta function deﬁned 1. a compare ζs r 1 xs dx show ζs 1 1 o1 1. b prove consequence x p 1 ps log µ 1 1 o1 1. 9. let χ0 denote trivial dirichlet character mod q p1 . . . pk distinct prime divisors q. recall ls χ0 1 ps 1 1 ps k ζs show consequence ls χ0 ϕq q 1 1 o1 1. hint use asymptotics ζ exercise 8. 10. show ℓis relatively prime q x pℓ 1 ps 1 ϕq log µ 1 1 o1 1. quantitative version dirichlets theorem. hint recall 4. 11. use characters z3 z4 z5 z6 verify directly l1 χ 0 nontrivial dirichlet characters modulo q q 3 4 5 6. hint consider case appropriate alternating series.
ibookroot october 20 2007 278 chapter 8. dirichlets theorem 12. suppose χ real nontrivial assuming theorem l1 χ 0 show directly l1 χ 0. hint use product formula ls χ. 13. let an nbe sequence complex numbers n mod q. show series x n1 n converges pq n1 0. hint sum parts. 14. series fθ x n0 einθ n θ π converges every θ fourier series function deﬁned π π f0 0 fθ ½ iπ θ π θ 0 iπ θ 0 θ π extended periodicity period 2π r see exercise 8 chapter 2. show also θ 0 mod 2π series eθ x n1 einθ n converges eθ 1 2 log µ 1 2 2 cos θ 2fθ. 15. sum series p n1 ann n mod q pq n1 0 proceed follows. a deﬁne am q x n1 anζmn ζ e2πiq.
ibookroot october 20 2007 5. problems 279 note aq 0. notation previous exercise prove x n1 n 1 q q1 x m1 ame2πmq. hint use fourier inversion zq. b am odd am am z observe a0 aq 0 show am x 1nq2 anζmn ζmn. c still assuming am odd show x n1 n 1 2q q1 x m1 amf2πmq. hint deﬁne am pq n1 anζmn apply fourier inversion for mula. 16. use previous exercises show π 3 3 1 1 2 1 4 1 5 1 7 1 8 l1 χ nontrivial odd dirichlet character modulo 3. 5 problems 1.here series summed methods exercise 15. a nontrivial dirichlet character modulo 6 l1 χ equals π 2 3 1 1 5 1 7 1 11 1 13 . b χ odd dirichlet character modulo 8 l1 χ equals π 2 2 1 1 3 1 5 1 7 1 9 1 11 . c odd dirichlet character modulo 7 l1 χ equals π 7 1 1 2 1 3 1 4 1 5 1 6 .
ibookroot october 20 2007 280 chapter 8. dirichlets theorem d even dirichlet character modulo 8 l1 χ equals log1 2 2 1 1 3 1 5 1 7 1 9 1 11 . e even dirichlet character modulo 5 l1 χ equals 2 5 log µ1 5 2 1 1 2 1 3 1 4 1 6 1 7 1 8 1 9 1 11 . 2. let dk denote number positive divisors k. a show k pa1 1 pan n prime factorization k dk a1 1 an 1. although theorem 3.12 shows average dk order log k prove following basis a b dk 2 inﬁnitely many k. c positive integer n constant c 0 dk clog kn inﬁnitely many k. hint let p1 . . . pn n distinct primes consider k form p1p2 pnm 1 2 . . . 3. show p relatively prime q χ µ 1 χp ps µ 1 1 pfs g g ϕqf f order p zq that is smallest n pn 1 mod q. product taken dirichlet characters modulo q. 4. prove consequence previous problem χ ls χ x n1 ns 0 product dirichlet characters modulo q.
ibookroot october 20 2007 appendix integration appendix meant quick review deﬁnition main properties riemann integral r integration appropriate continuous functions rd. exposition brief since assume reader already familiarity material. begin theory riemann integration closed bounded interval real line. besides standard results integral also discuss notion sets measure 0 give necessary suﬃcient condition set discontinuities function guarantee integrability. also discuss multiple repeated integrals. particular extend notion integration entire space rd restricting functions decay fast enough inﬁnity. 1 deﬁnition riemann integral let f bounded realvalued function deﬁned closed interval a b r. partition p a b mean ﬁnite sequence num bers x0 x1 . . . xn x0 x1 xn1 xn b. given partition let ij denote interval xj1 xj write ij length namely ij xj xj1. deﬁne upper lower sums f respect p up f n x j1 sup xij fx ij lp f n x j1 inf xij fx ij. note inﬁmum supremum exist assumption f bounded. clearly up f lp f function f said riemann integrable simply integrable every ϵ 0 exists partition p up f lp f ϵ. deﬁne value integral f need make simple yet important observation. partition p said reﬁnement partition p p obtained p adding points. then adding one
ibookroot october 20 2007 282 appendix integration point time easy check up f up f lp f lp f. this see p1 p2 two partitions a b up1 f lp2 f since possible take p common reﬁnement p1 p2 obtain up1 f up f lp f lp2 f. since f bounded see u inf p up f l sup p lp f exist where inﬁmum supremum taken partitions a b also u l . moreover f integrable must u l deﬁne r b fx dx common value. finally bounded complexvalued function f u iv said integrable real imaginary parts u v integrable deﬁne z b fx dx z b ux dx z b vx dx. example constants integrable functions clear c c r b c dx cb a. also continuous functions inte grable. continuous function closed bounded interval a b uniformly continuous is given ϵ 0 exists δ x y δ fx fy ϵ. choose n b an δ partition p given a b a n . . . kb a n . . . n 1b a n b satisﬁes up f lp f ϵb a. 1.1 basic properties proposition 1.1 f g integrable a b then i f g integrable r b fx gx dx r b fx dx r b gx dx.
ibookroot october 20 2007 1. definition riemann integral 283 ii c c r b cfx dx c r b fx dx. iii f g realvalued fx gx r b fx dx r b gx dx. iv c a b r b fx dx r c fx dx r b c fx dx. proof. property i may assume f g realvalued. p partition a b up f g up f up g lp f g lp f lp g. given ϵ 0 exist partitions p1 p2 up1 f lp1 f ϵ up2 g lp2 g ϵ p0 common reﬁnement p1 p2 get up0 f g lp0 f g 2ϵ. f g integrable let infp up f g supp lp f g see up0 f g 2ϵ up0 f up0 g 2ϵ z b fx dx z b gx dx 4ϵ. similarly r b fx dx r b gx dx 4ϵ proves r b fx gx dx r b fx dx r b gx dx. second third parts proposition easy prove. last property simply reﬁne partitions a b adding point c. another important property need prove fg integrable whenever f g integrable. lemma 1.2 f realvalued integrable a b ϕ realvalued continuous function r ϕ f also integrable a b. proof. let ϵ 0 remember f bounded say f m. since ϕ uniformly continuous m m may choose δ 0 s m m s t δ ϕs ϕt ϵ. choose par tition p x0 . . . xn a b up f lp f δ2. let ij xj1 xj distinguish two classes write j λ supxij fx infxij fx δ construction sup xij ϕ fx inf xij ϕ fx ϵ.
ibookroot october 20 2007 284 appendix integration otherwise write j λ note δ x jλ ij x jλ sup xij fx inf xij fx ij δ2 p jλ ij δ. therefore separating cases j λ j λ ﬁnd up ϕ f lp ϕ f ϵb a 2bδ b bound ϕ m m. since also choose δ ϵ see proposition proved. from lemma get following facts . f g integrable a b product fg integrable a b. follows lemma ϕt t2 fact fg 1 4 f g2 f g2. . f integrable a b function f integrable r b fx dx r b fx dx. take ϕt t see f integrable. moreover in equality follows iii proposition 1.1. record two results imply integrability. proposition 1.3 bounded monotonic function f interval a b integrable. proof. may assume without loss generality 0 b 1 f monotonically increasing. then n choose uniform partition pn given xj jn j 0 . . . n. αj fxj upn f 1 n n x j1 αj lpn f 1 n n x j1 αj1. therefore fx b x upn f lpn f αn α0 n 2b n proposition proved.
ibookroot october 20 2007 1. definition riemann integral 285 proposition 1.4 let f bounded function compact interval a b. c a b small δ 0 function f integrable intervals a c δ c δ b f integrable a b. proof. suppose f m let ϵ 0. choose δ 0 small 4δm ϵ3. let p1 p2 partitions a c δ c δ b 1 2 upi f lpi f ϵ3. possible since f integrable one intervals. taking partition p p1 c δ c δ p2 immediately see up f lp f ϵ. end section useful approximation lemma. recall function circle 2πperiodic function r. lemma 1.5 suppose f integrable circle f bounded b. exists sequence fk k1 continuous functions circle sup xππ fkx b k 1 2 . . . z π π fx fkx dx 0 k . proof. assume f realvalued in general apply following argu ment real imaginary parts separately. given ϵ 0 may choose partition π x0 x1 xn π interval π π upper lower sums f diﬀer ϵ. denote f step function deﬁned f x sup xj1yxj fy x xj1 xj 1 j n. construction f b moreover 1 z π π f x fx dx z π π f x fx dx ϵ. modify f to make continuous periodic yet still ap proximate f sense lemma. small δ 0 let fx f x distance x division points x0 . . . xn δ. δneighborhood xj j 1 . . . n 1 deﬁne fx linear function fxj δ f xj δ. near x0 π f
ibookroot october 20 2007 286 appendix integration f x3 x1 x2 f x1 x2 x3 x2 δ x2 δ x0 x0 figure 1. portions functions f and f linear fπ 0 fπ δ f π δ. similarly near xn π function f linear fπ 0 fπ δ f π δ. figure 1 illustrate situation near x0 π. second pic ture graph f shifted slightly clarify situation. then since fπ fπ may extend f continuous 2π periodic function r. absolute value extension also bounded b. moreover f diﬀers f only n intervals length 2δ surrounding division points. thus z π π f x fx dx 2bn2δ. choose δ suﬃciently small get 2 z π π f x fx dx ϵ. result equations 1 2 triangle inequality yield z π π fx fx dx 2ϵ. denoting fk f constructed 2ϵ 1k see sequence fk properties required lemma. 1.2 sets measure zero discontinuities integrable func tions observed continuous functions integrable. modifying argument slightly one show piecewise continuous func tions also integrable. fact consequence proposition 1.4
ibookroot october 20 2007 1. definition riemann integral 287 applied ﬁnitely many times. turn careful study discontinuities integrable functions. start deﬁnition1 subset e r said measure 0 every ϵ 0 exists countable family open intervals ik k1 i e s k1 ik ii p k1 ik ϵ ik denotes length interval ik. ﬁrst condition says union intervals covers e second union small. reader diﬃculty proving ﬁnite set points measure 0. subtle argument needed prove countable set points measure 0. fact result contained following lemma. lemma 1.6 union countably many sets measure 0 mea sure 0. proof. say e1 e2 . . . sets measure 0 let e i1ei. let ϵ 0 choose open interval ii1 ii2 . . . ei k1 iik x k1 iik ϵ2i. clearly e s ik1 iik x i1 x k1 iik x i1 ϵ 2i ϵ shown. important observation e measure 0 com pact possible ﬁnd ﬁnite number open intervals ik k 1 . . . n satisfy two conditions i ii above. prove characterization riemann integrable functions terms discontinuities. theorem 1.7 bounded function f a b integrable set discontinuities measure 0. 1a systematic study measure sets arises theory lebesgue integration taken book iii.
ibookroot october 20 2007 288 appendix integration write j a b ic r c r c r open interval centered c radius r 0. deﬁne oscillation f ic r oscf c r sup fx fy supremum taken x j ic r. quantity exists since f bounded. deﬁne oscillation f c oscf c lim r0 oscf c r. limit exists oscf c r 0 decreasing function r. point f continuous c oscf c 0. clear deﬁnitions. ϵ 0 deﬁne set aϵ aϵ c j oscf c ϵ. done that see set points j f discon tinuous simply ϵ0 aϵ. important step proof theorem. lemma 1.8 ϵ 0 set aϵ closed therefore compact. proof. argument simple. suppose cn aϵ converges c assume c aϵ. write oscf c ϵ δ δ 0. select r oscf c r ϵ δ2 choose n cn c r2. oscf cn r2 ϵ implies oscf cn ϵ contradiction. ready prove ﬁrst part theorem. suppose set discontinuities f measure 0 let ϵ 0. since aϵ d cover aϵ ﬁnite number open intervals say i1 . . . in whose total length ϵ. complement union intervals compact around point z complement ﬁnd interval fz supxyfz fx fy ϵ simply z aϵ. may choose ﬁnite subcovering ziciz denote in1 . . . in. now taking end points intervals i1 i2 . . . obtain partition p a b up f lp f 2m n x j1 ij ϵb a cϵ. hence f integrable a b shown. conversely suppose f integrable a b let set discontinuities. since equals n1a1n suﬃces prove
ibookroot october 20 2007 2. multiple integrals 289 a1n measure 0. let ϵ 0 choose partition p x0 x1 . . . xn up f lp f ϵn. then a1n inter sects ij xj1 xj must supxij fx infxij fx 1n shows 1 n x jija1n ij up f lp f ϵn. taking intervals intersecting a1n making slightly larger cover a1n open intervals total length 2ϵ. therefore a1n measure 0 done. note incidentally gives another proof fg integrable whenever f g are. 2 multiple integrals assume reader familiar standard theory multi ple integrals functions deﬁned bounded sets. here give quick review main deﬁnitions results theory. then de scribe notion improper multiple integration range integration extended rd. relevant study fourier transform. spirit chapters 5 6 shall deﬁne integral functions continuous satisfy adequate decay condition inﬁnity. recall vector space rd consists dtuples real numbers x x1 x2 . . . xd xj r addition multiplication scalars deﬁned componentwise. 2.1 riemann integral rd deﬁnitions notion riemann integration rectangle r rd imme diate generalization notion riemann integration interval a b r. restrict attention continuous functions always integrable. closed rectangle rd mean set form r aj xj bj 1 j d aj bj r 1 j n. words r product onedimensional intervals aj bj r a1 b1 ad bd.
ibookroot october 20 2007 290 appendix integration pj partition closed interval aj bj call p p1 . . . pd partition r sj subinterval partition pj s1 sd subrectangle partition p. volume s subrectangle naturally given product length sides s s1 sd sj denotes length interval sj. ready deﬁne notion integral r. given bounded realvalued function f deﬁned r partition p deﬁne upper lower sums f respect p up f x sup xs fx s lp f x inf xs fx s sums taken subrectangles partition p. deﬁnitions direct generalizations analogous notions one di mension. partition p p 1 . . . p d reﬁnement p p1 . . . pd p j reﬁnement pj. arguing reﬁnements onedimensional case see deﬁne u inf p up f l sup p lp f u l exist ﬁnite u l. say f rie mann integrable r every ϵ 0 exists partition p up f lp f ϵ. implies u l common value shall denote either z r fx1 . . . xd dx1 dxd z r fx dx z r f deﬁnition integral f r. f complexvalued say fx ux ivx u v realvalued naturally deﬁne z r fx dx z r ux dx z r vx dx. results follow primarily interested continuous functions. clearly f continuous closed rectangle r f integrable since uniformly continuous r. also note f continuous on say closed ball b may deﬁne integral
ibookroot october 20 2007 2. multiple integrals 291 b following way g extension f deﬁned gx 0 x b g integrable rectangle r contains b may set z b fx dx z r gx dx. 2.2 repeated integrals fundamental theorem calculus allows us compute many one dimensional integrals since possible many instances ﬁnd antiderivative integrand. rd permits calculation multiple integrals since ddimensional integral actually reduces onedimensional integrals. precise statement describing fact given following. theorem 2.1 let f continuous function deﬁned closed rect angle r rd. suppose r r1 r2 r1 rd1 r2 rd2 d1 d2. write x x1 x2 xi rdi fx1 r r2 fx1 x2 dx2 continuous r1 z r fx dx z r1 µz r2 fx1 x2 dx2 dx1. proof. continuity f follows uniform continuity f r fact fx1 fx 1 z r2 fx1 x2 fx 1 x2 dx2. prove identity let p1 p2 partitions r1 r2 respec tively. subrectangles p1 p2 respectively key observation sup st fx1 x2 sup x1s ã sup x2t fx1 x2 inf st fx1 x2 inf x1s µ inf x2t fx1 x2 .
ibookroot october 20 2007 292 appendix integration then up f x st sup st fx1 x2 s t x x sup x1s sup x2t fx1 x2 t s x sup x1s µz r2 fx1 x2 dx2 s u µ p1 z r2 fx1 x2 dx2 . arguing similarly lower sums ﬁnd lp f lp1 z r2 fx1 x2 dx2 up1 z r2 fx1 x2 dx2 up f theorem follows inequalities. repeating argument ﬁnd corollary f continuous rectangle r rd given r a1 b1 ad bd z r fx dx z b1 a1 ãz b2 a2 ãz bd ad fx1 . . . xd dxd . . . dx2 dx1 righthand side denotes diterates onedimensional integrals. also clear theorem interchange order integration repeated integral desired. 2.3 change variables formula diﬀeomorphism class c1 g b mapping contin uously diﬀerentiable invertible whose inverse g1 b a also continuously diﬀerentiable. denote dg jacobian derivative g. then change variables formula says following. theorem 2.2 suppose b compact subsets rd g b diﬀeomorphism class c1. f continuous b z ga fx dx z fgy detdgy dy. proof theorem consists ﬁrst analysis special situation g linear transformation l. case r rectangle gr detl r
ibookroot october 20 2007 2. multiple integrals 293 explains term detdg. indeed term corresponds new inﬁnitesimal element volume change variables. 2.4 spherical coordinates important application change variables formula case polar coordinates r2 spherical coordinates r3 general ization rd. particularly important function set integrating over exhibit rotational or spherical symme tries. cases 2 3 given chapter 6. generally spherical coordinates system rd given x gr θ1 . . . θd1 x1 r sin θ1 sin θ2 sin θd2 cos θd1 x2 r sin θ1 sin θ2 sin θd2 sin θd1 . . . xd1 r sin θ1 sin θ2 xd r cos θ1 0 θi π 1 i d 2 0 θd1 2π. determinant jacobian transformation given rd1 sind2 θ1 sind3 θ2 sin θd2. point x rd 0 written uniquely rγ γ sd1 unit sphere rd. deﬁne z sd1fγ dσγ z π 0 z π 0 z 2π 0 fgr θ sind2 θ1 sind3 θ2 sin θd2 dθd1 dθ1 see b0 n denotes ball radius n centered origin 3 z b0n fx dx z sd1 z n 0 frγ rd1 dr dσγ. fact deﬁne area unit sphere sd1 rd ωd z sd1 dσγ. important application spherical coordinates calculation integral r ar1r2 xλ dx ar1 r2 denotes annulus
ibookroot october 20 2007 294 appendix integration ar1 r2 r1 x r2 λ r. applying polar coordinates ﬁnd z ar1r2 xλ dx z sd1 z r2 r1 rλd1 drdσγ. therefore z ar1r2 xλ dx ωd λdrλd 2 rλd 1 λ d ωdlogr2 logr1 λ d. 3 improper integrals. integration rd theorems discussed extend functions integrated rd impose decay inﬁnity functions integrate. 3.1 integration functions moderate decrease ﬁxed n 0 consider closed cube rd centered origin sides parallel axis side length n qn xj n2 1 j d. let f continuous function rd. limit lim n z qn fx dx exists denote z rd fx dx. deal special class functions whose integrals rd exist. continuous function f rd said moderate decrease exists 0 fx 1 xd1 . note 1 recover deﬁnition given chapter 5. important example function moderate decrease r poisson kernel given pyx 1 π x2y2 . claim f moderate decrease limit exists. let r qn fx dx. exists f continuous hence integrable. n im in z qmqn fx dx.
ibookroot october 20 2007 3. improper integrals. integration rd 295 observe set qm qn contained annulus aan bm an x bm b constants de pend dimension d. cube qn contained annulus n2 x n d2 take 12 b d2. therefore using fact f moderate decrease yields im in a z anxbm xd1 dx. putting λ d 1 calculation integral previous section ﬁnd im in c µ 1 1 bm . f moderate decrease conclude in n1 cauchy sequence therefore r rd fx dx exists. instead rectangles qn could chosen balls bn cen tered origin radius n. then f moderate decrease reader diﬃculties proving limn r bn fx dx exists limit equals limn r qn fx dx. elementary properties integrals functions moderate decrease summarized chapter 6. 3.2 repeated integrals chapters 5 6 claimed multiplication formula held functions moderate decrease. required appropriate interchange integration. similarly operators deﬁned terms convolutions with poisson kernel example. justify necessary formula iterated integrals. consider case 2 although reader diﬃculty ex tending result arbitrary dimensions. theorem 3.1 suppose f continuous r2 moderate decrease. fx1 z r fx1 x2 dx2 moderate decrease r z r2 fx dx z r µz r fx1 x2 dx2 dx1.
ibookroot october 20 2007 296 appendix integration proof. see f moderate decrease note ﬁrst fx1 z r dx2 1 x2 1 x2 232 z x2x1 z x2x1 . ﬁrst integral observe integrand a1 x13 z x2x1 dx2 1 x2 1 x2 232 1 x13 z x2x1 dx2 a 1 x12 . second integral z x2x1 dx2 1 x2 1 x2 232 a z x2x1 dx2 1 x23 a x12 thus f moderate decrease. fact argument together theorem 2.1 shows f uniform limit continuous functions thus also continuous. establish identity simply use approximation theo rem 2.1 ﬁnite rectangles. write sc denote complement set s. given ϵ 0 choose n large z r2 fx1 x2 dx1dx2 z inin fx1 x2 dx1dx2 ϵ n n. know z inin fx1 x2 dx1dx2 z µz fx1 x2 dx2 dx1. last iterated integral written z r µz r fx1 x2 dx2 dx1 z ic n µz r fx1 x2 dx2 dx1 z ãz ic n fx1 x2 dx2 dx1. estimate z ãz ic n fx1 x2 dx2 dx1 o µ 1 n 2 c z 1x1n ãz x2n dx2 x1 y13 dx1 o µ 1 n .
ibookroot october 20 2007 3. improper integrals. integration rd 297 similar argument shows z ic n µz r fx1 x2 dx2 dx1 c n . therefore ﬁnd n large z inin fx1 x2 dx1dx2 z r µz r fx1 x2 dx2 dx1 ϵ done. 3.3 spherical coordinates rd spherical coordinates given x rγ r 0 γ belongs unit sphere sd1. f moderate decrease ﬁxed γ sd1 function f given frγrd1 also moderate decrease r. indeed frγrd1 a rd1 1 rγd1 b 1 r2 . result letting r in 3 obtain formula z rd fx dx z sd1 z 0 frγ rd1 dr dσγ. consequence combine fact z rd frx dx z rd fx dx whenever r rotation identity 3 obtain 4 z sd1 frγ dσγ z sd1 fγ dσγ .
ibookroot october 20 2007 notes references seeley 29 gives elegant brief introduction fourier series fourier transform. authoritative text fourier series zygmund 36. applications fourier analysis variety topics see dym mckean 8 k orner 21. reader also consult book kahane lemari erieusset 20 contains many historical facts results related fourier series. chapter 1 citation taken letter fourier unknown correspondent probably lagrange see herivel 15. facts early history fourier series found sections iiii riemanns memoir 27. chapter 2 quote translation excerpt riemanns paper 27. proof littlewoods theorem problem 3 well related tauberian theorems see chapter 7 titchmarsh 32. chapter 3 citation translation passage dirichlets memoir 6. chapter 4 quote translated hurwitz 17. problem ray light reﬂecting inside square discussed chap ter 23 hardy wright 13. relationship diameter curve fourier coeﬃcients problem 1 explored pﬂuger 26. many topics concerning equidistribution sequences including results problems 2 3 taken kuipers niederreiter 22. chapter 5 citation free translation passage schwartz 28. topics ﬁnance see duﬃe 7 particular chapter 5 black scholes theory problems 1 2. results problems 4 5 6 worked john 19 wid der 34. problem 7 see chapter 2 wiener 35. original proof nowhere diﬀerentiability f1 problem 8 hardy 12. chapter 6 quote excerpt cormacks nobel prize lecture 5. wave equation well results problems 3 4 5 found chapter 5 folland 9. 298
ibookroot october 20 2007 notes references 299 discussion relationship rotational symmetry fourier transform bessel functions chapter 4 stein weiss 31. radon transform see chapter 1 john 18 helgason 14 ludwig 25. chapter 7 citation taken bingham tukey 2. proofs structure theorem ﬁnite abelian groups problem 2 found chapter 2 herstein 16 chapter 2 lang 23 chapter 104 k orner 21. problem 4 see andrews 1 contains short proof. chapter 8 citation bochner 3. divisor function see chapter 18 hardy wright 13. another elementary proof l1 χ 0 found chapter 3 gelfond linnik 11. alternate proof l1 χ 0 based algebraic number theory weyl 33. also two analytic variants proof l1 χ 0 found chapter 109 k orner 21 chapter 6 serre 30. see also latter reference problems 3 4. appendix details results integration reviewed appendix found folland 10 chapter 4 buck 4 chapter 4 lang 24 chap ter 20.
ibookroot october 20 2007 bibliography 1 g. e. andrews. number theory. dover publications new york 1994. corrected reprint 1971 originally published w.b. saunders company. 2 c. bingham j.w. tukey. fourier methods frequency analysis data. collected works john w tukey volume ii time series 19651984wadsworth advanced books software 1984. 3 s. bochner. role mathematics rise science. prince ton university press princeton nj 1966. 4 r. c. buck. advanced calculus. mcgrawhill new york third edition 1978. 5 a. m. cormack. nobel prize physiology medicine lecture volume volume 209. science 1980. 6 g. l. dirichlet. sur la convergence des s eries trigonometriques qui servent representer une fonction arbitraire entre des limites donn ees. crelle journal f ur die reine angewandte mathematik 4157169 1829. 7 d. duﬃe. dynamic asset pricing theory. princeton university press princeton nj 2001. 8 h. dym h. p. mckean. fourier series integrals. academic press new york 1972. 9 g. b. folland. introduction partial diﬀerential equations. princeton university press princeton nj 1995. 10 g. b. folland. advanced calculus. prentice hall englewood cliﬀs nj 2002. 11 a. o. gelfond yu. v. linnik. elementary methods analytic number theory. rand mcnally compagny chicago 1965. 12 g. h. hardy. weierstrasss nondiﬀerentiable function. transac tions american mathematical society 17301325 1916. 13 g. h. hardy e. m. wright. introduction theory numbers. oxford university press london ﬁfth edition 1979. 300
ibookroot october 20 2007 bibliography 301 14 s. helgason. radon transform euclidean spaces com pact twopoint homogeneous spaces grassman manifolds. acta. math. 113153180 1965. 15 j. herivel. joseph fourier man physicist. clarendon press oxford 1975. 16 i. n. herstein. abstract algebra. macmillan new york second edition 1990. 17 a. hurwitz. sur quelques applications g eometriques des s eries de fourier. annales de lecole normale sup erieure 193357408 1902. 18 f. john. plane waves spherical mean applied partial dif ferential equations. interscience publishers new york 1955. 19 f. john. partial diﬀerential equations. springerverlag new york fourth edition 1982. 20 j.p. kahane p. g. lemari erieusset. s eries de fourier et on delettes. cassini paris 1998. english version gordon breach 1995. 21 t. w. k orner. fourier analysis. cambridge university press cam bridge uk 1988. 22 l. kuipers h. niederreiter. uniform distribution sequences. wiley new york 1974. 23 s. lang. undergraduate algebra. springerverlag new york second edition 1990. 24 s. lang. undergraduate analysis. springerverlag new york sec ond edition 1997. 25 d. ludwig. radon transform euclidean space. comm. pure appl. math. 194981 1966. 26 a. pﬂuger. diameter planar curves fourier coeﬃ cients. colloquia mathematica societatis j anos bolyai functions series operators 35957965 1983. 27 b. riemann. ueber die darstellbarkeit einer function durch eine trigonometrische reihe. habilitation der universit zu g ottingen 1854. collected works springer verlag new york 1990.
ibookroot october 20 2007 302 bibliography 28 l. schwartz. th eorie des distributions volume volume i. hermann paris 1950. 29 r. t. seeley. introduction fourier series integrals. w. a. benjamin new york 1966. 30 j.p. serre. course arithmetic. gtm 7. springer verlag new york 1973. 31 e. m. stein g. weiss. introduction fourier analysis euclidean spaces. princeton university press princeton nj 1971. 32 e. c. titchmarsh. theory functions. oxford university press london second edition 1939. 33 h. weyl. algebraic theory numbers volume volume 1 annals mathematics studies. princeton university press princeton nj 1940. 34 d. v. widder. heat equation. academic press new york 1975. 35 n. wiener. fourier integral certain applications. cambridge university press cambridge uk 1933. 36 a. zygmund. trigonometric series volume volumes ii. cambridge university press cambridge uk second edition 1959. reprinted 1993.
ibookroot october 20 2007 symbol glossary page numbers right indicate ﬁrst time symbol notation deﬁned used. usual z q r c denote integers rationals reals complex numbers respectively. laplacian 20 185 z z absolute value complex conjugate 22 ez complex exponential 24 sinh x cosh x hyperbolic sine hyperbolic cosine 28 ˆ fn fourier coeﬃcient 34 fθ p aneinθ fourier series 34 snf partial sum fourier series 35 dn dr dn d n dirichlet kernel conjugate modiﬁed 37 95 165 pr py pd poisson kernels 37 149 210 o big little notation 42 62 ck functions k times dif ferentiable 44 f g convolution 44 139 184 239 σn σnf ces aro mean 52 53 fn fr fej er kernel 53 163 ar arf abel mean 54 55 χab characteristic function 61 fθ fθ onesided limits jump dis continuities 63 rd cd euclidean spaces 71 x y orthogonal vectors 72 ℓ2z square summable sequences 73 r riemann integrable functions 75 ζs zeta function 98 x x integer fractional parts 106 n σnk n delayed means 114 127 174 ht ht hd heat kernels 120 146 209 mr space functions moderate decrease r 131 303
ibookroot october 20 2007 304 symbol glossary ˆ fξ fourier transform 134 181 s sr srd schwartz space 134 180 r2 r2 upper halfplane closure 149 ϑs θzτ theta functions 155 156 γs gamma function 165 x x x y x norm inner product rd 71 176 xα α x α monomial order diﬀerential operator 176 s1 s2 sd1 unit circle r2 unit spheres r3 rd 179 180 mt f mt spherical mean 194 216 jn bessel function 197 213 p ptγ plane 202 r r radon dual radon transforms 203 205 ad vd area volume unit sphere rd 208 zn group n th roots unity 219 znz group integers modulo n 221 g g abelian group or der 226 228 g h isomorphic groups 227 g1 g2 direct product groups 228 zq group units modulo q 227 229 244 ˆ g dual group g 231 ab divides b 242 gcda b greatest common divisor b 242 ϕq number integers rela tively prime q 254 χ χ0 dirichlet character trivial dirichlet character 254 ls χ dirichlet lfunction 256 log1 ³ 1 1z log2 ls χ logarithms 258 264 dk number positive divi sors k 269
ibookroot october 20 2007 index relevant items also arise book listed index preceeded numeral i. abel means 54 summable 54 abelian group 226 absolute value 23 absorption coeﬃcient 199 amplitude 3 annihilation operator 169 approximation identity 49 attenuation coeﬃcient 199 bernoulli numbers 97 167 polynomials 98 bernsteins theorem 93 bessel function 197 bessels inequality 80 best approximation lemma 78 blackscholes equation 170 bump functions 162 cauchy problem wave equa tion 185 cauchy sequence 24 cauchyschwarz inequality 72 ces aro means 52 sum 52 summable 52 character 230 trivial unit 230 class ck 44 closed rectangle 289 complete vector space 74 complex conjugate 23 exponential 24 congruent integers 220 conjugate dirichlet kernel 95 convolution 44 139 239 coordinates spherical rd 293 creation operator 169 curve 102 area enclosed 103 closed 102 diameter 125 length 102 simple 102 dalemberts formula 11 delayed means 114 generalized 127 descent method of 194 dilations 133 177 direct product groups 228 dirichlet characters 254 complex 265 real 265 dirichlet kernel conjugate on circle 95 modiﬁed on real line 165 circle 37 dirichlet problem 305
ibookroot october 20 2007 306 index annulus 64 rectangle 28 strip 170 unit disc 20 dirichlet product formula 256 dirichlets test 60 dirichlets theorem 128 discontinuity jump 63 riemann integrable function 286 divisibility integers 242 divisor 242 greatest common 242 divisor function 269 280 dual xray transform 212 group 231 radon transform 205 eigenvalues eigenvectors 233 energy 148 187 string 90 equidistributed sequence 107 ergodicity 111 euclids algorithm 241 euler constant γ 268 identities 25 phifunction 254 276 product formula 249 even function 10 expectation 160 exponential function 24 exponential sum 112 fast fourier transform 224 fej er kernel circle 53 real line 163 fibonacci numbers 122 fourier coeﬃcient discrete 236 coeﬃcient 16 34 zn 223 ﬁnite abelian group 235 series 34 sine coeﬃcient 15 fourier inversion ﬁnite abelian group 235 zn 223 r 141 rd 182 fourier series 34 235 abel means 55 ces aro mean 53 delayed means 114 generalized delayed means 127 lacunary 114 partial sum 35 uniqueness 39 fourier series convergence mean square 70 pointwise 81 128 fourier series divergence 83 fourier transform 134 136 181 fractional part 106 function bessel 197 exponential 24 gamma 165 moderate decrease 131 179 294 radial 182 rapidly decreasing 135 178 sawtooth 60 83 schwartz 135 180 theta 155 zeta 98 gamma function 165
ibookroot october 20 2007 index 307 gaussian 135 181 gibbss phenomenon 94 good kernels 48 greatest common divisor 242 group abelian 226 cyclic 238 dual 231 homomorphism 227 isomorphic 227 integers modulo n 221 units 227 229 244 order 228 h older condition 43 harmonic function 20 mean value property 152 harmonics 13 heat equation 20 ddimensions 209 real line 146 steadystate 20 timedependent 20 heat kernel ddimensions 209 real line 146 156 circle 120 124 156 heisenberg uncertainty princi ple 158 168 209 hermite functions 173 operator 168 hermitian inner product 72 hilbert space 75 homomorphism 227 hookes law 2 huygens principle 193 hyperbolic cosine sine func tions 28 hyperbolic sums 269 inner product 71 hermitian 72 strictly positive deﬁnite 71 integer part 106 integrable function riemann 31 281 inverse linear operator 177 isoperimetric inequality 103 122 jump discontinuity 63 kirchhoﬀs formula 211 lfunction 256 landau kernels 164 laplace operator 20 laplacian 20 149 185 polar coordinates 27 legendre expansion 96 polynomial 95 light cone backward 193 213 forward 193 linearity 6 22 lipschitz condition 82 logarithm log1 258 log2 264 mean value propery 152 measure zero 287 moderate decrease function 131 modulus 23 monomial 176 multiindex 176 multiplication formula 140 183 natural frequency 3 newtons law 3 cooling 19 nowhere diﬀerentiable function 113 126
ibookroot october 20 2007 308 index odd function 10 order group 228 orthogonal elements 72 orthogonality relations 232 orthonormal family 77 oscillation of function 288 overtones 6 13 parametrization arclength 103 reverse 121 parsevals identity 79 ﬁnite abelian group 236 part fractional 106 integer 106 partition rectangle 290 interval 281 period 3 periodic function 10 periodization 153 phase 3 plancherel formula zn 223 r 143 rd 182 plancks constant 161 plucked string 17 poincar es inequality 90 poisson integral formula 57 poisson kernel ddimensions 210 unit disc 37 55 upper halfplane 149 poisson kernelscomparison 157 poisson summation formula 154156 165 174 polar coordinates 179 polynomials bernoulli 98 legendre 95 prehilbert space 75 prime number 242 primes arithmetic progres sion 245 252 275 probability density 160 proﬁle 5 pure tones 6 pythagorean theorem 72 radon transform 200 203 rapid decrease 135 reﬁnement partition 281 290 relatively prime integers 242 repeated integrals 295 reuleaux triangle 125 riemann integrable function 31 281 290 riemann localization principle 82 riemannlebesgue lemma 80 root unity 219 rotation 177 improper 177 proper 177 sawtooth function 60 83 84 94 99 278 scaling 8 schwartz space 134 180 separation variables 4 11 simple harmonic motion 2 space variables 185 spectral theorem 233 commuting family 233 speed propagation ﬁnite 194 spherical coordinates rd 293 mean 189 wave 210 spring constant 3 standing wave 4
ibookroot october 20 2007 index 309 subordination principle 210 subrectangle 290 summable abel 54 ces aro 52 summation parts 60 superposition 6 14 symmetrybreaking 83 theta function 155 functional equation 155 tone fundamental 13 pure 11 translations 133 177 transpose linear operator 177 traveling wave 4 trigonometric polynomial 35 degree 35 series 35 tychonoﬀs uniqueness theo rem 172 uncertainty 160 unit 229 unitary transformation 143 233 variance 160 vector space 70 velocity wave 5 velocity motion 7 vibrating string 90 wave standing 4 13 traveling 4 velocity 5 wave equation 184 ddimensional 185 dalemberts formula 11 linear 9 onedimensional 7 time reversal 11 weierstrass approximation theo rem 54 63 144 163 weyl criterion 112 123 estimate 125 theorem 107 wirtingers inequality 90 122 xray transform 200 zeta function 98 155 166 248
10 5 0 5 10 10 5 0 5 10 0.4 0.2 0 0.2 0.4 0.6 0.8 1 z x z corrals vector calculus michael corral anton petrunin

corrals vector calculus michael corral anton petrunin
author michael corral adjunct faculty member department mathematics schoolcraft college. received b.a. mathematics university california berkeley received m.a. mathematics m.s. industrial operations engineering university michigan. text typeset l ex 2ε komascript bundle using gnu emacs text editor fedora linux system. graphics created using metapost pgf gnuplot. copyright 2016 anton petrunin. permission granted copy distribute andor modify document terms gnu free documentation license version 1.2 later version published free software foundation invariant sections frontcover texts backcover texts. copy license included section entitled gnu free documentation license.
preface book covers calculus two three variables. suitable onesemester course normally known vector calculus multivariable calculus simply calculus iii. prerequisites standard courses singlevariable calculus also known cal culus ii. exercises divided three categories a b c. exercises mostly routine computational nature b exercises slightly involved c exercises usually require effort insight solve. crude way describing a b c would easy moderate challenging respectively. however many b exercises easy c exercises difﬁcult. answers hints oddnumbered evennumbered exercises provided appendix a. exercises require student write computer program ex ample monte carlo method approximating multiple integrals section 4.4. code samples text java programming language hopefully enough comments reader ﬁgure done even without knowing java. exercises mandate use java students free implement solu tions using language choice. would simple use scripting language like python perhaps even easier functional programming language such haskell scheme java chosen due ubiquity relatively clear syntax easy availability multiple platforms. book released gnu free documentation license gfdl allows others copy distribute book also modify it. details see included copy gfdl. ambiguity matter anyone make many copies book desired distribute desired without needing permission. book downloaded older original version michael corral also obtained iii
contents preface iii 1 vectors euclidean space 1 1.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 vector algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.3 dot product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 1.4 cross product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 1.5 lines planes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 1.6 elementary surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 1.7 curvilinear coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 2 curves 56 2.1 vectorvalued functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 2.2 arc length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 2.3 curvature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 3 functions several variables 74 3.1 functions two three variables . . . . . . . . . . . . . . . . . . . . . . . . . . 74 3.2 partial derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 3.3 tangent plane surface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 3.4 directional derivatives gradient . . . . . . . . . . . . . . . . . . . . . . . 87 3.5 maxima minima . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 3.6 numerical methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 3.7 lagrange multipliers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 4 multiple integrals 114 4.1 double integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 4.2 double integrals general region . . . . . . . . . . . . . . . . . . . . . . . . 119 4.3 triple integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 4.4 numerical approximation multiple integrals . . . . . . . . . . . . . . . . . . . 130 4.5 change variables multiple integrals . . . . . . . . . . . . . . . . . . . . . . . 135 4.6 application center mass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 4.7 application probability expected value . . . . . . . . . . . . . . . . . . . . . 147 5 line surface integrals 155 5.1 line integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 iv
contents v 5.2 properties line integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 5.3 greens theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172 5.4 surface integrals divergence theorem . . . . . . . . . . . . . . . . . . . . 179 5.5 stokes theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 5.6 gradient divergence curl laplacian . . . . . . . . . . . . . . . . . . . . . . . 202 5.7 coordinate systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207 bibliography 213 appendix a answers hints selected exercises 215 gnu free documentation license 218 history 226 index 227

1 vectors euclidean space 1.1 introduction singlevariable calculus functions one encounters functions variable usually x t varies subset real number line which denote r. function say f x graph function f consists points x y x f x. points lie euclidean plane which cartesian rectangular coordinate system consists ordered pairs real numbers ab. use word euclidean denote system usual rules euclidean geometry hold. denote euclidean plane r2 2 represents number dimensions plane. euclidean plane two perpendicular coordinate axes xaxis yaxis. vector or multivariable calculus deal functions two three variables usually x x y z respectively. graph function two variables say z f x y lies euclidean space cartesian coordinate system consists ordered triples real numbers ab c. since euclidean space 3dimensional denote r3. graph f consists points x y z x y f x y. 3dimensional coordinate system euclidean space represented ﬂat surface page black board giving illusion three dimensions manner shown figure 1.1.1. euclidean space three mutually perpendicular coordinate axes x z three mutually perpendicular coordinate planes xyplane yzplane xzplane see figure 1.1.2. x z 0 pab c b c figure 1.1.1 x z 0 yzplane xyplane xzplane figure 1.1.2 coordinate system shown figure 1.1.1 known righthanded coordinate system possible using right hand point index ﬁnger positive 1
2 chapter 1. vectors euclidean space direction xaxis middle ﬁnger positive direction yaxis thumb positive direction zaxis figure 1.1.3. x z 0 figure 1.1.3 righthanded coordinate system. equivalent way deﬁning righthanded system point thumb up wards positive zaxis direction using remaining four ﬁngers rotate xaxis towards yaxis. thing left hand deﬁnes left handed coordinate system. notice switching x yaxes righthanded system results lefthanded system rotating either type system change handedness. throughout book use righthanded system. functions three variables graphs exist 4dimensional space r4 see 3dimensional space let alone simulate 2dimensional space. think 4dimensional space abstractly. entertaining discussion subject see book abbott.1 far discussed position object 2dimensional 3dimensional space. something velocity object acceleration gravitational force acting object phenomena seem involve motion direction way. idea vector comes in. already dealt velocity acceleration singlevariable calculus. example motion along straight line f t gives displacement object time t dydt f t velocity object time t. derivative f t 1one thing learn 4dimensional creature would able reach inside egg remove yolk without cracking shell
1.1 introduction 3 number positive object moving agreedupon positive direction negative moves opposite direction. think number called velocity object two components magnitude indicated nonnegative number preceded direction indicated plus minus symbol representing motion positive direction negative direction respectively is f t a number 0. magnitude velocity normally called speed object represents direction velocity though usually omitted positive direction. motion along straight line which 1dimensional space velocities also contained 1dimensional space since numbers. general motion along curve 2 3dimensional space however velocity need represented multidimensional object magnitude direction. geomet ric object features arrow elementary geometry called directed line segment. motivation deﬁne vector. deﬁnition 1.1. nonzero vector directed line segment drawn point p called initial point point q called terminal point p q distinct points. vector denoted pq. magnitude length line segment denoted pq direction directed line segment. zero vector point denoted 0. indicate direction vector draw arrow initial point terminal point. often denote vector single boldfaced letter for instance v use terms magnitude length interchangeably. note deﬁnition could apply systems number dimensions see figure 1.1.4 ac. 0 x p q r pq rs a one dimension x 0 p q r pq rs v b two dimensions x z 0 p q r pq rs v c three dimensions figure 1.1.4 vectors different dimensions. things need noted zero vector. motivation vector included notions magnitude direction. magnitude zero vector deﬁne zero is 0 0. agrees deﬁnition zero vector point zero length. direction zero vector single point really welldeﬁned direction. notice careful deﬁne
4 chapter 1. vectors euclidean space direction nonzero vector welldeﬁned since initial terminal points distinct. everyone agrees direction zero vector. contend zero vector arbitrary direction say indeterminate direction that is direction determined others say direction. deﬁnition zero vector however require direction leave that.2 know vector is need way determining two vectors equal. leads us following deﬁnition. deﬁnition 1.2. two nonzero vectors equal magnitude direction. vector zero magnitude equal zero vector. deﬁnition vectors magnitude direction different initial points would equal. example figure 1.1.5 vectors u v w magnitude p 5 by pythagorean theorem. see u w parallel since lie lines slope 1 2 point direction. u w even though different initial points. also see v parallel u points opposite direction. u v. 1 2 3 4 1 2 3 4 x 0 u v w figure 1.1.5 see inﬁnite number vectors given magnitude direction vectors equal differing initial terminal points. single vector choose represent equal vectors answer yes suggested vector w figure 1.1.5. unless otherwise indicated speaking the vector given magnitude direction mean one whose initial point origin coordinate system. thinking vectors starting origin provides way dealing vectors standard way since every coordinate system origin. times 2in subject linear algebra abstract way deﬁning vector concept direction really used. see anton rorres.
1.1 introduction 5 convenient consider different initial point vector for example adding vectors next section. another advantage using origin initial point provides natural correspondence vector terminal point. example 1.1. let v vector r3 whose initial point origin whose ter minal point 345. though point 345 vector v different objects convenient write v 345. this understood initial point v origin 000 terminal point 345. x z 0 p345 a point 345 x z 0 v 345 b vector 345 figure 1.1.6 correspondence points vectors. unless otherwise stated refer vectors v ab r2 v ab c r3 mean vectors cartesian coordinates starting origin. also write zero vector 0 r2 r3 00 000 respectively. pointvector correspondence provides way check two vectors equal without determine magnitude direction. similar seeing two points same seeing terminal points vectors starting origin same. vector ﬁnd unique vector equals whose initial point origin. compare coordinates terminal points new vectors coordinates same original vectors equal. get new vectors starting origin translate vector start origin subtracting coordinates original initial point original terminal point. resulting point terminal point new vector whose initial point origin. original vector compare. example 1.2. consider vectors pq rs r3 p 215q 357r 132 210. pq rs solution vector pq equal vector v initial point 000 terminal point q p 357215 325175 142. similarly rs equal vector w initial point 000 terminal point r 210132 211302 142.
6 chapter 1. vectors euclidean space pq v 142 rs w 142. pq rs z x 0 pq rs translate pq v translate rs w p 215 q 357 r 132 210 142 v w figure 1.1.7 recall distance formula points euclidean plane points p x1 y1 q x2 y2 r2 distance p q is q x2 x12 y2 y12. 1.1 formula following result vector pq r2 initial point p x1 y1 terminal point q x2 y2 magnitude pq is pq q x2 x12 y2 y12. 1.2 finding magnitude vector v ab r2 special case formula 1.2 p 00 q ab vector v ab r2 magnitude v is v p a2 b2. 1.3 calculate magnitude vectors r3 need distance formula points euclidean space we postpone proof next section
1.1 introduction 7 theorem 1.1. distance points p x1 y1 z1 q x2 y2 z2 r3 is q x2 x12 y2 y12 z2 z12. 1.4 proof use following result theorem 1.2. vector v ab c r3 magnitude v is v p a2 b2 c2. 1.5 proof four cases consider case 1 b c 0. v 0 v 0 p 02 02 02 p a2 b2 c2. case 2 exactly two ab c 0. without loss generality assume b 0 c 0 the two possibilities handled similar manner. v 00 c vector length c along zaxis. v c p c2 p 02 02 c2 p a2 b2 c2. case 3 exactly one ab c 0. without loss generality assume 0 b 0 c 0 the two possibilities handled similar manner. v 0b c vector yzplane pythagorean theorem v p b2 c2 p 02 b2 c2 p a2 b2 c2. x z 0 qab c p r b c v figure 1.1.8 case 4 none ab c 0. without loss generality as sume ab c positive the seven possibilities handled similar manner. consider points p 000 q ab c r ab0 a00 shown figure 1.1.8. applying pythagorean theorem right trian gle psr gives pr2 a2 b2. second application pythagorean theorem time right triangle pqr gives v pq q pr2 qr2 p a2 b2 c2. proves theorem. qed example 1.3. calculate following a magnitude vector pq r2 p 12 q 55. solution formula 1.2 pq p 512 522 p 369 p 45 3 p 5. b magnitude vector v 83 r2. solution formula 1.3 v p 82 32 p 73. c distance points p 214 q 423 r2. solution formula 1.4 distance p 422 212 342 p 4949 p 62.
8 chapter 1. vectors euclidean space d magnitude vector v 582 r3. solution formula 1.5 v p 52 82 22 p 25644 p 93. exercises 1. calculate magnitudes following vectors a v 21 b v 210 c v 322 d v 001 e v 644. 2. points p 111 q 222 r 201 312 pq rs 3. points p 000 q 132 r 101 234 pq rs b 4. let v 100 w a00 vectors r3. show w av. 5. let v ab c w 3a3b3c vectors r3. show w 3v. c x z 0 px1 y1 z1 qx2 y2 z2 rx2 y2 z1 sx1 y10 tx2 y20 ux2 y10 figure 1.1.9 6. though see simple proof theorem 1.1 next section possible prove using methods similar proof theorem 1.2. prove special case theorem 1.1 points p x1 y1 z1 q x2 y2 z2 satisfy fol lowing conditions x2 x1 0 y2 y1 0 z2 z1 0. hint think case 4 proof theorem 1.2 consider figure 1.1.9.
1.2 vector algebra 9 1.2 vector algebra know vectors are start perform usual algebraic operations including addition subtraction. that introduce notion scalar. deﬁnition 1.3. scalar quantity represented single number. purposes scalars always real numbers.3 examples scalar quantities mass electric charge speed not velocity.4 deﬁne scalar multiplication vector. deﬁnition 1.4. scalar k nonzero vector v scalar multiple v k denoted kv vector whose magnitude kv points direction v k 0 points opposite direction v k 0 zero vector 0 k 0. zero vector 0 deﬁne k0 0 scalar k. two vectors v w parallel denoted v w one scalar multiple other. think scalar multiplication vector stretching shrinking vector ﬂipping vector opposite direction scalar negative number see figure 1.2.1. v 2v 3v 0.5v v 2v figure 1.2.1 recall translating nonzero vector means initial point vector changed magnitude direction preserved. ready deﬁne sum two vectors. deﬁnition 1.5. sum vectors v w denoted vw obtained translating w initial point terminal point v initial point vw initial point v terminal point new terminal point w. 3the term scalar invented 19th century irish mathematician physicist astronomer william rowan hamilton convey sense something could represented point scale graduated ruler. word vector comes latin means carrier. 4an alternate deﬁnition scalars vectors used physics certain types coordinate trans formations for example rotations quantity affected scalar quantity affected in certain way vector. see marion details.
10 chapter 1. vectors euclidean space intuitively adding w v means tacking w end v see figure 1.2.2. v w a vectors v w v w b translate w end v v w vw c sum vw figure 1.2.2 adding vectors v w. notice deﬁnition valid zero vector which point hence translated see v0 v 0v vector v. particular 00 0. also easy see v v 0 would expect. general since scalar multiple v 1v welldeﬁned vector deﬁne vector subtraction follows vw vw. see figure 1.2.3. v w a vectors v w v w b translate w end v v w vw c difference vw figure 1.2.3 subtracting vectors v w. figure 1.2.4 shows use geometric proofs various laws vector algebra is uses laws elementary geometry prove statements vectors. example a shows vw wv vectors v w. c shows think vw vector tacked end w add v. v v w w wv vw a add vectors w w vw vw v b subtract vectors v w vw vw c combined addsubtract figure 1.2.4 geometric vector algebra. notice temporarily abandoned practice starting vectors origin. fact even mentioned coordinates section far. since deal mostly cartesian coordinates book following two theorems useful performing vector algebra vectors r2 r3 starting origin.
1.2 vector algebra 11 theorem 1.3. let v v1v2 w w1w2 vectors r2 let k scalar. a kv kv1kv2 b vw v1 w1v2 w2. proof a without loss generality assume v1v2 0 the possibilities handled similar manner. k 0 kv 0v 0 00 0v10v2 kv1kv2 needed show. k 0 kv1kv2 lies line slope kv2 kv1 v2 v1 slope line v and hence kv lies kv1kv2 points direction line kv. also formula 1.3 magnitude kv1kv2 p kv12 kv22 q k2v2 1 k2v2 2 q k2v2 1 v2 2 k q v2 1 v2 2 kv. kv kv1kv2 magnitude direction. proves a. x 0 w2 v2 w1 v1 v1 w1 v2 w2 w2 w1 v v w w vw figure 1.2.5 b without loss generality assume v1v2w1w2 0 the possibilities han dled similar manner. figure 1.2.5 see translating w start end v new terminal point w v1w1v2w2 deﬁnition vw must ter minal point vw. proves b. qed theorem 1.4. let v v1v2v3 w w1w2w3 vectors r3 let k scalar. a kv kv1kv2kv3 b vw v1 w1v2 w2v3 w3. following theorem summarizes basic laws vector algebra. theorem 1.5. vectors u v w scalars kl a vw wv commutative law b uvw uvw associative law c v0 v 0v additive identity d vv 0 additive inverse e klv klv associative law f kvw kv kw distributive law g k lv kv lv distributive law. proof a already presented geometric proof figure 1.2.4a. b illustrate difference analytic proofs geometric proofs vector alge bra present types here. analytic proof use vectors r3 the proof r2 similar.
12 chapter 1. vectors euclidean space let u u1u2u3 v v1v2v3 w w1w2w3 vectors r3. uvw u1u2u3v1v2v3w1w2w3 u1u2u3v1 w1v2 w2v3 w3 theorem 1.4b u1 v1 w1u2 v2 w2u3 v3 w3 theorem 1.4b u1 v1 w1u2 v2 w2u3 v3 w3 properties real numbers u1 v1u2 v2u3 v3w1w2w3 theorem 1.4b uvw completes analytic proof b. figure 1.2.6 provides geometric proof. u v w uv vw uvw uvw figure 1.2.6 associative law vector addition c already discussed p.10. d already discussed p.10. e prove vector v v1v2v3 r3 the proof r2 similar klv klv1lv2lv3 theorem 1.4a klv1klv2klv3 theorem 1.4a klv1v2v3 theorem 1.4a klv f g left exercises reader. qed unit vector vector magnitude 1. notice nonzero vector v vector v vis unit vector points direction v since 1 v 0 v v v v 1. dividing nonzero vector v vis often called normalizing v. speciﬁc unit vectors often use called basis vectors 100 j 010 k 001 r3 10 j 01 r2. useful several reasons mutually perpendicular since lie distinct coordinate axes unit vectors i j k 1 every vector written unique scalar combination basis vectors v ab ai bj r2
1.2 vector algebra 13 v ab c ai bj ck r3. see figure 1.2.7. 1 2 1 2 x 0 j a r2 x 0 ai bj v ab b v ai bj 1 2 1 2 1 2 x z 0 j k c r3 x z 0 ai bj ck v ab c d v ai bj ck figure 1.2.7 basis vectors different dimensions. vector v ab c written v ai bj ck say v component form a b c i j k components respectively v. have v v1 i v2 j v3 kk scalar kv kv1 i kv2 j kv3 k v v1 i v2 j v3 kw w1 i w2 j w3 k vw v1 w1iv2 w2jv3 w3k v v1 i v2 j v3 k v q v2 1 v2 2 v2 3 . example 1.4. let v 211 w 342 r3. a find vw. solution vw 231412 153. b find 3v2w. solution 3v2w 633684 1251. c write v w component form. solution v 2ijk w 3i4j2k. d find vector u uv w. solution theorem 1.5 u wv vw 153 153 parta. e find vector u uvw 0. solution theorem 1.5 u wv 342211 531. f find vector u 2ui2j k. solution 2u i2jk u 1 2 ij 1 2 k. g find unit vector v v. solution v v 1 p 221212 211 ³ 2 p 6 1 p 6 1 p 6 .
14 chapter 1. vectors euclidean space easily prove theorem 1.1 previous section. distance two points p x1 y1 z1 q x2 y2 z2 r3 length vector wv vectors v w deﬁned v x1 y1 z1 w x2 y2 z2 see figure 1.2.8. since wv x2x1 y2y1 z2z1 wv p x2 x12 y2 y12 z2 z12 theorem 1.2. x z 0 px1 y1 z1 qx2 y2 z2 v w wv figure 1.2.8 proof theorem 1.2 wv. exercises 1. let v 152 w 311. a find vw. b find vw. c find v v. d find 1 2vw . e find 1 2vw . f find 2v4w. g find v2w. h find vector u uvw i. i find vector u uvw 2jk. j scalar mv2w k so ﬁnd it. 2. vectors v w exercise 1 vw vw not quantity larger 3. vectors v w exercise 1 vw vw not quantity larger b 4. prove theorem 1.5f r3. 5. prove theorem 1.5g r3. c 6. know every vector r3 written scalar combination vectors i j k. every vector r3 written scalar combination j vector v r3 scalars m n v mi nj justify answer.
1.3 dot product 15 1.3 dot product may noticed deﬁne multiplication vector scalar previous section vector algebra deﬁne multiplication vector vector. see one type multiplication vectors called dot product. deﬁnition 1.6. let v v1v2v3 w w1w2w3 vectors r3. dot product v w denoted v w given by v w v1w1 v2w2 v3w3. 1.6 similarly vectors v v1v2 w w1w2 r2 dot product is v w v1w1 v2w2. 1.7 notice dot product two vectors scalar vector. associative law holds multiplication numbers addition vectors see theorem 1.5be hold dot product vectors. why vectors u v w dot product u v scalar u v w deﬁned since left side dot product the part parentheses scalar vector. vectors v v1 iv2 jv3 k w w1 iw2 jw3 k component form dot product still v w v1w1 v2w2 v3w3. also notice deﬁned dot product analytic way is referencing vector coordinates. geometric way deﬁning dot product develop consequence analytic deﬁnition. deﬁnition 1.7. angle two nonzero vectors initial point smallest angle them. deﬁne angle zero vector vector. two nonzero vectors initial point two angles them θ 360θ. always choose smallest nonnegative angle θ them 0θ 180. see figure 1.3.1. θ 360θ a 0 θ 180 θ 360θ b θ 180 θ 360θ c θ 0 figure 1.3.1 angle vectors. take geometric view dot product establishing relationship dot product two vectors angle them.
16 chapter 1. vectors euclidean space theorem 1.6. let v w nonzero vectors let θ angle them. cosθ v w vw 1.8 prove theorem assuming notion angle well law cosines known. rigorous approach one could deﬁne angles vectors using statement theorem above. proof prove theorem vectors r3 the proof r2 similar. let v v1v2v3 w w1w2w3. law cosines see figure 1.3.2 vw2 v2 w2 2vwcosθ. 1.9 note equation 1.9 holds even degenerate cases θ 0and 180. θ x z 0 v w vw figure 1.3.2 since vw v1 w1v2 w2v3 w3 expanding vw2 equation 1.9 gives v2 w2 2vwcosθ v1 w12 v2 w22 v3 w32 v2 1 2v1w1 w2 1 v2 2 2v2w2 w2 2 v2 3 2v3w3 w2 3 v2 1 v2 2 v2 3 w2 1 w2 2 w2 3 2v1w1 v2w2 v3w3 v2 w2 2v w 2vwcosθ 2v w since v 0 w 0 cosθ v w vw. qed example 1.5. find angle θ vectors v 211 w 341. solution since v w 231411 1 v p 6 w p 26 cosθ v w vw 1 p 6 p 26 1 2 p 39 0.08 θ 85.41. two nonzero vectors perpendicular angle 90. since cos90 0 following important corollary theorem 1.6
1.3 dot product 17 corollary 1.7. two nonzero vectors v w perpendicular v w 0. write v w indicate v w perpendicular. since 0 w 0 convenient assume zero vector 0 perpendicular vector. write 0 w despite angle 0 w undeﬁned. since cosθ 0 0θ 90and cosθ 0 90 θ 180 also have corollary 1.8. θ angle nonzero vectors v w v w 0 0θ 90 0 θ 90 0 90 θ 180. corollary 1.8 dot product thought way telling angle be tween two vectors acute obtuse right angle depending whether dot product positive negative zero respectively. see figure 1.3.3. 0θ 90 v w a v w 0 90 θ 180 v w b v w 0 θ 90 v w c v w 0 figure 1.3.3 sign dot product angle vectors. example 1.6. vectors v 152 w 311 perpendicular solution yes v w since v w 135121 0. following theorem summarizes basic properties dot product. theorem 1.9. vectors u v w scalar k a v w w v commutative law b kv w v kw kv w associative law c v 0 0 0 v d u vw u vu w distributive law e uv w u wv w distributive law f v w vw cauchyschwarz inequality.5 5also known cauchyschwarzbuniakovski inequality.
18 chapter 1. vectors euclidean space proof proofs parts ae straightforward applications deﬁnition dot product left reader exercises. prove part f. f either v 0 w 0 v w 0 part c inequality holds trivially. assume v w nonzero vectors. theorem 1.6 v w cosθvw v w cosθvw v w vwsince cosθ 1. qed using theorem 1.9 see u v 0 u w 0 u kv lw ku v lu w k0 l0 0 scalars kl. thus following fact u v u w u kv lw scalars kl. vectors v w collection scalar combinations kv lw called span v w. nonzero vectors v w parallel span line parallel span plane. showed vector perpendicular two vectors also perpendicular span. dot product used derive properties magnitudes vectors important triangle inequality given following theorem theorem 1.10. vectors v w a v2 v v b vwvw triangle inequality c vwvw. proof a left exercise reader. b part a theorem 1.9 vw2 vw vw v vv ww vw w v2 2v ww2 since a real number a v2 2v ww2 theorem 1.9f v2 2vww2 vw2 vwvwafter taking square roots sides proves b. c since v wvw v wvwwvwby triangle inequality subtracting wfrom sides gives vwvw. qed
1.3 dot product 19 v w vw figure 1.3.4 triangle inequality gets name fact triangle one side longer sum lengths two sides see figure 1.3.4. another way saying familiar statement the shortest distance two points straight line. exercises 1. let v 512 w 443. calculate v w. 2. let v 3i2jk w 6i4j2k. calculate v w. exercises 38 ﬁnd angle θ vectors v w. 3. v 512 w 443 4. v 7210 w 264 5. v 214 w 120 6. v 421 w 842 7. v i2jk w 3i6j3k 8. v i w 3i2j4k. 9. let v 843 w 214. v w justify answer. 10. let v 604 w 021. v w justify answer. 11. v w exercise 5 verify cauchyschwarz inequality v w vw. 12. v w exercise 6 verify cauchyschwarz inequality v w vw. 13. v w exercise 5 verify triangle inequality vwvw. 14. v w exercise 6 verify triangle inequality vwvw. b 15. prove theorem 1.9a. 16. prove theorem 1.9b. 17. prove theorem 1.9c. 18. prove theorem 1.9d. 19. prove theorem 1.9e. 20. prove theorem 1.10a. 21. prove give counterexample u v u w v w. c 22. prove give counterexample v w 0 v w 0. 23. prove give counterexample u v u w u v w. 24. prove vw vwfor v w.
20 chapter 1. vectors euclidean space l w v u figure 1.3.5 25. nonzero vectors v w projection v onto w some times written pro jwv vector u along line l w whose terminal point obtained dropping perpendic ular line terminal point v l see figure 1.3.5. show u v w w. hint consider angle v w. 26. assume v w. show vw vw. 27. let α β γ angles nonzero vector v r3 vectors i j k respectively. show cos2 α cos2 β cos2 γ 1. the angles α β γ often called direction angles v cosα cosβ cosγ called direction cosines.
1.4 cross product 21 1.4 cross product section 1.3 deﬁned dot product gave way multiplying two vectors. resulting product however scalar vector. section deﬁne product two vectors result another vector. product called cross product deﬁned vectors r3. deﬁnition may appear strange lacking motivation see geometric basis shortly. deﬁnition 1.8. let v v1v2v3 w w1w2w3 vectors r3. cross product v w denoted v w vector r3 given by v w v2w3 v3w2v3w1 v1w3v1w2 v2w1. 1.10 1 1 1 x z 0 j k i j figure 1.4.1 example 1.7. find i j. solution since 100 j 010 i j 000100101100 001 k. similarly shown j k k i j. example cross product given vectors perpendicular vectors. turns always case. theorem 1.11. cross product v w two nonzero vectors v w also nonzero vector perpendicular v w. proof show v w v 0 v w v v2w3 v3w2v3w1 v1w3v1w2 v2w1 v1v2v3 v2w3v1 v3w2v1 v3w1v2 v1w3v2 v1w2v3 v2w1v3 v1v2w3 v1v2w3 w1v2v3 w1v2v3 v1w2v3 v1w2v3 0 rearranging terms. v w v corollary 1.7. proof v w w similar. qed consequence theorem theorem 1.9 following corollary 1.12. cross product v w two nonzero vectors v w also nonzero vector perpendicular span v w.
22 chapter 1. vectors euclidean space span two nonzero nonparallel vectors v w r3 plane p corollary shows v w perpendicular plane. shown figure 1.4.2 two possible directions v w one opposite other. choice direction v w visualized using righthand rule is vectors v w v w form righthanded system. recall section 1.1 means point thumb upwards direction v w rotating v towards w remaining four ﬁngers. x z 0 θ v w v w v w p figure 1.4.2 direction v w. derive formula magnitude v w nonzero vectors v w v w2 v2w3 v3w22 v3w1 v1w32 v1w2 v2w12 v2 2 w2 3 2v2w2v3w3 v2 3 w2 2 v2 3 w2 1 2v1w1v3w3 v2 1 w2 3 v2 1 w2 2 2v1w1v2w2 v2 2 w2 1 v2 1 w2 2 w2 3 v2 2 w2 1 w2 3 v2 3 w2 1 w2 2 2v1w1v2w2 v1w1v3w3 v2w2v3w3 adding subtracting v2 1 w2 1 v2 2 w2 2 v2 3 w2 3 right side gives v2 1 w2 1 w2 2 w2 3 v2 2 w2 1 w2 2 w2 3 v2 3 w2 1 w2 2 w2 3 v2 1 w2 1 v2 2 w2 2 v2 3 w2 3 2v1w1v2w2 v1w1v3w3 v2w2v3w3 v2 1 v2 2 v2 3 w2 1 w2 2 w2 3 v1w12 v2w22 v3w32 2v1w1v2w22v1w1v3w32v2w2v3w3 using a b c2 a2 b2 c2 2ab 2ac 2bc subtracted term gives v2 1 v2 2 v2 3 w2 1 w2 2 w2 3 v1w1 v2w2 v3w32 v2 w2 v w2 v2 w2³ 1 v w2 v2 w2 since v 0 w 0 theorem 1.6 v2 w21cos2 θ θ angle v w v w2 v2 w2 sin2 θ since 0θ 180 sinθ 0 have
1.4 cross product 23 θ angle nonzero vectors v w r3 v w vwsinθ. 1.11 may seem strange bother formula magnitude cross product calculated directly like vector. formula useful applications geometry following example. example 1.8. let pqr pqrs triangle parallelogram respectively shown figure 1.4.3. b h h θ θ p p q q r r v w figure 1.4.3 think triangle existing r3 identify sides qr qp vectors v w respectively r3. let θ angle v w. area apqr pqr 1 2bh b base triangle h height. see b v h wsinθ apqr 1 2 vwsinθ 1 2 v w. since area apqrs parallelogram pqrs twice area triangle pqr apqrs vwsinθ. discussion example 1.8 proved following theorem theorem 1.13. area triangles parallelograms a area triangle adjacent sides v w as vectors r3 is 1 2 v w b area parallelogram adjacent sides v w as vectors r3 is v w.
24 chapter 1. vectors euclidean space may seem ﬁrst glance since formulas derived example 1.8 adjacent sides qp qr only general statements theorem 1.13 formulas hold adjacent sides justiﬁed. would get different formula area picked pq pr adjacent sides shown see exercise 26 different formulas would yield value choice adjacent sides indeed matter theorem 1.13 valid. theorem 1.13 makes simpler calculate area triangle 3dimensional space using traditional geometric methods. example 1.9. calculate area triangle pqr p 247 q 3718 r 5128. z x 0 v w r5128 q3718 p247 figure 1.4.4 solution let v pq w pr figure 1.4.4. v 3718247 1325 w 5128247 7815 area triangle pqr 1 2 v w 1 2 1325 7815 1 2 3152582571151837 1 2 15519029 1 2 p 1552 1902 292 1 2 p 60966. 123.46. example 1.10. calculate area parallelogram pqrs p 11 q 23 r 54 42.
1.4 cross product 25 x 0 1 2 3 4 1 2 3 4 5 p q r v w figure 1.4.5 solution let v sp w sr figure 1.4.5. v 1142 31 w 5442 12. vectors r2 cross product de ﬁned vectors r3. however r2 thought subset r3 zcoordinate always 0. write v 310 w 120. area parallelogram pqrs v w 310 120 100201303211 005 . 5. following theorem summarizes basic properties cross product. theorem 1.14. vectors u v w r3 scalar k a v w w v anticommutative law b u vw u vu w distributive law c uv w u wv w distributive law d kv w v kw kv w associative law e v 0 0 0 v f v v 0 g v w 0 v w proof proofs properties bf straightforward. prove parts a g leave rest reader exercises. x z 0 v w v w w v figure 1.4.6 a deﬁnition cross product scalar multipli cation have v w v2w3 v3w2v3w1 v1w3v1w2 v2w1 v3w2 v2w3v1w3 v3w1v2w1 v1w2 w2v3 w3v2w3v1 w1v3w1v2 w2v1 w v
26 chapter 1. vectors euclidean space note says v w w v mag nitude opposite direction see figure 1.4.6. g either v w 0 v w 0 part e either v 0 0w w 0 0v v w scalar multiples case parallel. v w nonzero θ angle them formula 1.11 v w 0 vwsinθ 0 true sinθ 0 since v 0 w 0. since 0θ 180 sinθ 0 θ 0or 180. angle v w 0or 180if v w. qed example 1.11. adding example 1.7 i j k j k i k i j j i k k j i i k j i i j j k k 0. recall parallelepiped 3dimensional solid 6 faces parallel ograms.6 example 1.12. volume parallelepiped let vectors u v w r3 represent adjacent sides parallelepiped p u v w forming righthanded system figure 1.4.7. show volume p scalar triple product u v w. h θ u w v v w figure 1.4.7 parallelepiped p solution recall volume volp par allelepiped p area base parallel ogram times height h. theorem 1.13b area base parallelogram v w. see since v w perpendicular base parallelogram determined v w height h ucosθ θ angle u v w. theorem 1.6 know cosθ u v w uv w. 6an equivalent deﬁnition parallelepiped is collection scalar combinations k1v1 k2v2 k3v3 vectors v1 v2 v3 r3 0 k1k2k3 1.
1.4 cross product 27 hence volp h v wuu v w uv w u v w. example 1.12 height h parallelepiped ucosθ ucosθ vector u side base parallelograms plane vector v w so cosθ 0. since volume matter base height use repeating steps using base determined u v since w side bases plane u v volume w u v. repeating base determined w u following result vectors u v w r3 u v w w u v v w u. 1.12 note equalities hold trivially vectors 0. since v w w v vectors v w r3 picking wrong order three adjacent sides scalar triple product formula 1.12 give negative volume parallelepiped. taking absolute value scalar triple product order three adjacent sides always give volume theorem 1.15. vectors u v w r3 represent three adjacent sides paral lelepiped volume parallelepiped u v w. another type triple product vector triple product u v w. proof following theorem left exercise reader theorem 1.16. vectors u v w r3 u v w u wvu vw. 1.13 examination formula theorem 1.16 gives idea geometry vector triple product. right side formula 1.13 see u v w scalar combination v w hence lies plane containing v w that is vectors u v w v w coplanar. makes sense since theorem 1.11 u v w perpendicular u v w. particular perpendicular v w means u v w lies plane containing v w since plane perpendicular v w. u v w also perpendicular u could vector following example may help see works.
28 chapter 1. vectors euclidean space example 1.13. find u v w u 124 v 220 w 130. solution since u v 6 u w 7 u v w u wvu vw 72206130 141406180 840. note v w lie xyplane u v w also lies plane. also u v w perpendicular u v w 004 see figure 1.4.8. z x 0 u v w v w u v w figure 1.4.8 vectors v v1 iv2 jv3 k w w1 iw2 jw3 k component form cross product written as v w v2w3v3w2iv3w1v1w3jv1w2v2w1k. often easier use component form cross product represented determinant. go deeply theory determinants7 cover essential purposes. 2 2 matrix array two rows two columns scalars written a b c µa b c ab cd scalars. determinant matrix written b c det a b c scalar deﬁned following formula b c ad bc. may help remember formula product scalars downward diagonal minus product scalars upward diagonal. 7see anton rorres fuller development.
1.4 cross product 29 example 1.14. 1 2 3 4 1423 46 2. 3 3 matrix array three rows three columns scalars written a1 a2 a3 b1 b2 b3 c1 c2 c3 a1 a2 a3 b1 b2 b3 c1 c2 c3 determinant given formula a1 a2 a3 b1 b2 b3 c1 c2 c3 a1 b2 b3 c2 c3 a2 b1 b3 c1 c3 a3 b1 b2 c1 c2 . 1.14 one way remember formula following multiply scalar ﬁrst row determinant 22 matrix remains removing row column contain scalar sum products up putting alternating plus minus signs front starting plus. example 1.15. 1 0 2 4 1 3 1 0 2 1 1 3 0 2 0 4 3 1 2 2 4 1 1 0 120083201 0. deﬁned determinant scalar derived algebraic operations scalar entries matrix. however put three vectors ﬁrst row 3 3 matrix deﬁnition still makes sense since would performing scalar multiplication three vectors they would multiplied 22 scalar determinants before. gives us determinant vector lets us write cross product v v1 iv2 jv3 k w w1 i w2 j w3 k determinant v w j k v1 v2 v3 w1 w2 w3 v2 v3 w2 w3 i v1 v3 w1 w3 j v1 v2 w1 w2 k v2w3 v3w2iv3w1 v1w3jv1w2 v2w1k . example 1.16. let v 4ij3k w i2k. v w j k 4 1 3 1 0 2 1 3 0 2 i 4 3 1 2 j 4 1 1 0 k 2i5jk .
30 chapter 1. vectors euclidean space scalar triple product also written determinant. fact example 1.12 following theorem provides alternate deﬁnition determinant 33 matrix volume parallelepiped whose adjacent sides rows matrix form righthanded system a lefthanded system would give negative volume. theorem 1.17. vectors u u1u2u3 v v1v2v3 w w1w2w3 r3 u v w u1 u2 u3 v1 v2 v3 w1 w2 w3 . 1.15 example 1.17. find volume parallelepiped adjacent sides u 213 v 132 w 112 see figure 1.4.9. z x 0 u v w figure 1.4.9 p solution theorem 1.15 volume volp parallelepiped p absolute value scalar triple product three adjacent sides in order. theorem 1.17 u v w 2 1 3 1 3 2 1 1 2 2 3 2 1 2 1 1 2 1 2 3 1 3 1 1 281034 28 volp 28 28. interchanging dot cross products useful proving vector identities example 1.18. prove u v w z u w u z v w v z vectors u v w z r3. solution let x u v. u v w z x w z w z x by formula 1.12 w z u v w z vuz uv by theorem 1.16 z vw uz uw v u wv zu zv w by commutativity dot product. u w u z v w v z .
1.4 cross product 31 exercises exercises 16 calculate v w. 1. v 512 w 443 2. v 7210 w 264 3. v 214 w 120 4. v 132 w 7210 5. v i2jk w 3i6j3k 6. v i w 3i2j4k. exercises 78 calculate area triangle pqr. 7. p 512 q 443 r 240 8. p 402 q 215 r 101. exercises 910 calculate area parallelogram pqrs. 9. p 213 q 145 r 253 321 10. p 22 q 14 r 66 30. exercises 1112 ﬁnd volume parallelepiped adjacent sides u v w. 11. u 113 v 214 w 512 12. u 132 v 7210 w 101. exercises 1314 calculate u v w u v w. 13. u 111 v 302 w 222 14. u 102 v 103 w 202. 15. calculate u v w z u 111 v 302 w 222 z 214. b 16. v w unit vectors r3 conditions would v w also unit vector r3 justify answer. 17. show v w 0 w r3 v 0. 18. prove theorem 1.14b. 19. prove theorem 1.14c. 20. prove theorem 1.14d. 21. prove theorem 1.14e. 22. prove theorem 1.14f. 23. prove theorem 1.16. 24. prove theorem 1.17. hint expand sides equation. 25. prove following vectors v w r3 a v w2 v w2 v2 w2
32 chapter 1. vectors euclidean space b v w 0 v w 0 v 0 w 0. c 26. prove example 1.8 formula area triangle pqr yields value matter two adjacent sides chosen. this show 1 2 u w 1 2 v w u pr w pq v qr w qp before. similarly show 1 2 u v 1 2 v w u rp v rq. 27. assume vector equation x b r3 unknown x 0 solution. show that a a b 0. b x b a a2 ka solution equation scalar k. 28. prove jacobi identity u v wv w uw u v 0. 29. show u v w lie plane r3 u v w 0. 30. vectors u v w z r3 show u v w z z u vww u vz u v w z u w zvv w zu equations make sense geometrically 31. describe geometrically set points position vector x satisfying equation v x x v given vector v 0
1.5 lines planes 33 1.5 lines planes know perform operations vectors start deal familiar geometric objects like lines planes language vectors. see using vectors makes easier study objects 3dimensional euclidean space. ﬁrst consider lines. line point parallel vector let p x0 y0 z0 point r3 let v ab c nonzero vector let l line p parallel v see figure 1.5.1. x z 0 l 0 0 px0 y0 z0 r v tv r tv r tv figure 1.5.1 let r x0 y0 z0 vector pointing origin p. since multiplying vector v scalar lengthens shrinks v preserving direction 0 reversing direction 0 see figure 1.5.1 every point line l obtained adding vector tv vector r scalar t. is varies real numbers vector r tv point every point l. summarize vector representation l follows point p x0 y0 z0 nonzero vector v r3 line l p parallel v given r tv 1.16 r x0 y0 z0 vector pointing p. note used correspondence vector terminal point. since v ab c terminal point vector r tv x0 at y0 bt z0 ct. get parametric representation l parameter t point p x0 y0 z0 nonzero vector v ab c r3 line l p parallel v consists points x y z given x x0 at y0 bt z z0 ct . 1.17 note representations get point p l letting 0.
34 chapter 1. vectors euclidean space formula 1.17 0 solve parameter t xx0a. also solve terms terms z neither b c respectively zero yy0b zz0c. three values equal value t write following system equalities called symmetric representation l point p x0 y0 z0 vector v ab c r3 a b c nonzero line l p parallel v consists points x y z given equations xx0 yy0 b z z0 c . 1.18 x z 0 x x0 x0 l figure 1.5.2 if say 0 scenario divide zero know x x0at x x00t x0. symmetric representation l would be x x0 yy0 b z z0 c . 1.19 note says line l lies plane x x0 parallel yzplane see figure 1.5.2. similar equations derived cases b 0 c 0. may noticed vector representation l formula 1.16 compact parametric symmetric formulas. advantage using vector notation. technically though vector representation gives us vectors whose terminal points make line l l itself. remember identify vectors r tv terminal points. hand parametric representation always gives points l nothing else. example 1.19. write line l point p 235 parallel vector v 416 following forms a vector b parametric c symmetric. lastly d ﬁnd two points l distinct p. solution a let r 235. formula 1.16 l given by r tv 235 t416 . b l consists points x y z x 24t 3t z 56t . c l consists points x y z x2 4 y3 1 z 5 6 . d letting 1 2 partb yields points 6211 10117 l.
1.5 lines planes 35 line two points x z 0 l p1x1 y1 z1 p2x2 y2 z2 r1 r2 r2 r1 r1 tr2 r1 figure 1.5.3 let p1 x1 y1 z1 p2 x2 y2 z2 distinct points r3 let l line p1 p2. let r1 x1 y1 z1 r2 x2 y2 z2 vectors pointing p1 p2 respectively. see figure 1.5.3 r2 r1 vector p1 p2. multiply vector r2 r1 scalar add vector r1 get entire line l varies real numbers. following summary vector para metric symmetric forms line l let p1 x1 y1 z1 p2 x2 y2 z2 distinct points r3 let r1 x1 y1 z1 r2 x2 y2 z2. line l p1 p2 following representations vector r1 tr2 r1 . 1.20 parametric x x1 x2 x1t y1 y2 y1t z z1 z2 z1t . 1.21 symmetric xx1 x2 x1 yy1 y2 y1 z z1 z2 z1 if x1 x2 y1 y2 z1 z2. 1.22 example 1.20. write line l points p1 314 p2 446 parametric form. solution formula 1.21 l consists points x y z x 37t 13t z 42t . distance point line θ l v w q p figure 1.5.4 let l line r3 vector form r tv for let p point l. distance p l length line segment p l perpendicular l see figure 1.5.4. pick point q l let w vector q p. θ angle w v wsinθ. since v w vwsinθ v 0 then v w v . 1.23 words hight parallelogram adjacent sides v w. since area v wand base v get expression 1.23.
36 chapter 1. vectors euclidean space example 1.21. find distance point p 111 line l example 1.20. solution example 1.20 see represent l vector form as r tv r 314 v 732. since point q 314 l w qp 111314 405 have v w j k 7 3 2 4 0 5 3 2 0 5 i 7 2 4 5 j 7 3 4 0 k 15i43j12k v w v 15i43j12k 732 p 152 432 122 p 72 32 22 p 2218 p 62 5.98. two lines clear two lines l1 l2 represented vector form r1 sv1 r2 tv2 respectively parallel denoted l1 l2 v1 v2 parallel. also l1 l2 perpendicular denoted l1 l2 v1 v2 perpendicular. x z 0 l1 l2 figure 1.5.5 2dimensional space two lines either identical parallel intersect. 3dimensional space additional possibility two lines skew is intersect parallel. however even though parallel skew lines parallel planes see figure 1.5.5. determine whether two lines r3 intersect often easier use parametric representation lines. case use dif ferent parameter variables usually t lines since values parameters may point intersection. setting two x y z triples equal result system 3 equations 2 unknowns s t. example 1.22. find point intersection if any following lines x1 3 y2 2 z 1 1 x3 y8 3 z 3 2 . solution first write lines parametric form parameters t x 13s 22s z 1s x 3 t 83t z 32t. lines intersect 13s22s1s 3 t83t32t s t 13s 3 t 23s 22s 83t 22s 8323s 29s 2s 9s s 0 t 230 2 1s 32t 10 322 1 1. note check this. letting 0 equations ﬁrst line letting 2 equations second line gives point intersection 121.
1.5 lines planes 37 plane point perpendicular vector let p plane r3 suppose contains point p0 x0 y0 z0. let n ab c nonzero vector perpendicular plane p. vector called normal vector or normal plane. let x y z point plane p. vector r xx0 yy0 zz0 lies plane p see figure 1.5.6. r 0 r n hence n r 0. r 0 still n r 0. x0 y0 z0 x y z n r figure 1.5.6 plane p. conversely x y z point r3 r x x0 yy0 z z0 0 n r 0 r n x y z lies p. proves following theorem theorem 1.18. let p plane r3 let x0 y0 z0 point p let n ab c nonzero vector perpendicular p. p consists points x y z satisfying vector equation n r 0 1.24 r xx0 yy0 z z0 equivalently axx0 byy0 cz z0 0. 1.25 equation called pointnormal form plane p. example 1.23. find equation plane p containing point 313 perpen dicular vector n 248. solution formula 1.25 plane p consists points x y z that 2x34y18z 3 0. multiply terms formula 1.25 combine constant terms get equation plane normal form ax by cz 0. 1.26 example normal form plane example 1.23 2x4y8z 22 0.
38 chapter 1. vectors euclidean space plane containing three noncollinear points 2dimensional 3dimensional space two points determine line. two points determine plane r3. fact three collinear points that is three line determine plane inﬁnite number planes would contain line three points lie. however three noncollinear points determine plane. q r noncollinear points r3 qr qs nonzero vectors parallel by noncollinearity cross product qr qs perpendicular qr qs. qr qs and hence q r s lie plane point q normal vector n qr qs see figure 1.5.7. q r n qr qs qr qs figure 1.5.7 noncollinear points q r s. example 1.24. find equation plane p containing points 213 112 321. solution let q 213 r 112 321. vectors qr 121 qs 112 plane p normal vector n qr qs 121 112 531. using formula 1.25 point q we could also use r s plane p consists points x y z that 5x23y1z 3 0 normal form 5x3y z 10 0. mentioned earlier skew lines r3 lie separate parallel planes. two skew lines determine plane. two nonidentical lines either intersect parallel determine plane. cases ﬁnd equation plane contains two lines simply pick two lines total three noncollinear points one point one line two points other use technique above example 1.24 write equation. leave examples exercises reader.
1.5 lines planes 39 distance point plane distance point r3 plane length line segment point plane perpendicular plane. following theorem gives formula distance. theorem 1.19. let q x0 y0 z0 point r3 let p plane normal form ax by cz 0 contain q. distance q p is ax0 by0 cz0 d p a2 b2 c2 . 1.27 proof let r x y z point plane p so ax cz 0 let r rq x0 x y0 y z0 z. r 0 since q lie p. normal form equation p know n ab c normal vector p. now plane divides r3 two disjoint parts. assume n points toward side p point q located. place n initial point r let θ angle r n. 0 θ 90 cosθ 0. thus distance cosθr cosθrsee figure 1.5.8. q r n r θ p figure 1.5.8 theorem 1.6 section 1.3 know cosθ n r nr cosθr n r nrr n r n ax0 x by0 y cz0 z p a2 b2 c2 ax0 by0 cz0 ax by cz p a2 b2 c2 ax0 by0 cz0 d p a2 b2 c2 ax0 by0 cz0 d p a2 b2 c2 . n points away side p point q located 90 θ 180and cosθ 0. distance cosθr thus repeating argument still gives result. qed example 1.25. find distance 245 plane example 1.24. solution recall plane given 5x3y z 10 0. 52341510 p 52 32 12 17 p 35 17 p 35 2.87.
40 chapter 1. vectors euclidean space line intersection two planes l figure 1.5.9 note two planes parallel normal vectors parallel planes perpendicular normal vectors perpendicular. suppose two planes p1 p2 normal vectors n1 n2 respectively intersect line l see figure 1.5.9. since n1 n2 n1 n1 n2 parallel plane p1. likewise n1 n2 n2 means n1 n2 also parallel p2. thus n1 n2 parallel intersection p1 p2 l. thus write l following vector form l r tn1 n2 1.28 r vector pointing point belonging planes. ﬁnd point planes ﬁnd common solution x y z two normal form equations planes. often made easier setting one coordinate variables zero leaves solve two equations two unknowns. example 1.26. find line intersection l planes 5x3y z10 0 2x4y z 3 0. solution plane 5x 3y z 10 0 normal vector n1 531 plane 2x 4yz3 0 normal vector n2 241. since n1 n2 scalar multiples two planes parallel hence intersect. point x y z planes satisfy following system two equations three unknowns 5x3y z 10 0 2x4yz 3 0. set x 0 why good choice. equations reduced to 3y z 10 0 4yz 3 0. second equation gives z 4y 3 substituting ﬁrst equation gives 7. z 31 point 0731 l. since n1 n2 1726 l given by r tn1 n2 0731 t1726 parametric form x t 77t z 3126t projections
1.5 lines planes 41 assume need ﬁnd orthogonal projection given point q position vector q line l given parametric equation r tv. note point intersection line l plane p thru q perpendicular v. plane p given equation xq v 0 unknown x. since belongs l position vector r tv t. since lies plane get r tvq v 0. solving t get qr v v v . therefore r qr v v v v position vector projection p l. note also projecction point r position vector r plane p. therefore formula used ﬁnd projection plane. example 1.27. find projections point q 111 line x 14t 25t z 36t. solution vector form parametric equation 123 t456. applying formula above get 123 114125136 42 52 62 456 123 17 77456 145 77 239 77 333 77 1.883.14.32 position vector projection. exercises exercises 14 write line l point p parallel vector v following forms a vector b parametric c symmetric. 1. p 232 v 543 2. p 312 v 281 3. p 213 v 101 4. p 000 v 7210. exercises 56 write line l points p1 p2 parametric form. 5. p1 123 p2 355 6. p1 415 p2 213. exercises 78 a ﬁnd distance point p line l b ﬁnd orthogonal projection p l
42 chapter 1. vectors euclidean space 7. p 111 l x 22t 4t z 7 t 8. p 000 l x 32t 43t z 54t. exercises 910 ﬁnd point intersection if any given lines. 9. x 73s 43s z 75s x 16t 2 t z 32t 10. x6 4 y3 z x11 3 y14 6 z 9 2 . exercises 1112 write normal form plane p containing point q per pendicular vector n. 11. q 512 n 443 12. q 620 n 264. exercises 1314 write normal form plane containing given points. 13. 103 121 616 14. 313 443 001. 15. write normal form plane containing lines exercise 9. 16. write normal form plane containing lines exercise 10. exercises 1718 a ﬁnd distance point q plane p b ﬁnd projection q plane p 17. q 412 p 3xy5z 8 0 18. q 020 p 5x2y7z 1 0. exercises 1920 ﬁnd line intersection if any given planes. 19. x3y2z 6 0 2xy z 2 0 20. 3x y5z 0 x2y z 4 0. b 21. find points intersection if any line x6 4 3 z plane x3y2z 6 0. hint put equations line equation plane. 22. explain following formula p pq ab pq ab gives distance skew lines ab pq.
1.6 elementary surfaces 43 1.6 elementary surfaces previous section discussed planes euclidean space. plane example surface deﬁne informally8 solution set equation fx y z 0 r3 realvalued function f. example plane given ax cz 0 solution set fx y z 0 function fx y z ax cz d. surfaces 2dimensional. plane simplest surface since ﬂat. section look surfaces complex important sphere cylinder. deﬁnition 1.9. sphere set points x y z r3 ﬁxed distance r called radius ﬁxed point p0 x0 y0 z0 called center sphere x y z xx02 yy02 z z02 r2 . 1.29 using vector notation written equivalent form x xx0 r 1.30 x x y z x0 x0 y0 z0 vectors. figure 1.6.1 illustrates vectorial approach spheres. z x 0 x r x a radius r center 000 z x 0 xx0 r x x0 xx0 x0 y0 z0 b radius r center x0 y0 z0 figure 1.6.1 spheres r3. note figure 1.6.1a intersection sphere xyplane circle radius r that is great circle given x2 y2 r2 subset r2. similarly intersections xzplane yzplane. general plane intersects sphere either single point circle. 8see oneill deeper rigorous discussion surfaces.
44 chapter 1. vectors euclidean space example 1.28. find intersection sphere x2 y2 z2 169 plane z 12. z x 0 z 12 figure 1.6.2 solution sphere centered origin radius 13 p 169 intersect plane z 12. putting z 12 equation sphere gives x2 y2 122 169 x2 y2 169144 25 52 circle radius 5 centered 0012 parallel xyplane see figure 1.6.2. equation formula 1.29 multiplied out get equation form x2 y2 z2 ax by cz 0 1.31 constants a b c d. conversely equation form may describe sphere determined completing square x z variables. note equation 1.31 could written x2 v x 0 x x y z v ab c. example 1.29. 2x2 2y2 2z2 8x4y16z 10 0 equation sphere solution dividing sides equation 2 gives x2 y2 z2 4x2y8z 5 0 x2 4x4y2 2y1z2 8z 1654116 0 x22 y12 z 42 16 sphere radius 4 centered 214. example 1.30. find pointss intersection if any sphere example 1.29 line x 3 t 12t z 3t. solution put equations line equation sphere x 22 y12 z 42 16 solve t 3 t22 12t12 3t42 16 t12 2t22 t12 16 6t2 12t10 0. quadratic formula gives solutions 1 4 p 6 . putting two values equations line gives following two points intersection µ 2 4 p 6 1 8 p 6 44 p 6 µ 24 p 6 18 p 6 4 4 p 6 .
1.6 elementary surfaces 45 two spheres intersect either single point circle. example 1.31. find intersection if any spheres x2 y2z2 25 x2 y2z 22 16. solution point x y z spheres see x2 y2 z2 25 x2 y2 25z2 x2 y2 z 22 16 x2 y2 16z 22 16z 22 25z2 4z 4 9 z 134 x2 y2 251342 23116. the intersection circle x2 y2 231 16 plane z 134. radius p 231 4 3.8 centered 00 13 4 . cylinders consider right circular cylinders. cylinders ob tained moving line l along circle c r3 way l always perpendicular plane containing c. consider cases plane containing c parallel one three coordinate planes see figure 1.6.3. z x 0 r a x2 y2 r2 z z x 0 r b x2 z2 r2 z x 0 r c y2 z2 r2 x figure 1.6.3 cylinders r3. example equation cylinder whose base circle c lies xyplane centered ab0 radius r xa2 yb2 r2 1.32 value z coordinate unrestricted. similar equations written base circle lies one coordinate planes. plane intersects right circular cylinder circle ellipse one two lines depending whether plane parallel oblique9 perpendicular respectively plane containing c. intersection surface plane called trace surface. 9that is angle strictly 0and 90.
46 chapter 1. vectors euclidean space equations spheres cylinders examples seconddegree equations r3 is equations form ax2 by2 cz2 dxy exz f yz gx h y iz j 0 1.33 constants a b . j. equation sphere cylinder plane line point resulting surface called quadric surface. z x 0 b c figure 1.6.4 ellipsoid one type quadric surface ellipsoid given equation form x2 a2 y2 b2 z2 c2 1. 1.34 case b c sphere. general ellipsoid eggshaped think ellipse rotated around major axis. traces coordinate planes ellipses. two types quadric surfaces hyperboloid one sheet given equation form x2 a2 y2 b2 z2 c2 1 1.35 hyperboloid two sheets whose equation form x2 a2 y2 b2 z2 c2 1. 1.36 z x 0 figure 1.6.5 hyperboloid one sheet. z x 0 figure 1.6.6 hyperboloid two sheets.
1.6 elementary surfaces 47 hyperboloid one sheet trace plane parallel xyplane ellipse. traces planes parallel xz yzplanes hyperbolas see figure 1.6.5 except special cases x a b planes traces pairs intersecting lines see exercise 8. hyperboloid two sheets trace plane parallel xy xzplane hyperbola see figure 1.6.6. trace yzplane. plane parallel yzplane x a trace ellipse. z x 0 figure 1.6.7 paraboloid elliptic paraboloid another type quadric surface whose equation form x2 a2 y2 b2 z c. 1.37 traces planes parallel xyplane ellipses though xyplane trace single point. traces planes parallel xz yzplanes parabolas. figure 1.6.7 shows case c 0. c 0 surface turned downward. case b surface called paraboloid revolution often used reﬂecting sur face vehicle headlights.10 complicated quadric surface hyperbolic paraboloid given by x2 a2 y2 b2 z c. 1.38 hyperbolic paraboloid tricky draw using graphing software computer make easier. example figure 1.6.8 created using free gnuplot package. shows graph hyperbolic paraboloid z y2 x2 special case b 1 c 1 equation 1.38. mesh lines surface traces planes parallel coordinate planes. see traces planes parallel xzplane parabolas pointing upward traces planes parallel yzplane parabolas pointing downward. also notice traces planes parallel xy plane hyperbolas though xyplane trace pair intersecting lines origin. true general c 0 equation 1.38. c 0 surface would similar figure 1.6.8 rotated 90around zaxis nature traces planes parallel xz yzplanes would reversed. 10for discussion see pp. 157158 hecht.
48 chapter 1. vectors euclidean space 10 5 0 5 10 10 5 0 5 10 100 50 0 50 100 z x z figure 1.6.8 hyperbolic paraboloid. z x 0 figure 1.6.9 elliptic cone last type quadric surface consider elliptic cone equation form x2 a2 y2 b2 z2 c2 0. 1.39 traces planes parallel xyplane ellipses ex cept xyplane trace single point. traces planes parallel xz yzplanes hyper bolas except xz yzplanes traces pairs intersecting lines. notice every point elliptic cone line lies entirely surface figure 1.6.9 lines go origin. makes elliptic cone example ruled surface. cylinder also ruled surface. may obvious hyperboloid one sheet hyperbolic paraboloid ruled surfaces. fact surfaces two lines point surface see exercises 1112. surfaces called doubly ruled surfaces pairs lines called regulus.
1.6 elementary surfaces 49 clear six types quadric surfaces discussed surface translated away origin say replacing x2 xx02 equation. proved11 every quadric surface translated andor rotated equation matches one six types described. example z kxy case equation 1.33 mixed variables namely 0 get xy term. equation match types considered. however rotating x yaxes 45in xyplane means coordinate transformation x x y p 2 x y p 2 z z z kxy becomes hyperbolic paraboloid z kx2 ky2 x y z coordinate system. is equation z kxy 1.40 describes hyperbolic paraboloid equation 1.38 rotated 45in xyplane. exercises exercises 14 determine given equation describes sphere. so ﬁnd radius center. 1. x2 y2 z2 4x6y10z 37 0 2. x2 y2 z2 2x2y8z 19 0 3. 2x2 2y2 2z2 4x4y4z 44 0 4. x2 y2 z2 12x2y4z 32 0. 5. find points intersection sphere x 32 y12 z 32 9 line x 12t 23t z 3 t. b 6. find intersection spheres x2 y2 z2 9 x42 y22 z 42 9. 7. find intersection sphere x2 y2 z2 9 cylinder x2 y2 4. 8. find trace hyperboloid one sheet x2 a2 y2 b2 z2 c2 1 plane x a trace plane b. 9. find trace hyperbolic paraboloid x2 a2 y2 b2 z c xyplane. c 10. shown four noncoplanar points that is points lie plane determine sphere.12 find equation sphere passes points 000 002 143 013. hint equation 1.31 11see ch. 7 pogorelov. 12see welchons krickenberger p. 160 proof.
50 chapter 1. vectors euclidean space 11. show hyperboloid one sheet doubly ruled surface is point surface two lines lying entirely surface. hint write equation 1.35 x2 a2 z2 c2 1y2 b2 factor side. recall two planes intersect line. 12. show hyperbolic paraboloid doubly ruled surface. hint exercise 11 z x 0 002 x y0 ab c 1 figure 1.6.10 13. let sphere radius 1 centered 001 let sbe without north pole point 002. let ab c arbitrary point s. line passing 002 ab c intersects xyplane point x y0 figure 1.6.10. find point x y0 terms a b c. note every point xyplane matched point s vice versa manner. method called stereographic projection essentially identiﬁes r2 punctured sphere. 14. given two points p q space consider set points x distance x p twice larger distance x q. show set sphere. find radius center p 123 q 245. 15. show equidistant set plane point plane formed elliptic paraboloid. hintuse coordinate system given pane xyplane.
1.7 curvilinear coordinates 51 1.7 curvilinear coordinates x z 0 x y z x z figure 1.7.1 cartesian coordinates point x y z determined following straight paths starting origin ﬁrst along xaxis parallel yaxis parallel zaxis figure 1.7.1. curvilinear coordinate systems paths curved. two types curvilinear coordinates consider cylindrical spherical coordinates. instead ref erencing point terms sides rectangular parallelepiped cartesian coordinates think point ly ing cylinder sphere. cylindrical coordinates often used symmetry around zaxis spherical coordinates useful symmetry origin. problem given curvilinear coordinates typical solution consist 1 convert ing data cartesian coordinates 2 solving cartesian coordinates 3 converting results back original curvilinear coordinates necessarily. unless know doing suggest follow procedure. let p x y z point cartesian coordinates r3. cylindrical coordi nates rθ z spherical coordinates ρθφ px y z deﬁned follows13 x z 0 px y z p0x y0 θ x z r figure 1.7.2 cylindrical coordinates cylindrical coordinates rθ z x rcosθ r q x2 y2 rsinθ θ tan1 x z z z z 0 θ π 0 π θ 2π 0. x z 0 px y z p0x y0 θ x z ρ φ figure 1.7.3 spherical coordinates spherical coordinates ρθφ x ρsinφ cosθ ρ q x2 y2 z2 ρsinφ sinθ θ tan1 x z ρcosφ φ cos1³ z p x2y2z2 0 θ π 0 π θ 2π 0. θ φ measured radians. note r 0 0 θ 2π ρ 0 0 φ π. also θ undeﬁned x y 00 φ undeﬁned x y z 000. 13this standard deﬁnition spherical coordinates used mathematicians results lefthanded system. reason physicists usually switch deﬁnitions θ φ make ρθφ righthanded system.
52 chapter 1. vectors euclidean space assume p0 x y0 projection p upon xyplane rθ z cylindrical coordinates p x y z. rθ polar coordinates p0 see figure 1.7.2. spherical coordinates ρ length line segment origin p φ angle line segment positive zaxis see figure 1.7.3. angle φ called zenith angle. example 1.32. convert point 221 cartesian coordinates a cylindrical b spherical coordinates. solution a r p 22 22 2 p 2 θ tan1 2 2 tan11 5π 4 since 2 0. rθ z 2 p 2 5π 4 1 . b ρ p 22 22 12 p 9 3 φ cos1 1 3 1.23 radians. ρθφ 3 5π 4 1.23 . cylindrical coordinates rθ z constants r0 θ0 z0 see figure 1.7.4 surface r r0 cylinder radius r0 centered along zaxis surface θ θ0 halfplane emanating zaxis surface z z0 plane parallel xyplane. z x 0 r0 a r r0 z x 0 θ0 b θ θ0 z x 0 z0 c z z0 figure 1.7.4 cylindrical coordinate surfaces. spherical coordinates ρθφ constants ρ0 θ0 φ0 see figure 1.7.5 surface ρ ρ0 sphere radius ρ0 centered origin surface θ θ0 halfplane emanating zaxis surface φ φ0 circular cone whose vertex origin. figures 1.7.4a 1.7.5a show coordinate systems got names. sometimes equation surface cartesian coordinates transformed simpler equation coordinate system following example. example 1.33. write equation cylinder x2 y2 4 cylindrical coordinates. solution since r p x2 y2 equation cylindrical coordinates r 2. using spherical coordinates write equation sphere necessarily make equation simpler sphere centered origin.
1.7 curvilinear coordinates 53 z x 0 ρ0 a ρ ρ0 z x 0 θ0 b θ θ0 z x 0 φ0 c φ φ0 figure 1.7.5 spherical coordinate surfaces. example 1.34. write equation x22 y12 z2 9 spherical coordinates. solution multiplying equation gives x2 y2 z2 4x2y5 9 get ρ2 4ρsinφ cosθ 2ρsinφ sinθ 4 0 ρ2 2sinφ2cosθ sinθρ 4 0 combining terms. note actually makes difﬁcult ﬁgure surface is opposed cartesian equation could immediately identify surface sphere radius 3 centered 210. example 1.35. describe surface given θ z cylindrical coordinates. solution surface called helicoid. vertical z coordinate increases angle θ radius r unrestricted. sweeps ruled surface shaped like spiral staircase spiral inﬁnite radius. figure 1.7.6 shows section surface restricted 0 z 4π 0 r 2. exercises exercises 14 ﬁnd a cylindrical b spherical coordinates point whose cartesian coordinates given. 1. 22 p 31 2. 556 3. p 21 p 70 4. 0 p 22. exercises 57 write given equation a cylindrical b spherical coordinates. 5. x2 y2 z2 25 6. x2 y2 2y 7. x2 y2 9z2 36. b 8. describe intersection surfaces whose equations spherical coordinates θ π 2 φ π 4.
54 chapter 1. vectors euclidean space 2 1.5 1 0.5 0 0.5 1 1.5 2 2 1.5 1 0.5 0 0.5 1 1.5 2 0 2 4 6 8 10 12 14 z x z figure 1.7.6 helicoid θ z. 9. show 0 equation ρ 2asinφ cosθ spherical coordinates describes sphere centered a00 radius a. c 10. let p aθφ point spherical coordinates 0 0 φ π. p lies sphere ρ a. since 0 φ π line segment origin p extended intersect cylinder given r in cylindrical coordinates. find cylindrical coordinates point intersection. 11. let p1 p2 points whose spherical coordinates ρ1θ1φ1 ρ2θ2φ2 respec tively. let v1 vector origin p1 let v2 vector origin p2. angle γ v1 v2 show cosγ cosφ1 cosφ2 sinφ1 sinφ2 cosθ2 θ1 . formula used electrodynamics prove addition theorem spherical har monics provides general expression electrostatic potential point due unit charge. see pp. 100102 jackson. 12. show distance points p1 p2 cylindrical coordinates
1.7 curvilinear coordinates 55 r1θ1 z1 r2θ2 z2 respectively q r2 1 r2 2 2r1 r2 cosθ2 θ1 z2 z12 . 13. show distance points p1 p2 spherical coordinates ρ1θ1φ1 ρ2θ2φ2 respectively q ρ2 1 ρ2 2 2ρ1 ρ2sinφ1 sinφ2 cosθ2 θ1 cosφ1 cosφ2.
2 curves 2.1 vectorvalued functions familiar vectors operations begin discussing func tions whose values vectors. deﬁnition 2.1. vectorvalued function real variable rule associates vector ft real number t r interval called domain f. write f r3 denote f mapping r3. example ft ti t2j t3k vectorvalued function r3 deﬁned real num bers t. would write f r r3. 1 value function vector j k cartesian coordinates terminal point 111. vectorvalued function real variable written component form ft f1ti f2tj f3tk ft f1t f2t f3t realvalued functions f1t f2t f3t called component functions f. ﬁrst form often used emphasizing ft vector second form useful considering terminal points vectors. z x 0 f0 f2π figure 2.1.1 example 2.1. deﬁne f r r3 ft costsintt. parametric equation helix see figure 1.8.1. value increases terminal points ft spiraling upward. t x ycoordinates ft x cost sint x2 y2 cos2 tsin2 1. thus ft lies surface right circular cylinder x2y2 1 t. since three component functions realvalued sometimes case results singlevariable calculus simply applied component functions yield similar result vectorvalued function. however times generalizations hold see exercise 13. concept limit though extended naturally vectorvalued functions following deﬁnition. 56
2.1 vectorvalued functions 57 deﬁnition 2.2. let ft vectorvalued function let real number let c vector. say limit ft approaches equals c written lim taft c lim taftc 0. equivalently ft f1t f2t f3t lim taft ³ lim ta f1tlim ta f2tlim ta f3t provided three limits right side exist. deﬁnition shows continuity derivative vectorvalued functions also deﬁned terms component functions. deﬁnition 2.3. let ft f1t f2t f3t vectorvalued function let real number domain. ft continuous lim taft fa. equivalently ft continuous f1t f2t f3t continuous a. derivative ft a denoted fa df dta limit fa lim h0 fa hfa h limit exists. equivalently fa f 1a f 2a f 3a component derivatives exist. say ft differentiable fa exists. realvalued function whose ﬁrst derivative continuous called continuously differ entiable or c1 function function whose derivatives orders continuous called smooth or cfunction. functions consider smooth. continuous vector valued functions also called curves case vector ft usually regarded terminal point. regular curve ft one whose derivative ft never zero vector. example consider plane curve ft t2t3 called semicubical parabola shown picture. curve smooth components regular since ft 2t3t2 vanish 0. fact curve look smooth 0 called cusp point. recall derivative realvalued function single vari able real number representing slope tangent line graph function point. similarly derivative vector valued function tangent vector curve space function represents lies tangent line curve see figure 2.1.2. example 2.2. let ft costsintt. ft sintcost1 t. tangent line
58 chapter 2. curves z x 0 l ft fa fa fa h fa hfa figure 2.1.2 tangent vector fa tangent line l fa sfa. l curve f2π 102π l f2π sf2π 102π s011 parametric form x 1 s z 2π . scalar function realvalued function. note ut scalar function ft vectorvalued function product deﬁned uft utft t vectorvalued function since product scalar vector vector. basic properties derivatives vectorvalued functions summarized fol lowing theorem. theorem 2.1. let ft gt differentiable vectorvalued functions let ut differentiable scalar function let k scalar let c constant vector. a dtc 0 b dtkf k df dt c dtfg df dt dg dt d dtfg df dt dg dt e dtuf du dt f u df dt f dtf g df dt g f dg dt g dtf g df dt g f dg dt . proof proofs parts ae follow easily differentiating component functions using rules derivatives singlevariable calculus. prove part f leave proof part g exercise reader.
2.1 vectorvalued functions 59 f write ft f1t f2t f3t gt g1t g2t g3t component functions f1t f2t f3t g1t g2t g3t differentiable realvalued functions. dtft gt dtf1t g1t f2t g2t f3t g3t dtf1t g1t dtf2t g2t dtf3t g3t f1 dt t g1t f1t dg1 dt t f2 dt t g2t f2t dg2 dt t f3 dt t g3t f3t dg3 dt t ³ f1 dt t f2 dt t f3 dt t g1t g2t g3t f1t f2t f3t ³dg1 dt t dg2 dt t dg3 dt t df dtt gt ft dg dt t t. qed example 2.3. suppose ft differentiable. find derivative ft. solution since ftis realvalued function t chain rule realvalued functions know dtft2 2ftd dtft. ft2 ft ft dtft2 dtft ft. hence 2ftd dtft dtft ft ft ft ft ft theorem 2.1f 2ft ft ft 0 dtft ft ft ft. know ftis constant dtft 0 t. also ft ft ft ft 0. thus example shows important fact ft 0 ftis constant ft ft t. means curve lies completely sphere or circle centered origin tangent vector ft always perpendicular position vector ft. example 2.4. spherical spiral ft ³ cost p 1 a2t2 sint p 1 a2t2 at p 1 a2t2 0. figure 2.1.3 shows graph curve 0.2. exercises reader asked show curve lies sphere x2 y2 z2 1 verify directly ft ft 0 t.
60 chapter 2. curves 1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1 1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1 1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1 z x z figure 2.1.3 spherical spiral 0.2. singlevariable calculus higherorder derivatives vectorvalued functions obtained repeatedly differentiating ﬁrst derivative function ft dtft ft dtft . dnf dtn dt ³ dn1f dtn1 for n 234. use vectorvalued functions represent physical quantities velocity ac celeration force momentum etc. example let real variable represent time elapsed initial time t 0 suppose object constant mass subjected force moves space position x y z time function t. is x xt yt z zt realvalued functions xt yt zt. call rt xt yt zt position vector object. deﬁne various physical quan tities associated object follows1 position rt xt yt zt velocity vt rt rt dr dt xt yt zt 1we often use older dot notation derivatives physics involved.
2.1 vectorvalued functions 61 acceleration at vt vt dv dt rt rt d2r dt2 xt yt zt momentum pt mvt force ft pt pt dp dt newtons second law motion. magnitude vtof velocity vector called speed object. note since mass constant force equation becomes familiar ft mat. example 2.5. let rt 5cost3sint4sint position vector object time 0. find a velocity b acceleration vectors. solution a vt rt 5sint3cost4cost. b at vt 5cost3sint4sint. note rt p 25cos2 t25sin2 5 t example 2.3 know rt rt 0 which verify part a. fact vt 5 also. rt lie sphere radius 5 centered origin perhaps obvious lies completely within circle radius 5 centered origin. also note at rt. turns see exercise 16 whenever object moves circle constant speed acceleration vector point towards center circle. recall section 1.5 r1 r2 position vectors distinct points r1tr2r1 represents line two points varies real numbers. vector sum written 1 tr1 tr2. function lt 1 tr1 tr2 line terminal points r1 r2 restricted interval 01 line segment points l0 r1 l1 r2. general function form ft a1tb1a2tb2a3tb3 represents line r3. function form ft a1t2 b1t c1a2t2 b2t c2a3t2 b3t c3 represents possibly degenerate parabola r3. example 2.6. bézier curves used computer aided design approximate shape polygonal path space called bézier polygon control polygon. instance given three points or position vectors b0 b1 b2 r3 deﬁne b 1 0t 1tb0 tb1 b 1 1t 1tb1 tb2 b 2 0t 1tb 1 0t tb 1 1t 1t2b0 2t1tb1 t2b2 real t. interval 01 see b1 0t line segment b0 b1 b1 1t line segment b1 b2. function b2 0t bézier curve
62 chapter 2. curves points b0 b1 b2. note last formula curve parabola goes b0 when 0 b2 when 1. example let b0 000 b1 123 b2 452. explicit formula bézier curve b2 0t 2t2t24t t26t4t2 shown figure 2.1.4 line segments b1 0t b1 1t curve b2 0t. 0 0.5 1 1.5 2 2.5 3 3.5 4 0 1 2 3 4 5 0 0.5 1 1.5 2 2.5 3 z x z 000 123 452 figure 2.1.4 bézier curve approximation three points. general polygonal path determined n 3 noncollinear points r3 used deﬁne bézier curve recursively process called repeated linear interpolation. curve vectorvalued function whose components polynomials degree n 1 formula given de casteljaus algorithm.2 exercises reader given algorithm case n 4 points asked write explicit formula bézier curve four points shown figure 2.1.5. example 2.7. pedal curve traced orthogonal projection ﬁxed point p tangent lines given curve ft. write parametric expression ht pedal curve unit circle ft costsint point p 10 position vector i. this curve called cardioid. 2see pp. 2730 farin.
2.1 vectorvalued functions 63 p denote wt projection vt ft tangent line ft ht ftwt. velocity vector ft sintcost parallel tangent line ft. note ft 1 t. therefore vector wt found useing following formula compare example 1.27 ex ercise 25 page 20 wt ftvtft ftiftft sin2 tsintcost. ht ftwt costsin2 tsintsintcost. exercises exercises 14 calculate ft ﬁnd tangent line f0. 1. ft t1t2 1t3 1 2. ft et 1 e2t 1 et2 1 3. ft cos2tsin2tt 4. ft sin2t2sin2 t2cost. exercises 56 ﬁnd velocity vt acceleration at object given position vector rt. 5. rt ttsint1cost 6. rt 3cost2sint1. b 7. let ft ³ cost p 1 a2t2 sint p 1 a2t2 at p 1 a2t2 0. a show ft 1 t. b show directly ft ft 0 t. 8. ft 0 interval ab show ft constant vector ab. 9. constant vector c 0 function ft tc represents line parallel c. a kind curve gt t3c represent explain. b kind curve ht etc represent explain.
64 chapter 2. curves c compare f0 g0. given answer part a explain differ ence two derivatives 10. show dt ³ f df dt f d2f dt2 . 11. let particle constant mass position vector rt velocity vt acceleration at momentum pt time t. angular momentum lt particle respect origin time deﬁned lt rt pt. ft force acting particle time t deﬁne torque nt acting particle respect origin nt rt ft. show lt nt. 12. show dtf g h df dt g h f ³dg dt h f ³ g dh dt . 13. mean value theorem hold vectorvalued functions show ft costsintt interval 02π ft f2πf0 2π0 . 14. wrie parametric equation pedal curve ft tt2t3 respect origin. c 15. bézier curve b3 0t four noncollinear points b0 b1 b2 b3 r3 deﬁned following algorithm going left column right b 1 0t 1tb0 tb1 b 1 1t 1tb1 tb2 b 2 0t 1tb 1 0t tb 1 1t b 1 2t 1tb2 tb3. b 2 1t 1tb 1 1t tb 1 2t b 3 0t 1tb 2 0t tb 2 1t. a show b3 0t 1t3b0 3t1t2b1 3t21tb2 t3b3. b write explicit formula as example 2.6 bézier curve points b0 000 b1 011 b2 230 b3 452. 16. let rt position vector particle moving r3 vt velocity at acceleration. show dtr v r r2ar vvv2 r ar. 17. let rt position vector r3 particle moves constant speed c 0 circle radius 0 centered origin xyplane. show acceleration at points opposite direction rt t. hint use example 2.3 show rt vt at vt hence at rt.
2.1 vectorvalued functions 65 18. prove theorem 2.1g. 19. show plane tangent3 curve ft tt2t3 two distinct points. 0 0.5 1 1.5 2 2.5 3 3.5 4 0 1 2 3 4 5 0 0.5 1 1.5 2 z x z 000 011 230 452 figure 2.1.5 bézier curve approximation four points. 3a plane called tangent curve ft point ft0 contains tangent line ft0.
66 chapter 2. curves 2.2 arc length deﬁnition 2.4. let ft xt yt zt curve r3 whose domain includes interval ab. suppose interval ab ﬁrst derivative component function xt yt zt exists continuous. arc length l curve b l b w ftdt b w q x t2 yt2 z t2 dt. 2.1 ft xt yt zt position vector object moving r3 speed time ft magnitude velocity vector. therefore seems natural deﬁne distance traveled deﬁnite integral speed time interval 2.1. example 2.8. find length l helix ft costsintt 0 2π. solution formula 2.1 l 2π w 0 p sint2 cost2 12 dt 2π w 0 p sin2 tcos2 t1dt 2π w 0 p 2dt p 22π0 2 p 2π. notice set traced curve ft costsintt example 2.8 also traced function gt cos2tsin2t2t. example interval 0π gt traces section curve ft interval 02π. intuitively says gt traces curve twice fast ft. makes sense since viewing functions position vectors derivatives velocity vectors speeds ft gt ft p 2 gt 2 p 2 respectively. say gt reparametrization curve ft. deﬁnition 2.5. let ft smooth curve r3 deﬁned interval ab let α cd ab smooth onetoone mapping interval cd onto ab. function g cd r3 deﬁned gs fαs reparametrization ft param eter s. derivative α vanish say reparametrization regular gs equivalent ft. ft cd ab r3 α f gs fαs ft note differentiability gs follows version chain rule vector valued functions the proof left exercise
2.2 arc length 67 theorem 2.2. chain rule ft differentiable vectorvalued function t αs differentiable scalar function s gs fαs differentiable vectorvalued function s dg ds df dt dt ds equivalently gs fαsαs 2.2 composite function fαs deﬁned. example 2.9. following regular reparametrizations one curve ft costsintt 02π gs cos2ssin2s2s 0π hs cos2πssin2πs2πs 01. see gs regular reparametrization ft deﬁne α 0π 02π αs 2s. α smooth onetoone maps 0π onto 02π strictly increasing since αs 2 0 s. likewise deﬁning α 01 02π αs 2πs shows hs regular reparametrization ft. curve reparametrized different speeds one best use situations arc length parametrization useful. idea behind replace parameter t given smooth parametrization ft deﬁned ab parameter given st w fudu. 2.3 terms motion along curve distance traveled along curve time elapsed. new parameter distance instead time. natural correspondence t starting point curve distance traveled along curve in one direction uniquely determined amount time elapsed vice versa. since arc length curve interval at ab function t. fundamental theorem calculus derivative t ds dt dt w fudu ftfor ab. since ft smooth ft 0 ab. thus t 0 hence st strictly increasing interval ab. recall means onetoone mapping interval ab onto interval sasb. see sa w fudu 0 sb b w fudu l arc length b.
68 chapter 2. curves 0l ab αs st figure 2.2.1 αs function ab 0l onetoone differentiable mapping onto interval 0l. singlevariable calculus know means exists inverse function α 0l ab differentiable inverse ab 0l. is ab unique 0l st αs. know derivative α αs 1 αs 1 fαs. deﬁne arc length parametrization g 0l r3 gs fαs 0l. gs smooth chain rule. fact gs unit speed gs fαsαs chain rule fαs 1 fαs gs 1 0l. arc length parametrization traverses curve normal rate. practice parametrizing curve ft arc length requires evaluate integral r afudu explicitly function t could solve terms s. done would substitute expression terms which called αs formula ft get gs fαs. example 2.10. parametrize helix ft costsintt 02π arc length. solution example 2.8 formula 2.3 w 0 fudu w 0 p 2du p 2t 02π. solve terms s αs p 2 . gs ³ cos p 2 sin p 2 p 2 02 p 2π. note gs 1. exercises exercises 13 calculate arc length ft given interval. 1. ft 3cos2t3sin2t3t 0π2
2.2 arc length 69 2. ft t2 1costt2 1sint2 p 2t 01 3. ft 2cos3t2sin3t2t32 01. 4. parametrize curve exercise 1 arc length. 5. parametrize curve exercise 3 arc length. b 6. assume gs regular reparametrization ft. show curves length. 7. let ft differentiable curve ft 0 t. show dt µ ft ft ft ft ft ft3 . 8. show arc length l curve whose spherical coordinates ρ ρt θ θt φ φt interval ab l b w q ρ t2 ρt2 sin2 φtθ t2 ρt2φt2 dt. hint convert data cartesian coordinates. 9. let ft smooth curve. pedal curve ft traced orthogonal projections origin tangent lines f. write parametric equation pedal curve ht given smooth curve ft. c 10. assume trajectory back wheel ideal bicycle given smooth plane curve bt denotes time. assume ideal bicycle distance back wheel front wheel ﬁxed let us denote r back wheel always moves direction front wheel. a write expression trajectory front wheel ft. b show speed back wheel exceed speed front wheel.
70 chapter 2. curves 2.3 curvature ﬁeld mathematics known differential geometry4 special attention given parametrizationindependent constructions. example depending parametriza tion velocity vector curve given point multiplied scalar parametrizationindependent hand tangent line given point parametrizationindependent although deﬁned using parametrization resulting line same. example called osculating plane. given smooth regular curve f oscu lating plane ft plane passing thru ft containing velocity vector ft acceleration ft. osculating plane deﬁned ft parallel ft. note case cross product ft ft perpendicular osculating plane. therefore equation osculating plane ft written xft ft ft 0 unknown x. example 2.11. let us show osculating plane given point depend parametrization. is gs fαs regular reparametrization plane thru gs containing velocity vector gs acceleration gs plane thru ft containing velocity vector ft acceleration ft αs. since ft gs need show ft ft gs gs. chain rule gs fαsαs chain rule gs fαsαs2 fαsαs. since f f 0 αs get gs gs ftαs ftαs2 ftαs αs3ft ft. since reparametrization regular αs 0. therefore ft ft gs gs re qured. yet example called curvature. assume smooth regular curve g arc length parametrization. note g parametrize straight line gs constant unit vector therefore gs 0 points. therefore value κs gscan used measure fast curve deviates straight line. value κs vector gs called curvature curvature vector curve g point gs. 4see oneill introduction elementary differential geometry.
2.3 curvature 71 κs 0 value rs 1 κs called curvature radius g point gs. called way since best approximation curve g point gs circle called osculating circle radius rs. circle lying osculating plane center lies direction curvature vector gs gs distance rs. κs 0 osculating circle degenerates tangent line. osculating circle sinusoid two points. assume want ﬁnd curvature given curve using deﬁnition above. ﬁrst ﬁnd arc length parametrization apply formula given point. finding parametrization often leads integral either difﬁcult impossible evaluate explicitly. simple integral example 2.10 exception norm. general arc length parametrizations useful theoretical purposes practical computations.5 following theorem provides direct way calculate curvature without passing reparametrization. exercises 9 guides similar calculations. theorem 2.3. curvature κ smooth curve f point ft found using following formula κ ft ft ft3 . 2.4 proof let gs arc length parametrization ft particular gs 1 s. above assume αs therefore gs fαs αs 1 ft. 5for example usual parametrizations bézier curves discussed section 1.8 polynomial functions r3. makes computation relatively simple which computeraided design desir able. arc length parametrizations polynomials fact usually impossible calculate all.
72 chapter 2. curves applying chain product rules get gs fαs fαsαs fαsαs2 fαsαs ft ft2 ftαs since ft ft 0 get ft ft ft3 µ ft ft2 ftαs ft ft gs gs. since gs 1 0 gs gs 2gs gs. is gs gs s. since gs 1 continue ft ft ft3 gs gs gsgs κ. qed exercises exercises 14 ﬁnd tangent line osculating plane curvature point curve ft. 1. ft costsintt 2. ft tt2t3 3. ft tsinttcost 4. ft et sint et cost. b 5. let ft smooth regular curve gs fαs regular reparametrization. show osculating plane f ft coinsides osculating plane g gs αs.
2.3 curvature 73 6. let ft smooth regular curve particular ft 0 t. deﬁne unit tangent vector tt ft ft. a show tt ft ft ft ft3 . b use formula get proof theorem 2.3. 7. let gs smooth curve arc length parametrization κs curvature. show gs gs κs2. 8. let g smooth plane curve arc length parametrization. curve hs gssgs called involute gs. a show hs sκs κs curvature g gs b show curvature h hs equals 1 0. hint use exercise 7. c 9. let ft smooth curve plane. assume curvature κt increasing t. show curve selfintersections is t0 t1 ft0 ft1. hint write expression center radius osculating circles use show intersect other.
3 functions several variables 3.1 functions two three variables section 1.8 discussed vectorvalued functions single real variable. examine realvalued functions point or vector r2 r3. part functions deﬁned sets points r2 times use points r3 also times convenient think points vectors or terminal points vectors. realvalued function f deﬁned subset r2 rule assigns point x y real number f x y. largest possible set r2 f deﬁned called domain f range f set real numbers f x y x y varies domain d. similar deﬁnition holds functions f x y z deﬁned points x y z r3. example 3.1. domain function f x y xy r2 range f r. example 3.2. domain function f x y 1 xy r2 except points x y x y. is domain set x y x y. range f real numbers except 0. example 3.3. domain function f x y q 1x2 y2 set x y x2 y2 1 since quantity inside square root nonnegative 1x2 y2 0. see consists points inside unit circle r2 d sometimes called closed unit disk. range f interval 01 r. 74
3.1 functions two three variables 75 example 3.4. domain function f x y z exyz r3 range f positive real numbers. function f x y deﬁned r2 often written z f x y mentioned section 1.1 graph f x y set x y z z f x y r3. see graph surface r3 since satisﬁes equation form fx y z 0 namely fx y z f x yz. traces surface planes z c c varies r called level curves function. equivalently level curves solution sets equations f x y c c r. level curves often projected onto xyplane give idea various elevation levels surface as done topography. example 3.5. graph function f x y sin p x2 y2 p x2 y2 shown below. note level curves shown surface projected onto xyplane groups concentric circles. may wondering happens function example 3.5 point x y 00 since numerator denominator 0 point. function deﬁned 00 limit function exists and equals 1 x y approaches 00. state explicitly meant limit function two variables. deﬁnition 3.1. let ab point r2 let f x y realvalued function deﬁned set containing ab but necessarily deﬁned ab itself. say limit f x y equals l x y approaches ab written lim xyab f x y l 3.1 given ǫ 0 exists δ 0 f x yl ǫ whenever 0 q xa2 yb2 δ. similar deﬁnition made functions three variables. idea behind deﬁnition values f x y get arbitrarily close l that is within ǫ l pick x y sufﬁciently close ab that is inside circle centered ab sufﬁciently small radius δ. recall epsilondelta proofs limits realvalued functions single variable may remember awkward be usually done easily
76 chapter 3. functions several variables 10 5 0 5 10 10 5 0 5 10 0.4 0.2 0 0.2 0.4 0.6 0.8 1 z x z figure 3.1.1 function f x y sinp x2y2 p x2y2 . simple functions. general multivariable cases least equally awkward go through bother proofs. instead simply state function f x y given single formula deﬁned point ab for example indeterminate form like 00 substitute x y ab formula f x y ﬁnd limit. example 3.6. lim xy12 xy x2 y2 12 12 22 2 5 since f x y xy x2y2 properly deﬁned point 12. major difference limits one variable limits two variables point approached. singlevariable case statement x a means x gets closer value two possible directions along real number line see figure 3.1.2a. two dimensions however x y approach point ab along inﬁnite number paths see figure 3.1.2b. example 3.7. lim xy00 xy x2 y2 exist
3.1 functions two three variables 77 0 x x x a x a r x 0 ab b x y ab r2 figure 3.1.2 approaching point different dimensions. note simply substitute x y 00 function since gives indeterminate form 00. show limit exist show function approaches different values x y approaches 00 along different paths r2. see this suppose x y 00 along positive xaxis 0 along path. f x y xy x2 y2 x0 x2 02 0 along path since x 0 denominator. x y 00 along straight line x origin x 0 see f x y xy x2 y2 x2 x2 x2 1 2 means f x y approaches different values x y 00 along different paths. hence limit exist. limits realvalued multivariable functions obey algebraic rules singlevariable case shown following theorem state without proof. theorem 3.1. suppose lim xyab f x y lim xyab gx y exist k scalar. then a lim xyabf x y gx y h lim xyab f x y h lim xyab gx y b lim xyabk f x y k h lim xyab f x y c lim xyabf x ygx y h lim xyab f x y ih lim xyab gx y d lim xyab f x y gx y lim xyab f x y lim xyab gx y lim xyab gx y 0 e f x yl gx y x y lim xyab gx y 0 lim xyab f x y l.
78 chapter 3. functions several variables note part e sufﬁces f x yl gx y x y sufﬁciently close ab but excluding ab itself. example 3.8. show lim xy00 y4 x2 y2 0. since substituting x y 00 function gives indeterminate form 00 need alternate method evaluating limit. use theorem 3.1e. first notice y4 p y24 0 y4 p x2 y24 x y. p x2 y24 x2 y22. thus x y 00 y4 x2 y2 x2 y22 x2 y2 x2 y2 0 x y 00. therefore lim xy00 y4 x2 y2 0. continuity deﬁned similarly singlevariable case. deﬁnition 3.2. realvalued function f x y domain r2 continuous point ab lim xyab f x y f ab. say f x y continuous function continuous every point domain d. unless indicated otherwise assume functions deal con tinuous. fact modify function example 3.8 continuous r2. example 3.9. deﬁne function f x y r2 follows f x y 0 x y 00 y4 x2 y2 x y 00 f x y welldeﬁned x y r2 that is indeterminate forms x y see lim xyab f x y b4 a2 b2 f ab ab 00. since lim xy00 f x y 0 f 00 example 3.8 f x y continuous r2. exercises exercises 16 state domain range given function.
3.1 functions two three variables 79 1. f x y x2 y2 1 2. f x y 1 x2 y2 3. f x y p x2 y2 4 4. f x y x2 1 5. f x y z sinxyz 6. f x y z p x1yz 1. exercises 718 evaluate given limit. 7. lim xy00 cosxy 8. lim xy00 exy 9. lim xy00 x2 y2 x2 y2 10. lim xy00 xy2 x2 y4 11. lim xy11 x2 2xy y2 xy 12. lim xy00 xy2 x2 y2 13. lim xy11 x2 y2 xy 14. lim xy00 x2 2xy y2 xy 15. lim xy00 y4 sinxy x2 y2 16. lim xy00 x2 y2cos µ 1 xy 17. lim xy00 x y 18. lim xy00 cos µ 1 xy . b 19. show f x y 1 2πσ2 ex2y22σ2 σ 0 constant circle radius r 0 centered origin. function called gaussian blur used ﬁlter image processing software produce blurred effect. 20. suppose f x y f yx x y r2. show f x y f yx x y r2. 21. use substitution r p x2 y2 show lim xy00 sin p x2 y2 p x2 y2 1 . hint need use lhôpitals rule singlevariable limits. c 22. prove theorem 3.1a case addition. hint use deﬁnition 3.1. 23. prove theorem 3.1b.
80 chapter 3. functions several variables 3.2 partial derivatives idea functions several variables are limit function is start develop idea derivative function two variables. start notion partial derivative. deﬁnition 3.3. let f x y realvalued function domain r2 let ab point d. partial derivative f ab respect x denoted f x ab deﬁned f x ab lim h0 f a hbf ab h 3.2 partial derivative f ab respect y denoted f yab deﬁned f yab lim h0 f ab hf ab h . 3.3 note symbol is pronounced del.1 recall derivative function f x interpreted rate change function positive x direction. deﬁnitions above see partial derivative function f x y respect x rate change f x y positive x direction respectively. means partial derivative function f x y respect x calculated treating variable constant simply differentiating f x y function x alone using usual rules singlevariable calculus. likewise partial derivative f x y respect obtained treating x variable constant differentiating f x y function alone. example 3.10. find f x x y f yx y function f x y x2y y3. solution treating constant differentiating f x y respect x gives f x x y 2xy treating x constant differentiating f x y respect gives f yx y x2 3y2 . often simply write f x f y instead f x x y f yx y. 1it greek letter. symbol ﬁrst used mathematicians a. clairaut l. euler around 1740 distinguish letter used usual derivative.
3.2 partial derivatives 81 example 3.11. find f x f y function f x y sinxy2 x2 1 . solution treating constant differentiating f x y respect x gives f x x2 1y2 cosxy22x sinxy2 x2 12 treating x constant differentiating f x y respect gives f y 2xy cosxy2 x2 1 . since f x f y functions x y take partial derivatives respect x y. yields higherorder partial derivatives 2 f x2 x ³f x 2 f y2 y ³f y 2 f yx y ³f x 2 f xy x ³f y 3 f x3 x ³2 f x2 3 f y3 y ³2 f y2 3 f yx2 y ³2 f x2 3 f xy2 x ³2 f y2 3 f y2 x y ³ 2 f yx 3 f x2 y x ³ 2 f xy 3 f xyx x ³ 2 f yx 3 f yxy y ³ 2 f xy . . . example 3.12. find partial derivatives f x f y 2 f x2 2 f y2 2 f yx 2 f xy function f x y ex2 xy3.
82 chapter 3. functions several variables solution proceeding before f x 2xyex2 y3 f y x2ex2 3xy2 2 f x2 x2xyex2 y3 2 f y2 yx2ex2 3xy2 2yex2 4x2y2ex2 y x4ex2 6xy 2 f yx y2xyex2 y3 2 f xy xx2ex2 3xy2 2xex2 2x3yex2 3y2 2xex2 2x3yex2 3y2. higherorder partial derivatives taken respect different variables 2 f yx 2 f xy called mixed partial derivatives. notice example 2 f yx 2 f xy. turns usually case. speciﬁcally whenever 2 f yx 2 f xy continuous point ab equal point.2 functions deal continuous partial derivatives orders assume remainder text 2 f yx 2 f xy x y domain f . words doesnt matter order take partial derivatives. applies even mixed partial derivatives order 3 higher. notation partial derivatives varies. following equivalent f x fxx y f1x y dxx y d1x y f y f yx y f2x y yx y d2x y 2 f x2 fxxx y f11x y dxxx y d11x y 2 f y2 f yyx y f22x y yyx y d22x y 2 f yx fxyx y f12x y dxyx y d12x y 2 f xy f yxx y f21x y yxx y d21x y . 2see pp. 214216 taylor mann proof.
3.2 partial derivatives 83 exercises exercises 116 ﬁnd f x f y. 1. f x y x2 y2 2. f x y cosx y 3. f x y p x2 y4 4. f x y x1 y1 5. f x y exy xy 6. f x y x2 y2 6xy4x8y2 7. f x y x4 8. f x y x2y 9. f x y p x2 y2 10. f x y sinx y 11. f x y 3 p x2 y4 12. f x y xy1 x 13. f x y ex2y2 14. f x y lnxy 15. f x y sinxy 16. f x y tanx y. exercises 1726 ﬁnd 2 f x2 2 f y2 2 f yx use exercises 18 14 15. 17. f x y x2 y2 18. f x y cosx y 19. f x y p x2 y4 20. f x y x1 y1 21. f x y exy xy 22. f x y x2 y2 6xy4x8y2 23. f x y x4 24. f x y x2y 25. f x y lnxy 26. f x y sinxy. b 27. show function f x y sinx ycosxy satisﬁes wave equation 2 f x2 2 f y2 0 . wave equation example partial differential equation. 28. let u v twicedifferentiable functions single variable let c 0 con stant. show f x y ux cyvxcy solution general onedimensional wave equation3 2 f x2 1 c2 2 f y2 0 . 3conversely turns solution must form. see ch. 1 weinberger.
84 chapter 3. functions several variables 3.3 tangent plane surface previous section mentioned partial derivatives f x f y thought rate change function z f x y positive x directions respectively. recall derivative dy dx function f x geometric meaning namely slope tangent line graph f point x f x r2. similar geometric meaning partial derivatives f x f y function z f x y given point ab domain f x y trace surface described z f x y plane b curve r3 point ab f ab slope tangent line lx curve point f x ab. similarly f yab slope tangent line l trace surface z f x y plane x see figure 3.3.1. z x 0 ab lx b ab f ab slope f x ab z f x y a tangent line lx plane b z x 0 ab l ab f ab slope f yab z f x y b tangent line l plane x figure 3.3.1 partial derivatives slopes. since derivative dy dx function f x used ﬁnd tangent line graph f which curve r2 might expect partial derivatives used deﬁne tangent plane graph surface z f x y. indeed turns case. first need deﬁnition tangent plane. intuitive idea tangent plane just touches surface point. formal deﬁnition mimics intuitive notion tangent line curve. deﬁnition 3.4. let z f x y equation surface r3 let p ab c point s. let plane contains point p let q x y z represent generic point surface s. acute angle vector pq plane approaches zero point q approaches p along surface s call tangent plane p. note since two lines r3 determine plane two tangent lines surface z f x y x directions described figure 3.3.1 contained tangent plane point tangent plane exists point. existence two
3.3 tangent plane surface 85 tangent lines guarantee existence tangent plane. possible take trace surface plane xy 0 which makes 45angle positive xaxis resulting curve plane may tangent line plane determined two tangent lines may tangent line point. luckily turns out4 f x f y exist region around point ab continuous ab tangent plane surface z f x y exist point ab f ab. text conditions always hold. z x 0 ab f ab z f x y lx l figure 3.3.2 tangent plane suppose want equation tangent plane surface z f x y point ab f ab. let lx l tangent lines traces surface planes b x a respectively as figure 2.3.2 suppose conditions exist hold. equation axabybcz f ab 0 3.4 n abc normal vector plane t. since contains lines lx l y need vectors vx vy parallel lx l y respectively let n vx vy. x z 0 vx 10 f x ab f x ab 1 figure 3.3.3 since slope lx f x ab vector vx 10 f x ab parallel lx since vx lies xzplane lies line slope f x ab 1 f x ab. see figure 2.3.3. similarly vector vy 01 f yab parallel l y. hence vector n vx vy j k 1 0 f x ab 0 1 f yab f x abif yabjk normal plane t. thus equation f x abxaf yabyb z f ab 0 . 3.5 multiplying sides 1 following result equation tangent plane surface z f x y point ab f ab f x abxa f yabybz f ab 0 3.6 example 3.13. find equation tangent plane surface z x2 y2 point 125. 4see taylor mann 6.4.
86 chapter 3. functions several variables solution function f x y x2 y2 f x 2x f y 2y equation tangent plane point 125 21x122y2z 5 0 2x4yz 5 0 . similar fashion shown surface deﬁned implicitly equation form fx y z 0 tangent plane surface point ab c given equation f x ab cxa f y ab cyb f z ab cz c 0 . 3.7 note formula 3.6 special case formula 3.7 fx y z f x yz. example 3.14. find equation tangent plane surface x2 y2 z2 9 point 221. solution function fx y z x2 y2 z2 9 f x 2x f y 2y f z 2z equation tangent plane 221 22x222y221z 1 0 2x2yz 9 0 . exercises exercises 16 ﬁnd equation tangent plane surface z f x y point p. 1. f x y x2 y3 p 112 2. f x y xy p 111 3. f x y x2y p 111 4. f x y xey p 101 5. f x y x2y p 214 6. f x y p x2 y2 p 345. exercises 710 ﬁnd equation tangent plane given surface point p. 7. x2 4 y2 9 z2 16 1 p ³ 12 2 p 11 3 8. x2 y2 z2 9 p 003 9. x2 y2 z2 0 p 345 10. x2 y2 4 p p 310. b 11. find angles curve ft tt2t3 surface x6 y3 z2 3 intersections.
3.4 directional derivatives gradient 87 3.4 directional derivatives gradient function z f x y learned partial derivatives f x f y represent instantaneous rate change f positive x directions respectively. directions turns ﬁnd rate change direction using general type derivative called directional derivative. deﬁnition 3.5. let f x y realvalued function domain r2 let ab point d. let v vector r2. directional derivative f ab direction v denoted dv f ab deﬁned dv f ab lim h0 f ab hvf ab h . 3.8 notice deﬁnition seem treating point ab vector since adding vector hv it. usual idea identifying vectors terminal points reader used now. write vector v v v1v2 dv f ab lim h0 f a hv1b hv2f ab h . 3.9 immediately recognize partial derivatives f x f y special cases directional derivative v 10 v j 01 respectively. is f x di f f y dj f . f x y continuous partial derivatives f x f y which always case text simple formula directional derivative theorem 3.2. let f x y realvalued function domain r2 partial derivatives f x f y exist continuous d. let ab point d. dv f ab v1 f x ab v2 f yab . 3.10 vector v v1v2 r2 proof note v 10 formula reduces dv f ab f x ab know true since di f f x noted earlier. similarly v j 01 formula reduces dv f ab f yab true since dj f f y. fix vector v v1v2 ﬁx number h 0. f a hv1b hv2f ab f a hv1b hv2f a hv1b f a hv1bf ab . 3.11
88 chapter 3. functions several variables since gα f a hv1 αhv2 realvalued function apply mean value theorem singlevariable calculus interval 01. provides number 0 α 1 g α g1g0 10 f a hv1b hv2f a hv1b. chain rule g α f ya hv1b αhv2hv2. therefore f a hv1b hv2f a hv1b hv2 f x a hv1b αhv2. similar argument exists number 0 β 1 f a hv1bf ab hv1 f x aβhv1b . thus equation 3.11 f a hv1b hv2f ab h hv2 f ya hv1b αhv2 hv1 f x aβhv1b h v2 f ya hv1b αhv2 v1 f x aβhv1b formula 3.9 dv f ab lim h0 f a hv1b hv2f ab h lim h0 v2 f ya hv1b αhv2 v1 f x aβhv1b v2 f yab v1 f x ab continuity f x f y dv f ab v1 f x ab v2 f yab reversing order summation. qed along lines one prove following generalization chain rule.
3.4 directional derivatives gradient 89 theorem 3.3. let f x y realvalued function domain r2 partial derivatives f x f y exist continuous ht h1th2t smooth function values d. f ht dht f ht h 1tf x h1th2t h 2tf yh1th2t. 3.12 note dv f ab v ³ f x ab f yab . second vector special name deﬁnition 3.6. realvalued function f x y gradient f denoted f vector f ³f x f y 3.13 r2. realvalued function f x y z gradient vector f ³f x f y f z 3.14 r3. symbol is pronounced del nabla.5 corollary 3.4. assumptions theorems 3.2 3.3 a dv f v f b f ht ht f example 3.15. find directional derivative f x y xy2 x3y point 12 direction v ³ 1 p 2 1 p 2 . solution see f y2 3x2y2xy x3 dv f 12 v f 12 ³ 1 p 2 1 p 2 22 312221213 15 p 2 realvalued function z f x y whose partial derivatives f x f y exist con tinuous called continuously differentiable. assume f x y function f 0. let c real number range f let v vector r2 tangent level curve f x y c see figure 3.4.1. 5sometimes notation gradf used instead f .
90 chapter 3. functions several variables x 0 v f f x y c figure 3.4.1 value f x y constant along level curve since v tangent vector curve rate change f direction v 0 is dv f 0. know dv f v f . words f v means f normal level curve. general unit vector v r2 dv f f cosθ θ angle v f . ﬁxed point x y length f is ﬁxed value dv f varies θ varies. largest value dv f take cosθ 1 θ 0 smallest value occurs cosθ 1 θ 180. words value function f increases fastest direction f since θ 0in case value f decreases fastest direction f since θ 180in case. thus proved following theorem theorem 3.5. let f x y continuously differentiable realvalued function f 0. then a gradient f normal level curve f x y c. b value f x y increases fastest direction f . c value f x y decreases fastest direction f . example 3.16. direction function f x y xy2 x3y increase fastest point 12 direction decrease fastest solution since f y2 3x2y2xy x3 f 12 105 0. unit vector direction v f f ³ 2 p 5 1 p 5 . thus f increases fastest direction ³ 2 p 5 1 p 5 decreases fastest direction ³ 2 p 5 1 p 5 .
3.4 directional derivatives gradient 91 though proved theorem 3.5 functions two variables similar argument used show also applies functions three variables. likewise directional derivative threedimensional case also deﬁned formula dv f v f . example 3.17. temperature solid given function tx y z ex e2y e4z x y z space coordinates relative center solid. direction point 111 temperature decrease fastest solution since f ex2e2y4e4z temperature decrease fastest direction f 111 e12e24e4. exercises exercises 110 compute gradient f . 1. f x y x2 y2 1 2. f x y 1 x2 y2 3. f x y p x2 y2 4 4. f x y x2ey 5. f x y lnxy 6. f x y 2x5y 7. f x y z sinxyz 8. f x y z x2eyz 9. f x y z x2 y2 z2 10. f x y z p x2 y2 z2. exercises 1114 ﬁnd directional derivative f point p direction v ³ 1 p 2 1 p 2 . 11. f x y x2 y2 1 p 11 12. f x y 1 x2 y2 p 11 13. f x y p x2 y2 4 p 11 14. f x y x2ey p 11. exercises 1516 ﬁnd directional derivative f point p direction v ³ 1 p 3 1 p 3 1 p 3 . 15. f x y z sinxyz p 111 16. f x y z x2eyz p 111. 17. repeat example 2.16 point 23. 18. repeat example 2.17 point 312. b exercises 1926 let f x y gx y continuously differentiable realvalued func tions let c constant let v unit vector r2. show that
92 chapter 3. functions several variables 19. cf cf 20. f g f g 21. f g f g gf 22. f g gf f g g2 gx y 0 23. dv f dv f 24. dvcf cdv f 25. dvf g dv f dvg 26. dvf g f dvg gdv f . 27. function rx y p x2 y2 length position vector r xi yj point x y r2. show r 1 r r x y 00 r2 2r. c 28. let gx f x y smooth function f x gx 0. show f x x gx gx f yx gx 0. hint apply theorem 3.3 curve ht t gt.
3.5 maxima minima 93 3.5 maxima minima gradient used ﬁnd extreme points realvalued functions several variables is points function local maximum local minimum. consider functions two variables functions three variables require methods using linear algebra. deﬁnition 3.7. let f x y realvalued function let x0 y0 point domain f . say f local maximum x0 y0 f x y f x0 y0 x y inside disk positive radius centered x0 y0 is sufﬁciently small r 0 f x y f x0 y0 x y xx02 yy02 r2. likewise say f local minimum x0 y0 f x y f x0 y0 x y inside disk positive radius centered x0 y0. f x y f x0 y0 x y domain f f global maximum x0 y0. f x y f x0 y0 x y domain f f global minimum x0 y0. suppose x0 y0 local maximum point f x y ﬁrstorder partial derivatives f exist x0 y0. know f x0 y0 largest value f x y x y goes directions point x0 y0 sufﬁciently small disk centered x0 y0. particular f x0 y0 largest value f x direction around point x0 y0 is singlevariable function gx f xb local maximum x a. know g a 0. since g x f x xb f x x0 y0 0. similarly f x0 y0 largest value f near x0 y0 direction f yx0 y0 0. thus following theorem theorem 3.6. let f x y realvalued function f x x0 y0 f yx0 y0 exist. necessary condition f x y local maximum minimum x0 y0 f x0 y0 0. note theorem 3.6 extended apply functions three variables. point x0 y0 f x0 y0 0 called critical point function f x y. given function f x y ﬁnd critical points f solve equations f x x y 0 f yx y 0 simultaneously x y. similar singlevariable case necessary condition f x0 y0 0 always sufﬁcient guarantee critical point local maximum minimum. example 3.18. function f x y xy critical point 00 f x 0 y 0 f y x 0 x 0 00 critical point. clearly f local maximum minimum 00 since disk around 00 contains points x y values x sign so f x y xy 0 f 00 different signs so f x y xy 0 f 00. fact along path x r2 f x y x2
94 chapter 3. functions several variables local minimum 00 along path x f x y x2 local maximum 00. 00 example saddle point is local maximum one direction local minimum another direction. graph f x y shown figure 3.5.1 hyperbolic paraboloid. 10 5 0 5 10 10 5 0 5 10 100 50 0 50 100 z x z figure 3.5.1 f x y xy saddle point 00. course singlevariable calculus may remember second derivative test. f x0 0 f 0 0 realtoreal function f local minimum x0. order explain multivariable analog test let us introduce second direc tional derivative. fix vector v r2 smooth function two variables f x y. directional derivative hx y dv f x y smooth function two variables take directional derivative dvhx y called second directional derivative denoted d2 v f x y. dv f x0 y0 0 d2 v f x0 y0 0 vector v 0 smooth function f x y two variables local minimum x0 y0. form second derivative test useful since requires check inequality d2 v f x0 y0 0 inﬁnite number vectors v. let us try remove weak point.
3.5 maxima minima 95 note v ab dv f x0 y0 f x b f y. d2 v f x0 y0 dva f x b f y a2 2 f x2 ab 2 f xy ab 2 f yx b2 2 f y2 a2 2 f x2 2ab 2 f yx b2 2 f y2 last equality holds since function f smooth and therefore 2 f xy 2 f yx therefore condition d2 v f x0 y0 0 v 0 means a2 2 f x2 2ab 2 f yx b2 2 f y2 0 pair real numbers ab least one zero. analyzing last inequality possible pairs ab leads following theorem true analog second derivative test smooth functions two variables gives sufﬁcient conditions critical point local maximum minimum smooth function that is function whose partial derivatives orders exist continuous. theorem proved here.6 theorem 3.7. let f x y smooth realvalued function critical point x0 y0 that is f x0 y0 0. deﬁne 2 f x2 x0 y0 2 f y2 x0 y0 ³ 2 f yxx0 y0 2 a 0 2 f x2 x0 y0 0 f local minimum x0 y0 b 0 2 f x2 x0 y0 0 f local maximum x0 y0 c 0 f neither local minimum local maximum x0 y0 d 0 test fails. condition c holds x0 y0 saddle point is second directional deriva tive d2 v f x0 y0 positive negative different vectors v. 6see taylor mann 7.6.
96 chapter 3. functions several variables recall assumption f x y smooth implies 2 f yx 2 f xy. therefore 2 f x2 x0 y0 2 f yxx0 y0 2 f xyx0 y0 2 f y2 x0 y0 . also 0 2 f x2 x0 y0 2 f y2 x0 y0 d ³ 2 f yxx0 y0 2 0 2 f x2 x0 y0 2 f y2 x0 y0 sign. means parts a b theorem one replace 2 f x2 x0 y0 2 f y2 x0 y0 desired. example 3.19. find local maxima minima f x y x2 xy y2 3x. solution first ﬁnd critical points is solve f 0. since f x 2x y3 f y x2y critical points x y common solutions equations 2x y3 0 x2y 0 unique solution x y 21. 21 critical point. use theorem 3.7 need secondorder partial derivatives 2 f x2 2 2 f y2 2 2 f yx 1 2 f x2 21 2 f y2 21 ³ 2 f yx21 2 2212 3 0 2 f x2 21 2 0. thus 21 local minimum. example 3.20. find local maxima minima f x y xyx3 y2. solution first ﬁnd critical points is solve f 0. since f x y3x2 f y x2y critical points x y common solutions equations y3x2 0 x2y 0
3.5 maxima minima 97 ﬁrst equation yields 3x2 substituting second equation yields x6x2 0 solutions x 0 x 1 6. x 0 y 30 0 x 1 6 y 3 1 6 2 1 12. critical points x y 00 x y 1 6 1 12 . use theorem 3.7 need secondorder partial derivatives 2 f x2 6x 2 f y2 2 2 f yx 1 2 f x2 00 2 f y2 00 ³ 2 f yx00 2 60212 1 0 thus 00 saddle point. also 2 f x2 1 6 1 12 2 f y2 1 6 1 12 ³ 2 f yx 1 6 1 12 2 6 1 6 212 1 0 2 f x2 1 6 1 12 1 0. thus 1 6 1 12 local maximum. example 3.21. find local maxima minima f x y x24 x2y2. solution first ﬁnd critical points is solve f 0. since f x 4x23 2x2y f y 4x2y critical points x y common solutions equations 4x23 2x2y 0 4x2y 0 second equation yields x 2y substituting ﬁrst equation yields 42y23 0 solution 1 x 21 2. thus 21 critical point. use theorem 3.7 need secondorder partial derivatives 2 f x2 12x22 2 2 f y2 8 2 f yx 4 2 f x2 21 2 f y2 21 ³ 2 f yx21 2 2842 0 test fails. done situation sometimes possible examine function see directly nature critical point. case see f x y 0 x y since f x y sum fourth second powers numbers hence must nonnegative. also see f 21 0. thus f x y 0 f 21 x y hence 21 is fact global minimum f .
98 chapter 3. functions several variables example 3.22. find local maxima minima f x y x2 y2ex2y2. solution first ﬁnd critical points is solve f 0. since f x 2x1x2 y2ex2y2 f y 2y1x2 y2ex2y2 critical points 00 points x y unit circle x2 y2 1. use theorem 3.7 need secondorder partial derivatives 2 f x2 21x2 y22x2 2x21x2 y2ex2y2 2 f y2 21x2 y22y2 2y21x2 y2ex2y2 2 f yx 4xy2x2 y2ex2y2 00 4 0 2 f x2 00 2 0 00 local minimum. however points x y unit circle x2 y2 1 4x2e14y2e14xye12 0 test fails. look graph f x y shown figure 3.5.2 looks like might local maximum x y unit circle x2 y2 1. switch using polar coordinates rθ instead x y r2 r2 x2y2 see write f x y function gr variable r alone gr r2er2. g r 2r1 r2er2 critical point r 1 check g 1 4e1 0 second derivative test singlevariable calculus says r 1 local maximum. r 1 corresponds unit circle x2 y2 1. thus points x y unit circle x2 y2 1 local maximum points f . exercises exercises 110 ﬁnd local maxima minima function f x y.
3.5 maxima minima 99 3 2 1 0 1 2 3 3 2 1 0 1 2 3 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 z x z figure 3.5.2 f x y x2 y2ex2y2. 1. f x y x3 3x y2 2. f x y x3 12x y2 8y 3. f x y x3 3x y3 3y 4. f x y x3 3x2 y3 3y2 5. f x y 2x3 6xy3y2 6. f x y 2x3 6xy y2 7. f x y p x2 y2 8. f x y x2y 9. f x y 4x2 4xy2y2 10x6y 10. f x y 4x2 4xy2y2 16x12y. b 11. rectangular solid volume 1000 cubic meters ﬁnd dimensions min imize surface area. hint use volume condition write surface area function two variables. 12. prove x0 y0 local maximum local minimum point smooth function f x y tangent plane surface z f x y point x0 y0 f x0 y0 parallel xyplane. hint use theorem 3.6. c 13. find three positive numbers x y z whose sum 10 x2y2z maximum.
100 chapter 3. functions several variables 3.6 numerical methods types problems solved previous section examples unconstrained optimization problems. is tried ﬁnd local and perhaps even global maximum minimum points realvalued functions f x y points x y could points domain f . method used required us ﬁnd critical points f meant solve equation f 0 general system two equations two unknowns x y. relatively simple examples did general case. might impossible solve equations elementary means.7 situation this choice may ﬁnd solution using numer ical method gives sequence numbers converge actual solution. example newtons method solving equations f x 0 probably learned singlevariable calculus. section describe another method newton ﬁnd ing critical points realvalued functions two variables. let f x y smooth realvalued function deﬁne dx y 2 f x2 x y 2 f y2 x y ³ 2 f yxx y 2 . newtons algorithm pick initial point x0 y0. n 0123. deﬁne xn1 xn 2 f y2 xn yn 2 f xyxn yn f yxn yn f x xn yn dxn yn yn1 yn 2 f x2 xn yn 2 f xyxn yn f x xn yn f yxn yn dxn yn . 3.15 sequence points xn yn n1 typically converges critical point. several critical points try different initial points ﬁnd them. choice formulas 3.15 motivated following fact checked direct calculations. assume partial derivatives 2 f x2 x y 2 f xyx y 2 f y2 x y constants words function f x y expressed quadratic polynomial x y say f x y a bx cy lx2 mxy ny2 constants ab clmn. choice x0 y0 formulas 3.15 returns critical point x1 y1 unique case. 7this also problem equivalent method the second derivative test singlevariable calculus though one usually emphasized.
3.6 numerical methods 101 example 3.23. find local maxima minima f x y x3 xyx xy3 y4. solution first calculate necessary partial derivatives f x 3x2 y1 y3 f y x3xy2 4y3 2 f x2 6x 2 f y2 6xy12y2 2 f yx 13y2 notice solving f 0 would involve solving two thirddegree polynomial equations x y case done easily. need pick initial point x0 y0 algorithm. looking graph z f x y large region may help see figure 3.6.1 below though may hard tell critical points are. 20 15 10 5 0 5 10 15 20 20 15 10 5 0 5 10 15 20 350000 300000 250000 200000 150000 100000 50000 0 50000 z x z figure 3.6.1 f x y x3 xyx xy3 y4 20 x 20 20 y 20. notice formulas 3.15 divide d pick initial point zero. see d00 0012 1 0 take 00 initial point. since may take large number iterations newtons algorithm sure close enough actual critical point since computations quite tedious let computer computing. this write simple program using java programming language take given initial point parameter
102 chapter 3. functions several variables perform 100 iterations newtons algorithm. iteration new point printed see convergence. full code shown listing 3.1. program find critical points fxyx3xyxxy3y4 public class newton public static void mainstring args get initial point xy commandline parameters double x double.parsedoubleargs0 initial x value double double.parsedoubleargs1 initial value system.out.printlninitial point x go 100 iterations newtons algorithm int n1 n100 n double fxxxyfyyxy math.powfxyxy2 double xn x double yn y the current x values d 0 we divide 0 system.out.printlnerror 0 iteration n n system.exit0 end program else calculate new values x x xn fyyxnynfxxnyn fxyxnynfyxnynd yn fxxxnynfyxnyn fxyxnynfxxnynd system.out.printlnn n x below parts specific function f the first partial derivative f wrt x 3x2y1y3 public static double fxdouble x double y return 3math.powx2 1 math.powy3 the first partial derivative f wrt y x3xy24y3 public static double fydouble x double y return x 3xmath.powy2 4math.powy3 the second partial derivative f wrt x 6x public static double fxxdouble x double y return 6x the second partial derivative f wrt y 6xy12y2 public static double fyydouble x double y return 6xy 12math.powy2 the mixed second partial derivative f wrt x y 13y2 public static double fxydouble x double y return 1 3math.powy2 listing 3.1 program listing newton.java
3.6 numerical methods 103 use program ﬁrst save code listing 3.1 plain text ﬁle called newton.java. need java development kit8 compile code. directory newton.java saved run command command prompt compile code javac newton.java run program initial point 00 command java newton 0 0 output program using 00 initial point truncated show ﬁrst 10 lines last 5 lines java newton 0 0 initial point 0.00.0 n 1 0.01.0 n 2 1.00.5 n 3 0.60658578856152510.44194107452339687 n 4 0.4845065729665450.405341511995805 n 5 0.471239726826344850.3966334583092305 n 6 0.471135585103495350.39636450001936047 n 7 0.47113563434497050.3963643379632247 n 8 0.47113563434498740.39636433796318005 n 9 0.47113563434498740.39636433796318005 n 10 0.47113563434498740.39636433796318005 . n 96 0.47113563434498740.39636433796318005 n 97 0.47113563434498740.39636433796318005 n 98 0.47113563434498740.39636433796318005 n 99 0.47113563434498740.39636433796318005 n 100 0.47113563434498740.39636433796318005 see appear converged fairly quickly after 8 iterations appears actual critical point up javas level precision namely point 0.47113563434498740.39636433796318005. easy conﬁrm f 0 point either evaluating f x f y point modifying program also print values partial derivatives point. turns partial derivatives indeed close enough zero considered zero f x 0.47113563434498740.39636433796318005 4.857225732735061017 f y0.47113563434498740.39636433796318005 8.3266726846886741017 also d0.47113563434498740.39636433796318005 8.776075636032301 0 theorem 3.7 know 0.47113563434498740.39636433796318005 saddle point. 8available free
104 chapter 3. functions several variables since f consists cubic polynomials seems likely may three critical points. computer program makes experimenting initial points easy trying different values indeed lead different sequences converge java newton 1 1 initial point 1.01.0 n 1 0.50.5 n 2 0.492957746478873250.08450704225352113 n 3 0.18556747524613831.2047647348546167 n 4 0.45400605745313830.8643989895639324 n 5 0.36721605344440.5426077421319053 n 6 0.47946222228564170.24529117721011612 n 7 0.115707439929545912.4319791238981274 n 8 0.058378517655333171.6536079835854451 n 9 0.1298412986500071.121516233310142 n 10 1.0044530149672080.9206128022529645 n 11 0.51612099146124750.4176293491131443 n 12 0.57886640438638840.2918236503332734 n 13 0.69851771242307150.49848120123515316 n 14 0.67336189165787020.4345777963475479 n 15 0.67043929134134440.4252025996474051 n 16 0.67038326791502860.4250147307973365 n 17 0.67038324592387010.42501465652421205 n 18 0.67038324592386670.4250146565242004 n 19 0.67038324592386670.42501465652420045 n 20 0.67038324592386670.42501465652420045 . n 98 0.67038324592386670.42501465652420045 n 99 0.67038324592386670.42501465652420045 n 100 0.67038324592386670.42501465652420045 again easy conﬁrm f x f y vanish point 0.67038324592386670.42501465652420045 means critical point. d0.67038324592386670.42501465652420045 15.3853578526055 0 2 f x2 0.67038324592386670.42501465652420045 4.0222994755432 0 know 0.67038324592386670.42501465652420045 local maximum. idea graph f looks like near point shown figure 3.6.2 suggest local maximum around point. finally running computer program initial point 55 yields critical point 7.5409627569925515.595509445899435 0 point makes saddle point.
3.6 numerical methods 105 1 0.8 0.6 0.4 0.2 0 0 0.2 0.4 0.6 0.8 1 1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 z x z 0.670.420.57 figure 3.6.2 f x y x3 xyx xy3 y4 1 x 0 0 y 1. summarize ﬁndings function f x y x3 xyx xy3 y4 0.47113563434498740.39636433796318005 saddle point 0.67038324592386670.42501465652420045 local maximum 7.5409627569925515.595509445899435 saddle point derivation newtons algorithm proof converges given reason able choice initial point requires techniques beyond scope text. see ralston rabinowitz detail discussion numerical methods. description newtons algorithm special twovariable case general algorithm applied functions n 2 variables. case functions global maximum minimum newtons algorithm used ﬁnd points. general global maxima minima tend interesting local versions least practical applications. maximization problem always turned minimization problem why large number methods developed ﬁnd global minimum functions number variables. ﬁeld study called nonlinear programming. many methods based steepest descent technique based idea discussed section 2.4. recall negative gradient f gives
106 chapter 3. functions several variables direction fastest rate decrease function f . crux steepest descent idea then starting initial point move certain amount direction f point. wherever takes becomes new point keep repeating procedure eventually hopefully reach point f smallest value. pure steepest descent method multitude variations improve rate convergence ease calculation etc. discussion this nonlinear programming general see bazaraa sherali shetty. exercises c 1. recall example 3.21 previous section showed point 21 global minimum function f x y x 24 x 2y2. notice computer program modiﬁed fairly easily use function just change return values fx fy fxx fyy fxy function deﬁnitions use appropriate partial derivative. either modify program write one programming language choice show newtons algorithm lead point 21. first use initial point 03 use initial point 32 compare results. make sure program attempts 100 iterations algorithm. anything strange happen program ran so explain it hint something strange happen. 2. version newtons algorithm solving system two equations f1x y 0 f2x y 0 f1x y f2x y smooth realvalued functions pick initial point x0 y0. n 0123. deﬁne xn1 xn f1xn yn f2xn yn f1 y xn yn f2 y xn yn dxn yn yn1 yn f1xn yn f2xn yn f1 x xn yn f2 x xn yn dxn yn dxn yn f1 x xn yn f2 y xn ynf1 y xn yn f2 x xn yn . sequence points xn yn n1 converges solution. write computer program uses algorithm ﬁnd approximate solutions system equations f1x y sinxyxy 0 f2x y e2x 2x3y 0 . show get two different solutions using 00 11 initial point x0 y0.
3.7 lagrange multipliers 107 3.7 lagrange multipliers sections 2.5 2.6 concerned ﬁnding maxima minima functions without constraints variables other domain function. would constraints variables following example illus trates simple case type problem. example 3.24. rectangle whose perimeter 20 m ﬁnd dimensions max imize area. solution area rectangle width x height xy. perimeter p rectangle given formula p 2x2y. since given perimeter p 20 problem stated as maximize f x y xy given 2x2y 20 reader probably familiar simple method using singlevariable calculus solving problem. since must 2x 2y 20 solve for say terms x using equation. gives 10x substitute f get f x y xy x10x 10x x2. function x alone maximize function f x 10xx2 interval 010. since f x 102x 0 x 5 f 5 2 0 second derivative test tells us x 5 local maximum f hence x 5 must global maximum interval 010 since f 0 endpoints interval. since 10x 5 maximum area occurs rectangle whose width height 5 m. notice example ease solution depended able solve one variable terms equation 2x2y 20. possible which often case section use general method called lagrange multiplier method9 solving constrained optimization problems maximize or minimize f x y or f x y z given gx y c or gx y z c constant c equation gx y c called constraint equation say x con strained gx y c. points x y maxima minima f x y con dition satisfy constraint equation gx y c called constrained maximum constrained minimum points respectively. similar deﬁnitions hold functions three variables. lagrange multiplier method solving problems stated 9named french mathematician joseph louis lagrange 17361813.
108 chapter 3. functions several variables theorem 3.8. let f x y gx y smooth functions suppose c scalar constant gx y 0 x y satisfy equation gx y c. solve constrained optimization problem maximize or minimize f x y given gx y c ﬁnd points x y solve equation f x y λgx y constant λ. number λ called lagrange multiplier point x y called critical point f x y constrained gx y c. constrained maximum minimum must critical point f x y constrained gx y c. recall gx y perpendicular tangent line curve gx y c point x y. therefore condition f x y λgx y simply means f x y perpen dicular tangent line curve gx y c point x y. intuitively clear f x y perpendicular tangent line slight movement along curve gx y c increase decrease value f x y particular x y nei ther minimum maximum point. rigorous proof however requires use implicit function theorem beyond scope text.10 note theorem gives necessary condition point constrained maximum minimum. is minimum maximum achieved point point must critical theorem says nothing existence minimum maximum points. let us discuss two important cases existence minima maxima guaranteed follow called extreme value theorem also beyond scope text. recall set called bounded completely lies ball sufﬁciently large radius. following condition guarantees existence maximum minimum proof given taylor mann. constraint equation gx y c describes bounded set r2 constrained maximum minimum f x y occur points. condition holds theorem maximum minimum points one critical. remains ﬁnd critical points x y compare values f x y maximum values global maximum f x y constraint gx y c analogously minimal value global minimum. let us formulate general condition guarantees existance minimum maximum. 10see taylor mann 6.8 detail.
3.7 lagrange multipliers 109 set described system gx y c f x y d empty bounded constrained minimum f x y occur points. again ﬁnd minimum ﬁnd values f x y critical points x y minimal value global minimum f x y constraint gx y c. example f x y x2 y2 set described f x y d bounded d. therefore condition guarantees existence minimum constrain gx y c. analogous condition maximum. set described system gx y c f x y d empty bounded constrained maximum f x y occur points. x 2 x 6 x 2 xy 1 sometimes answer depend hidden constants. example consider func tion f x y x constraint xy 1. equation f λg theorem 3.8 takes form 11 λyx. since yx 1 two critical points 11 multiplier 1 11 multiplier 1. non points maximum minimum fact f positive negative values arbitrary large absolute value. hand might happen nature problem x positive it called implicit constraints one point 11 satisﬁes constrained minimum point minimum 2. general case type maximum minimum f x y occur either point x y satisfying f x y λgx y boundary point set described hidden constraints. similar thing happens example 3.24 constraint equation 2x2y 20 describes line r2 bounded. however hidden constraints due
110 chapter 3. functions several variables nature problem namely 0 x 10 cause line restricted line segment r2 bounded endpoints line segment form boundary. example 3.25. rectangle whose perimeter 20 m use lagrange multiplier method ﬁnd dimensions maximize area. solution saw example 3.24 x representing width height respectively rectangle problem stated as maximize f x y xy given gx y 2x2y 20 solving system scalar vector equations gx y 20 f x y λgx y λ means solving system three scalar equations gx y 20 f x λg x f y λg y namely 2x2y 20 2λ x 2λ. general idea solve λ last two equations set expressions equal since equal λ solve x y. get 2 λ x 2 x substitute either expressions x constraint equation solve x y 20 gx y 2x2y 2x2x 4x x 5 5 must maximum area since minimum area 0 f 55 25 0 point 55 found called constrained critical point must constrained maxi mum. the maximum area occurs rectangle whose width height 5 m.
3.7 lagrange multipliers 111 example 3.26. find points circle x2 y2 80 closest farthest point 12. solution distance point x y point 12 q x12 y22 minimizing distance equivalent minimizing square distance. thus problem stated as maximize and minimize f x y x12 y22 given gx y x2 y2 80 solving f x y λgx y means solving following equations 2x1 2λx 2y2 2λy note x 0 since otherwise would get 2 0 ﬁrst equation. similarly 0. solve equations λ follows x1 x λ y2 xyy xy2x 2x x 0 48 12 48 x2 y2 80 figure 3.7.1 substituting gx y x2 y2 80 yields 5x2 80 x 4. two constrained critical points 48 48. since f 48 45 f 48 125 since must points circle closest farthest 12 must case 48 point circle clos est 12 48 farthest 12 see figure 2.7.1. notice since constraint equation x2y2 80 describes circle bounded set r2 guaranteed constrained critical points found indeed constrained maximum minimum. lagrange multiplier method extended functions three variables. example 3.27. maximize and minimize f x y z x z given gx y z x2 y2 z2 1
112 chapter 3. functions several variables solution solve system equations gx y z 1 f x y z λgx y z x2 y2 z2 1 1 2λx 0 2λy 1 2λz second equation implies λ 0 otherwise would 1 0 divide λ third equation get 0 divide λ ﬁrst last equations get x 1 2λ z. substituting expressions constraint equation x2 y2 z2 1 yields constrained critical points 1 p 20 1 p 2 1 p 20 1 p 2. since f 1 p 20 1 p 2 f 1 p 20 1 p 2 since constraint equation x2 y2 z2 1 describes sphere which bounded r3 ³ 1 p 20 1 p 2 constrained maximum point ³ 1 p 20 1 p 2 constrained minimum point. note solving equation f x y λgx y means solve system two possibly nonlinear equations three unknowns seen before may possible do. 3variable case get even complicated. somewhat restricts usefulness lagranges method relatively simple functions. luckily many numerical methods solving constrained optimization problems though discuss here.11 exercises 1. find constrained maxima minima f x y 2x given x2 y2 4. 2. find constrained maxima minima f x y xy given x2 3y2 6. 3. find constrained minima f x y x23y2 given xy 1 show constrained maxima. 4. find points circle x2 y2 100 closest farthest point 23. 11see bazaraa sherali shetty.
3.7 lagrange multipliers 113 b 5. find constrained maxima minima f x y z x y2 2z given 4x2 9y2 36z2 36. 6. find volume largest rectangular parallelepiped edges parallel coor dinate axis inscribed ellipsoid x2 a2 y2 b2 z2 c2 1 . c 7. let x0 y0 minimum point smooth function f x y constraint gx y c. assume gx0 y0 c gx0 y0 0. show f x0 y0 λgx0 y0 λ 0. hint note x0 y0 also minimum point smooth function f x y constraint gx y c points x0 y0tgx0 y0 satisfy constraint inequality small positive t.
4 multiple integrals 4.1 double integrals singlevariable calculus differentiation integration thought inverse opera tions. instance integrate function f x necessary ﬁnd antiderivative f is another function fx whose derivative f x. similar way deﬁning in tegration realvalued functions two variables answer yes see shortly. recall also deﬁnite integral nonnegative function f x 0 represented area under curve f x. see double integral nonnegative realvalued function f x y 0 represents volume under surface z f x y. let f x y continuous function f x y 0 x y rectangle r x y x b c y d r2. often write r abcd. number xin interval ab slice surface z f x y plane x xparallel yzplane. trace surface plane curve f x y xis ﬁxed varies. area curve that is area region curve xyplane varies interval cd depends value x. using variable x instead x let ax area see figure 4.1.1. z x 0 ax r x b c z f x y figure 4.1.1 area ax varies x ax r c f x ydy since treating x ﬁxed varies. makes sense since ﬁxed x function f x y continuous function interval cd know area curve deﬁnite integral. area ax function x slice crosssection method singlevariable calculus know 114
4.1 double integrals 115 volume v solid surface z f x y xyplane rectangle r integral ab crosssectional area ax v b w axdx b w w c f x ydy dx 4.1 always refer volume the volume surface. expression uses called iterated integrals. first function f x y integrated func tion y treating variable x constant this called integrating respect y. occurs inner integral square brackets equation 4.1. ﬁrst iterated integral. integration performed result expression involving x integrated respect x. occurs outer integral the second iterated integral. ﬁnal result number the volume. process going two iterations integrals called double integration last expression equation 4.1 called double integral. notice integrating f x y respect inverse operation taking partial derivative f x y respect y. also could taken area crosssections surface parallel xzplane would depend variable y volume would v w c b w f x ydx dy . 4.2 turns general1 order iterated integrals matter. also usually discard brackets simply write v w c b w f x ydxdy 4.3 understood fact dx written dy means function f x y ﬁrst integrated respect x using inner limits integration b resulting function integrated respect using outer limits integration c d. order integration changed convenient. example 4.1. find volume v plane z 8x6y rectangle r 01 02. 1due fubinis theorem. see ch. 18 taylor mann.
116 chapter 4. multiple integrals solution see f x y 8x6y 0 0 x 1 0 y 2 so v 2 w 0 1 w 0 8x6ydxdy 2 w 0 µ 4x2 6xy x1 x0 dy 2 w 0 46ydy 4y3y2 2 0 20 suppose switched order integration. verify still get answer v 1 w 0 2 w 0 8x6ydydx 1 w 0 µ 8xy3y2 y2 y0 dx 1 w 0 16x12dx 8x2 12x 1 0 20 example 4.2. find volume v surface z exy rectangle r 23 12. solution know f x y exy 0 x y v 2 w 1 3 w 2 exy dxdy 2 w 1 µ exy x3 x2 dy 2 w 1 ey3 ey2dy ey3 ey2 2 1
4.1 double integrals 117 e5 e4 e4 e3 e5 2e4 e3 recall general function f x integral r b f xdx represents difference area curve f x xaxis f x 0 area curve xaxis f x 0. similarly double integral continuous function f x y represents difference volume surface z f x y xyplane f x y 0 volume surface xyplane f x y 0. thus method double integration means iterated integrals used evaluate double integral continuous function rectangle regardless whether f x y 0 not. example 4.3. evaluate 2π w 0 π w 0 sinx ydxdy. solution note f x y sinx y positive negative rectangle 0π 02π. still evaluate double integral 2π w 0 π w 0 sinx ydxdy 2π w 0 ³ cosx y xπ x0 dy 2π w 0 cosyπcos ydy sinyπsin 2π 0 sin3πsin2πsinπsin0 0 exercises exercises 14 ﬁnd volume surface z f x y rectangle r. 1. f x y 4xy r 0101 2. f x y exy r 0111 3. f x y x3 y2 r 0101 4. f x y x4 xy y3 r 1202 exercises 512 evaluate given double integral. 5. 1 w 0 2 w 1 1yx2 dxdy 6. 1 w 0 2 w 0 xx ydxdy
118 chapter 4. multiple integrals 7. 2 w 0 1 w 0 x2dxdy 8. 2 w 1 1 w 1 xxysinxdxdy 9. π2 w 0 1 w 0 xycosx2ydxdy 10. π w 0 π2 w 0 sinxcosyπdxdy 11. 2 w 0 4 w 1 xydxdy 12. 1 w 1 2 w 1 1dxdy 13. let constant. show w c b w dxdy md cb a.
4.2 double integrals general region 119 4.2 double integrals general region previous section got idea double integral rectangle represents. deﬁne double integral realvalued function f x y general regions r2. suppose region r xyplane bounded left vertical line x a bounded right vertical line x b where b bounded curve g1x bounded curve g2x figure 4.2.1a. assume g1x g2x intersect open interval ab they could intersect endpoints x x b though. b x 0 g2x g1x r a vertical slice r b r g2x g1x f x ydydx x 0 x h1y x h2y r c b horizontal slice r c r h2y h1y f x ydxdy figure 4.2.1 double integral nonrectangular region r using slice method previous section double integral realvalued function f x y region r denoted x r f x yda given x r f x yda b w g2x w g1x f x ydy dx 4.4 means take vertical slices region r curves g1x g2x. symbol da sometimes called area element inﬁnitesimal signifying area. note f x y ﬁrst integrated respect y functions x limits integration. makes sense since result ﬁrst iterated integral function x alone allows us take second iterated integral respect x. similarly region r xyplane bounded left curve x h1y bounded right curve x h2y bounded horizontal line
120 chapter 4. multiple integrals c bounded horizontal line where c d figure 4.2.1b assuming h1y h2y intersect open interval cd taking horizontal slices gives x r f x yda w c h2y w h1y f x ydx dy 4.5 notice deﬁnitions include case region r rectangle. also f x y 0 x y region r r f x yda volume surface z f x y region r. example 4.4. assume region r deﬁned inequalities x2 y y2 x. rewrite double integral x r f x yda iterated integral. solution let us try write iterated integral 4.5. first need ﬁnd projection r yaxis. words need ﬁnd x y belongs r y. later holds y4 y equivalently 01 words c 0 1. ﬁxed y 0 y 1 need ﬁnd values x x y belongs r. later holds y2 x py. words h1y y2 h2y py. is x r f x yda 1 w 0 py w y2 f x ydx dy. note region r change switch x y. therefore integral could written x r f x yda 1 w 0 px w x2 f x ydy dx. example 4.5. find volume v plane z 8x6y plane region r deﬁned inequalities 0 x 1 0 y 2x2.
4.2 double integrals general region 121 x 0 2x2 r 1 figure 4.2.2 solution region r shown figure 3.2.2. using vertical slices get v x r 8x6yda 1 w 0 2x2 w 0 8x6ydy dx 1 w 0 µ 8xy3y2 y2x2 y0 dx 1 w 0 16x3 12x4dx 4x4 12 5 x5 1 0 4 12 5 32 5 6.4 x 0 2 x p y2 r 1 figure 4.2.3 get answer using horizontal slices see figure 3.2.3 v x r 8x6yda 2 w 0 1 w p y2 8x6ydx dy 2 w 0 µ 4x2 6xy x1 xp y2 dy 2 w 0 46y2y 6 p 2 ypydy 2 w 0 44y3 p 2y32dy 4y2y2 6 p 2 5 y52 2 0 886 p 2 p 32 5 1648 5 32 5 6.4 example 4.6. find volume v solid bounded three coordinate planes plane 2x y4z 4. solution solid shown figure 4.2.4a typical vertical slice. volume v given r f x yda f x y z 1 442x y region r shown figure
122 chapter 4. multiple integrals z x 0 040 001 200 2x y4z 4 a x 0 2x4 r 2 4 b figure 4.2.4 4.2.4b r x y 0 x 2 0 y 2x4. using vertical slices r gives v x r 1 442xyda 2 w 0 2x4 w 0 1 442xydy dx 2 w 0 µ 1 842xy2 y2x4 y0 dx 2 w 0 1 842x2 dx 1 4842x3 2 0 64 48 4 3 general region r may one types regions considered far double integral r f x yda deﬁned follows. assume f x y nonnega tive realvalued function r bounded region r2 enclosed rectangle abcd. divide rectangle grid subrectangles. consider subrectangles enclosed completely within region r shown shaded subrectangles figure 4.2.5a. subrectangle xixi1 yj yj1 pick point xi yj. volume surface z f x y subrectangle approxi mately f xi yjxi yj xi xi1 xi yj yj1 yj f xi yj height xi yj base area parallelepiped shown figure 4.2.5b. total vol ume surface approximately sum volumes parallelepipeds namely x j x f xi yjxi yj 4.6
4.2 double integrals general region 123 summation occurs indices subrectangles inside r. take smaller smaller subrectangles length largest diagonal subrect angles goes 0 subrectangles begin ﬁll region r sum approaches actual volume surface z f x y region r. deﬁne r f x yda limit double summation the limit taken subdivisions rectangle abcd largest diagonal subrectangles goes 0. b xi xi1 x 0 c yj yj1 xi yj a subrectangles inside region r z x 0 r xi xi1 yj yj1 z f x y yj xi xi yj f xi yj b parallelepiped subrectangle volume f xi yjxi yj figure 4.2.5 double integral general region r similar deﬁnition made function f x y necessarily always non negative replace mention volume negative volume description f x y 0. case region type shown figure 4.2.1 using def inition riemann integral singlevariable calculus deﬁnition r f x yda reduces sequence two iterated integrals. finally region r bounded. evaluate improper double inte grals that is unbounded region region contains points function f x y deﬁned sequence iterated improper singlevariable integrals. example 4.7. evaluate w 1 1x2 w 0 2ydydx.
124 chapter 4. multiple integrals solution w 1 1x2 w 0 2ydydx w 1 µ y2 y1x2 y0 dx w 1 x4 dx 1 3 x3 1 01 3 1 3 exercises exercises 18 evaluate given double integral. 1. 1 w 0 1 w px 24x2ydydx 2. π w 0 w 0 sinxdxdy 3. 2 w 1 lnx w 0 4xdydx 4. 2 w 0 2y w 0 ey2 dxdy 5. π2 w 0 w 0 cosx sin ydxdy 6. w 0 w 0 xyex2y2 dxdy 7. 2 w 0 w 0 1dxdy 8. 1 w 0 x2 w 0 2dydx exercises 910 evaluate x r f x yda 9. f x y xy r intersection unit disc x2 y2 1 positive quadrant. 10. f x y x2 r triangle vertices 00 20 01. 11. find volume v solid bounded three coordinate planes plane x y z 1. 12. find volume v solid bounded three coordinate planes plane 3x2y5z 6. b 13. explain double integral r 1da gives area region r. simplicity assume r region type shown figure 4.2.1a. c
4.2 double integrals general region 125 b c figure 4.2.6 14. prove volume tetrahedron mutually perpendic ular adjacent sides lengths a b c figure 3.2.6 abc 6 . hint mimic example 4.6 recall section 1.5 three noncollinear points determine plane. 15. show exercise 12 used solve exercise 10. b exercises 1617 rewrite double integral x r f x yda iterated integral. hint try visualize region. 16. region r deﬁned inequalities 2y2 x 1 y2. 17. region r deﬁned inequalities x 2y 4x 1.
126 chapter 4. multiple integrals 4.3 triple integrals deﬁnition double integral realvalued function f x y region r r2 extended deﬁne triple integral realvalued function f x y z solid r3. simply proceed before solid enclosed rectangular parallelepiped divided subparallelepipeds. subparallelepiped inside s sides lengths x y z pick point x y z. deﬁne triple integral f x y z s denoted f x y zdv f x y zdv lim xxx f x y zxyz 4.7 limit divisions rectangular parallelepiped enclosing sub parallelepipeds whose largest diagonal going 0 triple summation subparallelepipeds inside s. shown limit depend choice rectangular parallelepiped enclosing s. symbol dv often called volume element. physically triple integral represent saw double integral could thought volume twodimensional surface. turns triple integral simply generalizes idea thought representing hypervolume threedimensional hypersurface w f x y z whose graph lies r4. general word volume often used general term signify concept n dimensional object including length r1 area r2 volume r3. may hard get grasp concept volume fourdimensional object least know calculate volume case rectangular parallelepiped x1x2 y1 y2 z1 z2 is x y z x1 x x2 y1 y y2 z1 z z2 triple integral sequence three iterated integrals namely f x y zdv z2 w z1 y2 w y1 x2 w x1 f x y zdxdydz 4.8 order integration matter. simplest case. complicated case solid bounded surface z g1x y bounded surface z g2x y bounded two curves h1x h2x x varies b. f x y zdv b w h2x w h1x g2xy w g1xy f x y zdz dydx . 4.9 notice case ﬁrst iterated integral result function x since limits integration functions x y leaves double integral type learned evaluate section 3.2. are course many variations
4.3 triple integrals 127 case for example changing roles variables x y z probably tell triple integrals quite tricky. point learning evaluate triple integral regardless represents important thing. see ways triple integrals used later text. example 4.8. evaluate 3 w 0 2 w 0 1 w 0 xy zdxdydz. solution 3 w 0 2 w 0 1 w 0 xy zdxdydz 3 w 0 2 w 0 µ 1 2 x2y xz x1 x0 dydz 3 w 0 2 w 0 1 2 y z dydz 3 w 0 µ 1 4 y2 yz y2 y0 dz 3 w 0 12zdz z z2 3 0 12 example 4.9. evaluate 1 w 0 1x w 0 2xy w 0 x y zdz dydx. solution 1 w 0 1x w 0 2xy w 0 x y zdz dydx 1 w 0 1x w 0 µ x yz 1 2 z2 z2xy z0 dydx 1 w 0 1x w 0 x y2xy 1 22xy2 dydx 1 w 0 1x w 0 21 2 x2 xy1 2 y2 dydx 1 w 0 µ 2y1 2 x2yxy1 2 xy2 1 6 y3 y1x y0 dx 1 w 0 11 6 2x 1 6 x3 dx
128 chapter 4. multiple integrals 11 6 xx2 1 24 x4 1 0 7 8 note volume v solid r3 given v 1dv . 4.10 since function integrated constant 1 triple integral reduces double integral types considered previous section solid bounded surface z f x y bounded xyplane z 0. many possibilities. example solid could bounded surfaces z g1x y z g2x y respectively bounded two curves h1x h2x x varies b. v 1dv b w h2x w h1x g2xy w g1xy 1dz dydx b w h2x w h1x g2x yg1x y dydx like equation 4.9. see exercise 10 example. exercises exercises 18 evaluate given triple integral. 1. 3 w 0 2 w 0 1 w 0 xyz dxdydz 2. 1 w 0 x w 0 w 0 xyz dz dydx 3. π w 0 x w 0 xy w 0 x2 sin z dz dydx 4. 1 w 0 z w 0 w 0 zey2 dxdydz 5. e w 1 w 0 1y w 0 x2z dxdz dy 6. 2 w 1 y2 w 0 z2 w 0 yz dxdz dy 7. 2 w 1 4 w 2 3 w 0 1dxdydz 8. 1 w 0 1x w 0 1xy w 0 1dz dydx 9. let constant. show z2 w z1 y2 w y1 x2 w x1 dxdydz mz2 z1y2 y1x2 x1. b
4.3 triple integrals 129 10. find volume v solid bounded three coordinate planes bounded plane x y z 2 bounded plane z x y. 11. let solid deﬁned inequalities x2 1 y 1 z2. rewrite triple integral f x y zdv iterated integral. c 12. show b w z w w f xdxdydz b w bx2 2 f xdx. hint think changing order integration triple integral changes limits integration.
130 chapter 4. multiple integrals 4.4 numerical approximation multiple integrals seen calculating multiple integrals tricky even simple functions regions. complicated functions may possible evaluate one iterated in tegrals simple closed form. luckily numerical methods approximating value multiple integral. method discuss called monte carlo method. idea behind based concept average value function learned singlevariable calculus. recall continuous function f x average value f f interval ab deﬁned f 1 b a b w f xdx . 4.11 quantity b a length interval ab thought volume interval. applying reasoning functions two three variables deﬁne average value f x y region r f 1 ar x r f x yda 4.12 ar area region r deﬁne average value f x y z solid f 1 vs f x y zdv 4.13 vs volume solid s. thus example x r f x yda ar f . 4.14 average value f x y r thought representing sum values f divided number points r. however take sum literary since inﬁnite number points region in fact uncounably many one enumerate natural numbers. took large number n random points region r which generated computer took average values f points used average value f exactly monte carlo method does. formula 4.14 approximation get x r f x yda ar f ar f 2 f 2 n 4.15 f 1 n n x i1 f xi yi f 2 1 n n x i1 f xi yi2 4.16
4.4 numerical approximation multiple integrals 131 sums taken n random points x1 y1 . xn yn. error term formula 4.15 really provide hard bounds approximation. represents single standard deviation expected value integral. is provides likely bound error. due use random points monte carlo method example probabilistic method as opposed deterministic methods newtons method use speciﬁc formula generating points. example use formula 4.15 approximate volume v plane z 8x 6y rectangle r 01 02. example 4.1 section 3.1 showed actual volume 20. code listing montecarlo.java java program calculates volume using number points n passed command line parameter. program approximate double integral fxy8x6y over rectangle 01x02. public class montecarlo public static void mainstring args get number n random points commandline parameter int n integer.parseintargs0 double x 0 xcoordinate random point double 0 ycoordinate random point double f 0.0 value f random point double mf 0.0 mean values f double mf2 0.0 mean values f2 int i0ini get random coordinates x math.random x 0 1 2 math.random y 0 2 f 8x 6y value function mf mf f add sum f values mf2 mf2 ff add sum f2 values mf mfn compute mean f values mf2 mf2n compute mean f2 values system.out.printlnn n integral volmf volmath.sqrtmf2 math.powmf2n print result the volume rectangle 01x02 public static double vol return 12 listing 4.1 program listing montecarlo.java results running program various numbers random points for instance java montecarlo 100 shown below
132 chapter 4. multiple integrals n 10 19.36543087722646 2.7346060413546147 n 100 21.334419561385353 0.7547037194998519 n 1000 19.807662237526227 0.26701709691370235 n 10000 20.080975812043256 0.08378816229769506 n 100000 20.009403854556716 0.026346782289498317 n 1000000 20.000866994982314 0.008321168748642816 see approximation fairly good. n shown monte carlo approximation converges actual volume on order o p n com putational complexity terminology. example region r rectangle. use monte carlo method nonrectangular bounded region r slight modiﬁcation needed. pick rectangle r encloses r generate random points rectangle before. use points calculation f inside r. need calculate area r formula 4.15 case since exclusion points inside r allows use area rectangle r instead similar before. instance example 4.5 showed volume surface z 8x 6y nonrectangular region r x y 0 x 1 0 y 2x2 6.4. since rectangle r 01 02 contains r use program before change check see 2x2 random point x y 01 02. listing 4.2 contains code montecarlo2.java program approximate double integral fxy8x6y region bounded x0 x1 y0 y2x2 public class montecarlo2 public static void mainstring args get number n random points commandline parameter int n integer.parseintargs0 double x 0 xcoordinate random point double 0 ycoordinate random point double f 0.0 value f random point double mf 0.0 mean values f double mf2 0.0 mean values f2 int i0ini get random coordinates x math.random x 0 1 2 math.random y 0 2 y 2math.powx2 the point region f 8x 6y value function mf mf f add sum f values mf2 mf2 ff add sum f2 values mf mfn compute mean f values mf2 mf2n compute mean f2 values system.out.printlnn n integral volmf
4.4 numerical approximation multiple integrals 133 volmath.sqrtmf2 math.powmf2n the volume rectangle 01x02 public static double vol return 12 listing 4.2 program listing montecarlo2.java results running program various numbers random points for instance java montecarlo2 1000 shown below n 10 integral 6.95747529014894 2.9185131565120592 n 100 integral 6.3149056229650355 0.9549009662159909 n 1000 integral 6.477032813858756 0.31916837260973624 n 10000 integral 6.349975080015089 0.10040086346895105 n 100000 integral 6.440184132811864 0.03200476870881392 n 1000000 integral 6.417050897922222 0.01009454409789472 use monte carlo method evaluate triple integrals need generate random triples x y z parallelepiped instead random pairs x y rectangle use volume parallelepiped instead area rectangle formula 4.15 see exercise 2. detailed discussion numerical integration methods see press et al. exercises c 1. write program uses monte carlo method approximate double integral x r exy da r 0101. show program output n 10 100 1000 10000 100000 1000000 random points. 2. write program uses monte carlo method approximate triple integral exyz dv 01 01 01. show program output n 10 100 1000 10000 100000 1000000 random points.
134 chapter 4. multiple integrals 3. repeat exercise 1 region r x y 1 x 1 0 y x2. 4. repeat exercise 2 solid x y z 0 x 1 0 y 1 0 z 1xy. 5. use monte carlo method approximate volume sphere radius 1. 6. use monte carlo method approximate volume ellipsoid x2 9 y2 4 z2 1 1.
4.5 change variables multiple integrals 135 4.5 change variables multiple integrals given difﬁculty evaluating multiple integrals reader may wondering possible simplify integrals using suitable substitution variables. an swer yes though bit complicated substitution method learned singlevariable calculus. recall given example deﬁnite integral 2 w 1 x3p x2 1dx would make substitution u x2 1 x2 u 1 du 2xdx changes limits integration x 1 u 0 x 2 u 3 get 2 w 1 x3p x2 1dx 2 w 1 1 2 x2 2x p x2 1dx 3 w 0 1 2u 1 pu du 1 2 3 w 0 ³ u32 u12 du integrated give 14 p 3 5 . let us take different look happened substitution give motivation substitution works multiple integrals. first let u x2 1. interval integration 12 function x 7x2 1 strictly increasing and maps 12 onto 03 hence inverse function deﬁned interval 03. is 03 deﬁne x function u namely x gu p u 1 . substituting expression x function f x x3p x2 1 gives f x f gu u 132pu
136 chapter 4. multiple integrals see dx du g u dx g udu dx 1 2u 112 du since g0 1 0 g11 g3 2 3 g12 performing substitution earlier gives 2 w 1 f xdx 2 w 1 x3p x2 1dx 3 w 0 1 2u 1 pu du written 3 w 0 u 132pu 1 2u 112 du means 2 w 1 f xdx g12 w g11 f gu g udu . general x gu onetoone differentiable function interval cd which think uaxis onto interval ab on xaxis means g u 0 interval cd gc b gd c g1a g1b b w f xdx g1b w g1a f gu g udu . 4.17 called change variable formula integrals singlevariable functions implicitly using integration substitution. formula turns special case general formula used evaluate multiple integrals. state formulas double triple integrals involving realvalued functions two three variables respectively. assume functions involved continuously differentiable regions solids involved reasonable boundaries. proof following theorem beyond scope text.2 2see taylor mann 15.32 15.62 details.
4.5 change variables multiple integrals 137 theorem 4.1. change variables formula multiple integrals let x xuv yuv deﬁne onetoone mapping region r uvplane onto region r xyplane determinant juv x u x v y u y v 4.18 never 0 r. x r f x ydax y x r f xuv yuvjuvdauv . 4.19 use notation dax y dauv denote area element x y uv coordinates respectively. similarly x xuvw yuvw z zuvw deﬁne onetoone mapping solid s uvwspace onto solid xyzspace determinant juvw x u x v x w y u y v y w z u z v z w 4.20 never 0 s f x y zdvx y z s f xuvw yuvw zuvwjuvwdvuvw . 4.21 determinant juv formula 4.18 called jacobian x respect u v sometimes written juv x y uv . 4.22 similarly jacobian juvw three variables sometimes written juvw x y z uvw . 4.23 notice formula 4.19 saying dax y juvdauv think twovariable version relation dx g udu singlevariable case. following example shows change variables formula used. example 4.10. evaluate x r e xy xy da r x y x 0 0x 1.
138 chapter 4. multiple integrals solution first note evaluating double integral without using substitution prob ably impossible least closed form. looking numerator denominator exponent e try substitution u x y v x y. use change variables formula 4.19 need write x terms u v. solving x gives x 1 2u v 1 2v u. figure 4.5.1 below see mapping x xuv 1 2uv yuv 1 2vu maps region r onto r onetoone manner. x 0 x 1 1 1 r u v 0 1 1 1 r u v u v x 1 2u v 1 2vu figure 4.5.1 regions r r see juv x u x v y u y v 1 2 1 2 1 2 1 2 1 2 juv 1 2 1 2 using horizontal slices r x r e xy xy da x r f xuv yuvjuvda 1 w 0 v w v e u v 1 2 du dv 1 w 0 ³ v 2 e u v uv uv dv 1 w 0 v 2e e1dv v2 4 e e1 1 0 1 4 µ e 1 e e2 1 4e change variables formula used evaluate double integrals polar coordi nates. letting x xrθ rcosθ yrθ rsinθ
4.5 change variables multiple integrals 139 juv x r x θ y r y θ cosθ rsinθ sinθ rcosθ rcos2 θ rsin2 θ r juv r r following formula double integral polar coordinates x r f x ydxdy x r f rcosθrsinθr dr dθ 4.24 mapping x rcosθ rsinθ maps region r rθplane onto region r xyplane onetoone manner. example 4.11. find volume v inside paraboloid z x2 y2 0 z 1. z x 0 x2 y2 1 1 figure 4.5.2 z x2 y2 solution using vertical slices see v x r 1zda x r 1x2 y2da r x y x2 y2 1 unit disk r2 see figure 3.5.2. polar coordinates rθ know x2 y2 r2 unit disk r set r rθ 0 r 10 θ 2π. thus v 2π w 0 1 w 0 1r2r dr dθ 2π w 0 1 w 0 r r3dr dθ 2π w 0 µ r2 2 r4 4 r1 r0 dθ 2π w 0 1 4 dθ π 2 example 4.12. find volume v inside cone z p x2 y2 0 z 1.
140 chapter 4. multiple integrals z x 0 x2 y2 1 1 figure 4.5.3 z p x2 y2 solution using vertical slices see v x r 1zda x r µ 1 q x2 y2 da r x y x2 y2 1 unit disk r2 see figure 3.5.3. polar coordinates rθ know p x2 y2 r unit disk r set r rθ 0 r 10 θ 2π. thus v 2π w 0 1 w 0 1rr dr dθ 2π w 0 1 w 0 r r2dr dθ 2π w 0 µ r2 2 r3 3 r1 r0 dθ 2π w 0 1 6 dθ π 3 similar fashion shown see exercises 56 triple integrals cylindrical spherical coordinates take following forms triple integral cylindrical coordinates f x y zdxdydz s f rcosθrsinθ zr dr dθ dz 4.25 mapping x rcosθ rsinθ z z maps solid s rθzspace onto solid xyzspace onetoone manner. triple integral spherical coordinates f x y zdxdydz s f ρsinφ cosθρsinφ sinθρcosφρ2 sinφdρ dφdθ 4.26 mapping x ρsinφ cosθ ρsinφ sinθ z ρcosφ maps solid s ρφθ space onto solid xyzspace onetoone manner. example 4.13. 0 ﬁnd volume v inside sphere x2 y2 z2 a2.
4.5 change variables multiple integrals 141 solution see set ρ spherical coordinates v 1dv 2π w 0 π w 0 w 0 1ρ2 sinφdρ dφdθ 2π w 0 π w 0 µρ3 3 ρa ρ0 sinφdφdθ 2π w 0 π w 0 a3 3 sinφdφdθ 2π w 0 µ a3 3 cosφ φπ φ0 dθ 2π w 0 2a3 3 dθ 4πa3 3 . exercises 1. find volume v inside paraboloid z x2 y2 0 z 4. 2. find volume v inside cone z p x2 y2 0 z 3. b 3. find volume v solid inside x2 y2 z2 4 x2 y2 1. 4. find volume v inside sphere x2 y2 z2 1 cone z p x2 y2. 5. prove formula 4.25. 6. prove formula 4.26. 7. evaluate r sin xy 2 cos xy 2 da r triangle vertices 00 20 11. hint use change variables u x y2 v xy2. 8. find volume solid bounded z x2 y2 z2 4x2 y2. 9. find volume inside elliptic cylinder x2 a2 y2 b2 1 0 z 2. c 10. show volume inside ellipsoid x2 a2 y2 b2 z2 c2 1 4πabc 3 . hint use change variables x au bv z cw consider example 4.13. 11. show w w f x ydxdy w w f x yx2ydxdy smooth function f x y vanishes outside bounded region plane.
142 chapter 4. multiple integrals 4.6 application center mass b x 0 f x r x y figure 4.6.1 center mass r recall singlevariable calculus region r x y x b0 y f x r2 represents thin ﬂat plate see figure 3.6.1 f x con tinuous function ab center mass r coordinates x y given x mx mx b w f x2 2 dx b w xf xdx b w f xdx 4.27 assuming r uniform density i.e mass r uniformly distributed region. case area region considered mass r the density constant taken 1 simplicity. general case density region or lamina r continuous function δ δx y coordinates x y points inside r where r region r2 coordinates x y center mass r given x mx 4.28 x r xδx yda mx x r yδx yda x r δx yda 4.29 quantities mx called moments or ﬁrst moments region r xaxis yaxis respectively. quantity mass region r. see this think taking small rectangle inside r dimensions x y close 0. mass rectangle approximately δx yxy point x y rectangle. mass r limit sums masses rectangles inside r diagonals rectangles approach 0 double integral r δx yda. note formulas 4.27 represent special case δx y 1 throughout r formulas 4.29. example 4.14. find center mass region r x y 0 x 1 0 y 2x2 density function x y δx y x y.
4.6 application center mass 143 x 0 2x2 r 1 figure 4.6.2 solution region r shown figure 3.6.2. x r δx yda 1 w 0 2x2 w 0 x ydydx 1 w 0 ã xy y2 2 y2x2 y0 dx 1 w 0 2x3 2x4dx x4 2 2x5 5 1 0 9 10 mx x r yδx yda x r xδx yda 1 w 0 2x2 w 0 yx ydydx 1 w 0 2x2 w 0 xx ydydx 1 w 0 ã xy2 2 y3 3 y2x2 y0 dx 1 w 0 ã x2y xy2 2 y2x2 y0 dx 1 w 0 2x5 8x6 3 dx 1 w 0 2x4 2x5dx x6 3 8x7 21 1 0 5 7 2x5 5 x6 3 1 0 11 15 center mass x y given x 1115 910 22 27 mx 57 910 50 63 . note center mass little towards upper corner region r density uniform use formulas 4.27 show x y 3 4 3 5 case. makes sense since density function δx y x increases x y approaches upper corner quite bit area.
144 chapter 4. multiple integrals special case density function δx y constant function region r center mass x y called centroid r. formulas center mass region r2 generalized solid r3. let solid continuous mass density function δx y z point x y z s. center mass coordinates x y z x myz mxz z mxy 4.30 myz xδx y zdv mxz yδx y zdv mxy zδx y zdv 4.31 δx y zdv . 4.32 case myz mxz mxy called moments or ﬁrst moments around yzplane xzplane xyplane respectively. also mass s. example 4.15. find center mass solid x y z z 0 x2 y2 z2 a2 density function x y z δx y z 1. z x 0 x y z figure 4.6.3 solution solid upper hemisphere inside sphere radius centered origin see figure 3.6.3. since density function constant symmetric zaxis clear x 0 0 need ﬁnd z. δx y zdv 1dv v olumes. since volume half volume sphere radius a know example 4.13 4πa3 3 2πa3 3 . mxy zδx y zdv z dv spherical coordinates 2π w 0 π2 w 0 w 0 ρ cosφρ2 sinφdρ dφdθ 2π w 0 π2 w 0 sinφ cosφ ã w 0 ρ3 dρ dφdθ
4.6 application center mass 145 2π w 0 π2 w 0 a4 4 sinφ cosφdφdθ mxy 2π w 0 π2 w 0 a4 8 sin2φdφdθ since sin2φ 2sinφ cosφ 2π w 0 µ a4 16 cos2φ φπ2 φ0 dθ 2π w 0 a4 8 dθ πa4 4 z mxy πa4 4 2πa3 3 3a 8 . thus center mass x y z 00 3a 8 . exercises exercises 15 ﬁnd center mass region r given density function δx y. 1. r x y 0 x 2 0 y 4 δx y 2y 2. r x y 0 x 1 0 y x2 δx y x 3. r x y 0 x2 y2 a2 δx y 1 4. r x y 0 x 0 1 x2 y2 4 δx y p x2 y2 5. r x y 0 x2 y2 1 δx y b exercises 610 ﬁnd center mass solid given density function δx y z. 6. x y z 0 x 1 0 y 1 0 z 1 δx y z xyz 7. x y z z 0 x2 y2 z2 a2 δx y z x2 y2 z2
146 chapter 4. multiple integrals 8. x y z x 0 0 z 0 x2 y2 z2 a2 δx y z 1 9. x y z 0 x 1 0 y 1 0 z 1 δx y z x2 y2 z2 10. x y z 0 x 1 0 y 1 0 z 1xy δx y z 1 c 11. let f ﬁgure upper halfplane denote x0 y0 center mass area. show 2πay0 volume body revolution obtained rotating f around xaxis.
4.7 application probability expected value 147 4.7 application probability expected value section brieﬂy discuss applications multiple integrals ﬁeld probability theory. particular see ways multiple integrals used calculate probabilities expected values. probability suppose standard sixsided fair die let variable x represent value rolled. probability rolling 3 written px 3 1 6 since six sides die one equally likely rolled hence particular 3 one six chance rolled. likewise probability rolling 3 written px 3 3 6 1 2 since six numbers die three equally likely numbers 1 2 3 less equal 3. note px 3 px 1 px 2 px 3. call x discrete random variable sample space or probability space ωconsisting possible outcomes. case ω 123456. event subset sample space. example case die event x 3 set 123. let x variable representing random real number interval 01. note real number x 01 makes sense consider px x since must 0 why. instead consider probability px x given px x x. reasoning this interval 01 length 1 x 01 interval 0x length x. since x represents random number 01 hence uniformly distributed 01 px x length 0x length 01 x 1 x . call x continuous random variable sample space ω 01. event subset sample space. example case event x x set 0x. case discrete random variable saw probability event sum probabilities individual outcomes comprising event for instance px 3 px 1 px 2 px 3 die example. continuous random variable probability event instead integral function describe. let x continuous realvalued random variable sample space ωin r. simplic ity let ω ab. deﬁne distribution function f x fx px x x 4.33 1 x b px x x b 0 x a . 4.34
148 chapter 4. multiple integrals suppose nonnegative continuous realvalued function f r fx x w f ydy x 4.35 w f xdx 1 . 4.36 call f probability density function x. thus px x x w f ydy x b . 4.37 also fundamental theorem calculus f x f x x . 4.38 example 4.16. let x represent randomly selected real number interval 01. say x uniform distribution 01 distribution function fx px x 1 x 1 x 0 x 1 0 x 0 4.39 probability density function f x f x 1 0 x 1 0 elsewhere. 4.40 general x represents randomly selected real number interval ab x uniform distribution function fx px x 1 x b x ba x b 0 x a 4.41 probability density function f x f x 1 ba x b 0 elsewhere. 4.42
4.7 application probability expected value 149 example 4.17. famous distribution function given standard normal distribution whose probability density function f f x 1 p 2π ex22 x . 4.43 often called bell curve used widely statistics. since claiming f probability density function w 1 p 2π ex22 dx 1 4.44 formula 4.36 equivalent w ex22 dx p 2π . 4.45 use double integral polar coordinates verify integral. first w w ex2y22 dxdy w ey22 ã w ex22 dx dy ã w ex22 dx ã w ey22 dy ã w ex22 dx 2 since function integrated twice middle equation different variables. using polar coordinates see w w ex2y22 dxdy 2π w 0 w 0 er22 r dr dθ 2π w 0 µ er22 r r0 dθ 2π w 0 0e0dθ 2π w 0 1dθ 2π
150 chapter 4. multiple integrals ã w ex22 dx 2 2π hence w ex22 dx p 2π . addition individual random variables consider jointly distributed random variables. this let x z three realvalued continuous random variables deﬁned sample space ωin r the discussion two random variables similar. joint distribution function f x z given fx y z px x y z z x y z . 4.46 nonnegative continuous realvalued function f r3 fx y z z w w x w f uvwdu dvdw x y z 4.47 w w w f x y zdxdydz 1 4.48 call f joint probability density function x z. general a1 b1 a2 b2 a3 b3 pa1 x b1 a2 b2 a3 z b3 b3 w a3 b2 w a2 b1 w a1 f x y zdxdydz 4.49 and symbols interchangeable combination. triple integral then thought representing probability for function f probability density function. example 4.18. let a b c real numbers selected randomly interval 01. probability equation ax2 bx c 0 least one real solution x
4.7 application probability expected value 151 c 0 c 1 4a 1 1 1 4 r1 r2 figure 4.7.1 region r r1 r2 solution know quadratic formula least one real solution b24ac 0. need calculate pb24ac 0. use three jointly distributed random variables this. first since 0 ab c 1 b2 4ac 0 0 4ac b2 1 0 2 pa pc b 1 last relation holds 0 a c 1 0 4ac 1 0 c 1 4a . considering a b c real variables region r acplane relation holds given r a c 0 1 0 c 1 0 c 1 4a see union two regions r1 r2 figure 3.7.1 above. let x z continuous random variables representing randomly se lected real number interval 01 think x z representing a b c respectively. then similar showed f x 1 probability density func tion uniform distribution 01 shown f x y z 1 x y z 01 0 elsewhere joint probability density function x z. now pb2 4ac 0 pa c r 2 pa pc b 1 probability triple integral f ab c 1 b varies 2papc 1 a c varies region r. since r divided two regions r1 r2 required triple integral split sum two triple integrals using vertical slices r pb2 4ac 0 14 w 0 1 w 0 z r1 1 w 2papc 1db dc da 1 w 14 14a w 0 z r2 1 w 2papc 1db dc da 14 w 0 1 w 0 12 pa pcdc da 1 w 14 14a w 0 12 pa pcdc da 14 w 0 µ c 4 3 pa c32 c1 c0 da 1 w 14 µ c 4 3 pa c32 c14a c0 da 14 w 0 14 3 pa da 1 w 14 1 12a da a8 9a32 14 0 1 12 lna 1 14
152 chapter 4. multiple integrals µ1 4 1 9 µ 01 12 ln 1 4 5 36 1 12 ln4 pb2 4ac 0 53ln4 36 0.2544 words equation ax2 bx c 0 25 chance solved expected value expected value ex random variable x thought average value x varies sample space. x discrete random variable ex x x xpx x 4.50 sum taken elements x sample space. example x repre sents number rolled sixsided die ex 6 x x1 xpx x 6 x x1 x 1 6 3.5 4.51 expected value x average integers 16. x realvalued continuous random variable probability density function f ex w x f xdx . 4.52 example x uniform distribution interval 01 probability density function f x 1 0 x 1 0 elsewhere 4.53 ex w x f xdx 1 w 0 xdx 1 2 . 4.54 pair jointly distributed realvalued continuous random variables x joint probability density function f x y expected values x given ex w w x f x ydxdy ey w w f x ydxdy 4.55 respectively.
4.7 application probability expected value 153 example 4.19. pick n 2 random real numbers interval 01 expected values smallest largest numbers solution let u1.un n continuous random variables representing randomly selected real number 01 uniform distribution 01. deﬁne random vari ables x x minu1.un maxu1.un . shown3 joint probability density function x f x y nn1yxn2 0 x y 1 0 elsewhere. 4.56 thus expected value x ex 1 w 0 1 w x nn1xyxn2 dydx 1 w 0 µ nxyxn1 y1 yx dx 1 w 0 nx1xn1 dx integration parts yields x1xn 1 n11xn1 1 0 ex 1 n1 similarly see exercise 3 shown ey 1 w 0 w 0 nn1yyxn2 dxdy n n1 . so example repeatedly take samples n 3 random real numbers 01 time store minimum maximum values sample aver age minimums would approach 1 4 average maximums would approach 3 4 number samples grows. would relatively simple see exercise 4 write computer program test this. exercises b 3see ch. 6 hoel port stone.
154 chapter 4. multiple integrals 1. evaluate integral w ex2 dx using anything learned far. 2. σ 0 µ 0 evaluate w 1 σ p 2π exµ22σ2 dx. 3. show ey n n1 example 4.19 c 4. write computer program in language choice veriﬁes results example 4.19 case n 3 taking large numbers samples. 5. repeat exercise 4 case n 4. 6. continuous random variables x joint probability density function f x y deﬁne second moments ex 2 ey 2 ex 2 w w x2 f x ydxdy ey 2 w w y2 f x ydxdy variances varx vary varx ex 2ex2 vary ey 2ey 2 . find varx vary x example 4.19. 7. continuing exercise 6 correlation ρ x deﬁned ρ exy exey p varxvary exy w w xy f x ydxdy. find ρ x example 4.19. note quantity exy exey called covariance x . 8. example 4.18 would answer change interval 0100 used instead 01 explain.
5 line surface integrals 5.1 line integrals singlevariable calculus learned integrate realvalued function f x interval ab r1. integral usually called riemann integral thought integral path r1 since interval or collection intervals really kind path r1. may also recall f x represented force applied along xaxis object position x ab work w done moving object position x x b deﬁned integral w b w f xdx. section see deﬁne integral function either realvalued vectorvalued two variables general curve also called path r2. deﬁnition motivated physical notion work. begin realvalued functions two variables. physics intuitive idea work work force distance . assume move object unit weight along curve c r2 want ﬁnd work force works friction. suppose f x y coefﬁcient friction point x y. case force magnitude f x y applied direction motion along c see figure 5.1.1 below. x 0 c b si q xi2 yi2 ti ti1 yi xi figure 5.1.1 curve c x xt yt ab 155
156 chapter 5. line surface integrals assume function f x y continuous realvalued consider magnitude force. partition interval ab follows t0 t1 t2 tn1 tn b integer n 2 see figure 5.1.1 typical subinterval titi1 distance si traveled along curve approximately q xi2 yi2 pythagorean theorem. thus subinterval small enough work done moving object along piece curve approximately force distance f xi yi q xi2 yi2 5.1 xi yi xti yti tiin titi1 w n1 x i0 f xi yi q xi2 yi2 5.2 approximately total amount work done entire curve. since q xi2 yi2 sµxi ti 2 µyi ti 2 ti ti ti1 ti w n1 x i0 f xi yi sµxi ti 2 µyi ti 2 ti . 5.3 taking limit sum length largest subinterval goes 0 sum subintervals becomes integral b xi ti yi ti become x t yt respectively f xi yi becomes f xt yt w b w f xt yt q x t2 yt2 dt . 5.4 integral right side equation gives us idea deﬁne realvalued function f x y integral f x y along curve c called line integral deﬁnition 5.1. realvalued function f x y curve c r2 parametrized x xt yt t b line integral f x y along c respect arc length w c f x yds b w f xt yt q x t2 yt2 dt . 5.5
5.1 line integrals 157 symbol ds differential arc length function st w q x u2 yu2 du 5.6 may recognize section 1.9 length curve c interval at ab. is ds tdt q x t2 yt2 dt 5.7 fundamental theorem calculus. general realvalued function f x y line integral r c f x yds rep resent preceding discussion ds gives us clue. think differentials inﬁnitesimal lengths. think f x y height picket fence along c f x yds thought approximately area section fence inﬁnitesimally small section curve thus line integral r c f x yds total area picket fence see figure 5.1.2. x 0 c ds f x y figure 5.1.2 area shaded rectangle heightwidth f x yds example 5.1. use line integral show lateral surface area right circular cylinder radius r height h 2πrh.
158 chapter 5. line surface integrals z x 0 r h f x y c x2 y2 r2 figure 5.1.3 solution use right circular cylinder base circle c given x2 y2 r2 height h positive z direction see figure 4.1.3. parametrize c follows x xt rcost yt rsint 0 t 2π let f x y h x y. w c f x yds b w f xt yt q x t2 yt2 dt 2π w 0 h p rsint2 rcost2 dt h 2π w 0 r p sin2 tcos2 dt rh 2π w 0 1dt 2πrh note example 5.1 traversed circle c twice that is let vary 0 4π would gotten area 4πrh twice desired area even though curve still namely circle radius r. also notice traversed circle counterclockwise direction. gone clockwise direction using parametrization x xt rcos2πt yt rsin2πt 0 t 2π 5.8 easy verify see exercise 12 value line integral unchanged. general shown see exercise 15 reversing direction curve c traversed leaves r c f x yds unchanged f x y. curve c parametriza tion x xt yt t b denote c curve c traversed opposite direction. c parametrized x xa b t ya b t t b 5.9 w c f x yds w c f x yds . 5.10 notice deﬁnition line integral respect arc length parameter s. also deﬁne w c f x ydx b w f xt ytx tdt 5.11
5.1 line integrals 159 line integral f x y along c respect x w c f x ydy b w f xt yt ytdt 5.12 line integral f x y along c respect y. derivation formula line integral used idea work force multiplied distance. however know force actually vector. would helpful develop vector form line integral. this suppose function fx y deﬁned r2 fx y px yi qx yj continuous realvalued functions px y qx y r2. function f called vector ﬁeld r2. deﬁned points r2 values vectors r2. curve c smooth parametrization x xt yt t b let rt xti ytj position vector point xt yt c. rt xti ytj w c px ydx w c qx ydy b w pxt ytx tdt b w qxt yt ytdt b w pxt ytx tqxt yt ytdt b w fxt yt rtdt deﬁnition fx y. notice function fxt yt rt realvalued function ab last integral right looks somewhat similar earlier deﬁnition line integral. leads us following deﬁnition deﬁnition 5.2. vector ﬁeld fx y px yi qx yj curve c smooth parametrization x xt yt t b line integral f along c w c f dr w c px ydx w c qx ydy 5.13 b w fxt yt rtdt 5.14 rt xti ytj position vector points c.
160 chapter 5. line surface integrals use notation dr rtdt dxi dyj denote differential vectorvalued function r. line integral deﬁnition 5.2 often called line integral vector ﬁeld distinguish line integral deﬁnition 5.1 called line integral scalar ﬁeld. convenience often write w c px ydx w c qx ydy w c px ydxqx ydy understood line integral along c applied p q. quantity px ydxqx ydy known differential form. realvalued function fx y differential f df f x dx f y dy. differential form px ydxqx ydy called exact equals df function fx y. recall points curve c position vector rt xti ytj rt tangent vector c point xt yt direction increasing which call direction c. since c smooth curve rt 0 ab hence tt rt rt unit tangent vector c xt yt. putting deﬁnitions 5.1 5.2 together get following theorem theorem 5.1. vector ﬁeld fx y px yi qx yj curve c smooth parametrization x xt yt t b position vector rt xti ytj w c f dr w c f tds 5.15 tt rt rtis unit tangent vector c xt yt. vector ﬁeld fx y represents force moving object along curve c work w done force w w c f tds w c f dr . 5.16 example 5.2. evaluate r cx2 y2dx2xydy where a c x 2t 0 t 1 b c x 2t2 0 t 1
5.1 line integrals 161 x 0 12 2 1 figure 5.1.4 solution figure 4.1.4 shows curves. a since x t 1 yt 2 w c x2 y2dx2xydy 1 w 0 xt2 yt2x t2xtyt yt dt 1 w 0 t2 4t212t2t2 dt 1 w 0 13t2 dt 13t3 3 1 0 13 3 b since x t 1 yt 4t w c x2 y2dx2xydy 1 w 0 xt2 yt2x t2xtyt yt dt 1 w 0 t2 4t412t2t24t dt 1 w 0 t2 20t4dt t3 3 4t5 1 0 1 3 4 13 3 cases vector ﬁeld fx y x2 y2i2xyj represents force moving object 00 12 along given curve c work done 13 3 . may lead think work and generally line integral vector ﬁeld independent path taken. however see next section always case. although deﬁned line integrals single smooth curve c piecewise smooth curve c c1 c2 .cn union smooth curves c1.cn deﬁne w c f dr w c1 f dr1 w c2 f dr2 . w cn f drn ri position vector curve ci. example 5.3. evaluate r cx2 y2dx 2xydy c polygonal path 00 02 12.
162 chapter 5. line surface integrals x 0 12 2 1 c1 c2 figure 5.1.5 solution write c c1 c2 c1 curve given x 0 t 0 t 2 c2 curve given x t 2 0 t 1 see figure 4.1.5. w c x2 y2dx2xydy w c1 x2 y2dx2xydy w c2 x2 y2dx2xydy 2 w 0 02 t2020t1 dt 1 w 0 t2 412t20 dt 2 w 0 0dt 1 w 0 t2 4dt t3 3 4t 1 0 1 3 4 13 3 line integral notation varies quite bit. example physics common see notation r b f dl understood limits integration b underlying parameter curve letter l signiﬁes length. also formulation r c f tds theorem 5.1 often preferred physics since emphasizes idea integrating tangential component f t f direction that is direction c useful physical interpretation line integrals. exercises exercises 14 calculate w c f x yds given function f x y curve c. 1. f x y xy c x cost sint 0 t π2 2. f x y x x2 1 c x t 0 0 t 1 3. f x y 2x y c polygonal path 00 30 32 4. f x y x y2 c path 20 counterclockwise along circle x2 y2 4 point 20 back 20 along xaxis 5. use line integral ﬁnd lateral surface area part cylinder x2 y2 4 plane x2y z 6 xyplane.
5.1 line integrals 163 exercises 611 calculate w c f dr given vector ﬁeld fx y curve c. 6. fx y ij c x 3t 2t 0 t 1 7. fx y yixj c x cost sint 0 t 2π 8. fx y xi yj c x cost sint 0 t 2π 9. fx y x2 yixy2j c x cost sint 0 t 2π 10. fx y xy2 i xy3 j c polygonal path 00 10 01 00 11. fx y x2 y2i c x 2cost sint 0 t 2π b 12. verify value line integral example 5.1 unchanged using parametrization circle c given formulas 5.8. 13. show f rt point rt along smooth curve c w c f dr 0. 14. show f points direction rt point rt along smooth curve c w c f dr w c fds. c 15. prove w c f x yds w c f x yds. hint use formulas 5.9. 16. let c smooth curve arc length l suppose fx y px yiqx yj vector ﬁeld fx ym x y c. show w c f dr ml. hint recall r b gxdx r b gxdx riemann integrals. 17. prove riemann integral r b f xdx special case line integral.
164 chapter 5. line surface integrals 5.2 properties line integrals know previous section line integrals realvalued functions scalar ﬁelds reversing direction integral taken along curve change value line integral w c f x yds w c f x yds 5.17 line integrals vector ﬁelds however value change. see this let fx y px yiqx yj vector ﬁeld p q continuously differentiable functions. let c smooth curve parametrized x xt yt t b position vector rt xti ytj we usually abbreviate saying c rt xti ytj smooth curve. know curve c traversed opposite direction parametrized x xa b t ya b t t b. w c px ydx b w pxa b t ya b t dtxa b tdt b w pxa b t ya b tx a b tdt by chain rule w b pxu yux udu by letting u a b t w b pxu yux udu b w pxu yux udu since w b b w w c px ydx w c px ydx since using different letter u line integral along c. similar argument shows w c qx ydy w c qx ydy hence w c f dr w c px ydx w c qx ydy w c px ydx w c qx ydy
5.2 properties line integrals 165 ãw c px ydx w c qx ydy w c f dr w c f dr . 5.18 formula interpreted terms work done force fx y treated vector moving object along curve c total work performed moving object along c initial point terminal point back initial point moving backwards along path zero. force considered vector direction accounted for. preceding discussion shows importance always taking direction curve account using line integrals vector ﬁelds. reason curves line integrals sometimes referred directed curves oriented curves. recall deﬁnition line integral required parametrization x xt yt t b curve c. know curve inﬁnitely many parametrizations. could get different value line integral using parametrization c say x xu yu c u d so would mean deﬁnition welldeﬁned. luckily turns value line integral vector ﬁeld unchanged long direction curve c preserved whatever parametrization chosen theorem 5.2. let fx y px yiqx yj vector ﬁeld let c smooth curve parametrized x xt yt t b. suppose αu c u d αc b αd αu 0 open interval cd that is αu strictly increasing cd. r c f dr value parametrizations x xt yt t b x xu xαu yu yαu c u d. proof since αu strictly increasing maps cd onto ab know αu inverse function u α1t deﬁned ab c α1a α1b du dt 1 αu. also dt αudu chain rule x u x du duxαu dx dt dt du x tαu x t x u αu making susbstitution αu gives b w pxt ytx tdt α1b w α1a pxαu yαu x u αu αudu w c p xu yu x udu
166 chapter 5. line surface integrals shows r c px ydx value parametrizations. similar argument shows r c qx ydy value parametrizations hence r c f dr value. qed notice condition αu 0 theorem 5.2 means two parametrizations move along c direction. case reverse parametriza tion c u a b t αu a b u αu 1 0. example 5.4. evaluate line integral r cx2 y2dx 2xydy example 5.2 section 4.1 along curve c x t 2t2 0 t 1 sinu 0 u π2. solution first notice 0 sin0 1 sinπ2 dt du cosu 0 0π2. theorem 5.2 know c parametrized x sinu 2sin2 u 0 u π2 r cx2 y2dx2xydy value found example 5.2 namely 13 3 . indeed verify this w c x2 y2dx2xydy π2 w 0 sin2 u 2sin2 u2cosu 2sinu2sin2 u4sinu cosu du π2 w 0 sin2 u 20sin4 u cosu du sin3 u 3 4sin5 u π2 0 1 3 4 13 3 words line integral unchanged whether u parameter c. closed curve mean curve c whose initial point terminal point same is c x xt yt t b xa ya xb yb. simple closed curve closed curve intersect itself. note closed curve regarded union simple closed curves think loops ﬁgure eight. use special notation z c f x yds z c f dr denote line integrals scalar vector ﬁelds respectively along closed curves. older texts may see notation w w indicate line integral traversing closed curve counterclockwise clockwise direction respectively.
5.2 properties line integrals 167 ï î c b a closed ï î c b b closed figure 5.2.1 closed vs nonclosed curves far examples seen line integrals for instance example 5.2 value different curves joining initial point terminal point. is line integral independent path joining two points. mentioned before always case. following theorem gives necessary sufﬁcient condition path independence theorem 5.3. region r line integral r c f dr independent path two points r u c f dr 0 every closed curve c contained r. proof suppose u c f dr 0 every closed curve c contained r. let p1 p2 two distinct points r. let c1 curve r going p1 p2 let c2 another curve r going p1 p2 figure 4.2.2. ï ï c1 c2 p1 p2 figure 5.2.2 c c1 c2 closed curve r from p1 p1 u c f dr 0. thus 0 z c f dr w c1 f dr w c2 f dr w c1 f dr w c2 f dr r c1 f dr r c2 f dr. proves path independence. conversely suppose line integral r c f dr independent path two points r. let c closed curve contained r. let p1 p2 two distinct points
168 chapter 5. line surface integrals c. let c1 part curve c goes p1 p2 let c2 remaining part c goes p1 p2 figure 4.2.2. path independence w c1 f dr w c2 f dr w c1 f dr w c2 f dr 0 w c1 f dr w c2 f dr 0 z c f dr 0 since c c1 c2 . qed clearly theorem give practical way determine path independence since impossible check line integrals around possible closed curves region. mostly give idea way line integrals behave seem ingly unrelated line integrals related in case speciﬁc line integral two points line integrals around closed curves. recall z f x y continuously differentiable function x y x xt yt differentiable functions t dz dt z x dx dt z y dy dt . 5.19 multivariable version chain rule see theorem 3.3 corollary 3.4. use version chain rule prove following sufﬁcient condition path independence line integrals theorem 5.4. let fx y px yiqx yj vector ﬁeld region r p q continuously differentiable functions r. let c smooth curve r parametrized x xt yt t b. suppose realvalued function fx y f f r. w c f dr fb fa 5.20 xa ya b xb yb endpoints c. thus line integral independent path endpoints since depends values f endpoints.
5.2 properties line integrals 169 proof deﬁnition r c f dr w c f dr b w pxt ytx tqxt yt yt dt b w µf x dx dt f y dy dt dt since f f f x p f y q b w fxt yt dt by chain rule theorem 3.3 fxt yt b fb fa fundamental theorem calculus. qed theorem 5.4 thought line integral version fundamental theorem calculus. realvalued function fx y fx y fx y called potential f. conservative vector ﬁeld one potential. example 5.5. recall examples 5.2 5.3 section 4.1 line integral r cx2 y2dx 2xydy found value 13 3 three different curves c going point 00 point 12. use theorem 5.4 show line integral indeed path independent. solution need ﬁnd realvalued function fx y f x x2 y2 f y 2xy . suppose f x x2 y2 must fx y 1 3 x3 xy2 gy function gy. f y 2xy g y satisﬁes condition f y 2xy g y 0 is gy k k constant. since choice k why pick k 0. thus potential fx y fx y x2 y2i2xyj exists namely fx y 1 3 x3 xy2 . hence line integral r cx2 y2dx2xydy path independent. note also verify value line integral f along curve c going 00 12 always 13 3 since theorem 5.4 w c f dr f12 f00 1 313 122 00 1 3 4 13 3 .
170 chapter 5. line surface integrals consequence theorem 5.4 special case c closed curve endpoints b point following important corollary corollary 5.5. vector ﬁeld f potential region r z c f dr 0 closed curve c r. equivalently z c f dr 0 realvalued function fx y. example 5.6. evaluate z c xdx ydy c x 2cost 3sint 0 t 2π. solution vector ﬁeld fx y xi yj potential fx y f x x fx y 1 2 x2 gy so f y g y gy 1 2 y2 k constant k fx y 1 2 x2 1 2 y2 potential fx y. thus z c xdx ydy z c f dr 0 corollary 5.5 since curve c closed it ellipse x2 4 y2 9 1. exercises 1. evaluate z c x2 y2dx2xydy c x cost sint 0 t 2π. 2. evaluate w c x2 y2dx2xydy c x cost sint 0 t π. 3. potential fx y fx y yixj so ﬁnd one. 4. potential fx y fx y xiyj so ﬁnd one. 5. potential fx y fx y xy2 i x3yj so ﬁnd one.
5.2 properties line integrals 171 b 6. let fx y gx y vector ﬁelds let b constants let c curve r2. show w c af bg dr w c f dr b w c g dr . 7. let c curve whose arc length l. show r c 1ds l. 8. let f x y gx y continuously differentiable realvalued functions region r. show z c f g dr z c gf dr closed curve c r. hint use exercise 21 section 2.4. 9. let fx y y x2y2 i x x2y2 j x y 00 c x cost sint 0 t 2π. a show f f fx y tan1yx. b show z c f dr 2π. contradict corollary 5.5 explain. c 10. let gx hy differentiable functions let fx y hyi gxj. gx hy vector ﬁeld f potential find potential fx y cases.
172 chapter 5. line surface integrals 5.3 greens theorem see way evaluating line integral smooth vector ﬁeld around simple closed curve. vector ﬁeld fx y px yiqx yj smooth component functions px y qx y smooth. greens theorem relates line integral around closed curve double integral region inside curve theorem 5.6. greens theorem let r region r2 whose boundary simple closed curve c piecewise smooth. let fx y px yiqx yj smooth vector ﬁeld deﬁned r c. z c f dr x r µq x p y da 5.21 c traversed r always left side c. proof prove theorem case simple region r is boundary curve c written c c1 c2 two distinct ways c1 curve y1x point x1 point x2 5.22 c2 curve y2x point x2 point x1 5.23 x1 x2 points c farthest left right respectively c1 curve x x1y point y2 point y1 5.24 c2 curve x x2y point y1 point y2 5.25 y1 y2 lowest highest points respectively c. see figure 4.3.1. b x î ï y2x y1x x x2y x x1y y2 y1 x2 x1 r c c figure 5.3.1 integrate px y around c using representation c c1 c2 given 4.23 4.24. since y1x along c1 as x goes b y2x along c2 as x goes b
5.3 greens theorem 173 a see figure 4.3.1 z c px ydx w c1 px ydx w c2 px ydx b w px y1xdx w b px y2xdx b w px y1xdx b w px y2xdx b w px y2x px y1x dx b w µ px y yy2x yy1x dx b w y2x w y1x px y y dydx by fundamental theorem calculus x r p y da . likewise integrate qx y around c using representation c c1 c2 given 4.25 4.26. since x x1y along c1 as goes c x x2y along c2 as goes c d see figure 4.3.1 z c qx ydy w c1 qx ydy w c2 qx ydy c w qx1y ydy w c qx2y ydy w c qx1y ydy w c qx2y ydy w c qx2y y qx1y y dy w c µ qx y xx2y xx1y dy
174 chapter 5. line surface integrals w c x2y w x1y qx y x dxdy by fundamental theorem calculus x r q x da z c f dr z c px ydx z c qx ydy x r p y da x r q x da x r µq x p y da . qed example 5.7. evaluate z c x2 y2dx2xydy c boundary traversed counterclockwise region r x y 0 x 1 2x2 y 2x. x 0 12 2 1 c figure 5.3.2 solution r shaded region figure 4.3.2. greens theorem px y x2 y2 qx y 2xy z c x2 y2dx2xydy x r µq x p y da x r 2y2yda x r 0da 0 . actually already knew answer zero. recall example 5.5 section 4.2 vector ﬁeld fx y x2 y2i2xyj potential function fx y 1 3 x3 xy2 u c f dr 0 corollary 5.5. though proved greens theorem simple region r theorem also proved general regions particular regions admit subdivision
5.3 greens theorem 175 simple regions.1 includes regions bounded closed curves. regions outer boundary inner boundaries traversed r always left side. c1 c2 r1 r2 ï î î ï a region r one hole c1 c2 c3 r1 r2 ï ï î î î ï b region r two holes figure 5.3.3 multiply connected regions idea greens theorem holds regions shown figure 5.3.3 above. idea cut region r divided simple subregions. example figure 5.3.3a region r union regions r1 r2 divided slits indicated dashed lines. slits part boundary r1 r2 traverse manner indicated arrows. notice along slit boundary r1 traversed opposite direction r2 means line integrals f along slits cancel out. assuming greens theorem holds r1 r2 get z bdy r1 f dr x r1 µq x p y da z bdy r2 f dr x r2 µq x p y da . since line integrals along slits cancel out z c1c2 f dr z bdy r1 f dr z bdy r2 f dr z c1c2 f dr x r1 µq x p y da x r2 µq x p y da x r µq x p y da shows greens theorem holds region r. similar argument shows theorem holds region two holes shown figure 5.3.3b. 1see taylor mann 15.31 discussion difﬁculties involved boundary curve complicated.
176 chapter 5. line surface integrals example 5.8. let fx y px yiqx yj px y y x2 y2 qx y x x2 y2 let r x y 0 x2 y2 1. boundary curve c x2 y2 1 traversed counter clockwise shown exercise 9b section 4.2 u c f dr 2π. q x y2 x2 x2 y22 p y x r µq x p y da x r 0da 0 . would seem contradict greens theorem. however note r entire region enclosed c since point 00 contained r. is r hole origin greens theorem apply. x 0 c1 c2 1 1 12 12 r ï î figure 5.3.4 annulus r modify region r annulus r x y 14 x2 y2 1 see figure 4.3.3 take boundary c r c c1 c2 c1 unit circle x2 y2 1 traversed counterclockwise c2 circle x2 y2 14 traversed clockwise shown see exercise 8 z c f dr 0 . would still r ³ q x p y da 0 r would z c f dr x r µq x p y da shows greens theorem holds annular region r. know corollary 5.5 smooth vector ﬁeld fx y px yiqx yj region r whose boundary piecewise smooth simple closed curve c potential r u c f dr 0. potential fx y smooth r f x p f y q know 2f yx 2f xy p y q x r. conversely p y q x r z c f dr x r µq x p y da x r 0da 0 .
5.3 greens theorem 177 simply connected region r that is region holes following shown following statements equivalent simply connected region r r2 a fx y px yiqx yj smooth potential fx y r b w c f dr independent path curve c r c z c f dr 0 every simple closed curve c r d p y q x r in case differential form p dxq dy exact exercises 14 use greens theorem evaluate given line integral around curve c traversed counterclockwise. 1. z c x2 y2dx2xydy c boundary r x y 0 x 1 2x2 y 2x 2. z c x2ydx2xydy c boundary r x y 0 x 1 x2 y x 3. z c 2ydx3xdy c circle x2 y2 1 4. z c ex2 y2dx ey2 x2dy c boundary triangle vertices 00 40 04 5. potential fx y fx y y2 3x2i2xyj so ﬁnd one. 6. potential fx y fx y x3 cosxy 2xsinxyi x2ycosxyj so ﬁnd one. 7. potential fx y fx y 8xy3i4x2 yj so ﬁnd one. 8. show z c adx b dy 0 constants a b closed simple curve c. b
178 chapter 5. line surface integrals 9. vector ﬁeld f example 5.8 show directly u c f dr 0 c boundary annulus r x y 14 x2 y2 1 traversed r always left. 10. evaluate z c ex sin ydxy3 ex cos ydy c boundary rectangle vertices 11 11 11 11 traversed counterclockwise. c 11. region r bounded simple closed curve c show area r z c ydx z c xdy 1 2 z c xdyydx c traversed r always left. hint use greens theorem fact r 1da. following exercises use exercise 11 ﬁnd area bounded curve. you ﬁgure curve traversed around region bounds. 12. curve sintsin2t 0 t π. 13. deltoid curve 2costcos2t2sintsin2t 0 t 2π. the deltoid curve shown diagram assume without proof selfintesections.
5.4 surface integrals divergence theorem 179 5.4 surface integrals divergence theorem section 4.1 learned integrate along curve. learn perform integration surface r3 sphere paraboloid. recall section 1.8 identiﬁed points x y z curve c r3 parametrized x xt yt z zt t b terminal points position vector rt xti ytj ztk ab. idea behind parametrization curve transforms subset r1 nor mally interval ab curve r2 r3 see figure 5.4.1. b r1 z x 0 xa ya za xt yt zt xb yb zb rt c x xt yt z zt figure 5.4.1 parametrization curve c r3 similar used parametrization curve deﬁne line integral along curve use parametrization surface deﬁne surface integral. use two variables u v parametrize surface σ r3 x xuv yuv z zuv uv region r r2 see figure 5.4.2. u v r r2 uv z x 0 σ ruv x xuv yuv z zuv figure 5.4.2 parametrization surface σ r3 case position vector point surface σ given vectorvalued
180 chapter 5. line surface integrals function ruv xuvi yuvj zuvk uv r. since ruv function two variables deﬁne partial derivatives r u r v uv r r uuv x uuvi y uuvj z uuvk r vuv x vuvi y v uvj z vuvk . parametrization σ thought transforming region r2 in uv plane 2dimensional surface r3. parametrization surface sometimes called patch based idea patching region r onto σ gridlike manner shown figure 5.4.2. fact gridlines r lead us deﬁne surface integral σ. along vertical gridlines r variable u constant. lines get mapped curves σ variable u constant along position vector ruv. thus tangent vector curves point uv r v. similarly horizontal gridlines r get mapped curves σ whose tangent vectors r u. take point uv r as say lower left corner one rectangular grid sections r shown figure 5.4.2. suppose rectangle small width height u v respectively. corner points rectangle uv u uv uuvv uvv. area rectangle uv. rectangle gets mapped parametrization onto section surface σ which u v small enough surface area close area parallelogram adjacent sides ru uvruv corresponding line segment uv u uv r ruv v ruv corresponding line segment uv uv v r. combining usual notion partial derivative see deﬁnition 3.3 section 2.2 derivative vectorvalued function see deﬁnition 2.3 section 1.8 applied function two variables r u ru uvruv u r v ruvvruv v surface area element dσ approximately ru uvruv ruvvruv u r u vr v r u r v uv theorem 1.13 section 1.4. thus total surface area σ approximately sum quantities r u r v uv summed rectangles r. taking limit
5.4 surface integrals divergence theorem 181 sum diagonal largest rectangle goes 0 gives x r r u r v du dv . 5.26 write double integral right using special notation x σ dσ x r r u r v du dv . 5.27 special case surface integral surface σ surface area element dσ thought 1dσ. replacing 1 general realvalued function f x y z deﬁned r3 following deﬁnition 5.3. let σ surface r3 parametrized x xuv yuv z zuv uv region r r2. let ruv xuvi yuvj zuvk position vector point σ let f x y z realvalued function deﬁned subset r3 contains σ. surface integral f x y z σ x σ f x y zdσ x r f xuv yuv zuv r u r v du dv . 5.28 particular surface area σ x σ 1dσ . 5.29 example 5.9. torus surface obtained revolving circle radius yzplane around zaxis circles center distance b zaxis 0 b figure 5.4.3. find surface area t. solution point circle line segment center circle point makes angle u yaxis positive direction see figure 5.4.3a. circle revolves around zaxis line segment origin center circle sweeps angle v positive xaxis see figure 5.4.3b. thus torus parametrized as x b acosucosv b acosusinv z asinu 0 u 2π 0 v 2π position vector ruv xuvi yuvj zuvk b acosucosvi b acosusinvj asinuk
182 chapter 5. line surface integrals z 0 yb2 z2 a2 u b a circle yzplane x z v xyz b torus figure 5.4.3 see r u asinu cosvi asinu sinvj acosuk r v b acosusinvi b acosucosvj 0k computing cross product gives r u r v ab acosucosv cosui ab acosusinv cosuj ab acosusinuk magnitude r u r v ab acosu . thus surface area x σ 1dσ 2π w 0 2π w 0 r u r v du dv 2π w 0 2π w 0 ab acosudu dv 2π w 0 µ abu a2 sinu u2π u0 dv 2π w 0 2πab dv
5.4 surface integrals divergence theorem 183 4π2ab z x 0 figure 5.4.4 assume surface σ given collection charts. note chart ruv collection vectors r u r v tangent surface. therefore crossproduct r uuv r vuv normal σ point position vector ruv. assume point p surface σ one choose unit normal vector n way every chart ruv collection n point position vector ruv crossproduct r uuv r vuv codirectional n. case σ called oriented vector ﬁeld n called outward unit normal vector σ. deﬁnition 5.4. let σ oriented surface r3 let fx y z vector ﬁeld deﬁned subset r3 contains σ. surface integral f σ x σ f dσ x σ f ndσ 5.30 where point σ n outward unit normal vector σ. particular σ given single chart ruv xuvi yuvj zuvk deﬁned plane region r x σ f dσ x r fxuv yuv zuv r uuv r vuv du dv. note deﬁnition dot product inside integral right real valued function hence use deﬁnition 5.3 evaluate integral. example 5.10. evaluate surface integral σ f dσ fx y z yzixzjxyk σ part plane x yz 1 x 0 0 z 0 outward unit normal n pointing positive z direction see figure 4.4.5.
184 chapter 5. line surface integrals z x 0 1 1 1 σ x y z 1 n figure 5.4.5 solution since vector v 111 normal plane x y z 1 why dividing v length yields outward unit normal vector n ³ 1 p 3 1 p 3 1 p 3 . need parametrize σ. see figure 4.4.5 projecting σ onto xyplane yields triangular region r x y 0 x 1 0 y 1x. thus using uv instead x y see x u v z 1u v 0 u 10 v 1u parametrization σ r since z 1x y σ. σ f n yzxzxy µ 1 p 3 1 p 3 1 p 3 1 p 3 yz xz xy 1 p 3 x yz xy 1 p 3 u v1u v uv 1 p 3 u vu v2 uv uv r ruv xuvi yuvj zuvk ui vj1u vk r u r v 101 011 111 r u r v p 3 . thus integrating r using vertical slices indicated dashed line figure 4.4.5 gives x σ f dσ x σ f ndσ x r fxuv yuv zuv n r u r v dvdu 1 w 0 1u w 0 1 p 3 u vu v2 uv p 3dvdu 1 w 0 ã u v2 2 u v3 3 uv2 2 v1u v0 du 1 w 0 µ1 6 u 2 3u2 2 5u3 6 du u 6 u2 4 u3 2 5u4 24 1 0 1 8 .
5.4 surface integrals divergence theorem 185 computing surface integrals often tedious especially formula outward unit normal vector point σ changes. following theorem provides easier way case σ closed surface is σ encloses bounded solid r3. example spheres cubes ellipsoids closed surfaces planes paraboloids not. theorem 5.7. divergence theorem let σ closed surface r3 bounds solid s let fx y z f1x y zi f2x y zj f3x y zk vector ﬁeld deﬁned subset r3 contains σ. x σ f dσ divf dv 5.31 divf f1 x f2 y f3 z 5.32 called divergence f. proof divergence theorem similar proof greens theorem. ﬁrst proved simple case solid bounded one surface bounded another surface bounded laterally one surfaces. proof extended general solids.2 example 5.11. evaluate σ f dσ fx y z xi yj zk σ unit sphere x2 y2 z2 1. solution see divf 111 3 x σ f dσ divf dv 3 dv 3 1 dv 3vols 3 4π13 3 4π . physical applications surface integral σ f dσ often referred ﬂux f surface σ. example f represents velocity ﬁeld ﬂuid ﬂux net quantity ﬂuid ﬂow surface σ per unit time. positive ﬂux means net ﬂow surface that is direction outward unit normal vector n negative ﬂux indicates net ﬂow inward in direction n. 2see taylor mann 15.6 details.
186 chapter 5. line surface integrals term divergence comes interpreting divf measure much vector ﬁeld diverges point. best seen using another deﬁnition divf equivalent3 deﬁnition given formula 5.32. namely point x y z r3 divfx y z lim v0 1 v x σ f dσ 5.33 v volume enclosed closed surface σ around point x y z. limit v 0 means take smaller smaller closed surfaces around x y z means volumes enclose going zero. shown limit independent shapes surfaces. notice limit taken ratio ﬂux surface volume enclosed surface gives rough measure ﬂow leaving point mentioned. vector ﬁelds zero divergence often called solenoidal ﬁelds. following theorem simple consequence formula 5.33. theorem 5.8. ﬂux vector ﬁeld f zero every closed surface containing given point divf 0 point. proof formula 5.33 given point x y z divfx y z lim v0 1 v x σ f dσ closed surfaces σ containing x y z lim v0 1 v 0 assumption ﬂux σ zero lim v0 0 0 . qed lastly note sometimes notation σ f x y zdσ σ f dσ used denote surface integrals scalar vector ﬁelds respectively closed sur faces. exercises 3see schey p. 3639 intuitive discussion this.
5.4 surface integrals divergence theorem 187 exercises 14 use divergence theorem evaluate surface integral x σ f dσ given vector ﬁeld fx y z surface σ. 1. fx y z xi2yj3zk σ x2 y2 z2 9 2. fx y z xi yj zk σ boundary solid cube x y z 0 x y z 1 3. fx y z x3i y3j z3k σ x2 y2 z2 1 4. fx y z 2i3j5k σ x2 y2 z2 1 b 5. show ﬂux constant vector ﬁeld closed surface zero. 6. evaluate surface integral exercise 2 without using divergence theorem is using deﬁnition 5.3 example 5.10. note different outward unit normal vector six faces cube. 7. evaluate surface integral σ f dσ fx y z x2i xyj zk σ part plane 6x 3y 2z 6 x 0 0 z 0 outward unit normal n pointing positive z direction. 8. use surface integral show surface area sphere radius r 4πr2. hint use spherical coordinates parametrize sphere. 9. use surface integral show surface area right circular cone radius r height h πr p h2 r2. hint use parametrization x rcosθ rsinθ z h r r 0 r r 0 θ 2π. 10. ellipsoid x2 a2 y2 b2 z2 c2 1 parametrized using ellipsoidal coordinates x asinφ cosθ bsinφ sinθ z ccosφ 0 θ 2π 0 φ π. show surface area ellipsoid π w 0 2π w 0 sinφ q a2b2 cos2 φ c2a2 sin2 θ b2 cos2 θsin2 φ dθ dφ . note double integral evaluated elementary means. speciﬁc values a b c evaluated using numerical methods. alternative express surface area terms elliptic integrals.4 4bowman f. introduction elliptic functions applications new york dover 1961 iii.7.
188 chapter 5. line surface integrals c 11. use deﬁnition 5.3 prove surface area region r r2 surface z f x y given formula x r r 1 ³ f x 2 ³ f y 2 da . hint think parametrization surface.
5.5 stokes theorem 189 5.5 stokes theorem far types line integrals discussed along curves r2. deﬁnitions properties covered sections 4.1 4.2 easily extended include functions three variables discuss line integrals along curves r3. deﬁnition 5.5. realvalued function f x y z curve c r3 parametrized x xt yt z zt t b line integral f x y z along c respect arc length w c f x y zds b w f xt yt zt q x t2 yt2 z t2 dt . 5.34 line integral f x y z along c respect x w c f x y zdx b w f xt yt ztx tdt . 5.35 line integral f x y z along c respect w c f x y zdy b w f xt yt zt ytdt . 5.36 line integral f x y z along c respect z w c f x y zdz b w f xt yt zt z tdt . 5.37 similar twovariable case f x y z 0 line integral r c f x y zds thought total area picket fence height f x y z point along curve c r3. vector ﬁelds r3 deﬁned similar fashion r2 allows us deﬁne line integral vector ﬁeld along curve r3.
190 chapter 5. line surface integrals deﬁnition 5.6. vector ﬁeld fx y z px y ziqx y zjrx y zk curve c r3 smooth parametrization x xt yt z zt t b line integral f along c w c f dr w c px y zdx w c qx y zdy w c rx y zdz 5.38 b w fxt yt zt rtdt 5.39 rt xti ytj ztk position vector points c. similar twovariable case fx y z represents force applied object point x y z line integral r c f dr represents work done force moving object along curve c r3. important results need line integrals r3 stated without proof the proofs similar twovariable equivalents. theorem 5.9. vector ﬁeld fx y z px y zi qx y zj rx y zk curve c smooth parametrization x xt yt z zt t b position vector rt xti ytj ztk w c f dr w c f tds 5.40 tt rt rtis unit tangent vector c xt yt zt. theorem 5.10. chain rule w f x y z continuously differentiable function x y z x xt yt z zt differentiable functions t w differentiable function t dw dt w x dx dt w y dy dt w z dz dt . 5.41 also x xt1t2 yt1t2 z zt1t2 continuously differentiable function t1t2 then5 w t1 w x x t1 w y y t1 w z z t1 5.42 w t2 w x x t2 w y y t2 w z z t2 . 5.43 5see taylor mann 6.5 proof.
5.5 stokes theorem 191 theorem 5.11. let fx y z px y zi qx y zj rx y zk vector ﬁeld solid s p q r continuously differentiable functions s. let c smooth curve parametrized x xt yt z zt t b. suppose realvalued function fx y z f f s. w c f dr fb fa 5.44 xa ya za b xb yb zb endpoints c. corollary 5.12. vector ﬁeld f potential solid s z c f dr 0 closed curve c that is z c f dr 0 realvalued function fx y z. example 5.12. let f x y z z let c curve r3 parametrized x tsint tcost z 0 t 8π . evaluate r c f x y zds. note c called conical helix. see figure 5.5.1. solution since x t sint tcost yt costtsint z t 1 x t2 yt2 z t2 sin2 t2tsintcost t2 cos2 tcos2 t2tsintcost t2 sin2 t1 t2sin2 tcos2 tsin2 tcos2 t1 t2 2 since f xt yt zt zt along curve c w c f x y zds 8π w 0 f xt yt zt q x t2 yt2 z t2 dt 8π w 0 p t2 2 dt µ1 3t2 232 8π 0 1 3 ³ 64π2 232 2 p 2 . example 5.13. let fx y z xi yj2zk vector ﬁeld r3. using curve c example 5.12 evaluate r c f dr.
192 chapter 5. line surface integrals 25 20 15 10 5 0 5 10 15 20 25 25 20 15 10 5 0 5 10 15 20 25 30 0 5 10 15 20 25 30 z 0 8π x z figure 5.5.1 conical helix c solution note fx y z x2 2 y2 2 z2 potential fx y z that is f f. theorem 5.11 know w c f dr fb fa x0 y0 z0 b x8π y8π z8π f8πsin8π8πcos8π8π f0sin00cos00 f08π8π f000 0 8π2 2 8π2 000 96π2 . discuss generalization greens theorem r2 orientable surfaces r3 called stokes theorem. surface σ r3 orientable continuous vector ﬁeld n r3 n nonzero normal σ that is perpendicular tangent plane point σ. say n normal vector ﬁeld.
5.5 stokes theorem 193 z x 0 n n figure 5.5.2 example unit sphere x2y2z2 1 orientable since continuous vector ﬁeld nx y z xi yjzk nonzero normal sphere point. fact nx y z another normal vector ﬁeld see figure 4.5.2. see case nx y z called outward normal vector nx y z inward normal vector. outward inward normal vec tor ﬁelds sphere correspond outer inner side respectively sphere. is say sphere two sided surface. roughly twosided means orientable. ex amples twosided hence orientable surfaces cylinders paraboloids ellipsoids planes. may wondering kind surface would two sides. example möbius strip constructed taking thin rectangle connecting ends opposite corners resulting twisted strip see figure 5.5.3. b b a connect b b along ends b orientable figure 5.5.3 möbius strip imagine walking along line center möbius strip figure 5.5.3b arrive back place started upside down is orientation changed even though motion continuous along center line. informally thinking vertical direction normal vector ﬁeld along strip discontinuity starting point and fact every point since vertical direction takes two different values there. möbius strip one side hence nonorientable.6 orientable surface σ boundary curve c pick unit normal vector n walked along c head pointing direction n surface would left. say situation n positive unit normal vector c traversed npositively. state stokes theorem 6for discussion orientability see oneill iv.7.
194 chapter 5. line surface integrals theorem 5.13. stokes theorem let σ orientable surface r3 whose boundary simple closed curve c let fx y z px y zi qx y zj rx y zk smooth vector ﬁeld deﬁned subset r3 contains σ. z c f dr x σ curlf ndσ 5.45 curlf µr y q z µp z r x j µq x p y k 5.46 n positive unit normal vector σ c traversed npositively. proof general case beyond scope text prove theorem special case σ graph z zx y smooth realvalued function zx y x y varying region r2. z x 0 n x y cd c σ z zx y figure 5.5.4 projecting σ onto xyplane see closed curve c the boundary curve σ projects onto closed curve cd boundary curve see fig ure 4.5.4. assuming c smooth parametriza tion projection cd xyplane also smooth parametrization say cd x xt yt t b c parametrized in r3 c x xt yt z zxt yt t b since curve c part surface z zx y. now chain rule theorem 3.3 z zxt yt function t know z t z x x t z y yt z c f dr w c px y zdxqx y zdy rx y zdz b w µ p x tq yt r µz x x t z y yt dt b w µµ p r z x x t µ q r z y yt dt w cd px ydx qx ydy
5.5 stokes theorem 195 px y px y zx y rx y zx y z xx y and qx y qx y zx y rx y zx y z yx y x y d. thus greens theorem applied region d z c f dr x µ q x p y da . 5.47 thus q x x µ qx y zx y rx y zx y z yx y product rule get x qx y zx y µ x rx y zx y z yx y rx y zx y x µ z yx y . now formula 5.42 theorem 5.10 x qx y zx y q x x x q y y x q z z x q x 1 q y 0 q z z x q x q z z x . similarly x rx y zx y r x r z z x . thus q x q x q z z x µr x r z z x z y rx y zx y 2z xy q x q z z x r x z y r z z x z y r 2z xy . similar fashion calculate p y p y p z z y r y z x r z z y z x r 2z yx . subtracting gives q x p y µq z r y z x µr x p z z y µq x p y 5.48
196 chapter 5. line surface integrals since 2z xy 2z yx smoothness z zx y. hence equation 5.47 z c f dr x µ µr y q z z x µp z r x z y µq x p y da 5.49 factoring 1 terms ﬁrst two products equation 5.48. now recall section 2.3 see p.76 vector n z x iz y jk normal tangent plane surface z zx y point σ. thus n n n z x iz y jk r 1 z x 2 ³ z y 2 is fact positive unit normal vector σ see figure 4.5.4. hence using parametrization rx y xi yj zx yk x y d surface σ r x z x k r y j z y k r x r y r 1 z x 2 ³ z y 2 . see us ing formula 5.46 curl f x σ curlf ndσ x curlf n r x r y da x µµr y q z i µp z r x j µq x p y k µ z xiz yjk da x µ µr y q z z x µp z r x z y µq x p y da which upon comparing equation 5.49 proves theorem. qed note condition stokes theorem surface σ continuously vary ing positive unit normal vector n boundary curve c traversed npositively expressed precisely follows rt position vector c tt rtrt unit tangent vector c vectors t n t n form righthanded system. also noted stokes theorem holds even boundary curve c piecewise smooth. example 5.14. verify stokes theorem fx y z zi xj yk σ paraboloid z x2 y2 z 1 see figure 4.5.5.
5.5 stokes theorem 197 z x 0 n c σ 1 figure 5.5.5 z x2 y2 solution positive unit normal vector surface z zx y x2 y2 n z x iz y jk r 1 z x 2 ³ z y 2 2xi2yjk p 14x2 4y2 curlf 10i10j10k ijk curlf n 2x2y1 q 14x2 4y2 . since σ parametrized rx y xi yj x2 y2k x y region x y x2 y2 1 x σ curlf ndσ x curlf n r x r y da x 2x2y1 p 14x2 4y2 q 14x2 4y2 da x 2x2y1da switching polar coordinates gives 2π w 0 1 w 0 2rcosθ 2rsinθ 1r dr dθ 2π w 0 1 w 0 2r2 cosθ 2r2 sinθ rdr dθ 2π w 0 µ 2r3 3 cosθ 2r3 3 sinθ r2 2 r1 r0 dθ 2π w 0 2 3 cosθ 2 3 sinθ 1 2 dθ 2 3 sinθ 2 3 cosθ 1 2θ 2π 0 π . boundary curve c unit circle x2 y2 1 laying plane z 1 see figure
198 chapter 5. line surface integrals 4.5.5 parametrized x cost sint z 1 0 t 2π. z c f dr 2π w 0 1sintcostcostsint0dt 2π w 0 µ sint 1cos2t 2 dt µ used cos2 1cos2t 2 cost 2 sin2t 4 2π 0 π . see z c f dr x σ curl f ndσ predicted stokes theorem. line integral preceding example far simpler calculate surface integral always case. example 5.15. let σ elliptic paraboloid z x2 4 y2 9 z 1 let c boundary curve. calculate u c f dr fx y z 9xz 2yi 2x y2j 2y2 2zk c traversed counterclockwise. solution surface similar one example 5.14 except boundary curve c ellipse x2 4 y2 9 1 laying plane z 1. case using stokes theorem easier computing line integral directly. example 5.14 point x y zx y surface z zx y x2 4 y2 9 vector n z x iz y jk r 1 z x 2 ³ z y 2 x 2 i2y 9 jk q 1 x2 4 4y2 9 positive unit normal vector σ. calculating curl f gives curlf 4y0i 9x0j 22k 4yi 9xj 0k curlf n 4yx 29x2y 9 01 q 1 x2 4 4y2 9 2xy2xy0 q 1 x2 4 4y2 9 0 stokes theorem z c f dr x σ curlf ndσ x σ 0dσ 0 . physical applications simple closed curve c line integral u c f dr often called circulation f around c. example e represents electrostatic ﬁeld due point charge turns out7 curle 0 means circulation u c e dr 0 7see ch. 2 reitz milford christy.
5.5 stokes theorem 199 stokes theorem. vector ﬁelds zero curl often called irrotational ﬁelds. fact term curl created 19th century scottish physicist james clerk maxwell study electromagnetism used extensively. physics curl interpreted measure circulation density. best seen using another deﬁnition curlf equivalent8 deﬁnition given formula 5.46. namely value n curlf point x y z lim s0 1 z c f dr 5.50 surface area surface σ containing point x y z simple closed boundary curve c positive unit normal vector n x y z. limit think curve c shrinking point x y z causes σ surface bounds smaller smaller surface area. ratio circulation surface area limit makes curl rough measure circulation density that is circulation per unit area. x 0 f figure 5.5.6 curl rotation idea curl vector ﬁeld related rotation shown figure 4.5.6. suppose vector ﬁeld fx y z always parallel xyplane point x y z vectors grow larger point x y z y axis. example fx y z 1 x2j. think vector ﬁeld representing ﬂow water imagine dropping two wheels paddles water ﬂow fig ure 4.5.6. since ﬂow stronger that is magnitude f larger move away yaxis wheel would rotate counterclockwise dropped right yaxis would rotate clockwise dropped left yaxis. cases curl would nonzero curl fx y z 2xk example would obey righthand rule is curlfx y z points direction thumb cup right hand direction rota tion wheel. curl points outward in positive zdirection x 0 points inward in negative zdirection x 0. notice vectors di rection magnitude wheels would rotate hence would curl which ﬁelds called irrotational meaning rotation. finally stokes theorem know c simple closed curve solid region 8see schey p. 7881 derivation.
200 chapter 5. line surface integrals r3 fx y z smooth vector ﬁeld curlf 0 s z c f dr x σ curlf ndσ x σ 0 ndσ x σ 0dσ 0 σ orientable surface inside whose boundary c such surface some times called capping surface c. similar twovariable case three dimensional version result section 4.3 solid regions r3 simply connected that is regions holes following statements equivalent simply connected solid region r3 a fx y z px y ziqx y zj rx y zk smooth potential fx y z s b w c f dr independent path curve c s c z c f dr 0 every simple closed curve c s d r y q z p z r x q x p y that is curlf 0 s. part d also way saying differential form p dxq dy r dz exact. example 5.16. determine vector ﬁeld fx y z xyzixzjxyk potential r3. solution since r3 simply connected need check whether curlf 0 throughout r3 is r y q z p z r x q x p y throughout r3 px y z xyz qx y z xz rx y z xy. see p z xy r x p z r x x y z r3. thus fx y z potential r3. exercises exercises 13 calculate r c f x y zds given function f x y z curve c. 1. f x y z z c x cost sint z t 0 t 2π 2. f x y z x y2yz c x t2 t z 1 1 t 2
5.5 stokes theorem 201 3. f x y z z2 c x tsint tcost z 2 p 2 3 t32 0 t 1 exercises 49 calculate r c f dr given vector ﬁeld fx y z curve c. 4. fx y z ijk c x 3t 2t z t 0 t 1 5. fx y z yixj zk c x cost sint z t 0 t 2π 6. fx y z xi yj zk c x cost sint z 2 0 t 2π 7. fx y z y2zi xyj2xz yk c x t 2t z t2 1 0 t 1 8. fx y z yzi xzj xyk c polygonal path 000 100 120 9. fx y z xyiz xj2yzk c polygonal path 000 100 120 122 exercises 1013 state whether vector ﬁeld fx y z potential r3 you need ﬁnd potential itself. 10. fx y z yixj zk 11. fx y z ai bj ck a b c constant 12. fx y z x yi xj z2 k 13. fx y z xyixyz2j y2zk b exercises 1415 verify stokes theorem given vector ﬁeld fx y z surface σ. 14. fx y z 2yixj zk σ x2 y2 z2 1 z 0 15. fx y z xyi xzj yzk σ z x2 y2 z 1 16. construct möbius strip piece paper draw line center like dotted line figure 5.5.3b. cut möbius strip along center line completely around strip. many surfaces result in would describe them orientable c 17. let σ closed surface fx y z smooth vector ﬁeld. show σ curlf ndσ 0. hint split σ half. 18. show greens theorem special case stokes theorem.
202 chapter 5. line surface integrals 5.6 gradient divergence curl laplacian ﬁnal section establish relationships gradient divergence curl also introduce new quantity called laplacian. show write quantities cylindrical spherical coordinates. realvalued function f x y z r3 gradient f x y z vectorvalued func tion r3 is value point x y z vector f x y z µf x f y f z f x f y j f z k r3 partial derivatives evaluated point x y z. way think symbol as applied realvalued function f produce vector f . turns divergence curl also expressed terms symbol . done thinking as vector r3 namely x y j z k . 5.51 here symbols x y z thought partial derivative operators get applied realvalued function say f x y z produce partial derivatives f x f y f z . instance x applied f x y z produces f x . really vector strictly speaking no since x y z actual numbers. helps think as vector especially divergence curl soon see. process applying x y z realvalued function f x y z normally thought multiplying quantities µ x f f x µ y f f y µ z f f z reason is often referred del operator since operates functions. example often convenient write divergence divf f since vector ﬁeld fx y z f1x y zi f2x y zj f3x y zk dot product f thought vector makes sense f µ x y j z k f1x y zi f2x y zj f3x y zk µ x f1 µ y f2 µ z f3 f1 x f2 y f3 z divf
5.6 gradient divergence curl laplacian 203 also write curlf terms namely f since vector ﬁeld fx y z px y ziqx y zj rx y zk have f j k x y z px y z qx y z rx y z µr y q z µr x p z j µq x p y k µr y q z µp z r x j µq x p y k curlf realvalued function f x y z gradient f x y z f x f y j f z k vector ﬁeld take divergence divf f µ x y j z k µf x f y j f z k x µf x y µf y z µf z 2 f x2 2 f y2 2 f z2 note realvalued function give special name deﬁnition 5.7. realvalued function f x y z laplacian f denoted f given f x y z f 2 f x2 2 f y2 2 f z2 . 5.52 example 5.17. let rx y z xiyjzk position vector ﬁeld r3. rx y z2 r r x2 y2 z2 realvalued function. find a gradient r2 b divergence r c curl r d laplacian r2
204 chapter 5. line surface integrals solution a r2 2xi2yj2zk 2r b r xx yy zz 111 3 c r j k x y z x z 00i 00j 00k 0 d r2 2 x2 x2 y2 z2 2 y2 x2 y2 z2 2 z2 x2 y2 z2 222 6 note could calculated r2 another way using notation along parts a b r2 r2 2r 2 r 23 6 notice example 5.17 take curl gradient r2 get r2 2r 2 r 20 0 . following theorem shows case general theorem 5.14. smooth realvalued function f x y z f 0. proof see smoothness f f j k x y z f x f y f z µ 2 f yz 2 f zy µ 2 f xz 2 f zx j µ 2 f xy 2 f yx k 0 since mixed partial derivatives component equal. qed corollary 5.15. vector ﬁeld fx y z potential curl f 0. another way stating theorem 5.14 gradients irrotational. also notice example 5.17 take divergence curl r trivially get r 0 0 . following theorem shows case general
5.6 gradient divergence curl laplacian 205 theorem 5.16. smooth vector ﬁeld fx y z f 0. proof straightforward left exercise reader. corollary 5.17. ﬂux curl smooth vector ﬁeld fx y z closed surface zero. proof let σ closed surface bounds solid s. ﬂux f σ x σ f dσ f dv by divergence theorem 0 dv by theorem 5.16 0 . qed another method proving theorem 5.14 useful often used physics. namely surface integral σ f x y zdσ 0 surfaces σ solid region usually r3 must f x y z 0 throughout region. proof trivial physicists usually bother prove it. result true also applied double triple integrals. instance prove theorem 5.14 assume f x y z smooth realvalued function r3. let c simple closed curve r3 let σ capping surface c that is σ orientable boundary c. since f vector ﬁeld x σ f ndσ z c f dr stokes theorem 0 corollary 5.12. since choice σ arbitrary must f n 0 throughout r3 n unit vector. using i j k place n see must f 0 r3 completes proof. example 5.18. system electric charges charge density ρx y z produces electrostatic ﬁeld ex y z points x y z space. gauss law states x σ e dσ 4π ρ dv closed surface σ encloses charges solid region enclosed σ. show e 4πρ. one maxwells equations.9 9in gaussian or cgs units.
206 chapter 5. line surface integrals solution divergence theorem gauss law e dv x σ e dσ 4π ρ dv. combining integrals gives e4πρ dv 0 . since σ hence arbitrary get e 4πρ. exercises exercises 16 ﬁnd laplacian function f x y z. 1. f x y z x y z 2. f x y z x5 3. f x y z x2 y2 z232 4. f x y z exyz 5. f x y z x3 y3 z3 6. f x y z ex2y2z2 b exercises 718 prove given formula r ris length position vector ﬁeld rx y z xi yj zk. 7. 1r rr3 8. 1r 0 9. rr3 0 10. lnr rr2 11. divfg divf divg 12. curlfg curl f curl g 13. divf g f divg g f 14. divf g g curlf f curlg 15. divf g 0 16. curlf g f curl g f g 17. curlcurlf divf f 18. f g f g gf 2f g c 19. prove theorem 5.16. 20. use f uv divergence theorem prove a greens ﬁrst identity uv u vdv x σ uv dσ b greens second identity uv vudv x σ uv vu dσ
5.7 coordinate systems 207 21. suppose u 0 that is u harmonic r3. show x σ u dσ 0 closed surface σ. 5.7 coordinate systems often especially physics convenient use coordinate systems dealing quantities gradient divergence curl laplacian. present formulas cylindrical spherical coordinates. recall section 1.7 point x y z represented cylindrical coordinates rθ z x rcosθ rsinθ z z. point rθ z let er eθ ez unit vectors direction increasing r θ z respectively see figure 5.7.1. er eθ ez form orthonormal set vectors. note righthand rule ez er eθ. x z 0 x y z x y0 θ x z r er eθ ez figure 5.7.1 orthonormal vectors er eθ ez cylindrical coordinates x z 0 x y z x y0 θ x z ρ φ eρ eθ eφ figure 5.7.2 orthonormal vectors eρ eθ eφ spherical coordinates similarly point x y z represented spherical coordinates ρθφ x ρsinφcosθ ρsinφsinθ z ρcosφ. point ρθφ let eρ eθ eφ unit vectors direction increasing ρ θ φ respectively see figure 5.7.2. vectors eρ eθ eφ orthonormal. righthand rule see eθ eρ eφ. summarize expressions gradient divergence curl laplacian cartesian cylindrical spherical coordinates following tables
208 chapter 5. line surface integrals cartesian x y z scalar function f vector ﬁeld f f1 i f2 j f3 k gradient f f x f y j f z k divergence f f1 x f2 y f3 z curl f µf3 y f2 z µf1 z f3 x j µf2 x f1 y k laplacian f 2f x2 2f y2 2f z2 cylindrical rθ z scalar function f vector ﬁeld f fr er fθ eθ fz ez gradient f f r er 1 r f θ eθ f z ez divergence f 1 r r rfr 1 r fθ θ fz z curl f µ1 r fz θ fθ z er µfr z fz r eθ 1 r µ r rfθfr θ ez laplacian f 1 r r µ r f r 1 r2 2f θ2 2f z2 spherical ρθφ scalar function f vector ﬁeld f fρ eρ fθ eθ fφ eφ gradient f f ρ eρ 1 ρsinφ f θ eθ 1 ρ f φ eφ divergence f 1 ρ2 ρ ρ2 fρ 1 ρsinφ fθ θ 1 ρsinφ φsinφ fθ curl f 1 ρsinφ µ φsinφ fθ fφ θ eρ 1 ρ µ ρ ρ fφ fρ φ eθ µ 1 ρsinφ fρ θ 1 ρ ρ ρ fθ eφ laplacian f 1 ρ2 ρ µ ρ2 f ρ 1 ρ2 sin2 φ 2f θ2 1 ρ2 sinφ φ µ sinφ f φ derivation formulas cylindrical spherical coordinates straight forward extremely tedious. basic idea take cartesian equivalent quantity question substitute formula using appropriate coordinate transformation. example derive formula gradient spherical coordinates.
5.7 coordinate systems 209 goal show gradient realvalued function fρθφ spherical coordinates is f f ρ eρ 1 ρsinφ f θ eθ 1 ρ f φ eφ idea cartesian gradient formula fx y z f x i f y j f z k put cartesian ba sis vectors i j k terms spherical coordinate basis vectors eρ eθ eφ functions ρ θ φ. put partial derivatives f x f y f z terms f ρ f θ f φ functions ρ θ φ. step 1 get formulas eρ eθ eφ terms i j k. see figure 5.7.2 unit vector eρ ρ direction general point ρθφ eρ r r r xi yj zk position vector point cartesian coordinates. thus eρ r r xi yj zk p x2 y2 z2 using x ρsinφcosθ ρsinφsinθ z ρcosφ ρ p x2 y2 z2 get eρ sinφ cosθi sinφ sinθj cosφk now since angle θ measured xyplane unit vector eθ θ direction must parallel xyplane. is eθ form ai bj0k. ﬁgure b are note since eθ eρ particular eθ eρ eρ xyplane. occurs angle φ π2. putting φ π2 formula eρ gives eρ cosθisinθj0k see vector perpendicular sinθicosθj0k. since vector also unit vector points positive θ direction must eθ eθ sinθi cosθj 0k lastly since eφ eθ eρ get eφ cosφ cosθi cosφ sinθj sinφk step 2 use three formulas step 1 solve i j k terms eρ eθ eφ. comes solving system three equations three unknowns. many ways this combining formulas eρ eφ eliminate k give us equation involving j. this formula eθ leave us system two equations two unknowns i j use solve ﬁrst j i. lastly solve k. first note sinφeρ cosφeφ cosθi sinθj sinθsinφeρ cosφeφ cosθeθ sin2 θ cos2 θj j
210 chapter 5. line surface integrals so j sinφ sinθeρ cosθeθ cosφ sinθeφ likewise see cosθsinφeρ cosφeφ sinθeθ cos2 θ sin2 θi so sinφ cosθeρ sinθeθ cosφ cosθeφ lastly see that k cosφeρ sinφeφ step 3 get formulas f ρ f θ f φ terms f x f y f z . chain rule f ρ f x x ρ f y y ρ f z z ρ f θ f x x θ f y y θ f z z θ f φ f x x φ f y y φ f z z φ yields f ρ sinφ cosθ f x sinφ sinθ f y cosφ f z f θ ρsinφ sinθ f x ρsinφ cosθ f y f φ ρcosφ cosθ f x ρcosφ sinθ f y ρsinφ f z step 4 use three formulas step 3 solve f x f y f z terms f ρ f θ f φ . again involves solving system three equations three unknowns. using similar process elimination step 2 get f x 1 ρsinφ µ ρsin2 φ cosθ f ρ sinθ f θ sinφ cosφ cosθ f φ f y 1 ρsinφ µ ρsin2 φ sinθ f ρ cosθ f θ sinφ cosφ sinθ f φ f z 1 ρ µ ρcosφ f ρ sinφ f φ step 5 substitute formulas i j k step 2 formulas f x f y f z step 4 cartesian gradient formula fx y z f x i f y j f z k.
5.7 coordinate systems 211 last step perhaps tedious since involves simplifying 3333 22 22 terms namely f 1 ρsinφ µ ρsin2 φ cosθ f ρ sinθ f θ sinφ cosφ cosθ f φ sinφ cosθeρ sinθeθ cosφ cosθeφ 1 ρsinφ µ ρsin2 φ sinθ f ρ cosθ f θ sinφ cosφ sinθ f φ sinφ sinθeρ cosθeθ cosφ sinθeφ 1 ρ µ ρcosφ f ρ sinφ f φ cosφeρ sinφeφ see 8 terms involving eρ 6 terms involving eθ 8 terms involving eφ. algebra straightforward yields desired result f f ρ eρ 1 ρsinφ f θ eθ 1 ρ f φ eφ example 5.19. example 5.17 showed r2 2r r2 6 rx y z xi yj zk cartesian coordinates. verify get answers switch spherical coordinates. solution since r2 x2 y2 z2 ρ2 spherical coordinates let fρθφ ρ2 so fρθφ r2. gradient f spherical coordinates f f ρ eρ 1 ρsinφ f θ eθ 1 ρ f φ eφ 2ρeρ 1 ρsinφ 0eθ 1 ρ 0eφ 2ρeρ 2ρ r r showed earlier 2ρ r ρ 2r expected. laplacian f 1 ρ2 ρ µ ρ2 f ρ 1 ρ2 sin2 φ 2f θ2 1 ρ2 sinφ φ µ sinφ f φ 1 ρ2 ρ ρ2 2ρ 1 ρ2 sinφ 0 1 ρ2 sinφ φ sinφ0 1 ρ2 ρ 2ρ3 0 0 1 ρ2 6ρ2 6 expected. exercises
212 chapter 5. line surface integrals 1. let f x y z x2 y2 z232 cartesian coordinates. find laplacian f spher ical coordinates. 2. let f x y z ex2y2z2 cartesian coordinates. find laplacian function spherical coordinates. 3. let f x y z z x2 y2 cartesian coordinates. find f cylindrical coordinates. 4. frθ z rer z sinθeθ rzez cylindrical coordinates ﬁnd divf curlf. 5. fρθφ eρ ρ cosθeθ ρeφ spherical coordinates ﬁnd divf curlf. c 6. derive gradient formula cylindrical coordinates f f r er 1 r f θ eθ f z ez.
bibliography abbott e.a. flatland 7th edition. new york dover publications inc. 1952 classic tale creature living 2dimensional world encounters higher dimensional creature lots humor thrown in. anton h. c. rorres elementary linear algebra applications version 8th edition. new york john wiley sons 2000 standard treatment elementary linear algebra. bazaraa m.s. h.d. sherali c.m. shetty nonlinear programming theory algo rithms 2nd edition. new york john wiley sons 1993 thorough treatment nonlinear optimization. farin g. curves surfaces computer aided geometric design practical guide 2nd edition. san diego ca academic press 1990 intermediatelevel book curve surface design. hecht e. optics 2nd edition. reading ma addisonwesley publishing co. 1987 intermediatelevel book optics covering wide range topics. hoel p.g. s.c. port c.j. stone introduction probability theory boston ma houghton mifﬂin co. 1971 excellent introduction elementary calculusbased probability theory. lots good exer cises. jackson j.d. classical electrodynamics 2nd edition. new york john wiley sons 1975 advanced book electromagnetism famous intimidating. mathemat ics understandable reading present book. marion j.b. classical dynamics particles systems 2nd edition. new york academic press 1970 standard intermediatelevel treatment classical mechanics. thorough. oneill b. elementary differential geometry new york academic press 1966 intermediatelevel book differential geometry modern approach based differential forms. 213
214 bibliography pogorelov a.v. analytical geometry moscow mir publishers 1980 intermediateadvanced book analytic geometry. press w.h. s.a. teukolsky w.t. vetterling b.p. flannery numerical recipes for tran art scientific computing 2nd edition. cambridge uk cambridge uni versity press 1992 excellent source information numerical methods solving wide variety problems. though examples fortran programming language code clear enough implement language choice. protter m.h. c.b. morrey analytic geometry 2nd edition. reading ma addison wesley publishing co. 1975 thorough treatment elementary analytic geometry rigor found recent books. ralston a. p. rabinowitz first course numerical analysis 2nd edition. new york mcgrawhill 1978 standard treatment elementary numerical analysis. reitz j.r. f.j. milford r.w. christy foundations electromagnetic theory 3rd edi tion. reading ma addisonwesley publishing co. 1979 intermediate text electromagnetism. schey h.m. div grad curl that informal text vector calculus new york w.w. norton co. 1973 intuitive approach subject physicists viewpoint. highly recommended. taylor a.e. w.r. mann advanced calculus 2nd edition. new york john wiley sons 1972 excellent treatment ndimensional calculus. good book study present book. many intriguing exercises. uspensky j.v. theory equations new york mcgrawhill 1948 classic subject discussing many interesting topics. weinberger h.f. first course partial differential equations new york john wiley sons 1965 good introduction vast subject partial differential equations. welchons a.m. w.r. krickenberger solid geometry boston ma ginn co. 1936 thorough treatment 3dimensional geometry elementary perspective in cludes many topics sadly seem taught anymore.
appendix answers hints selected exercises chapter 1 section 1.1 p. 8 1.a p 5 b p 5 c p 17 d 1 e 2 p 17 2. yes 3. no. section 1.2 p. 14 1.a 443 b 261 c ³ 1 p 30 5 p 30 2 p 30 d p 41 2 e p 41 2 f 1468 g 734 h 161 i 242 j no. 3. no v wis larger. section 1.3 p. 19 1. 10 3. 73.4 5. 90 7. 0 9. yes since v w 0 11. v w 0 p 21 p 5 vw 13. v w p 26 p 21 p 5 v w 15. hint use deﬁnition 1.6 24. hint see theorem 1.10c. section 1.4 p. 31 1. 52324 3. 845 5. 0 7. 16.72 9. 4 p 5 11. 9 13. 0 8102 15. 14 31. circle radius 1 vcentered origin normal plane v. section 1.5 p. 41 1. a 232 t543 b x 2 5t 3 4t z 2 3t c x2 5 y3 4 z2 3 3.a 213t101 b x 2t 1 z 3t c x2 z3 1 5. x 12t 2 7t z 3 8t 7. 7.65 9. 123 11. 4x 4y3z 10 0 13. x 2yz 2 0 15. 11x 24y 21z 26 0 17. 9 p 35 19. x 5t 23t z 7t 21. 1021. section 1.6 p. 49 1. radius 1 center 235 3. radius 5 center 111 5. intersection 7. circle x2 y2 4 planes z p 5 9. lines x b z 0 x y b z 0 13. ³ 2a 2c 2b 2c0 . section 1.7 p. 53 1.a 4 π 31 b p 17 π 31.816 3. a 2 p 7 11π 6 0 b 2 p 7 11π 6 π 2 5.a r2 z2 25 b ρ 5 7.a r2 9z2 36 b ρ21 8cos2 φ 36 10. aθacotφ 12. hint use distance formula carte sian coordinates. chapter 2 section 2.1 p. 63 1. ft 12t3t2 x 1 t z 1 3. ft 2sin2t2cos2t1 x 1 2t z t 5. vt 11 costsint at 0sintcost 9.a line parallel c b halfline parallel c c hint think 215
216 appendix a answers hints selected exercises functions position vectors 15. hint theorem 1.16. section 2.3 p. 72 1. 3π p 5 2 3. 2532 8 5. replace ³³ 27s16 2 23 4 . 9 6. hint use theo rem 2.1e example 2.3 theorem 1.16 7. hint use exercise 6. 9. hint use ft ftt differentiate get ft put expressions ft ft write tt terms nt. 11. tt 1 p 2sintcost1 nt costsint0 bt 1 p 2sintcost1 κt 12 chapter 3 section 3.1 p. 78 1. domain r2 range 1 3. domain x y x2 y2 4 range 0 5. domain r3 range 11 7. 1 9. exist 11. 2 13. 2 15. 0 17. exist. section 3.2 p. 83 1. f x 2x f y 2y 3. f x xx2 y412 f y 1 2x2 412 5. f x yexy y f y xexy x 7. f x 4x3 f y 0 9. f x xx2 y212 f y yx2 y212 11. f x 2x 3 x2 423 f y 1 3x2 423 13. f x 2xex2y2 f y 2yex2y2 15. f x ycosxy f y xcosxy 17. 2 f x2 2 2 f y2 2 2 f xy 0 19. 2 f x2 y4x2y432 2 f y2 1 4x2 432 2 f xy 1 2 xx2 432 21. 2 f x2 y2exy 2 f y2 x2exy 2 f xy 1 xyexy 1 23. 2 f x2 12x2 2 f y2 0 2 f xy 0 25. 2 f x2 x2 2 f y2 y2 2 f xy 0 section 3.3 p. 86 1. 2x 3y z 3 0 3. 2x z 2 0 5. x 2y z 7. 1 2x 1 4 9y 2 p 11 12 z 2 p 11 3 0 9. 3x4y5z 0. section 3.4 p. 91 1. 2x2y 3. x p x2y24 p x2y24 5. 1 x 1 y 7. yzcosxyzxzcosxyzxycosxyz 9. 2x2y2z 11. 2 p 2 13. 1 p 3 15. p 3 cos1 17. increase 4520 de crease 4520 section 3.5 p. 98 1. local min. 10 saddle pt. 10 3. lo cal min. 11 local max. 11 saddle pts. 1111 5. local min. 11 saddle pt. 00 7. local min. 00 9. local min. 112 11. width height depth10 13. x 4 z 2. section 3.6 p. 106 2. x0 y0 00 0.28580.3998 x0 y0 11 1.032561.94037 section 3.7 p. 112 1. min. ³ 4 p 5 2 p 5 max. ³ 4 p 5 2 p 5 3. min. ³ 20 p 13 30 p 13 max. ³ 20 p 1330 p 13 4. global maximum global minimum. 5. 8abc 3 p 3 chapter 4 section 4.1 p. 117 1. 1 3. 7 12 5. 7 6 7. 5 9. 1 2 11. 15.
217 section 4.2 p. 124 1. 1 3. 8ln23 5. π 4 6. 1 4 7. 2 9. 1 6 10. 6 5. section 4.3 p. 128 1. 9 2 3. 2cosπ2 π4 24 5. 1 6 7. 6 10. 1 3 section 4.4 p. 133 1. values converge 1.318. hint java exponential function ex obtained math.expx. languages similar functions otherwise use e 2.7182818284590455 pro gram. 2. 1.146 3. 0.705 4. 0.168. section 4.5 p. 141 1. 8π 3. 4π 3 8332 7. 1sin2 2 9. 2πab section 4.6 p. 145 1. 183 3. 0 4a 3π 5. 03π16 7. 005a12 9. 712712712 section 4.7 p. 153 1. pπ 2. 1 6. n n12n2 7. 1 n chapter 5 section 5.1 p. 162 1. 12 3. 23 5. 24π 7. 2π 9. 2π 11. 0 section 5.2 p. 170 1. 0 3. no 4. yes. fx y x2 2 y2 2 5. no 9. b no. hint think f deﬁned 10. yes. fx y axy bx cy section 5.3 p. 177 1. 1615 3. 5π 5. yes. fx y xy2 x3 7. yes. fx y 4x2y2y2 3x section 5.4 p. 186 1. 216π 2. 3 3. 12π5 7. 154 section 5.5 p. 200 1. 2 p 2π2 2. 17 p 175 p 53 3. 25 4. 2 5. 2ππ 1 7. 6715 9. 6 11. yes 13. no 19. hint think vector ﬁeld fx y px yi qx yj r2 extended natural way vector ﬁeld r3. section 5.6 p. 206 1. 0 3. 12 p x2 y2 z2 5. 6x y z section 5.7 p. 211 1. 12ρ 2. 4ρ2 6eρ2 3. 2z r3 er 1 r2 ez 5. divf 2 ρ sinθ sinφ cotφ curlf cotφ cosθeρ 2eθ 2cosθeφ 6. hint start showing er cosθi sinθj eθ sinθi cosθj ez k.
gnu free documentation license version 1.2 november 2002 copyright 200020012002 free software foundation inc. 51 franklin st fifth floor boston 021101301 usa everyone permitted copy distribute verbatim copies license document changing allowed. preamble purpose license make manual textbook functional useful document free sense freedom assure everyone effective freedom copy redistribute it without modifying it either commercially noncommercially. secondarily license preserves author publisher way get credit work considered responsible modiﬁcations made others. license kind copyleft means derivative works document must free sense. complements gnu general public license copyleft license designed free software. designed license order use manuals free software free software needs free documentation free program come manuals providing freedoms software does. license limited software manuals used textual work regardless subject matter whether published printed book. recommend license principally works whose purpose instruction reference. 1. applicability definitions license applies manual work medium contains notice placed copyright holder saying distributed terms license. notice grants worldwide royaltyfree license unlimited duration use work conditions stated herein. document below refers manual work. member public licensee addressed you. accept license copy modify distribute work way requiring permission copyright law. modiﬁed version document means work containing document portion it either copied verbatim modiﬁcations andor translated another language. 218
219 secondary section named appendix frontmatter section document deals exclusively relationship publishers authors document documents overall subject or related matters contains nothing could fall directly within overall subject. thus document part textbook mathematics secondary section may explain mathematics. relationship could matter historical connection subject related matters legal commercial philosophical ethical political position regarding them. invariant sections certain secondary sections whose titles designated invariant sections notice says document released license. section ﬁt deﬁnition secondary allowed designated invariant. document may contain zero invariant sections. document identify invariant sections none. cover texts certain short passages text listed frontcover texts backcover texts notice says document released li cense. frontcover text may 5 words backcover text may 25 words. transparent copy document means machinereadable copy represented format whose speciﬁcation available general public suitable revising document straightforwardly generic text editors for images composed pixels generic paint programs for drawings widely available drawing editor suitable input text formatters automatic translation variety formats suitable input text formatters. copy made otherwise transparent ﬁle format whose markup absence markup arranged thwart discourage subsequent modiﬁcation readers transparent. image format transparent used substantial amount text. copy transparent called opaque. examples suitable formats transparent copies include plain ascii without markup texinfo input format latex input format sgml xml using publicly available dtd standardconforming simple html postscript pdf designed human modiﬁca tion. examples transparent image formats include png xcf jpg. opaque formats include proprietary formats read edited proprietary word proces sors sgml xml dtd andor processing tools generally available machinegenerated html postscript pdf produced word processors output purposes only. title page means printed book title page itself plus following pages needed hold legibly material license requires appear title page. works formats title page such title page means text near prominent appearance works title preceding beginning body text. section entitled xyz means named subunit document whose title either precisely xyz contains xyz parentheses following text translates xyz an language. here xyz stands speciﬁc section name mentioned below acknowledgments dedications endorsements history. preserve
220 gnu free documentation license title section modify document means remains section entitled xyz according deﬁnition. document may include warranty disclaimers next notice states license applies document. warranty disclaimers considered included reference license regards disclaiming warranties implication warranty disclaimers may void effect meaning license. 2. verbatim copying may copy distribute document medium either commercially non commercially provided license copyright notices license notice say ing license applies document reproduced copies add conditions whatsoever license. may use technical measures obstruct control reading copying copies make distribute. how ever may accept compensation exchange copies. distribute large enough number copies must also follow conditions section 3. may also lend copies conditions stated above may publicly display copies. 3. copying quantity publish printed copies or copies media commonly printed covers document numbering 100 documents license notice requires cover texts must enclose copies covers carry clearly legibly cover texts frontcover texts front cover backcover texts back cover. covers must also clearly legibly identify publisher copies. front cover must present full title words title equally prominent visible. may add material covers addition. copying changes limited covers long preserve title document satisfy conditions treated verbatim copying respects. required texts either cover voluminous ﬁt legibly put ﬁrst ones listed as many ﬁt reasonably actual cover continue rest onto adjacent pages. publish distribute opaque copies document numbering 100 must either include machinereadable transparent copy along opaque copy state opaque copy computernetwork location general networkusing public access download using publicstandard network protocols com plete transparent copy document free added material. use latter option must take reasonably prudent steps begin distribution opaque copies quantity ensure transparent copy remain thus accessible stated lo cation least one year last time distribute opaque copy directly agents retailers edition public.
221 requested required contact authors document well redistributing large number copies give chance provide updated version document. 4. modifications may copy distribute modiﬁed version document conditions sections 2 3 above provided release modiﬁed version precisely license modiﬁed version ﬁlling role document thus licensing distribu tion modiﬁcation modiﬁed version whoever possesses copy it. addition must things modiﬁed version a. use title page and covers any title distinct document previous versions which should any listed history section document. may use title previous version original publisher version gives permission. b. list title page authors one persons entities responsible au thorship modiﬁcations modiﬁed version together least ﬁve principal authors document all principal authors fewer ﬁve unless release requirement. c. state title page name publisher modiﬁed version pub lisher. d. preserve copyright notices document. e. add appropriate copyright notice modiﬁcations adjacent copy right notices. f. include immediately copyright notices license notice giving public per mission use modiﬁed version terms license form shown addendum below. g. preserve license notice full lists invariant sections required cover texts given documents license notice. h. include unaltered copy license. i. preserve section entitled history preserve title add item stating least title year new authors publisher modiﬁed version given title page. section entitled history document create one stating title year authors publisher document given title page add item describing modiﬁed version stated previous sentence.
222 gnu free documentation license j. preserve network location any given document public access trans parent copy document likewise network locations given document previous versions based on. may placed history section. may omit network location work published least four years document itself original publisher version refers gives permission. k. section entitled acknowledgments dedications preserve title section preserve section substance tone contributor acknowledgments andor dedications given therein. l. preserve invariant sections document unaltered text titles. section numbers equivalent considered part section titles. m. delete section entitled endorsements. section may included modiﬁed version. n. retitle existing section entitled endorsements conﬂict title invariant section. o. preserve warranty disclaimers. modiﬁed version includes new frontmatter sections appendices qualify secondary sections contain material copied document may option designate sections invariant. this add titles list invariant sections modiﬁed versions license notice. titles must distinct section titles. may add section entitled endorsements provided contains nothing endorse ments modiﬁed version various partiesfor example statements peer review text approved organization authoritative deﬁnition standard. may add passage ﬁve words frontcover text passage 25 words backcover text end list cover texts modiﬁed version. one passage frontcover text one backcover text may added or arrangements made by one entity. document already includes cover text cover previously added arrangement made entity acting behalf of may add another may replace old one explicit permission previous publisher added old one. authors publishers document license give permission use names publicity assert imply endorsement modiﬁed version. 5. combining documents may combine document documents released license terms deﬁned section 4 modiﬁed versions provided include
223 combination invariant sections original documents unmodiﬁed list invariant sections combined work license notice preserve warranty disclaimers. combined work need contain one copy license multiple identical in variant sections may replaced single copy. multiple invariant sections name different contents make title section unique adding end it parentheses name original author publisher section known else unique number. make adjustment section titles list invariant sections license notice combined work. combination must combine sections entitled history various orig inal documents forming one section entitled history likewise combine sections en titled acknowledgments sections entitled dedications. must delete sections entitled endorsements. 6. collections documents may make collection consisting document documents released un der license replace individual copies license various documents single copy included collection provided follow rules license verbatim copying documents respects. may extract single document collection distribute individually license provided insert copy license extracted document follow license respects regarding verbatim copying document. 7. aggregation independent works compilation document derivatives separate independent documents works volume storage distribution medium called ag gregate copyright resulting compilation used limit legal rights compilations users beyond individual works permit. document included aggregate license apply works aggregate derivative works document. cover text requirement section 3 applicable copies document document less one half entire aggregate documents cover texts may placed covers bracket document within aggregate elec tronic equivalent covers document electronic form. otherwise must ap pear printed covers bracket whole aggregate. 8. translation translation considered kind modiﬁcation may distribute translations document terms section 4. replacing invariant sections translations re quires special permission copyright holders may include translations
224 gnu free documentation license invariant sections addition original versions invariant sections. may include translation license license notices document warranty disclaimers provided also include original english version license original versions notices disclaimers. case dis agreement translation original version license notice disclaimer original version prevail. section document entitled acknowledgments dedications history requirement section 4 preserve title section 1 typically require changing actual title. 9. termination may copy modify sublicense distribute document except expressly pro vided license. attempt copy modify sublicense distribute document void automatically terminate rights license. how ever parties received copies rights license licenses terminated long parties remain full compliance. 10. future revisions license free software foundation may publish new revised versions gnu free doc umentation license time time. new versions similar spirit present version may differ detail address new problems concerns. see http version license given distinguishing version number. document speciﬁes particular numbered version license or later version applies it option following terms conditions either speciﬁed version later version published not draft free software foundation. document specify version number license may choose version ever published not draft free software foundation. addendum use license documents use license document written include copy license document put following copyright license notices title page copyright year name. permission granted copy distribute andor modify document terms gnu free documentation license ver sion 1.2 later version published free software foundation invariant sections frontcover texts backcover texts. copy license included section entitled gnu free documentation license.
225 invariant sections frontcover texts backcover texts replace with.texts. line this invariant sections list titles frontcover texts list backcover texts list. invariant sections without cover texts combination three merge two alternatives suit situation. document contains nontrivial examples program code recommend releas ing examples parallel choice free software license gnu general public license permit use free software.
history section contains revision history book. persons making modiﬁcations book please record pertinent information here following format ﬁrst item below. 1. version 1.0 date 20080104 authors michael corral title vector calculus modiﬁcations initial version 2. version 1.1 date 20161204 authors anton petrunin title corrals vector calculus modiﬁcations minor corrections exercises. 226
index symbols c1 c. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 mx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .142 mxy mxz myz . . . . . . . . . . . . . . . . . . . . . . . . 144 r2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1 r3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1 x. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .142 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 z. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .144 δx y . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 x y z uvw . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 f x . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .126 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 119 r c . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 159 er eθ ez eρ eφ . . . . . . . . . . . . . . . . . . . . . . .207 i j k . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 202 v σ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 u c . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .166 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 dv f . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 dr . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . 2 61 angle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 annulus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176 arc length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 area element . . . . . . . . . . . . . . . . . . . . . . . . . . 119 average value . . . . . . . . . . . . . . . . . . . . . . . . . 130 b bézier curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 bounded set . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 c capping surface . . . . . . . . . . . . . . . . . . . . . . . 200 cauchyschwarz inequality . . . . . . . . . . . 17 center mass. . . . . . . . . . . . . . . . . . . . . . . . .142 centroid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 chain rule . . . . . . . . . . . . . . . . . . . . . . . . . 66 89 change variable. . . . . . . . . . . . . . . .135 137 circulation. . . . . . . . . . . . . . . . . . . . . . . . . . . . .198 closed curve . . . . . . . . . . . . . . . . . . . . . . . . . . . 166 closed surface . . . . . . . . . . . . . . . . . . . . . . . . . 185 collinear . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 conical helix. . . . . . . . . . . . . . . . . . . . . . . . . . .191 conservative ﬁeld . . . . . . . . . . . . . . . . . . . . . 169 constrained critical point . . . . . . . . . . . . . 107 continuity. . . . . . . . . . . . . . . . . . . . . . . . . . .57 78 continuously differentiable . . . . . . . . . 57 89 coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 cartesian . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 curvilinear . . . . . . . . . . . . . . . . . . . . . . . . . 51 cylindrical . . . . . . . . . . . . . . . . . . . . 51 208 ellipsoidal. . . . . . . . . . . . . . . . . . . . . . . . .187 lefthanded. . . . . . . . . . . . . . . . . . . . . . . . . . 2 polar . . . . . . . . . . . . . . . . . . . . . . . . . . 52 139 rectangular. . . . . . . . . . . . . . . . . . . . . . . . . .1 righthanded . . . . . . . . . . . . . . . . . . . . . . . . 1 spherical . . . . . . . . . . . . . . . . . . . . . . 51 208 coplanar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154 covariance. . . . . . . . . . . . . . . . . . . . . . . . . . . . .154 critical point. . . . . . . . . . . . . . . . . . . . . . . . . . . .93 227
228 index cross product . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 curl. . . . . . . . . . . . . . . . . . . . . . . . . .194 203 208 cylinder. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .45 density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 derivative. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .2 directional . . . . . . . . . . . . . . . . . . . . . . . . . 87 mixed partial . . . . . . . . . . . . . . . . . . . . . . 82 partial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 vectorvalued function . . . . . . . . . . . . . 57 determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 differential . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 differential form . . . . . . . . . . . . . . . . . . . . . . 160 directed curve . . . . . . . . . . . . . . . . . . . . . . . . . 165 direction angles . . . . . . . . . . . . . . . . . . . . . . . . 20 direction cosines. . . . . . . . . . . . . . . . . . . . . . . .20 directional derivative. . . . . . . . . . . . . . . . . . .87 distance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6 points . . . . . . . . . . . . . . . . . . . . . . 6 point line . . . . . . . . . . . . . . . . . . 35 point plane . . . . . . . . . . . . . . 39 44 45 distribution function . . . . . . . . . . . . . . . . . . 147 joint. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .150 normal. . . . . . . . . . . . . . . . . . . . . . . . . . . .149 divergence . . . . . . . . . . . . . . . . . . 185 202 208 divergence theorem . . . . . . . . . . . . . . . . . . 185 dot product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 double integral . . . . . . . . . . . . . . . . . . . 115 119 polar coordinates . . . . . . . . . . . . . . . . . 139 doubly ruled surface. . . . . . . . . . . . . . . . . . . .48 e ellipsoid . . . . . . . . . . . . . . . . . . . . . . 46 141 187 elliptic cone. . . . . . . . . . . . . . . . . . . . . . . . . . . . .48 elliptic paraboloid . . . . . . . . . . . . . . . . . . . . . . 47 euclidean space . . . . . . . . . . . . . . . . . . . . . . . . . 1 exact differential form . . . . . . 160 177 200 expected value . . . . . . . . . . . . . . . . . . . . . . . . 152 extreme point . . . . . . . . . . . . . . . . . . . . . . . . . . 93 f ﬂux. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .185 force . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1 continuous . . . . . . . . . . . . . . . . . . . . . . . . . 78 scalar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 vectorvalued . . . . . . . . . . . . . . . . . . . . . . 56 g gaussian blur . . . . . . . . . . . . . . . . . . . . . . . . . . 79 global maximum . . . . . . . . . . . . . . . . . . . . . . . 93 global minimum . . . . . . . . . . . . . . . . . . . . . . . . 93 gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 208 greens identities . . . . . . . . . . . . . . . . . . . . . 206 greens theorem . . . . . . . . . . . . . . . . . . . . . . 172 h harmonic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207 helicoid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 helix . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 66 191 hyperbolic paraboloid . . . . . . . . . . . . . . . . . . 47 hyperboloid. . . . . . . . . . . . . . . . . . . . . . . . . . . . .46 one sheet. . . . . . . . . . . . . . . . . . . . . . . . . . .46 two sheets. . . . . . . . . . . . . . . . . . . . . . . . . .46 hypersurface . . . . . . . . . . . . . . . . . . . . . . . . . . 126 hypervolume . . . . . . . . . . . . . . . . . . . . . . . . . . 126 improper integral . . . . . . . . . . . . . . . . . . . . . 123 integral double . . . . . . . . . . . . . . . . . . . . . . . 115 119 improper. . . . . . . . . . . . . . . . . . . . . . . . . .123 iterated . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 multiple . . . . . . . . . . . . . . . . . . . . . . . . . . 114 surface. . . . . . . . . . . . . . . . . . . . . . .179 181 triple. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .126 involute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 irrotational. . . . . . . . . . . . . . . . . . . . . . . . . . . .199 iterated integral . . . . . . . . . . . . . . . . . . . . . . 115 j jacobi identity. . . . . . . . . . . . . . . . . . . . . . . . . .32 jacobian. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .137 joint distribution . . . . . . . . . . . . . . . . . . . . . . 150
index 229 l lagrange multiplier. . . . . . . . . . . . . . . . . . .107 lamina . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 laplacian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208 level curve. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .75 limit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 vectorvalued function . . . . . . . . . . . . . 57 line . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 intersection planes . . . . . . . . . . . . . . 40 parallel . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 parametric representation . . . . . . . . . 33 perpendicular . . . . . . . . . . . . . . . . . . . . . . 36 skew . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 symmetric representation . . . . . . . . . 34 two points. . . . . . . . . . . . . . . . .35 vector representation . . . . . . . . . . . . . . 33 line integral. . . . . . . . . . . . . . . . . . . . . . 156 159 local maximum . . . . . . . . . . . . . . . . . . . . . . . . . 93 local minimum . . . . . . . . . . . . . . . . . . . . . . . . . 93 mass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 mixed partial derivative. . . . . . . . . . . . . . . .82 möbius strip . . . . . . . . . . . . . . . . . . . . . . . . . . 193 moment . . . . . . . . . . . . . . . . . . . . . . . . . . 142 144 momentum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 monte carlo method . . . . . . . . . . . . . . . . . . 130 multiple integral . . . . . . . . . . . . . . . . . . . . . . 114 multiply connected. . . . . . . . . . . . . . . . . . . .175 n npositive direction . . . . . . . . . . . . . . . . . . . 193 normal curve. . . . . . . . . . . . . . . . . . . . . . .90 normal vector ﬁeld . . . . . . . . . . . . . . . . . . . . 192 orientable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192 p paraboloid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 elliptic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 hyperbolic . . . . . . . . . . . . . . . . . . . . . . 47 94 revolution . . . . . . . . . . . . . . . . . . . . . . . 47 parallelepiped . . . . . . . . . . . . . . . . . . . . . . . . . . 26 volume . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 parameter . . . . . . . . . . . . . . . . . . . . . . . . . . 33 66 parametrization . . . . . . . . . . . . . . . . . . . . . . . . 66 partial derivative. . . . . . . . . . . . . . . . . . . . . . .80 partial differential equation. . . . . . . . . . . .83 path independence . . . . . . . . . . 167 177 200 pedal curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 piecewise smooth curve . . . . . . . . . . . . . . . 161 plane coordinate . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 euclidean . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 line intersection . . . . . . . . . . . . . . . . . 40 normal form . . . . . . . . . . . . . . . . . . . . . . . 37 normal vector . . . . . . . . . . . . . . . . . . . . . . 37 pointnormal form . . . . . . . . . . . . . . . . . 37 tangent . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 three points . . . . . . . . . . . . . . . 38 position vector . . . . . . . . . . . . . . . . . 59 60 159 potential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169 probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 probability density function. . . . . . . . . . .148 projection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .20 q quadric surface . . . . . . . . . . . . . . . . . . . . . . . . . 46 r random variable . . . . . . . . . . . . . . . . . . . . . . 147 regular reparametrization . . . . . . . . . . . . . 66 reparametrization . . . . . . . . . . . . . . . . . . . . . . 66 riemann integral . . . . . . . . . . . . . . . . . . . . . 155 righthand rule . . . . . . . . . . . . . . . . . . . . . . . . . 22 ruled surface . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 saddle point . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 sample space . . . . . . . . . . . . . . . . . . . . . . . . . . 147 scalar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 combination. . . . . . . . . . . . . . . . . . . . . . . .13 scalar function . . . . . . . . . . . . . . . . . . . . . . . . . 58 scalar triple product. . . . . . . . . . . . . . . . . . . .26
230 index second derivative test . . . . . . . . . . . . . . . . . 95 second moment. . . . . . . . . . . . . . . . . . . . . . . .154 seconddegree equation. . . . . . . . . . . . . . . . .46 simple closed curve . . . . . . . . . . . . . . . . . . . 166 simply connected. . . . . . . . . . . . . . . . .177 200 smooth function . . . . . . . . . . . . . . . . . . . . 57 95 solenoidal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 span . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 sphere . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 spherical spiral. . . . . . . . . . . . . . . . . . . . . . . . .59 standard normal distribution . . . . . . . . . 149 steepest descent. . . . . . . . . . . . . . . . . . . . . . .106 stereographic projection. . . . . . . . . . . . . . . .50 stokes theorem . . . . . . . . . . . . . . . . . 192 194 surface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 doubly ruled . . . . . . . . . . . . . . . . . . . . . . . 48 orientable. . . . . . . . . . . . . . . . . . . . . . . . .192 ruled . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 twosided . . . . . . . . . . . . . . . . . . . . . . . . . 193 surface integral . . . . . . . . . . . . . . . . . . 179 181 tangent plane . . . . . . . . . . . . . . . . . . . . . . . . . . 84 torus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 trace. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .45 triangle inequality . . . . . . . . . . . . . . . . . . . . . 18 triple integral . . . . . . . . . . . . . . . . . . . . . . . . . 126 cylindrical coordinates. . . . . . . . . . . .140 spherical coordinates . . . . . . . . . . . . . 140 u uniform density . . . . . . . . . . . . . . . . . . . . . . . 142 uniform distribution . . . . . . . . . . . . . . . . . . 148 uniformly distributed . . . . . . . . . . . . . . . . . 147 unit disk. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .74 v variance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .154 vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 angle between. . . . . . . . . . . . . . . . . . . . . .15 basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 components . . . . . . . . . . . . . . . . . . . . . . . . 13 direction. . . . . . . . . . . . . . . . . . . . . . . . . . . . .3 magnitude . . . . . . . . . . . . . . . . . . . . . 3 6 7 normal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 normalized. . . . . . . . . . . . . . . . . . . . . . . . .12 parallel. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9 perpendicular . . . . . . . . . . . . . . . . . . 16 17 positive unit normal . . . . . . . . . . . . . . 193 scalar multiplication . . . . . . . . . . . . . . . . 9 subtraction. . . . . . . . . . . . . . . . . . . . . . . . .10 tangent . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 translation. . . . . . . . . . . . . . . . . . . . . . . .5 9 unit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 zero . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 vector ﬁeld . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159 normal. . . . . . . . . . . . . . . . . . . . . . . . . . . .192 smooth. . . . . . . . . . . . . . . . . . . . . . . . . . . .172 vector triple product. . . . . . . . . . . . . . . . . . . .27 velocity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 61 volume element . . . . . . . . . . . . . . . . . . . . . . . 126 w wave equation . . . . . . . . . . . . . . . . . . . . . . . . . . 83 work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 190 z zenith angle . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
mathematics data science sushmitha shubham subhasis subhajit linear algebra

contents 1 vector matrices 1 1.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 vectors important . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2.1 visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2.2 visualization vector addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.2.3 vectors physical context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.2.4 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.3 matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 1.3.1 matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 1.3.2 linear equations matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.3.3 addition matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.3.4 scalar multiplication multiplying matrix number . . . . . . . 14 1.3.5 multiplication matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 1.3.6 properties matrix addition multiplication . . . . . . . . . . . . . . . 16 1.3.7 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 1.4 system linear equations . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 1.4.1 solutions linear system equations . . . . . . . . . . . . . . . . . . . . . . 21 1.4.2 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 1.5 determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 1.5.1 first order determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 1.5.2 second order determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 1.5.3 third order determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 ii
contents iii 1.5.4 invariance elementary row column operations . . . . . . . 30 1.5.5 determinant terms minors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 1.5.6 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 2 solving system linear equations 36 2.1 linear equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 2.2 system linear equations . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 2.2.1 matrix representation system linear equations . . . . . . . . . . 40 2.3 solution system linear equations . . . . . . . . . . . . . 41 2.3.1 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 2.4 cramers rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 2.4.1 cramers rule invertible coefficient matrix order 2 . . . . . . . . 51 2.4.2 cramers rule invertible coefficient matrix order 3 . . . . . . . . 53 2.4.3 cramers rule invertible coefficient matrix order n . . . . . . . . 55 2.4.4 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 2.5 finding solution system linear equations invertible coefficient matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 2.5.1 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 2.6 gauss elimination method . . . . . . . . . . . . . . . . . . . . . . 62 2.6.1 homogeneous nonhomogeneous system linear equations 62 2.6.2 row echelon form reduced row echelon form . . . . . . . . . . 64 2.6.3 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 2.6.4 solution ax b reduced row echelon form . . . . . 71 2.6.5 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 2.6.6 elementary row operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 2.6.7 row reduction reduced row echelon form . . . . . . . . . . . . . . . . 79 2.6.8 effect elementary row operations determinant matrix 83
contents iv 2.6.9 gauss elimination algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 2.6.10 augmented matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 2.6.11 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 3 introduction vector space 95 3.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 3.2 vector space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 3.2.1 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 3.3 properties vector spaces . . . . . . . . . . . . . . . . . . . . . . . . . . 108 3.4 subspaces vector spaces . . . . . . . . . . . . . . . . . . . . . . . . . 109 3.4.1 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 4 basis dimension 113 4.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 4.2 linear dependence independence . . . . . . . . . . . . . . 114 4.2.1 linear dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 4.2.2 linear independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118 4.2.3 ways check linear independence . . . . . . . . . . . . . . . . . . . 119 4.2.4 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 4.3 spanning sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 4.3.1 building spanning sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 4.3.2 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 4.4 basis vector space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 4.4.1 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 4.5 dimension vector space . . . . . . . . . . . . . . . . . . . . . . . . . 128
contents v 4.5.1 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 5 rank nullity matrix 131 5.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132 5.2 rank matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132 5.2.1 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 5.3 nullity matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 5.3.1 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 5.4 ranknullity theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 5.4.1 exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 6 linear transformation 141 6.1 linear mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 6.1.1 linear mapping formal definition . . . . . . . . . . . . . . . . . . . . . 144 6.1.2 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 6.2 linear transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 6.2.1 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148 6.2.2 images vectors basis vector space . . . . . . . . . . . 149 6.2.3 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150 6.3 injective surjective linear transformations . . . . . . 150 6.3.1 null space range space linear transformation . . . . . . . . 151 6.4 matrix representation linear transformation . . . . . 153 6.4.1 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 6.5 finding basis null space range space row reduced echelon form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
contents vi 6.6 ranknullity theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159 6.6.1 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 7 equivalence similarity matrices 163 7.1 equivalence matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 7.2 similar matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172 7.3 properties similar matrices . . . . . . . . . . . . . . . . . . . . . . . 180 7.4 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182 8 affine subspaces affine mapping 185 8.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 8.1.1 two dimensional affine subspaces . . . . . . . . . . . . . . . . . . . . . . . . 187 8.1.2 three dimensional affine subspaces . . . . . . . . . . . . . . . . . . . . . . . 188 8.1.3 visual representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 8.1.4 addition scalar multiplication affine subspaces . . . . . . . . 189 8.2 solution set system linear equations . . . 191 8.3 affine mappings affine subspaces . . . . . . . . . . . . . . . . 191 8.3.1 affine mapping corresponding linear transformation . . . . . . 192 8.3.2 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193 9 inner product space 195 9.1 dot product two vectors euclidean space dimension 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196 9.1.1 length vector euclidean space dimension 2 . . . . . . 197
contents vii 9.1.2 relation length dot product euclidean space dimension 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197 9.1.3 dot product angle two vectors euclidean space dimension 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198 9.2 dot product two vectors euclidean space dimension 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199 9.2.1 length vector euclidean space dimension 3 . . . . . . . . . 199 9.2.2 length dot product euclidean space dimension 3 . 200 9.2.3 angle two vectors euclidean space dimension 3 dot product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200 9.3 dot product euclidean space dimension n length angle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201 9.4 inner product vector space . . . . . . . . . . . . . . . . . . . . . 204 9.5 norm vector space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206 9.6 norm induced inner product . . . . . . . . . . . . . . . . . . . . . 208 9.7 orthogonality linear independence . . . . . . . . . . . . 212 9.7.1 obtaining orthonormal set orthogonal set . . . . . . . . . . . . . . 214 9.7.2 importance orthonormal basis . . . . . . . . . . . . . . . . . . . . . . . . . . 215 9.8 projections using inner products . . . . . . . . . . . . . . . . . . . . 220 9.8.1 projection vector along another vector . . . . . . . . . . . . . . . . . . 220 9.8.2 projection vector onto subspace . . . . . . . . . . . . . . . . . . . . . 221 9.8.3 projection linear transformation . . . . . . . . . . . . . . . . . . . . . . . 222 9.8.4 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224 9.9 gramschmidt orthonormalization . . . . . . . . . . . . . . . . . . 225 9.9.1 gramschmidt process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226 9.9.2 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 9.10 orthogonal transformations rotations . . . . . . . . 227 9.10.1 orthogonal transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
contents viii 9.10.2 rotation matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 9.10.3 orthogonal matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230 9.10.4 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
1
1.1. introduction 2 1. vector matrices what affirmed without proof also denied without proof. euclid 1.1 introduction vectors foundational elements linear algebra. often encounter data table. example following table get complete view indias gdp 200001 201213 sector wise breakups. figure 1.1 indias gdp data 200001 201213 sector wise breakups
1.1. introduction 3 following table view average run scored five players in dian cricket team v.kohli m.s.dhoni r. sharma k.l.rahul s.dhawan australia england new zealand south africa sri lanka pakistan. vs teams v.kohli m.s.dhoni r.sharma k.l.rahul s.dhawan australia 54.57 44.86 61.33 45.75 45.80 england 45.30 46.84 50.44 6.60 32.45 new zealand 59.91 49.47 33.47 68.33 32.72 south africa 64.35 31.92 33.30 26.00 49.87 sri lanka 60.00 64.40 46.25 34.75 70.21 pakistan 48.72 53.52 51.42 57.00 54.28 table 1.1 teamwise batting averages vector thought list. context examples vectors could columns rows. choose row corresponding year 201011 table given figure 1.1 4937006 713477 606848 1393879 108938 801476 2829650 considered row vector. similarly consider averages five players south africa given table 1.1 64.35 31.92 33.30 26.00 49.87 also row vector. choose column corresponding gross domestic product in rs. cr. 200405 prices table given figure 1.1 2342774 2472052 2570690 2777813 2971464 3253073 3564364 3896636 4158676 4516071 4937006 5243582 5503476 considered column vector. similarly consider averages r.sharma australia england new zealand south africa sri lanka pakistan
1.2. vectors important 4 given table 1.1 61.33 50.44 33.47 33.30 46.25 51.42 also column vector. 1.2 vectors important vectors used perform arithmetic operations lists table columns rows e.g. suppose want average sectoral gdp across years 200001 200910. one way adding elements column 200001 200910 divide total 10 get average. efficient way considering row vectors corresponding year starting 200001 till 200910 adding row vectors coordinate wise multiplying resultant vector 1 10 i.e. multiplying element resultant vector 1 10. let us describe process without using particular numbers table make cumbersome. 2000 01 a11 a12 a13 a14 a15 a16 a17 2001 02 a21 a22 a23 a24 a25 a26 a27 . . . . . . . . . . . . . . . . . . . . . . . . 2008 09 a91 a92 a93 a94 a95 a96 a97 2009 10 a10 1 a10 2 a10 3 a10 4 a10 5 a10 6 a10 7 adding row vectors corresponding year a11 a12 a13 a14 a15 a16 a17 a21 a22 a23 a24 a25 a26 a27 . . . a91 a92 a93 a94 a95 a96 a97 a10 1 a10 2 a10 3 a10 4 a10 5 a10 6 a10 7 get σ10 i1ai1 σ10 i1ai2 . . . σ10 i1ai6 σ10 i1ai7 multiplying 1 10 coordinatewise get 1 10 σ10 i1ai1 σ10 i1ai2 . . . σ10 i1ai6 σ10 i1ai7 1 10σ10 i1ai1 1 10σ10 i1ai2 . . . 1 10σ10 i1ai6 1 10σ10 i1ai7
1.2. vectors important 5 observe element vector 1 10σ10 i1ai1 1 10σ10 i1ai2 . . . 1 10σ10 i1ai6 1 10σ10 i1ai7 denotes average gdp corresponding sector. let us try understand usage vectors little details examples. example 1.2.1. suppose arun buy 3 kg rice 2 kg dal neela buy 5 kg rice 6 kg dal. vectors 3 2 arun 5 6 neela represent demands. items arun neela total rice kg 3 5 8 dal kg 2 6 8 add vectors get 3 2 5 6 8 8 nothing represents together buy 8 kg rice 8 kg dal. example 1.2.2. stocks grocery shops items stock buyer buyer b buyer c new stock rice kg 150 8 12 3 100 dal kg 50 8 5 2 75 oil liter 35 4 7 5 30 biscuits 70 10 10 5 80 soap bar 25 4 2 1 30 taking stock items grocery shop done easily using vector rep resentation 150 50 35 70 258 8 4 10412 5 7 10 2 3 2 5 5 1100 75 30 80 30 227 110 49 125 48. note add corresponding entries vectors. example addition vectors. buyer comes next day buys items quantities add vector two times multiply coordinate vector 2. 8 8 4 10 4 8 8 4 10 4 16 16 8 20 8 2.8 8 4 10 4 multiplying vector scalar i.e. entries list called scalar multiplication. 1.2.1 visualization section try visualize vectors r2. point a b r2 vector a b r2 visualized arrow joining origin 0 0 a b.
1.2. vectors important 6 pointa b v ectora b aˆ bˆ j x 1 2 1 1 ˆ ˆ j figure 1.2 vector visualization r2 write 1 2 1ˆ 2ˆ j similarly 1 1 1ˆ 1ˆ j. general vectors rn lists or rows columns n real entries. vectors n entries vectors rn points rn. 1.2.2 visualization vector addition add two vectors joining headtotail parallelogram law. v v w w figure 1.3 visualization vector addition r2
1.2. vectors important 7 want add two vectors 1 2 2 1 complete diagram shown figure 1.4. diagonal parallelogram denotes vector resulting addition two vectors. x 1 2 2 1 3 3 figure 1.4 visualization vector addition r2 1 2 2 1 3 3 observe figure 1.4. 1.2.3 vectors physical context high school physics studied vectors quite details may different approach. vector magnitude size direction. length line shows magnitude arrowhead points direction. although vector magnitude direction position. is long length changed vector altered displaced parallel itself. examples vectors appear physics . velocity . acceleration . force example 1.2.3. plane flying towards north wind blowing northwest.
1.2. vectors important 8 v w figure 1.5 caption v velocity flight w velocity wind 1.2.4 exercise question 1. choose set correct options using figure 1.6. x b 2 3 . . 1 2 figure 1.6 hint recall that vector addition scalar multiplication done coordinate wise. . option 1 2a vector 2 4. . option 2 3b vector 6 9. . option 3 b vector 3 5. . option 4 b vector 1 1.
1.2. vectors important 9 question 2. marks obtained karthika romy farzana quiz 1 quiz 2 end sem with maximum marks exam 100 shown table 1.2. use information answer following questions quiz 1 quiz 2 end sem karthika 51 50 61 romy 33 41 45 farzana 38 21 35 table 1.2 a choose following set correct options. option 1 marks obtained romy quiz 1 quiz 2 end sem represent row vector. option 2 quiz 2 marks karthika romy farzana represent column vector. option 3 number components column vector representing quiz 2 marks 9. option 4 number components row vector representing romys marks 3. b order improve marks farzana undertook project work succeeded increasing marks. marks became doubled exam. choose correct options. option 1 obtain marks obtained farzana completion project scalar multiplication done 2 row vector representing farzanas marks. option 2 obtain marks obtained farzana completion project scalar multiplication done 1 row vector representing farzanas marks. option 3 completion project row vector representing farzanas marks 76 42 70 option 4 completion project row vector representing farzanas marks 76 21 35. option 5 completion project row vector representing farzanas marks 66 82 90 c following farzanas improved marks due project i.e marks become doubled exam students given bonus marks quiz 2
1.2. vectors important 10 given column vector 10 12 15 . column vector repre senting final marks obtained quiz 2 karthika romy farzana option 1 60 53 57 option 2 60 53 36 . option 3 61 45 53 option 4 71 57 85 question 3. consider vectors a1 2 b2 2 r2 shown figure 1.7. x b v1 v2 v3 v4 v5 figure 1.7 choose set correct options. hint recall geometric representation vectors scalar multiplication vectors addition.
1.3. matrices 11 . option 1 v1 represents scalar multiple a. . option 2 v2 represents scalar multiple a. . option 3 v5 represents scalar multiple b. . option 4 v1 represents scalar multiple b. . option 5 v4 represents scalar multiple b. . option 6 v3 represents scalar multiple b. question 4. let 1 1 1 b 2 1 4 two vectors. suppose ca 3b 4 j k c j k real numbers scalars. find value c. ans 2 question 5. let 1 1 1 b 2 1 4 two vectors. suppose ca 3b 4 j k c j k real numbers scalars. find value j k. ans 5 1.3 matrices matrix set numbers arranged rows columns form rectangular array. numbers called elements entries matrix. matrices wide applications engineering physics economics statistics well var ious branches mathematics. term matrix introduced 19thcentury english mathematician james sylvester friend mathematician arthur cayley developed algebraic aspect matrices two papers 1850s. cayley first applied study systems linear equations still useful. 1.3.1 matrix definition 1.3.1. matrix rectangular array numbers arranged rows columns. plural matrices two important notions . n matrix rows n columns. . i jth entry matrix entry occuring ith row jth column. let us try understand notions using following example example 1.3.1. 1 2 3 2 3 4 23
1.3. matrices 12 . 2 3 matrix it 2 rows 3 columns. . 1 2th entry 2. . 2 3th entry 4. special types matrices. definition 1.3.2. square matrix matrix number rows number columns. example 1.3.2. 0.3 5 7 2.8 0 1 0 2.5 1 33 3 3 matrix 3 rows 3 columns. . ith diagonal entry square matrix i ith entry. . diagonal square matrix set diagonal entries. . 0.3 0 1 diagonal entries square matrix given above. definition 1.3.3. square matrix entries except diagonal 0 called diagonal matrix. example 1.3.3. 1 0 0 0 3 0 0 0 4.2 33 3 3 diagonal matrix diagonal entries 1 3 4.2 definition 1.3.4. diagonal matrix entries diagonal equal called scalar matrix. example 1.3.4. 3 0 0 0 3 0 0 0 3 33 3 3 diagonal matrix diagonal entries 3. definition 1.3.5. scalar matrix diagonal entries 1 called identity matrix denoted i. example 1.3.5. 1 0 0 0 1 0 0 0 1 3 3 identity matrix.
1.3. matrices 13 1.3.2 linear equations matrices one important applications matrices study system linear equa tions. set linear equations represented terms matrices. study later find solutions system using representation. now let us take example understand represent systems using matrices. example 1.3.6. set linear equations represented terms matrices. 3x 4y 5 4x 6y 10 represented matrix 3 4 5 4 6 10 example 1.3.7. set linear equations represented terms matrices. x 2y 5z 10 x 3y 4z 0 2x z 7 represented matrix 1 2 5 10 1 3 4 0 2 1 1 7 1.3.3 addition matrices matrix addition operation adding two matrices adding corresponding entries together. example 1.3.8. 1 9 0.6 7 4 1.5 32 0 7 0.6 7 2.5 0.6 32 1 16 1.2 0 6.5 2.1 32 definition 1.3.6. sum two n matrix b calculated entrywise i jth entry matrix b sum i jth entry i jth entry b abij aij bij remark 1.3.1. definition clear n matrix b k l matrix addition b possible k n l. example 1.3.9. 12 34 3 13 2 3 1 13 52 154 2 13
1.3. matrices 14 1.3.4 scalar multiplication multiplying matrix number multiplying matrix number means number multiplied every element matrix. example 1.3.10. 3 1 2 3 4 5 6 23 3 6 9 12 15 18 23 definition 1.3.7. product matrix number c denoted ca i jth entry ca product i jth entry number c. caij caij let us consider another example. example 1.3.11. 4 1 0 5 3 22 4 0 20 12 22 1.3.5 multiplication matrices r b c c e f product rc given ad cf suppose r1 r2 b c1 c2 c3 ris denote rows matrix cis denote columns matrix b. moreover assume number columns number rows b same. product ab given by ab r1c1 r1c2 r1c3 r2c1 r2c2 r2c3 let a11 a12 a13 a21 a22 a23 a31 a32 a33 and b b11 b12 b13 b21 b22 b23 b31 b32 b33 be two 3 3 matrices. product ab given ab a11b11 a12b21 a13b31 a11b12 a12b22 a13b32 a11b13 a12b23 a13b33 a21b11 a22b21 a23b31 a21b12 a22b22 a23b32 a21b13 a22b23 a23b33 a31b11 a32b21 a33b31 a31b12 a32b22 a33b32 a31b13 a32b23 a33b33
1.3. matrices 15 definition 1.3.8. let n matrix b n p matrix product ab p matrix amnbnp abmp i jth entry ab defined follows abij pn k1 aik.bkj remark 1.3.2. multiplication matrices b defined number columns number rows b. example 1.3.12. let 1 2 3 4 b 1 2 3 3 4 5 . let try calculate ab. 11th entry 1 2 3 4 22 1 2 3 3 4 5 7 23 12th entry 1 2 3 4 22 1 2 3 3 4 5 7 10 23 13th entry 1 2 3 4 22 1 2 3 3 4 5 7 10 13 23 21th entry 1 2 3 4 22 1 2 3 3 4 5 7 10 13 15 23 22th entry 1 2 3 4 22 1 2 3 3 4 5 7 10 13 15 22 23 23th entry 1 2 3 4 22 1 2 3 3 4 5 7 10 13 15 22 29 23 hence have 1 2 3 4 22 1 2 3 3 4 5 23 7 10 13 15 22 29 23
1.3. matrices 16 example 1.3.13. 1 2 3 4 22 5 6 21 17 39 21 example 1.3.14. 1 2 3 13 2 0.8 5 0.7 12 2 32 13.5 3.8 12 example 1.3.15. c 0 0 0 c 0 0 0 c 33 1 2 3 4 5 6 32 c 2c 3c 4c 5c 6c 32 c 1 2 3 4 5 6 32 remark 1.3.3. scalar multiplication c multiplication scalar matrix cinn inn denotes n n identity matrix. innann ann anninn innank ank amninn amn inn denotes n n identity matrix. 1.3.6 properties matrix addition multiplication subsection mention properties matrix addition multiplication with proof. want verify own. . a b c b c associativity addition . abc abc associativity multiplication . b b commutativity addition . general ab ba assuming make sense . λa b λa λb real number λ. . λab λab aλb real number λ. . ab c ab ac . a bc ac bc
1.3. matrices 17 1.3.7 exercise question 6. suppose 2 4 5 1 11 2 9 6 3 4 7 7 . following true matrix a hint i jth entry entry ith row jth column. . option 1 4 3 matrix. . option 2 3 4 matrix. . option 3 23th entry matrix 4. . option 4 23th entry matrix 9. question 7. following statements isare true hint recall definitions scalar matrix diagonal matrix identity ma trix. . option 1 diagonal matrix scalar matrix. . option 2 scalar matrices may square matrices. . option 3 scalar matrices must square matrices. . option 4 scalar matrix identity matrix. question 8. suppose p 3 1 7 4 0 1 2 5 2 q 1 4 9 r 0 3 10 2 4 5 hint matrix order n b matrix order n p order ab p. . option 1 matrix pd order 3 1. . option 2 matrix pd order 1 3. . option 3 matrix qd order 3 3. . option 4 matrix qd order 1 1. . option 5 matrix dq order 3 3.
1.3. matrices 18 . option 6 matrix dq order 1 1. . option 7 qd defined. . option 8 qr defined. . option 9 p q defined. . option 10 p defined. feedback . p 3 3 matrix 3 1 matrix. think order pd. . q 1 3 matrix 3 1 matrix. think order qd dq. . q 1 3 matrix r 1 3 matrix. think whether possible define qr not. . figure pair matrices number rows well number columns. question 9. suppose 1 0 0 1 b 0 1 0 0 following options true . option 1 a2 . option 2 a2 . option 3 b2 . option 4 b2 0 question 10. suppose 3 3 scalar matrix 11th entry matrix 4. suppose b 3 3 square matrix i jth entry equal i2 j2. find 22th entry matrix 2a b. ans 16 question 11. suppose 1 2 4 7 a2 αa 0 α r. find value α. ans 6
1.4. system linear equations 19 1.4 system linear equations mentioned earlier one applications matrices solve system linear equations. section study detail use matrices so. let us start example day day life. example 1.4.1. suppose purchases a b c given following table. items buyer buyer b buyer c rice in kg 8 12 3 dal in kg 8 5 2 oil in liter 4 7 5 suppose paid rs.1960 b paid rs.2215 c paid rs.1135. want find price items using data. suppose price rice rs.x per kg. price dal rs.y per kg. price oil rs.z per liter. hence following system linear equations 8x 8y 4z 1960 12x 5y 7z 2215 3x 2y 5z 1135 represented ax b 8 8 4 12 5 7 3 2 5 x x z b 1960 2215 1135 simple checking shows that x 45 125 z 150 satisfies equations. mentioning solution obtained next chapter. now want verify values satisfy three equations simultaneously. main question asked is linear equation is. definition 1.4.1. linear equation equation may put form a1x1 a2x2 . . . anxn b 0 x1 x2 . . . xn variables unknown b a1 a2 . . . coefficients often real numbers. example 1.4.2. 2x 3y 5z 9 0 x y z variables 2 3 5 9 coefficients.
1.4. system linear equations 20 example 1.4.3. x 3z 7 0 x z variables 1 3 7 coefficients. system linear equations collection one linear equations involving set variable. illustrated following example. example 1.4.4. 3x 2y z 6 x 1 2y 2 3z 7 6 4x 6y 10z 0 system three equations three variables x y z. solution linear system assignment values variables equations simultaneously satisfied. solution system given x 1 1 z 1 definition 1.4.2. general system linear equations n unknowns written a11x1 a12x2 . . . a1nxn b1 a21x1 a22x2 . . . a2nxn b2 . . . . . . am1x1 am2x2 . . . amnxn bm system linear equations equivalent matrix equation form ax b n matrix x column vector n entries b column vector entries. a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . am1 am2 . . . amn x x1 x2 . . . xn b b1 b2 . . . bm example 1.4.5. consider set linear equations mentioned example 1.4.4 follows 3x 2y z 6 x 1 2y 2 3z 7 6 4x 6y 10z 0
1.4. system linear equations 21 matrix representation system linear equations ax b 3 2 1 1 1 2 2 3 4 6 10 x x z b 6 7 6 0 example 1.4.6. consider set linear equations mentioned example 1.4.1 follows 8x 8y 4z 1960 12x 5y 7z 2215 3x 2y 5z 1135 represented ax b 8 8 4 12 5 7 3 2 5 x x z b 1960 2215 1135 1.4.1 solutions linear system equations linear system may behave one three possible ways 1 system infinitely many solutions. 2 system single unique solution. 3 system solution. section illustrate three cases examples geometrical representations. example 1.4.7. example infinitely many solutions suppose purchases b given following table items buyer buyer b rice kg 2 4 dal kg 1 2 suppose paid rs.215 b paid rs.430. want find price items using data. suppose price rice rs.x per kg. price dal rs.y per kg. hence following system linear equations 2x 215 4x 2y 430
1.4. system linear equations 22 infinitely many x satisfying equations. 6 4 2 0 2 4 6 205 210 215 220 225 figure 1.8 equations represent straight line two dimensional plane shown figure 1.8. example 1.4.8. example system equations solution suppose previous case b bought amount items mentioned table 1.4.7. reason seller gave discount b. let us assume paid rs.215 b paid rs.400. returning home decided find price items solving linear system equations earlier cases. suppose price rice rs.x per kg. price dal rs.y per kg. hence following system linear equations 2x 215 4x 2y 400 case solution system equations.
1.4. system linear equations 23 6 4 2 0 2 4 6 190 200 210 220 figure 1.9 equations represent two parallel straight lines two dimensional plane shown figure 1.9. example 1.4.9. example unique solution suppose purchases b given following table items buyer buyer b rice kg 2 3 dal kg 1 1 suppose paid rs.215 b paid rs.260. want find price items using data. suppose price rice rs.x per kg. price dal rs.y per kg. hence following system linear equations 2x 215 3x 260 infinitely many x satisfying equations.
1.4. system linear equations 24 0 20 40 60 80 100 0 100 200 figure 1.10 equations represent two straight lines intersecting one point shown figure 1.10. 1.4.2 exercise consider system linear equations system 1 2x1 3x2 x3 1 x1 x3 0 2x2 5 answer questions 12 13 based data. question 12. matrix representation system 1 ax b x x1 x2 x3 . option 1 2 3 1 1 0 1 0 2 0 . option 2 b 5 0 1 . option 3 2 1 0 3 0 2 1 1 0 b 1 0 5
1.4. system linear equations 25 . option 4 2 3 1 1 0 1 0 2 0 b 1 0 5 question 13. system 1 hint solve x1 x2 x3. . option 1 unique solution. . option 2 solution. . option 3 infinitely many solutions. . option 4 none above. question 14. plane 1 plane 2 figure m2w1aq3 correspond two different linear equations form system linear equations. plane 2 plane 1 figure 1.11 figure m2w1aq3 system linear equations hint system linear equations solution point corresponding solution must lie plane corresponding linear equation given system. . option 1 unique solution. . option 2 solution. . option 3 infinitely many solutions. . option 4 none above. question 15. consider geometric representations figures a b c three systems linear equations.
1.4. system linear equations 26 figure 1.12 figure a figure 1.13 figure b figure 1.14 figure c choose set correct options. . option 1 figure a represents system linear equations solution. . option 2 figure a represents system linear equations in finitely many solutions. . option 3 figure b represents system linear equations unique solution. . option 4 figure b represents system linear equations in finitely many solutions. . option 5 figure c represents system linear equations in finitely many solutions. . option 6 figure c represents system linear equations solution.
1.4. system linear equations 27 question 16. consider system equations 2x1 3x2 6 2x1 kx2 4x1 6x2 12 choose set correct options. . option 1 ax b represents system x x1 x2 2 3 2 k 4 6 b 6 12 . option 2 system solution k 3 0. . option 3 system unique solution k 3 0. . option 4 system infinitely many solutions k 3 6. . option 5 system infinitely many solutions k 3 6. question 17. let x1 x2 solutions system linear equations ax b. following options correct . option 1 x1 x2 solution system linear equations ax b. . option 2 x1 x2 solution system linear equations ax 2b. . option 3 x1 x2 solution system linear equations ax b. . option 4 x1 x2 solution system linear equations ax 0. question 18. let v solution systems linear equations a1x b a2x b. following options correct . option 1 v solution system linear equations a1 a2x b. . option 2 v solution system linear equations a1 a2x 2b. . option 3 v solution system linear equations a1 a2x 0. . option 4 v solution system linear equations a1 a2x b. question 19. consider system equations x1 3x2 4 3x1 kx2 12 k r. given system unique solution k equal ans 9
1.5. determinant 28 1.5 determinant every square matrix associated number called determinant denoted deta a. used . solving system linear equations . finding inverse matrix . calculus more. 1.5.1 first order determinant a 1 1 matrix deta 1.5.2 second order determinant consider 2 2 matrix a b c . calculate determinant matrix follows a b c deta ad bc example 1.5.1. 2 3 6 10 deta 20 18 2 example 1.5.2. 5 23 6 37 deta 157 4 137 1.5.3 third order determinant consider 3 3 matrix follows a11 a12 a13 a21 a22 a23 a31 a32 a33 calculate determinant matrix a. first consider element a11. consider 2 2 submatrix ignoring first row first column shown below. a11 a12 a13 a21 a22 a23 a31 a32 a33
1.5. determinant 29 calculate 111a11 det a22 a23 a32 a33 consider element a12. consider 2 2 submatrix ignoring first row second column shown below. a11 a12 a13 a21 a22 a23 a31 a32 a33 calculate 112a12 det a21 a23 a31 a33 consider element a13. consider 2 2 submatrix ignoring first row third column shown below. a11 a12 a13 a21 a22 a23 a31 a32 a33 calculate 113a13 det a21 a22 a31 a32 calculating three terms position calculate deter minant matrix a. deta 111a11 det a22 a23 a32 a33 112a12 det a21 a23 a31 a33 113a13 det a21 a22 a31 a32 a11a22a33 a23a32 a12a21a33 a23a31 a13a21a32 a22a31 example 1.5.3. consider following matrix 2 4 1 3 8 7 5 6 9 deta 2 det 8 7 6 9 4 det 3 7 5 9 1 det 3 8 5 6 272 42 427 35 118 40 230 48 122 60 32 22 70
1.5. determinant 30 example 1.5.4. consider following matrix observe upper triangular matrix 2 4 3 0 8 7 0 0 9 deta 2 det 8 7 0 9 4 det 0 7 0 9 3 det 0 8 0 0 272 0 40 0 30 0 272 40 30 144 observer that determinant case product diagonal elements. mention important properties determinant leave learners check verify. proposition 1.5.1. following important properties determinant matrices. . determinant identity matrix of order n 1. verify 2 2 3 3 matrices first. . detat deta denotes transpose matrix a. . detab detadetb b n n matrices verify 2 2 3 3 matrices first. proposition 1.5.2. determinant inverse matrix let denote inverse matrix a1 aa1 i. calculat ing determinant sides have detaa1 deti i.e. deta.deta1 1. hence get deta1 1 deta. 1.5.4 invariance elementary row column operations important properties determinant include following include invari ance elementary row column operations. 1 switching two rows columns changes sign. 2 multiples rows columns added together without changing determinants value.
1.5. determinant 31 3 scalar multiplication row constant multiplies determinant t. 4 determinant row column zeros value 0. calculate determinant expanding respect row column. switching two rows columns changes sign n n matrix switching two rows columns changes sign. verify n 2 suggest learners verify 3 3 matrices. 22 matrix show this. a b c switching first second row get c b deta ad bc det a cb da ad bc deta multiples rows columns added together without changing determinants value verify 2 2 matrix suggest learners check 3 3 matrix. let a b c 2 2 matrix. let us construct multiplying second row add first row a. hence a tc b td c det a a tcd b tdc ad tcd bc tdc ad bc deta scalar multiplication row constant multiplies determinant t simple checking shows 2 2 matrix. a tb c td a b c t a b c deta atd tbc tad bc t.det a 1.5.5 determinant terms minors definition 1.5.1. square matrix minor entry ith row jth column also called i j minor first minor determinant submatrix formed deleting ith row jth column. number often denoted mij.
1.5. determinant 32 consider 3 3 matrix follows a11 a12 a13 a21 a22 a23 a31 a32 a33 . minor entry 1st row 1st column i.e. 1 1 minor de terminant 2 2 submatrix obtained ignoring first row first column shown below. a11 a12 a13 a21 a22 a23 a31 a32 a33 1 1 minor given matrix a22a33 a23a32. . minor entry 2nd row 3rd column i.e. 2 3 minor deter minant 2 2 submatrix obtained ignoring second row third column shown below. a11 a12 a13 a21 a22 a23 a31 a32 a33 2 3 minor given matrix a11a32 a12a31. definition 1.5.2. i j cofactor obtained multiplying minor 1ij. . 1 1 cofactor given matrix 111a22a33a23a32 a22a33 a23a32. . 2 3 cofactor given matrix 123a11a32a12a31 a11a32 a12a31. observe 3 3 matrix a11 a12 a13 a21 a22 a23 a31 a32 a33 . calculate determinant expanding respect first row follows deta 111a11 det a22 a23 a32 a33 112a12 det a21 a23 a31 a33 113a13 det a21 a22 a31 a32 111a11m11 112a12m12 113a13m13
1.5. determinant 33 . also calculate determinant expanding respect second column row follows deta 112a12 det a21 a23 a31 a33 122a22 det a11 a13 a31 a33 132a32 det a11 a13 a21 a23 112a12m12 122a22m22 132a32m32 generalised n n matrices. let n n matrix deta σn j11ijaijmij fixed σn i11ijaijmij fixed j 1.5.6 exercise question 20. let 3 3 matrix nonzero determinant. det2a k deta value k ans 8 hint scalar c multiplied one row matrix a determinant new matrix c times determinant a. question 21. 2 1 1 1 3 4 1 0 2 and b 1 3 2 0 1 1 3 4 2 choose set correct options. . option 1 deta 9 detb 3. . option 2 deta 9 detb 3. . option 3 deta 9 detb 3. . option 4 detab 27 detba 27. . option 5 detab detba 27. question 22. let 2 2 matrix given a11 a12 a21 a22 . define following matrices b a11 a21 a12 a22 a21 a22 c a11 a12 a12 a21 a22 a22 a11 a21 a12 a22 a21 a22 e a11 a21 a12 a22 a21 a22
1.5. determinant 34 matrices among b c d e determinant matrix a real numbers a11 a12 a21 a22 . option 1 b . option 2 b e . option 3 b c . option 4 e . option 5 c e question 23. let a11 a12 a13 ta11 sa31 ta12 sa32 ta13 sa33 ra31 ra32 ra33 be matrix r s 0. find deta ans 0 question 24. suppose 1 4 2 0 1 3 1 0 2 and b 2 0 1 4 α 0 0 3 5 . detab 32 find value α. ans 1 question 25. let square matrix order 3 b matrix obtained adding 2 times first row third row adding 3 times second row first row a. value det6a2b1 . option 1 6 deta . option 2 6 detadetb . option 3 63 deta . option 4 63 detadetb question 26. choose set correct options . option 1 real 3 3 matrix deta detat . . option 2 aij real 4 4 matrix order sub matrix obtained deleting ith row jth column 4 4. . option 3 aij real 4 4 matrix order sub matrix obtained deleting ith row jth column 3 3. . option 4 real 3 3 matrix orders possible square sub matrices 1 1 2 2 3 3.
1.5. determinant 35 question 27. let 3 2 2 2 3 2 2 1 1 be 3 3 matrix. following isare correct hint determinant calculated expanding respect different rows columns. . option a deta 3 det 3 2 1 1 2 det 2 2 2 1 2 det 2 3 2 1 . option b deta 3 det 3 2 1 1 2 det 2 2 1 2 2 det 2 3 2 1 . option c deta 2 det 2 2 1 1 3 det 3 2 2 1 2 det 3 2 2 1 . option d deta 2 det 2 2 1 1 3 det 3 2 2 1 2 det 3 2 2 1 question 28. suppose 4 1 3 2 0 1 3 2 0 and cij i jth cofactor matrix a. let b 3 3 matrix i jth entry equal cij. find detb. ans 49 question 29. suppose b two 3 3 matrices deta 4 b 3a. find value 3 p deta2b. ans 12
36
37 2. solving system linear equations what know drop dont know ocean. isaac newton already solved equations one variable like x 5 3 etc. also know equation said linear equation fx c fx degree one quadratic equation fx c fx degree two cubic equation fx c fx degree three etc. previous section gone vectors matrices section going solve equations contains one variables one one equations equations one degree called linear equa tions i.e. let system linear equations two variables two equations fx y c gx y c c c r fx y gx y one degree polynomials. help matrix solve system linear equations know system linear equations unique solution solution infinitely many solutions visualize geometrically. lets understand precisely example. items buyer buyer b buyer c rice in kg 8 12 3 dal in kg 8 5 2 oil in liter 4 7 5 example 2.0.1. suppose three buyers a b c bought three items according given table form shop buyer paid 1960 buyer b paid 2215 buyer c paid 1135 shopkeeper. suppose buyer wants know prices rice dal per kg. oil per liter. assumes price
2.1. linear equation 38 rice x per kg. price dal y per kg. price oil z per liter. question 30. much amount paid buyer shopkeeper bought 8 kg. rice 8 kg. dal 4 litre oil total amount 8x 8y 4z equal 1960 i.e. 8x 8y 4z 1960. equation three variables x z left side equation polynomial three variables degree one. question 31. much amount paid buyer b shopkeeper bought 12 kg. rice 5 kg. dal 7 litre oil total amount 12x 5y 7z equal 2215 i.e. 12x 5y 7z 2215. also equation three variables x z left side equation polynomial three variables degree one. question 32. much amount paid buyer c shopkeeper bought 3 kg. rice 2 kg. dal 5 litre oil total amount 3x 2y 5z equal 1135 i.e. 12x 5y 7z 1135. also equation three variables x z left side equation polynomial three variable degree one. hence following system linear equations 8x 8y 4z 1960 12x 5y 7z 2215 12x 5y 7z 1135 observe three equations degree one involve three variables x z. 2.1 linear equation linear equation equation form a1x1 a2x2 . . . anxn b x1 x2 . . . xn variables or unknowns a1 a2 . . . co efficients real numbers b also real number. example 2.1.1. 8x 8y 4z 1960 linear equation x z variables 8 8 4 coefficients.
2.2. system linear equations 39 question 33. followings isare linear equation . option 1 2x2 1 . option 2 x 2y 1 . option 3 x z 1 . option 4 3x2 3y2 z2 1 2.2 system linear equations system linear equations collections one linear equations involving set variables example 8x 8y 4z 1960 12x 5y 7z 2215 12x 5y 7z 1135 system linear equations three variable x z. solution system linear equations assignment values variables equations simultaneously satisfied. system linear equations x 45 125 z 150 satisfies three equations simultaneously. general system linear equations n variables written a11x1 a12x2 . . . a1nxn b1 a21x1 a22x2 . . . a2nxn b2 . . . . . . am1x1 am2x2 . . . amnxn bm
2.2. system linear equations 40 2.2.1 matrix representation system linear equations general system linear equations n variables a11x1 a12x2 . . . a1nxn b1 a21x1 a22x2 . . . a2nxn b2 . . . . . . am1x1 am2x2 . . . amnxn bm written matrix equation ax b matrix order n called coefficient matrix x column vector n entries b column vector entries follows a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . am1 am2 . . . amn x x1 x2 . . . xn and b b1 b2 . . . bm example 2.2.1. consider system linear equations 2x1 3x2 x3 1 x1 x3 0 2x2 5 matrix representation system ax b 2 3 1 1 0 1 0 2 0 x x1 x2 x3 b 1 0 5 question 34. consider system equations 2x1 3x2 6 2x1 2x2 3 4x1 6x2 12 1 find matrix representation ax b x x1 x2 system linear equations. 2 order matrix a
2.3. solution system linear equations 41 2.3 solution system linear equations system linear equations may 1. unique solution. 2. infinitely many solutions. 3. solution. let understand 3 examples example 2.3.1. consider system linear equations x 1 x y 1 observe assigned values x 1 0 values x satisfying equations simultaneously. case say system linear unique solution. also two variables relate coordinate system x axis axis visualize geometrically follow 3 2 1 1 2 3 4 2 2 4 x 1 x y 1 1 0 figure 2.1 figure see two lines intersecting one point 10. general system linear equations two variables always related coordinate system xy plane equations represent lines plane. suppose system linear equations two variables three equations three equations system unique solution equations represents lines plane lines pass one point represent solution point. example 2.3.2. consider system linear equations x 1 2x 2y 2
2.3. solution system linear equations 42 observe assigned values x 1 0 values x satisfying equations simultaneously also x 2 1 another values x satisfying equations simultaneously similarly infinitely values x satisfying equations simultaneously. case say system linear infinitely solutions. visualize geometrically. observe equations represents line x 1. 3 2 1 1 2 3 2 1 1 2 3 4 x 1 2x 2y 2 figure 2.2 figure see equations represent one line x 1 point line solution system linear equation line infinitely many points system linear infinitely many solutions. general system linear equations two variables always related coordinate system xy plane equations represent lines plane. suppose system linear equations two variables three equations three equations system infinitely many solutions equations represent one line shows as argued above system lines infinitely many solutions. example 2.3.3. consider system linear equations x 1 x 0 since left side equations implies 1 0 absurd. means possible values x satisfying equations simultaneously. case say system linear solution. lets see geometrically.
2.3. solution system linear equations 43 3 2 1 1 2 3 2 2 4 x 1 x 0 figure 2.3 figure see two lines intersecting i.e. point lines lines means possible value x satisfies equations simultaneously solution system linear equations. example 2.3.4. find solution system linear equations 2x 3y 2 3x 5y 1 multiply sides equation 2x 3y 2 3 get equation 1 6x 9y 6 . . . 1 multiply sides equation 3x 5y 1 2 get equation 2 6x 10y 2 . . . 2 subtract equ 1 equ 2 get 19y 4 y 4 19 substitute 4 19 equation 1 2 get x 78 114 so system linear equations unique solution solution x 78 114 4 19 matrix representation system linear equations 2 3 3 5 x 2 1 solution column vector 78 119 4 19
2.3. solution system linear equations 44 lets see example system linear equations three variables one one equations example 2.3.5. consider system linear equations x z 1 x z 5 say nature solution system linear equations system linear equation three variables related coordinate system xaxis axis zaxis equation coordinate system represents plane see following figure. equation x z 1 represent blue colored plane equation x z 5 represents red colored plane coordinate system. figure planes intersecting means system linear equations solution. algebraically see left side equations equal system linear equations equate right side equations get 15 absurd also conclude system linear equations solution. figure 2.4
2.3. solution system linear equations 45 example 2.3.6. consider system linear equations x z 1 x y z 5 say nature solution system linear equations see figure equation x yz 1 represent blue colored plane equation x y z 5 represents red colored plane coordinate system. figure see planes intersecting intersection represented line yellow color i.e. infinitely many points yellow colored line also lie planes algebraically say infinitely many values x z points yellow colored line satisfy equations simultaneously system linear equation infinitely many solutions. figure 2.5 see also
2.3. solution system linear equations 46 write first equation x z 1 y z 1 x substitute z 1 x second equation get x 1 x 5 x 3 get x 3 z 1 hence 1 get z 0 0 get z 1 many more. x 3 1 z 0 x 3 0 z 1 solution system linear equations similarly get infinitely many solution system linear equations. we see precisely go ahead
2.3. solution system linear equations 47 example 2.3.7. consider system linear equations x z 1 2 x 1 say nature solution system linear equations see figure equation x z 1 represent blue colored plane equation x 1 represents red colored plane 2 represents green colored plan coordinate system. figure see three planes intersecting point represented black colored dot point i.e. one point 122 lies three planes algebraically say one value x z i.e. x 1 2 z 2 satisfy three equations simultaneously system linear equation unique solution. figure 2.6 see also system linear equation second equation 2 third
2.3. solution system linear equations 48 equation x 1 substitute first equation get z 2. x 1 2 z 2 unique solution system linear equations. question 35. consider system equations 2x1 3x2 6 2x1 kx2 4x1 6x2 12 choose set correct options. hint observe third equation multiple first one dividing 2 sides third equation gives first equation. enough check solutions first second equation. . option 1 ax b represents system linear equations x x1 x2 2 3 2 k 4 6 b 6 12 . option 2 system solution k 3 0. . option 3 system unique solution k 3 0. . option 4 system infinitely many solutions k 3 6. . option 5 system infinitely many solutions k 3 6. feedback . option 2 k 3 0 comparing first equation get 6 0 absurd. x1 x2 satisfying first second equation together. . option 4 k 3 6 comparing first equation get 6 6 absurd. x1 x2 satisfying first second equation together. . option 5 k 3 6 second equation multiple first equation. third equation also multiple first equation. set solutions infinitely many values x1 x2 satisfy first equation.
2.3. solution system linear equations 49 2.3.1 exercise question 36. choose set correct options. hint think 2 2 3 3 matrix b accordingly. . option 1 every system linear equations either unique solution solution infinitely many solutions. . option 2 equation system linear equations multiplied nonzero constant c solution new system equations c times solution old system equations. . option 3 ax b system linear equations solution system linear equations cax b c 0 also solution. . option 4 ax b system linear equations solution 1 cax b c 0 also solution. question 37. plane 1 plane 2 figure below correspond two different linear equations form system linear equations. plane 2 plane 1 figure 2.7 system linear equations hint system linear equations solution point corresponding solution must lie plane corresponding linear equation given system. . option 1 unique solution. . option 2 solution. . option 3 infinitely many solutions. . option 4 none above.
2.3. solution system linear equations 50 question 38. let x1 x2 solutions system linear equations ax b. following options correct . option 1 x1 x2 solution system linear equations ax b. . option 2 x1 x2 solution system linear equations ax 2b. . option 3 x1 x2 solution system linear equations ax b. . option 4 x1 x2 solution system linear equations ax 0. question 39. let v solution systems linear equations a1x b a2x b. following options correct . option 1 v solution system linear equations a1 a2x b. . option 2 v solution system linear equations a1 a2x 2b. . option 3 v solution system linear equations a1 a2x 0. . option 4 v solution system linear equations a1 a2x b. question 40. consider system equations x1 3x2 4 3x1 kx2 12 k r. given system unique solution k equal ans 9 question 41. consider two system linear equations system 1 x1 2x2 5 0x1 x2 5 system 2 2x3 x4 5 3x3 x4 5 suppose another system linear equations given 1 2x1 x3 2 1x2 x4 0 3x1 x3 1 1x2 x4 n real values n. find value n m. ans 46
2.4. cramers rule 51 2.4 cramers rule previous section gone matrices types matrices deter minant matrix. also gone system linear equations know types solution system linear equations i.e. solution unique solution infinitely many solutions. section try solve system linear equations unique solution using cramers rule. cramers rule algorithm used find solve system linear equations. every system linear need solved using cramers rule. cramers rule applicable specific types system linear equations follows 1. coefficient matrix system linear equations square matrix. 2. determinant coefficient matrix system linear equations nonzero i.e coefficient matrix invertible. 2.4.1 cramers rule invertible coefficient matrix order 2 consider system linear equations a11x1 a12x2 b1 a21x1 a22x2 b2 matrix representation system ax b a11 a12 a21 a22 x x1 x2 b b1 b2 . steps find solution system linear equations step1 find determinant matrix a deta a11a22 a12a21 step2 define ax1 ax1 matrix obtained replacing first column column matrix b i.e. ax1 b1 a12 b2 a22 find determinant matrix ax1 detax1 b1a22 b2a12
2.4. cramers rule 52 step3 define ax2 ax2 matrix obtained replacing second column column matrix b i.e. ax2 a11 b1 a21 b2 find determinant matrix ax2 detax2 b2a11 b1a21 step4 x1 detax1 deta b1a22 b2a12 a11a22 a12a21 x2 detax2 deta b2a11 b1a21 a11a22 a12a21 example 2.4.1. consider system linear equations 2x1 x2 1 3x1 4x2 1 find solution system linear equations. matrix representation system linear equations ax b 2 1 3 4 x x1 x2 b 1 1 deta 2.4 1.3 8 3 5 lets find ax1 ax1 1 1 1 4 detax1 4.1 1.1 4 1 5 now lets find ax2 ax2 2 1 3 1 detax2 2.1 3.1 2 3 5 finally x1 detax1 deta 5 5 1 x2 detax2 deta 5 5 1
2.4. cramers rule 53 2.4.2 cramers rule invertible coefficient matrix order 3 consider system linear equations follows a11x1 a12x2 a13x3 b1 a21x1 a22x2 a23x3 b2 a31x1 a32x2 a33x3 b3 let matrix representation system ax b a11 a12 a13 a21 a22 a23 a31 a32 a33 x x1 x2 x3 b b1 b2 b3 . steps find solution system linear equations step1 find determinant matrix a. step2 define ax1 ax1 matrix obtained replacing first column column matrix b i.e. ax1 b1 a12 a13 b2 a22 a23 b3 a32 a33 find determinant matrix ax1 step3 define ax2 ax2 matrix obtained replacing second column column matrix b i.e. ax2 a11 b1 a13 a21 b2 a23 a31 b3 a33 find determinant matrix ax2 step4 define ax3 ax3 matrix obtained replacing third column column matrix b i.e. ax3 a11 a12 b1 a21 a22 b2 a31 a32 b3 find determinant matrix ax3
2.4. cramers rule 54 step5 x1 detax1 deta x2 detax2 deta x3 detax3 deta example 2.4.2. consider system linear equations x1 x2 x3 1 2x1 x3 2x3 1 x1 2x2 4x3 1 find solution system linear equations. matrix representation system linear equation ax b 1 1 1 2 1 2 1 2 4 x x1 x2 x3 and b 1 1 1 determinant matrix a deta 3 lets find ax1 ax1 1 1 1 1 1 2 1 2 4 detax1 1 now lets find ax2 ax2 1 1 1 2 1 2 1 1 4 detax2 7 ax3 1 1 1 2 1 1 1 2 1
2.4. cramers rule 55 detax3 3 finally x1 detax1 deta 1 3 x2 detax2 deta 7 3 x3 detax3 deta 3 3 1 2.4.3 cramers rule invertible coefficient matrix order n consider system linear equations a11x1 a12x2 . . . a1nxn b1 a21x1 a22x2 . . . a2nxn b2 . . . . . . an1x1 an2x2 . . . annxn bn matrix representation system linear equation ax b a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . an1 an2 . . . ann x x1 x2 . . . xn and b b1 b2 . . . bn steps find solution system linear equations step1 find determinant matrix a. step2 define axi axi matrix obtained replacing ith column column matrix b find determinant matrix axi 1 2 . . . n step3 xi detaxi deta 1 2 . . . n
2.4. cramers rule 56 2.4.4 exercise question 42. consider system linear equations x1 x3 1 x1 x2 x3 1 x2 x3 1 let matrix representation system ax b 1 0 1 1 1 1 0 1 1 x x1 x2 x3 b 1 1 1 . let axi matrix obtained replacing ith column i.e. a1i a2i a3i b 1 2 3. use information answer questions 1 2. question 1 choose set correct options . option 1 ax1 1 1 0 1 1 1 0 1 1 . option 2 ax1 1 0 1 1 1 1 1 1 1 . option 3 ax2 1 0 1 1 1 1 0 1 1 . option 4 ax3 1 0 1 1 1 1 0 1 1 question 2 choose set correct options. . option 1 x1 2. . option 2 x2 2. . option 3 x3 3.
2.4. cramers rule 57 . option 4 none above. question 43. consider system linear equations ax b 1 0 1 2 1 0 x x1 0 x3 b 1 0 0 solution x partially known. value a2 0 3 3 given hint observe second row vector x given 0 implies x2 known x x1 x2 x3 answer 2 question 44. consider system linear equations ax b 1 2a 3 3a 2 1 x x1 0 b 1 1 solution x partially known. value 3a deta 0 answer 1
2.5. finding solution system linear equations invertible coefficient matrix 58 2.5 finding solution system linear equations invertible coefficient matrix previous section studied finding unique solution system linear equations using cramers rule. section learn another method find unique solution system linear equations using inverse matrix coefficient matrix. inverse coefficient matrix method e applicable specific types system linear equations follows 1. coefficient matrix system linear equations square matrix. 2. determinant coefficient matrix system linear equations nonzero i.e coefficient matrix invertible. consider system linear linear equations ax b invert ible matrix. solution system linear equations obtain follows ax b pre multiplication a1 sides get a1ax a1b x a1b steps find solution system linear equations using inverse coefficient matrix consider system linear equations a11x1 a12x2 . . . a1nxn b1 a21x1 a22x2 . . . a2nxn b2 . . . . . . an1x1 an2x2 . . . annxn bn matrix representation system linear equation ax b
2.5. finding solution system linear equations invertible coefficient matrix 59 a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . an1 an2 . . . ann x x1 x2 . . . xn and b b1 b2 . . . bn step1 find inverse matrix a. step2 find matrix multiplication a1 b i.e. a1b column matrix. step3 compare column matrix x find value x1 x2 . . . xn example 2.5.1. consider system linear equations x1 2x2 1 3x1 5x2 2 find solution system linear equations. matrix representation system linear equations ax b 1 2 3 5 x x1 x2 b 1 2 lets find inverse coefficient matrix adja 5 2 3 1 deta 1 a1 adja deta 5 2 3 1 x x1 x2 a1b 5 2 3 1 1 2 1 1 hence x1 1 x2 1
2.5. finding solution system linear equations invertible coefficient matrix 60 example 2.5.2. consider system linear linear equations 8x1 8x2 4x3 1960 12x1 5x2 7x3 2215 3x1 2x2 5x3 1135 matrix representation system linear equations ax b 8 8 4 12 5 7 3 2 5 x x1 x2 x3 and b 1960 2215 1135 see deta 188 0 matrix invertible. inverse a1 1 188 11 32 36 39 28 8 9 8 56 hence x x1 x2 x3 1 188 11 32 36 39 28 8 9 8 56 1960 2215 1135 45 125 150 x1 45 x2 125 x3 150
2.5. finding solution system linear equations invertible coefficient matrix 61 2.5.1 exercise question 45. consider system linear equations 2x1 x2 3 x1 x3 3 x2 x3 2 let matrix representation system ax b 2 1 0 1 0 1 0 1 1 x x1 x2 x3 b 3 3 2 . use information answer questions 1 2. question 1 choose set correct options. . option 1 a1 2 1 0 1 1 0 1 0 1 . option 2 a1 1 1 1 1 2 2 1 2 1 . option 3 adjoint matrix 1 1 1 1 2 2 1 2 1 . option 4 deta 1. question 2 choose set correct options. . option 1 x1 2. . option 2 x2 1. . option 3 x3 1 . option 4 none above.
2.6. gauss elimination method 62 2.6 gauss elimination method previous sections learned method solve system linear equations system linear equations unique solution i.e. coefficient matrix system linear equations square matrix determinant non zero. case system linear equations following things 1. coefficient matrix system linear equations square matrix determinant zero. 2. system linear equations consists equations n variables i.e. system linear equations whose coefficient matrix rectangular matrix. know algebraic method get given system linear equa tions unique solution solutions infinitely many solutions. solve kind system linear equations method called gauss elimination method learn method lets see forms matrices 1. echelon form row echelon form 2. reduced row echelon form. 2.6.1 homogeneous nonhomogeneous system linear equations homogeneous system linear equations system linear equations type a11x1 a12x2 . . . a1nxn 0 a21x1 a22x2 . . . a2nxn 0 . . . . . . am1x1 am2x2 . . . amnxn 0 matrix representation system linear equation ax 0
2.6. gauss elimination method 63 a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . am1 am2 . . . amn x x1 x2 . . . xn b 0 0 . . . 0 called homogeneous system linear equations. obvious 0 always solution system linear equations called trivial solution system. homogeneous system linear equations always two possibilities 1. 0 unique solution. reason 1 coefficient matrix homogeneous system linear equations invertible ax 0 a1a a10 premultiplication a1 sides equation x 0. 2 homogeneous system linear equations number equations greater number variables system linear equations unique solution 0. get using gauss elimination method. 2. infinitely many solutions 0. reason homogeneous system linear equations number variables greater number equations system linear equations infinitely many solutions 0. get using gauss elimination method. nonhomogeneous system linear equations system linear equations type a11x1 a12x2 . . . a1nxn b1 a21x1 a22x2 . . . a2nxn b2 . . . . . . am1x1 am2x2 . . . amnxn bm matrix representation system linear equation ax b
2.6. gauss elimination method 64 a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . am1 am2 . . . amn x x1 x2 . . . xn b b1 b2 . . . bm bi 0 system type called nonhomogeneous system linear equations. non homogeneous system linear equations always three possibilities 1. unique solution. 2. infinitely many solutions. 3. solution. solutions decided using gauss elimination method. 2.6.2 row echelon form reduced row echelon form matrix row echelon form . first nonzero element the leading entry row 1. . column containing leading 1 row right column containing leading 1 row it. different words subsequent nonzero rows also leading entries i.e. first nonzero entries 1 appear right leading entry previous row. . nonzero rows always rows zeros. matrix reduced row echelon form . first nonzero element first row the leading entry number 1. . column containing leading 1 row right column containing leading 1 row it. different words subsequent nonzero rows also leading entries i.e. first nonzero entries 1 appear right leading entry previous row.
2.6. gauss elimination method 65 . leading entry row must nonzero number column. . nonzero rows always rows zeros. note matrix reduced row echelon form also row echelon form. example 2.6.1. choose correct options matrix 1 2 0 0 0 1 0 0 0 . option 1 row echelon form reduced row echelon form. . option 2 row echelon form reduced row echelon form. . option 3 neither row echelon form reduced row echelon form. lets see observations matrix 1. first nonzero entry element first row 1 entry 11th first row top row first column left column also called leading 1 first row. 1 2 0 0 0 1 0 0 0 2. first nonzero entry second row 1 entry 2 3th also called leading 1 second row. 1 2 0 0 0 1 0 0 0 3. third row zero row nonzero entry leading entry. third row non zero row. 4. column containing leading 1 entry 1 1th first row first column non zero entry first column. see matrix corresponding column 1 2 0 0 0 1 0 0 0 5. column containing leading 1 entry 2 3th second row third column non zero entry third column. see matrix corresponding column 1 2 0 0 0 1 0 0 0
2.6. gauss elimination method 66 6. column containing leading 1 entry 2 3th second row third column right column containing leading 1 entry 1 1th first row. 1 2 0 0 0 1 0 0 0 observations infer matrix satisfies conditions row reduced row echelon form. matrix row echelon form reduced row echelon form. example 2.6.2. choose correct options matrix 0 1 0 1 0 1 0 1 0 . option 1 row echelon form reduced row echelon form. . option 2 row echelon form reduced row echelon form. . option 3 neither row echelon form reduced row echelon form. lets see observations matrix note nonzero row. 1. first nonzero entry element first row 1 entry 12th first row top row first column left column also called leading 1 first row . 0 1 0 1 0 1 0 1 0 2. first nonzero entry second row 1 entry 2 1th also called leading 1 second row. 0 1 0 1 0 1 0 1 0 3. first nonzero entry third row 1 entry 3 2th also called leading 1 third row. 0 1 0 1 0 1 0 1 0 4. column containing leading 1 entry 2 1th second row first column left column containing leading 1 entry 1 1th first row. 0 1 0 1 0 1 0 1 0
2.6. gauss elimination method 67 fourth observation infer matrix satisfying second condition row echelon form reduced row echelon form. matrix row echelon form reduced row echelon form. example 2.6.3. choose correct options matrix 1 1 0 0 1 0 0 0 1 . option 1 row echelon form reduced row echelon form. . option 2 row echelon form reduced row echelon form. . option 3 neither row echelon form reduced row echelon form. lets see observations matrix note nonzero row. 1. first nonzero entry element first row 1 entry 12th first row top row first column left column also called leading 1 first row. 1 1 0 0 1 0 0 0 1 2. first nonzero entry second row 1 entry 2 1th also called leading 1 second row. 1 1 0 0 1 0 0 0 1 3. first nonzero entry third row 1 entry 3 2th also called leading 1 third row. 1 1 0 0 1 0 0 0 1 4. conditions row echelon form a first nonzero element the leading entry row 1. also followed a. b column containing leading 1 row right column containing leading 1 row it. different words subsequent nonzero rows also leading entries i.e. first nonzero entries 1 appear right leading entry previous row. also followed a. c nonzero rows always rows zeros. zero row matrix need think this.
2.6. gauss elimination method 68 5. matrix row echelon form 6. observe second column 1 1 0 0 1 0 0 0 1 entry 2 1th 1 leading entry second row column containing leading entry second column red color non zero number column entry 12th 1 also follows 3rd condition reduced row echelon form column first third columns follows 3rd condition reduced row echelon form. observation 5 6 infer matrix row echelon form reduced row echelon form.
2.6. gauss elimination method 69 2.6.3 exercise question 46. let 0 1 3 1 0 2 0 0 0 be matrix where first row denotes top row ordering rows order top bottom. among given set options identify correct statements. . option 1 first nonzero element first row 3. . option 2 first nonzero element second row 1. . option 3 nonzero element third row. . option 4 since row elements zero deta 0. question 47. let i33 denote identity matrix order 3. answer questions 1 2 set defined a 0 1 0 1 0 1 0 1 0 b 1 2 0 0 0 1 0 0 0 c 1 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 e 0 1 3 0 1 1 0 0 1 f 1 1 0 0 1 0 0 0 1 g 0 1 3 0 0 1 0 0 0 h 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 j 0 1 0 0 0 1 0 0 0 k i33 l 1 0 1 0 1 1 . question 1 s1 subset consisting matrices row echelon form choose correct option following. . option 1 s1 . option 2 s1 a b c d f g i . option 3 s1 b c d e f g h i j k l . option 4 s1 c d e f g h i j k . option 5 s1 c d e f g h j k l . option 6 s1 c d f g h j k l
2.6. gauss elimination method 70 . option 7 cardinality s1 7. . option 8 cardinality s1 8. question 2 s2 subset consisting matrices reduced row echelon form choose correct option following. . option 1 s2 a c d f g l . option 2 s2 b c d e f g h j k . option 3 s2 c d g h j k l . option 4 s2 c d h j k l . option 5 s2 c d h i j k l . option 6 cardinality s2 7. . option 7 cardinality s2 6.
2.6. gauss elimination method 71 2.6.4 solution ax b reduced row echelon form let ax b system linear equations reduced row echelon form. note consider case ith row a reduced row echelon form zero row bi entry matrix b non zero case system linear equations ax b solution. lets see reason note consider system linear equations coefficient matrix reduced row echelon form ith row zero row ith entry b bi 0 see following example . . . . . . . . . . . . . . . . . . . . . . . . . . . 0 . . . 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . x1 x2 . . . xi . . . xn b1 b2 . . . bi . . . bn corresponding system linear equations form matrix form equation which obtain multiplying matrices 0x1 0x2 . . . 0xn bi 0 0 0 absurd. hence ith row a reduced row echelon form zero row bi entry matrix b non zero case system linear equations ax b solution system linear equation solution system linear equations called inconsistent. question 48. consider following system linear equations 0x1 x2 0x3 0x4 1 0x1 0x2 0x3 0x4 1 x1 x2 0x3 0x4 1 0x1 0x2 x3 x4 1. choose set correct options. . option 1 system linear equations solution.
2.6. gauss elimination method 72 . option 2 system linear equations solution. . option 3 deta 0 coefficient matrix given system linear equations. . option 4 none above. dependent variable independent variable let ax b system linear equations reduced row echelon form. assume zero row a corresponding entry b also 0 i.e. system linear equations solution system linear equation solution it matter unique solution infinitely many solutions system linear equations called consistent. . ith column leading entry row call xi dependent variable. . ith column leading entry row call xi independent variable. example 2.6.4. consider system linear equations 0x1 x2 0x3 0x4 1 0x1 0x2 x3 0x4 1 variables dependent independent variables matrix representation system linear equation 0 1 0 0 0 0 1 0 x1 x2 x3 x4 1 1 observe that coefficient matrix 0 1 0 0 0 0 1 0 reduced row echelon form entry 12th 1 leading entry column second column corresponding variable x2 x2 dependent variable. similarly see third column 0 1 0 0 0 0 1 0 entry 2 3th
2.6. gauss elimination method 73 one leading entry column third column corresponding variable x3 dependent variable. see coefficient matrix 0 1 0 0 0 0 1 0 reduced row echelon form first second column dont leading entry variables x1 x4 independent variables. example 2.6.5. let ax b system linear equations 1 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 b 3 2 0 0 . find number independent dependent variables observe given coefficient matrix reduced row echelon form entries 11th 23th matrix leading entries column corresponding variables x1 x3 resp. dependent variables. entry 2 4th leading entry entry 23th leading second row. second fourth columns dont leading entries corresponding variables x2 x4 independent variables. use dependent variable independent variable let ax b system linear equation coefficient matrix reduced row echelon form get variables dependent independent variables. let xi xj or many get independent variables system linear equations write rest variables linear function xi xj variables xi xj independent variables give value xi xj or many variables get get value rest variables substituting corresponding function. i.e. reduced row echelon form makes easier find find solution system linear equations. lets see example. example 2.6.6. let ax b system linear equations 1 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0
2.6. gauss elimination method 74 b 3 2 0 0 . observe coefficient matrix reduced row echelon form. now 1 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 x1 x2 x3 x4 3 2 0 0 x1 x3 2x4 0 0 3 2 0 0 comparing entries get system linear equations equation form x1 0x2 0x3 0x4 3 0x1 0x2 x3 2x4 2 one example seen x1 x3 dependent variables x2 x4 independent variables write dependent variable x1 linear function x2 x4 similarly dependent variable x3. x1 3 0x2 0x4 3 x3 2 2x4 0x1 0x2 2 2x4 see x1 3 let x2 1 let x4 0 get x3 2 3 2 0 2 is solution system linear equations. let x4 2 x3 0 let x2 0 3 0 0 2 is another solution. similarly x2 x4 independent variables change values x2 x4 get infinitely many solutions system linear equations. note system linear equations consistent least one independent variable system linear equations infinitely many solutions
2.6. gauss elimination method 75 2.6.5 exercise question 49. consider system linear equations 0x1 x2 0x3 0x4 1 0x1 0x2 x3 0x4 1 choose set correct options. hint recall definitions independent dependent variable respect reduced row echelon form. . option 1 x1 x2 dependent variables. . option 2 x2 dependent variable. . option 3 x3 x4 independent variables. . option 4 x4 independent variable. question 50. suppose system linear equations consists one equation four variables follows x1 x2 x3 x4 constant. find number independent variables. answer 3 hintthink about many variables expressed terms variables given system linear equations question 51. let ab denote augmented matrix system linear equations 2x1 x2 3 x1 3x2 4 where 2 1 1 3 b 3 4 . let matrix 1 0 0 1 b denote reduced row echelon form augmented matrix corresponding system linear equations above. following options are correct . option 1 values b cannot determined given infor mation.
2.6. gauss elimination method 76 . option 2 b exact values cannot determined given information. . option 3 b 1 . option 4 2 b 3 . option 5 solutions x1 x2 unique. . option 6 x1 x2 1 system linear equations unique solution. . option 7 x1 x2 1 solution. however possible determine whether system equations unique solution given information. question 52. let ax b matrix representation system linear equations 4 4 matrix x x1 x2 x3 x4 b b1 b2 b3 b4 . let reduced row echelon form 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 . following options correct hint recall definitions independent dependent variable respect reduced row echelon form. . option 1 x1 dependent x2. . option 2 x3 dependent x4. . option 3 x2 independent variable. . option 4 solution system linear equations if exists unique. . option 5 exist infinitely many solutions given system linear equations. question 53. let 1 0 3 0 1 2 0 0 0 be matrix where first row denotes top row ordering rows top bottom. consider system
2.6. gauss elimination method 77 linear equations given ax b x x1 x2 x3 b 4 3 0 . answer following questions 1 2. question 1 find number independent variables. answer 1 question 2 x1 0 given find number solutions given system linear equations. answer 1
2.6. gauss elimination method 78 2.6.6 elementary row operations let matrix. operation rows matrix a like inter change two rows multiply row real number adding row another row multiply row real number add another row kind operations used called elementary row operations. section see mainly three different types elementary row oper ations. three different types elementary row operations performed matrix are note ri denotes ith row matrix. . type 1 interchanging two rows. example interchanging r1 r2 1 2 3 4 5 6 7 8 9 r1 r2 4 5 6 1 2 3 7 8 9 . type 2 multiplying row constant. example multiply r2 c r 1 2 3 4 5 6 7 8 9 c.r2 1 2 3 4c 5c 6c 7 8 9 . type 3 adding scalar multiple row another row. example multiply r2 c r add r3 1 2 3 4 5 6 7 8 9 r3 c.r2 1 2 3 4 5 6 7 4c 8 5c 9 6c example 2.6.7. let 5 2 9 2 5 6 9 8 1 . find matrices performing ele mentary operations r2 5r1 r1 r3 2r3.
2.6. gauss elimination method 79 5 2 9 2 5 6 9 8 1 r2 5r1 5 2 9 23 10 39 9 8 1 5 2 9 2 5 6 9 8 1 r1 r3 9 8 1 2 5 6 5 2 9 5 2 9 2 5 6 9 8 1 2r3 5 2 9 2 5 6 18 16 2 question 54. three different types elementary row operations performed matrix are . type 1 interchanging two rows. . type 2 multiplying row constant. . type 3 adding scalar multiple row another row. consider four matrices given below 1 1 1 0 1 1 1 1 1 b 1 1 1 0 1 1 1 1 1 c 1 1 1 0 1 1 1 1 1 and 1 1 1 0 1 1 1 3 3 . choose set correct options. . option 1 matrix b obtained matrix elementary row operation type 1. . option 2 matrix c obtained matrix elementary row operation type 1. . option 3 matrix obtained matrix c elementary row operation type 3. . option 4 matrix obtained matrix c elementary row operation type 2. 2.6.7 row reduction reduced row echelon form seen system linear equation ax b coefficient reduced row echelon form makes easier solve system linear equa tions. lets see methods change matrix order mn row echelon form
2.6. gauss elimination method 80 reduced row echelon form using elementary operations. steps change matrix row echelon form till step4 row echelon form reduced row echelon form step1 find left non zero column example matrix 0 3 2 2 0 2 2 3 0 left column first column step2 use elementary row operation get 1 top position column example matrix using type1 elementary operation interchange first second rows. using type2 elementary operation make one top position column follows 0 3 2 2 0 2 2 3 0 r1 r2 2 0 2 0 3 2 2 3 0 r12 1 0 1 0 3 2 2 3 0 step3 use type3 elementary operation make entries 1 0. see last matrix 1 0 1 0 3 2 2 3 0 1 left top position entry 11th first row firs column 1 second row 0 i.e entry 21th need make 0 third row i.e entry 31th need make 0. 1 0 1 0 3 2 2 3 0 r3 2r1 1 0 1 0 3 2 0 3 0 step4 nonzero rows current row matrix row echelon form. else find next nonzero row using type1 elementary operation move zero rows remaining non zero rows. repeat process sub matrix current row change matrix row echelon form.
2.6. gauss elimination method 81 seen last matrix 1 0 1 0 3 2 0 3 0 1 left top position entry 11th first row firs column 1 second row 0 made 0 third row entry 31th. make entry 2 2th 1 entries column 0 repeat next row. see process below 1 0 1 0 3 2 0 3 0 r23 1 0 1 0 1 23 0 3 0 r3 3r2 1 0 1 0 1 23 0 0 23 3 2r3 1 0 1 0 1 23 0 0 1 obtained matrix row echelon form. step5 continue reduced row echelon form take columns containing 1 leading position row use type3 operation make entries column 0. get last matrix 1 0 1 0 1 23 0 0 1 see entry 11th 1 leading position nonzero column. similarly entry 22th 1 non zero entry column need performs operations entry 33 leading position and 1 also nonzero entry column make entries 0 1. see below 1 0 1 0 1 23 0 0 1 r2 2 3r3 1 0 1 0 1 0 0 0 1 r1 r3 1 0 0 0 1 0 0 0 1 obtain matrix reduced row echelon form. example 2.6.8. find row echalon form reduced row echelon form matrix 2 4 1 3 8 7 5 6 9 . 2 4 1 3 8 7 5 6 9 1 2r1 1 2 12 3 8 7 5 6 9 r2 3r1 r3 5r1 1 2 12 0 2 112 0 4 132 r22
2.6. gauss elimination method 82 1 2 12 0 1 114 0 4 132 r3 4r2 1 2 12 0 1 114 0 0 352 2 35r3 1 2 12 0 1 114 0 0 1 row echelon form r1 r32 r2 11 4 r3 1 2 0 0 1 0 0 0 1 r1 2r2 1 0 0 0 1 0 0 0 1 reduced row echelon form question 55. let 1 1 1 0 1 1 1 0 1 be square matrix order 3. statements true matrix a . option 1 transformed via elementary row operations matrix 1 1 1 0 1 1 0 0 1 which row echelon form. . option 2 reduced row echelon form matrix 1 0 1 0 1 0 0 0 1 . . option 3 reduced row echelon form matrix 1 0 0 0 1 0 0 0 1 . . option 4 transformed via elementary row operations matrix 1 0 0 0 1 0 0 0 1 which row echelon form. . option 5 transformed via elementary row operations matrix 1 1 1 0 1 0 0 0 1 which row echelon form. question 56. let reduced row echelon form matrix 1 0 1 0 1 1 . suppose first third columns 1 1 2 1 respectively. second column matrix given m1 m2 value m1 m2. answer 1
2.6. gauss elimination method 83 question 57. let 0 1 3 1 0 2 1 1 1 be matrix where first row denotes top row ordering rows top bottom. consider sys tem linear equations given ax b x x1 x2 x3 b 4 3 1 . answer following questions. q1 r reduced row echelon form a find number nonzero rows r. answer 2 q2 x1 0 given find value x2. answer 0.5 2.6.8 effect elementary row operations determinant matrix . type 1 interchanging two rows. let b matrix obtain interchanging two row detb deta. example interchanging r1 r2 let 1 2 3 4 5 6 7 8 9 r1 r2 4 5 6 1 2 3 7 8 9 b detb deta . type 2 multiplying row constant. let b matrix obtain multiplying row c r detb cdeta. example multiply r2 c r let 1 2 3 4 5 6 7 8 9 c.r2 1 2 3 4c 5c 6c 7 8 9 b detb cdeta
2.6. gauss elimination method 84 . type 3 adding scalar multiple row another row. let b matrix obtain multiplying row real number added another row detb deta. example multiply r2 c r add r3 let 1 2 3 4 5 6 7 8 9 r3 c.r2 1 2 3 4 5 6 7 4c 8 5c 9 6c b detb deta. question 58. let b square matrices order 3. consider three equa tions below. . equation 1 deta detb . equation 2 deta c detb c r . equation 3 deta detb choose set correct options. . option 1 matrix b obtained matrix elementary row operation type 1 equation 1 satisfied. . option 2 matrix b obtained matrix elementary row operation type 1 followed elementary operation type 2 equation 2 satisfied c. . option 3 matrix b obtained elementary row operation type 2 equation 3 satisfied. . option 4 matrix b obtained elementary row operation type 3 equation 3 satisfied. question 59. let 3 3 real matrix whose sum entries column 5 sum first two elements column 3. following statements are true hint row operation adding one row row. . option 1 determinant matrix multiple 5. . option 2 determinant matrix multiple 3.
2.6. gauss elimination method 85 . option 3 determinant matrix multiple 15. . option 4 determinant matrix multiple 2. . option 5 determinant matrix multiple 8. 2.6.9 gauss elimination algorithm learned change matrix row echelon form reduced row echelon form. suppose coefficient matrix given system linear equations reduced row echelon form easily find solution system linear equations. lets see via example. example 2.6.9. consider system linear equation ax b 1 0 0 5 0 1 6 7 0 0 1 0 0 0 0 1 x x1 x2 x3 x4 and b 2 0 1 1 find solution system linear equations. observe coefficient matrix row echelon form. lets write equation form matrix form system linear equations get x1 5x4 2 x2 6x3 7x4 0 x3 1 x4 1 observe already got values x3 x4. substituting x3 1 x4 1 rest two equations get x2 1 x1 7 2.6.10 augmented matrix let ax b system linear equations n matrix b 1 matrix. augmented matrix system od linear equations defined ma trix size mn1 whose first n columns column last column b.
2.6. gauss elimination method 86 denote augmented matrix ab put vertical line first n columns last column b writing it. a11x1 a12x2 . . . a1nxn b1 a21x1 a22x2 . . . a2nxn b2 . . . . . . am1x1 am2x2 . . . amnxn bm matrix representation system linear equation ax b a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . am1 am2 . . . amn x x1 x2 . . . xn b b1 b2 . . . bm augmented matrix system linear equations a11 a12 . . . a1n b1 a21 a22 . . . a2n b2 . . . . . . . . . . . . . . . am1 am2 . . . amn bm example 2.6.10. find augmented matrix system linear equations x1 2x2 5x3 2 3x2 x 3 1 coefficient matrix system linear equations 1 2 3 0 3 1 b 2 1 augmented matrix system linear equations 1 2 3 2 0 3 1 1
2.6. gauss elimination method 87 steps find solution system linear equations using gauss elimi nation method consider system linear equations ax b. step1 augmented matrix system ab. step2 perform elementary row operations matrix ab used bring matrix reduced row echelon form. step4 obtaining reduced row echelon form matrix ab let r submatrix obtained matrix first n columns c submatrix obtained matrix consisting last column. write reduced row echelon form ab rc. step5 form corresponding system linear equations rx c. step6 find solutions rx c solution solution ax b. example 2.6.11. consider system linear equations 3x1 2x2 x3 x4 6 x1 x2 2 7x2 x3 x4 8 augmented matrix system 3 2 1 1 6 1 1 0 0 2 0 7 1 1 8 r3 1 23 13 13 2 1 1 0 0 2 0 7 1 1 8 r2 r1 1 23 13 13 2 0 13 13 13 0 0 7 1 1 8 3r3 1 23 13 13 2 0 1 1 1 0 0 7 1 1 8 r3 7r2 1 23 13 13 2 0 1 1 1 0 0 0 8 8 8 r38 1 23 13 13 2 0 1 1 1 0 0 0 1 1 1 r2 r3 r1 r33 1 23 0 0 53 0 1 0 0 1 0 0 1 1 1 r1 2r23 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 matrix 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 is reduced row echelon form augmented matrix matrix get form rx c r 1 0 0 0 0 1 0 0 0 0 1 1 r reduced
2.6. gauss elimination method 88 row echelon form coefficient matrix c 1 1 1 . observe that r entries 11th 22th 33th leading entries variables x1 x2 x3 dependent variables x4 independent variable. equation form rx c is x1 1 x2 1 x3 x4 1 x3 1 x4 system infinitely many solutions. let x4 c c r get x3 1 c. hence 1 1 1 c c c r solution space system linear equations. alternative method find solution system linear equations found rc reduced row echelon form form augmented matrix change rx c equation form decided solution. get row echelon form augmented matrix write equation form still decide solution system linear equations. lets see this observe 1 23 13 13 2 0 1 1 1 0 0 0 1 1 1 is row echelon form augmented matrix. lets find equation form fro row echelon form augmented matrix. know last column 1 23 13 13 2 0 1 1 1 0 0 0 1 1 1 is right side equations system. so write 1 23 13 13 0 1 1 1 0 0 1 1 x1 x2 x3 x4 2 0 1
2.6. gauss elimination method 89 so x1 2 3x2 1 3x3 1 3x4 2 . . . 1 x2 x3 x4 0 . . . 2 x3 x4 1 . . . 3 adding equations 2 3 get x2 1 . multiplying sides second equation 13 adding 1 get x1 x2 1 x1 1 x2 1 as x2 1. form equation 3 get x3 x4 1 x3 1 x4 let x4 c c r get x3 1 c . hence 1 1 1 c c c r solution space system linear equations. example 2.6.12. consider system linear equations x1 x2 x3 2 x2 3x3 1 2x1 x2 5x3 0 augmented matrix system 1 1 1 2 0 1 3 1 2 1 5 0 r3 2r1 1 1 1 2 0 1 3 1 0 1 3 4 r3 r2
2.6. gauss elimination method 90 1 1 1 2 0 1 3 1 0 0 0 3 r1 r2 r33 1 0 4 1 0 1 3 1 0 0 0 1 matrix 1 0 4 1 0 1 3 1 0 0 0 1 is reduced row echelon form augmented matrix matrix get form rx c r 1 0 4 0 1 3 0 0 0 r reduced row echelon form coefficient matrix c 1 1 1 . equation form rx c is x1 4x3 1 x2 3x3 1 0x1 0x2 0x3 1 0 1 which absurd system solution. example 2.6.13. consider system linear equations 3x1 2x2 x3 6 x1 x2 2 7x2 x3 8 augmented matrix system 3 2 1 6 1 1 0 2 0 7 1 8 r3 1 23 13 2 1 1 0 2 0 7 1 8 r2 r1 1 23 13 2 0 13 13 0 0 7 1 8 3r3 1 23 13 2 0 1 1 0 0 7 1 8 r3 7r2 1 23 13 2 0 1 1 0 0 0 8 8 r38 1 23 13 2 0 1 1 0 0 0 1 1 r2 r3 r1 r33 1 23 0 53 0 1 0 1 0 0 1 1 r1 2r23 1 0 0 1 0 1 0 1 0 0 1 1
2.6. gauss elimination method 91 matrix 1 0 0 1 0 1 0 1 0 0 1 1 is reduced row echelon form augmented matrix matrix get form rx c r 1 0 0 0 1 0 0 0 1 r reduced row echelon form coefficient matrix c 1 1 1 . observe that r entries 11th 22th 33th leading entries variables x1 x2 x3 dependent. equation form rx c is x1 1 x2 1 x3 1 system unique solution. hence 1 1 1 is solution system. 2.6.11 exercise question 60. let ax b matrix representation system linear equations. let rc reduced row echelon augmented matrix ab corresponding system. choose set correct options. hint recall reduced row echelon form. . option 1 system rx c infinitely many solutions system ax b infinite solutions. . option 2 system rx c solutions system ax b unique solution. . option 3 system rx c unique solution system ax b solution. . option 4 system rx c unique solution system ax b unique solution.
2.6. gauss elimination method 92 question 61. consider system linear equations 2x1 x2 1 x1 x3 x4 1 x1 x2 x3 x4 2 x1 x3 x4 1. following matrix represents augmented matrix system answer q1 q2 q3 a11 a12 a13 a14 b1 a21 a22 a23 a24 b2 a31 a32 a33 a34 b3 a41 a42 a43 a44 b4 q1 find value a22. answer 0 q2 find sum elements 4th row augmented matrix. answer 2 q3 find sum elements 3rd column augmented matrix. answer 1 question 62. let ax b matrix representation system linear equations b 0. choose set correct options. hint recall definition trivial solution system linear equations applicability gauss elimination method. . option 1 invertible matrix system solution. . option 2 invertible matrix system unique solution. . option 3 invertible matrix trivial solution solution system. . option 4 deta 0 system infinitely many solutions. question 63. let ax b matrix representation system linear equations b 0. choose set correct options. . option 1 invertible matrix system solution. . option 2 invertible matrix system unique solution.
2.6. gauss elimination method 93 . option 3 invertible matrix trivial solution solution system. . option 4 deta 0 either system solution system infinitely many solutions. question 64. consider following systems linear equations system i x y 3 y 2z 1 x z 0 system ii x y 3 2y z 1 6y 3z 0 system iii x y 3 2y z 1 6y 3z 3 choose correct option. . option 1 three systems unique solution. . option 2 system unique solution. . option 3 system ii solution. . option 4 system iii infinitely many solutions. . option 5 system ii iii infinitely many solutions. . option 6 system ii iii solution. question 65. suppose p 3 3 real matrix follows 1 0 1 0 1 1 0 0 1
2.6. gauss elimination method 94 vector xn defined recurrence relation pxn1 xn. x2 7 8 3 sum elements x0 answer 6 question 66. consider system linear equations given below x y z 2 x z 3 x z 4. system linear equations hint use relation system linear equations determinant corresponding coefficient matrix. . option 1 solution. . option 2 infinitely many solutions. . option 3 unique solution. . option 4 finitely many solutions. . option 5 3 dependent variables. . option 6 2 dependent 1 independent variables. question 67. consider system linear equations x1 x2 2x3 1 2x1 x2 2x3 1 3x2 cx3 choose set correct options. hint recall gauss elimination method. . option 1 c 2 1 system infinitely many solutions. . option 2 c 1 1 system infinitely many solutions. . option 3 c 1 2 system solution. . option 4 c 3 2 system unique solution.
95
3.1. introduction 96 3. introduction vector space certainly permitted anyone put forward whatever hypotheses wishes develop logical consequences contained hypotheses. order work merit name geometry necessary hypotheses postulates express result simple elementary observations physical figures. giuseppe peano 3.1 introduction last two chapters studied methods solve system linear equations. vector spaces provide among things birds eye view solutions system linear equations. idea vector space formalized peano 1888. motivate abstract definition vector space discuss critical properties example. consider homogeneous system linear equations x 2z 0 x 2y z 0 2x z 0
3.2. vector space 97 matrix representation system linear equation 1 1 2 1 2 1 2 1 1 x z 0 0 0 3.1 solving system finding x y z solve equation x 1 1 2 1 2 1 z 2 1 1 0 0 0 . let denote matrix associated system. consider set b c r31 b c 0 0 0 consists solutions system 3.1. observe v1 a1 b1 c1 and v2 a2 b2 c2 s v1 v2 a1 a2 b1 b2 c1 c2 s v1 a1 b1 c1 s αv1 αa1 αb1 αc1 s α r. equivalently v1 v2 α1v1 α2v2 also s α1 α2 real numbers. call α1v1 α2v2 linear combination v1 v2. general v1 v2 . . . vn s α1 α2 . . . αn r α1v1 α2v2 αnvn linear combination v1 v2 . . . vn also s. roughly think vector space v collection objects behave similar vectors set s. perform two operations v . add elements v . . multiply scalars α r elements v . operations satisfy conditions axioms vector space. 3.2 vector space definition 3.2.1. vector space v r nonempty set v two operations
3.2. vector space 98 addition v v v scalar multiplication r v v . satisfy following conditions u v w v scalars b r. 1. additive closure u v v . 2. multiplicative closure u v . 3. additive commutativity u v v u. 4. additive associativity u v w u v w. 5. existence zero vector special vector 0v v called zero vector u 0v u. 6. additive inverse every u v vector u called nega tiveadditive inverse u u u 0v . 7. unity 1 u u. 8. associativity multiplication ab u b u. 9. distributivity a b u u b u u v u v. note . one vector space axiom fails hold v vector space. . zero element 0v vector space v always unique. . real number 0 r zero vector 0v vector space v commonly denoted symbol 0. one always tell context whether 0 means zero scalar 0 r zero vector 0v v . . standard suppress write au instead u. vector element vector space. example 3.2.1. let v rn set ntuples x1 x2 . . . xn real numbers. is v nx1 x2 . . . xn x1 x2 . . . xn r . consider usualstandard vector addition scalar multiplication v de fined follows
3.2. vector space 99 x1 x2 . . . xn y1 y2 . . . yn x1 y1 x2 y2 . . . xn yn c x1 x2 . . . xn cx1 cx2 . . . cxn note addition multiplication left hand side defined rn whereas addition multiplication inside bracket righthand side occur r. lets verify conditions vector space know whether v vector space respect given addition scalar multiplication. 1. additive closure clearly u x1 x2 . . . xn v v y1 y2 . . . yn v u v x1 y1 x2 y2 . . . xn yn also v . hence v closed addition. 2. multiplicative closure similarly c r u x1 x2 . . . xn v c u cx1 cx2 . . . cxn also v . hence v closed scalar multiplication. 3. additive commutativity consider two arbitrary vectors u x1 x2 . . . xn v y1 y2 . . . yn v u v x1 y1 x2 y2 . . . xn yn y1 x1 y2 x2 . . . yn xn v u. therefore u v v u u v v . 4. additive associativity consider three arbitrary vectors u x1 x2 . . . xn v y1 y2 . . . yn w z1 z2 . . . zn u v w x1 y1 w1 x2 y2 w2 . . . xn yn wn u v w x1 y1 w1 x2 y2 w2 . . . xn yn wn . since xi yi wi real numbers xi yi wi xi yi wi 1 2 . . . n u v w u v w u v v . 5. existence zero vector get zero vector recall zero vector vector space v vec tor 0v u 0v u u v . let u x1 x2 . . . xn 0v
3.2. vector space 100 m1 m2 . . . mn must x1 x2 . . . xn m1 m2 . . . mn x1 x2 . . . xn x1 m1 x2 m2 . . . xn mn x1 x2 . . . xn result obtain xi mi xi mi 0 1 n. hence 0v 0 0 . . . 0 zero vector v property u 0v u u v . 6. additive inverse find additive inverse element u v need find element u v u u 0v . let u x1 x2 . . . xn u a1 a2 . . . v hence must x1 x2 . . . xn a1 a2 . . . 0v 0 0 . . . 0 x1 a1 x2 a2 . . . xn 0 0 . . . 0 . gives xi ai 0 ai xi. hence x1 x2 . . . xn additive inverse u. 7. unity take u x1 x2 . . . xn v 1 r 1 u x1 x2 . . . xn u u v . 8. associativity multiplication let a b r u x1 x2 . . . xn v . ab u abx1 abx2 . . . abxn abx1 abx2 . . . abxn bx1 bx2 . . . bxn b x1 x2 . . . xn b u hence ab u b u a b r u v . 9. distributivity u x1 x2 . . . xn v y1 y2 . . . yn v a b r.
3.2. vector space 101 . a b u a bx1 a bx2 . . . a bxn ax1 bx1 ax2 bx2 . . . axn bxn ax1 ax2 . . . axn bx1 bx2 . . . bxn x1 x2 . . . xn b x1 x2 . . . xn u b v. . u v x1 y1 x2 y2 . . . xn yn ax1 ay1 ax2 ay2 . . . axn ayn ax1 ax2 . . . axn ay1 ay2 . . . ayn x1 x2 . . . xn y1 y2 . . . yn u v. axioms vector space satisfied. hence v vector space respect given operations. example 3.2.2. let v m23r set 2 3 matrices real numbers. v a11 a12 a13 a21 a22 a23 aij r . usualstandard vector addition scalar multiplication v defined follows a11 a12 a13 a21 a22 a23 b11 b12 b13 b21 b22 b23 a11 b11 a12 b12 a13 b13 a21 b21 a22 b22 a23 b23 c a11 a12 a13 a21 a22 a23 ca11 ca12 ca13 ca21 ca22 ca23 existence zero vector one verify vector 0v 0 0 0 0 0 0 zero vector vector space v respect given operations. additive inverse vector u a11 a12 a13 a21 a22 a23 v element u a11 a12 a13 a21 a22 a23 v additive inverse u. like previous example one check axioms show v vector space respect given operations.
3.2. vector space 102 example 3.2.3. consider real matrix order mn. let v set solutions homogeneous system ax0. is v x1 x2 . . . xn a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . am1 am2 . . . amn x1 x2 . . . xn 0 0 . . . 0 . addition scalar multiplication v defined follows u1 u2 . . . un v1 v2 . . . vn u1 v1 u2 v2 . . . un vn c u1 u2 . . . un cu1 cu2 . . . cun additive closure here want show u v v u v also v . show u v v need prove au v 0. let u u1 u2 . . . un v v1 v2 . . . vn v . definition v au 0 av 0 . result au v au av 0 0 0. therefore u v v v closed addition. multiplicative closure here want show u v c r c u also v . u v au 0. ac u c au c 0 0 therefore c u v v closed scalar multiplication. checking axioms vector space straightforward. hence v vector space respect given operations. example 3.2.4. let v r2 x1 x2 x1 x2 r addition scalar multiplication v defined follows x1 x2 y1 y2 x1 y1 x2 y2 c x1 x2 cx1 cx2.
3.2. vector space 103 example 1 1 2 3 1 2 1 3 3 2 2 3 1 1 2 1 3 1 3 2 4 1 1 4 4 1 1 4 5 3 want check whether v vector space respect given operations. additive closure multiplicative closure x1 x2 y1 y2 v x1 x2 y1 y2 x1 y1 x2 y2 v hence v closed addition. similarly one check v closed scalar multiplication. additive commutativity let x1 x2 y1 y2 v . x1 x2 y1 y2 x1 y1 x2 y2 y1 y2 x1 x2 y1 x1 y2 x2 clear x1 x2 y1 y2 y1 y2 x1 x2 hold x1 x2 y1 y2 v . hence additive commutativity fails hold. v vector space respect given operations. note fact one check many axioms vector space must satisfy hold set respect given operations. example 3.2.5. consider set v x 1 x r. addition scalar multiplication v defined follows x 1 y 1 x y 1 c x 1 cx 1 example 1 1 2 1 1 2 1 3 1 2 1 2 1 2 2 1 0 1 4 1 1 4 1 want check whether v vector space respect given operations. that need verify conditions one one.
3.2. vector space 104 1. additive closure x 1 y 1 v x 1 y 1 x y 1 also v . hence v closed addition. 2. multiplicative closure similarly c r x 1 v c x 1 cx 1 also v . hence v closed scalar multiplication. 3. additive commutativity let u x 1 v y 1 v u v x y 1 y x 1 v u therefore u v v u u v v . 4. additive associativity let u x 1 v y 1 w z 1 v u v w x y z 1 x y z 1 u v w 3. existence zero vector looking element 0v v u v u 0v u. let u x 1 0v m 1 v hence must x 1 m 1 x 1 x m 1 x 1 using addition defined v . gives x x m 0. hence zero vector 0v 0 1. 6. additive inverse find additive inverse element u v need find element u v u u 0v . let u x 1 u a 1 v hence must x 1 a 1 0v 0 1 x a 1 0 1 using addition defined v . gives x 0 a x. hence x 1 additive inverse u x 1.
3.2. vector space 105 7. unity take u x 1 v 1 r 1 u x 1 u u v . 8. associativity multiplication let a b r u x 1 v . ab u abx 1 abx 1 bx 1 b x 1 b u hence ab u b u a b r u v . 9. distributivity u x 1 v y 1 v a b r. . a b u a bx 1 ax bx 1 ax 1 bx 1 x 1 b x 1 u b v. . u v x y 1 ax ay 1 ax 1 ay 1 x 1 y 1 u v. axioms vector space true given set v . hence v vector space respect given operations. example 3.2.6. consider set v r2 x y x r. addition scalar multiplication v defined follows x1 y1 x2 y2 x1 x2 1 y1 y2 c x y cx c 1 cy
3.2. vector space 106 example 1 1 2 1 1 2 1 1 1 4 2 2 1 2 3 2 2 1 3 1 1 4 4 1 1 4 4 1 4 7 4 1. existence zero vector let u x y 0v m n v hence must x y m n x y x 1 n x y. gives x 1 x m 1 n n 0. hence zero vector 0v 1 0. 2. additive inverse find additive inverse element u v need find element u v u u 0v . let u x y u a b v hence must x y a b 0v 1 0 x 1 b 1 0. gives x 1 0 a x 1 b y. hence x 1 y additive inverse u x y. 3. unity take u x v 1 r 1 u x 1 1 x y u u v . 4. associativity multiplication let a b r u x v . ab u abx ab 1 aby bx b 1 b x y b u hence ab u b u a b r u v .
3.2. vector space 107 5. distributivity u x1 y1 v x2 y2 v a b r. . a b x1 y1 ax1 bx1 b 1 ay1 by1 ax1 1 ay1 bx1 b 1 by1 x1 y1 b x1 y1 u b v. . u v x1 x2 1 y1 y2 ax1 ax2 1 ay1 ay2 ax1 1 ay1 ax2 1 ay2 x1 y1 x2 y2 u v. checking axioms easy. hence v vector space respect given operations. note see 0 0 always zero vector r2. zero vector element set working satisfies corresponding axioms. 3.2.1 exercise question 68. set v x y x r vector space opera tions . x1 y1 x2 y2 x1 x2 0 rx y rx y. . x1 y1 x2 y2 x1 y1 x2 y2 rx y rx 0. question 69. consider set v 1 b 1 a b r . vector addition scalar multiplication v defined follows 1 b 1 1 c d 1 1 c b d 1 c 1 b 1 1 ca cb 1
3.3. properties vector spaces 108 set v vector space 3.3 properties vector spaces theorem 3.3.1 cancellation law vector addition. let v vector space v1 v2 v3 v . v1 v3 v2 v3 v1 v2. proof. since v vector space element v3 v additive inverse v 3 v3 v 3 0v . therefore v1 v3 v2 v3 by assumption v1 v3 v 3 v2 v3 v 3 by adding v 3 sides v1 v3 v 3 v2 v3 v 3 by additive associativity v1 0v v2 0v v 3 additive inverse v3 v1 v2 0v zero vector v . note similarly one show vector space v v3 v1 v3 v2 v1 v2. corollary 3.3.2. i zero vector vector space v unique is vectors 0v 0 v satisfy property zero vector 0v 0 v . ii vector u v additive inverse u u unique. proof. i suppose 0v 0 v two additive identities v . since 0v additive identity 0 v 0v 0 v 3.2 since 0 v also additive identity 0 v 0 v 0 v 3.3 therefore equations 3.2 3.3 0 v 0v 0 v 0 v 0 v 0v by cancellation law vector addition ii suppose u w two additive inverse u. u u 0v u w.
3.4. subspaces vector spaces 109 cancellation law vector addition obtain u w. hence additive inverse vector u v unique. note denote additive inverse u u. corollary 3.3.3. i u v 0 u 0v 0 zero scalar 0v zero vector. ii c r c 0v 0v . iii c r u v c u c u c u. proof. i since 0 0 0 obtain 0 u 0 0 u. since scalar multiplication distributive addition distributive prop erty 0 0 u 0 u 0 u. last two equalities get 0 u 0 u 0 u 0 u 0 u 0 u 0v 0v zero vector v 0 u 0v cancellation law vector addition ii proof similar proof i. iii since c c 0 obtain 0v 0 u c c u c u c u. hence c u additive inverse c u is c u c u. similarly one show c u c u. 3.4 subspaces vector spaces definition 3.4.1. nonempty subset w vector space v called subspace v w vector space operations addition scalar multiplication defined v . show non empty set w vector subspace one doesnt need check vector space axioms. enough check two axioms vector space.
3.4. subspaces vector spaces 110 theorem 3.4.1. w non empty subset vector space v w subspace v following conditions hold 1 w1 w2 w w1 w2 w . 2 c r w1 w c w1 w . subspace w vector space v called proper subspace w v . note every vector space v r two trivial subspaces . v subspace v . . subset consisting zero vector 0v v also subspace v. question 70. show v 0v subspaces vector space v . example 3.4.1. consider vector space v r2 x y x r respect usual addition scalar multiplication x1 y1 x2 y2 x1 x2 y1 y2 c x1 y1 cx1 cy1. want check whether w x y x 0 r2 vector subspace v r2 not. note w non empty proper subset v 1 1 v 1 1 w . theorem 3.4.1 enough show w closed addition scalar multiplication. let w1 x1 y1 w2 x2 y2 w . definition w x1y1 0 x2y2 0. show x1 y1x2 y2 x1 x2 y1 y2 w enough show x1 x2 y1 y2 0 obvious. next want show c x1 y1 cx1 cy1 w is cx1 cy1 0. since x1 y1 0 cx1 cy1 0 c r. hence w subspaceproper subspace v . geometrically set w represents straight line r2. geometrical repre sentation w given below.
3.4. subspaces vector spaces 111 3 2 1 1 2 3 3 2 1 1 2 3 l x 0 0 0 figure 3.1 graph clear line l passes origin. general line r2 subspace r2 passes origin. question 71. line r2 subspace r2 passes origin r3 vector space respect usual addition scalar multiplication. example 3.4.2. consider parabola w x y x2 v r2. want check whether w vector subspace v not v vector space respect usual addition scalar multiplication. easy see w closed addition scalar multiplication. prove this enough find one counterexample. . 1 1 1 1 w 1 1 1 1 0 2 w w closed addition. . 1 1 w 2 1 1 2 2 w w closed addition. hence w subspace v . example 3.4.3. check whether set w a m22r 2 2 real symmetric matrices subspace m22r standard addition scalar multiplication. a b w at a bt b a bt bt b. therefore ab symmetric w closed addition. similarly c r w cat cat ca. hence w closed scalar multiplication. example 3.4.4. check whether set w 2 2 invertible matrices real entries standard addition scalar multiplication subspace m22r
3.4. subspaces vector spaces 112 not. observe w closed addition 1 0 0 1 1 0 0 1 w 1 0 0 1 1 0 0 1 0 0 0 0 w . example 3.4.5. show w 0 y z y z r subspace real vector space r3 r3 vector space respect usual addition scalar multiplication. clearly w non empty proper subset r3. 0 y1 z1 0 y2 z2 w 0 y1 y2 z1 z2 0 cy1 cz1 w c r. hence w vector subspace. 3.4.1 exercise question 72. consider system linear equations n variables a11x1 a12x2 . . . a1nxn b1 a21x1 a22x2 . . . a2nxn b2 . am1x1 am2x2 . . . amnxn bm solution x1 x2 . . . xn system element rn. set solutions w system subspace rn iff b b1 b2 . . . bm 0 0 . . . 0. considering rn vector space respect usual addition scalar multiplication. hint check example 3.2.3. question 73. intersection two vector subspaces w1 w2 vector subspace v also vector subspace. question 74. show plane w x y z x z 1 r3 vector subspace r3 r3 vector space respect usual addition scalar multiplication. question 75. let w x y z x z subset vector space r3 with respect usual addition scalar multiplication. show w vector subspace r3.
113
4.1. introduction 114 4. basis dimension pure mathematics is way poetry logical ideas. albert einstein 4.1 introduction previous chapter concept vector space formally introduced. noticed nonzero vector space infinitely many elements vectors call. chapter try look smaller set finitely many elements called basis describes vector space completely. defining basis study concepts linear dependence span set. 4.2 linear dependence independence begin section defining linear combination is. definition 4.2.1. let v vector space v1 v2 . vn v . n x i1 αivi said linear combination vectors v1 v2 . vn coefficients α1 α2 . αn r. note linear combination set vectors another vector v since vector space closed addition scalar multiplication. r2 geometrically find new vector linear combination vectors. obtained using parallelogram law the sum two vectors diagonal parallelogram got taking vectors two adjacent sides. example given following figure.
4.2. linear dependence independence 115 figure 4.1 observe 21 2 obtained scaling vector 1 2. two adjacent sides parallelogram 21 2 2 4 2 1. resulting vector diagonal parallelogram 4 5. may obtained algebraically adding two vectors 2 4 2 1. get result either way. case 4 5 expressed linear combination two vectors v1 1 2 v2 2 1. scalars used α1 2 α2 1 i.e. 21 2 2 1 4 5. also see one vectors written linear combination two. 4 5 21 2 2 1 2 1 21 2 4 5 1 2 1 24 5 1 22 1 taking three vectors one side equation get 21 2 2 1 4 5 0 0. is zero vector written linear combination three vectors nonzero coefficients. say set vectors linearly dependent. shall formally define linear dependence vectors following example depicts linear combination vectors r3.
4.2. linear dependence independence 116 figure 4.2 example see 3 7 2 written linear combination two vectors 0 2 1 2 2 0. again note one vectors written linear combination two vectors. 3 7 2 20 2 1 3 22 2 0 0 2 1 1 23 7 2 3 42 2 0 2 2 0 2 33 7 2 4 30 2 1 case too zero vector written linear combination three vectors 0 2 1 2 2 0 3 7 2 nonzero coefficients. easily verify three vectors lie plane namely plane 2x 2y 4z 0. suppose choose vector lie plane example 1 1 1. case show vector 1 1 1 cannot written linear combination vectors. suppose possible is say 1 1 1 linear combination 0 2 1 2 2 0 1 1 1 a0 2 1 b2 2 0
4.2. linear dependence independence 117 a b r. 2b 1 2a 2b 1 1. possible. thus 1 1 1 cannot written linear combination vectors. this conclude α1 1 1 β0 2 1 γ2 2 0 0 0 0 possible α β γ 0. here zero linear combination vectors coefficients zero. set define linear dependence independence vectors. 4.2.1 linear dependence definition 4.2.2. set vectors v1 v2 . vn said linearly dependent exist scalars α1 α2 . αn r zero α1v1α2v2.αnvn 0. words set vectors linearly dependent zero vector written linear combination vectors least one coefficients nonzero. geometrically two vectors lie line three vectors lie plane etc. linearly dependent. earlier discussion 4 5 2 1 1 2 3 7 2 0 2 1 2 2 0 linearly dependent sets. notice sets contain 3 vectors lie plane. example 4.2.1. consider set 1 2 4 2 1 2 5 0 8. shall show set linearly dependent. suppose exist a b c r a1 2 4 b2 1 2 c5 0 8 0 0 0. homogeneous system linear equations 2b 5c 0 2a b 0 4a 2b 8c 0 solving get nonzero solutions system. eg. 1 b 2 c 1 satisfies system. thus 11 2 422 1 215 0 8 0 0 0 zero vector written linear combination elements set nonzero coefficients. hence set linearly dependent. now append another vector set say 1 1 1 new set 1 2 4 2 1 2 5 0 8 1 1 1. clearly linearly dependent 11 2 4 22 1 2 15 0 8 01 1 1 0 0 0.
4.2. linear dependence independence 118 example brings clarity fact every superset linearly dependent set linearly dependent. easy prove statement hence omitted. remark 4.2.1. suppose set v1 v2 . vn contains zero vector. say vi 0. taking αi 1 coefficients zero write zero linear combination vectors least one coefficient nonzero. thus set containing zero vector linearly dependent. 4.2.2 linear independence shall begin defining linearly independent set. definition 4.2.3. set vectors v1 v2 . vn said linearly independent linearly dependent. words exist α1 α2 . αn r α1v1 α2v2 . αnvn 0 αi 0 i. set linearly independent linear combination vectors yield zero vector coefficients zero. example 4.2.2. consider set vectors 1 1 1 1. suppose exist a b r a1 1 b1 1 0 0. following homoge neous system linear equations. b 0 b 0 solving this get b 0. is way zero written linear combination 1 1 1 1 coefficients zero. thus 1 1 1 1 linearly independent set. notice set two vectors linearly dependent one scalar multiple other. example 1 1 scalar multiple 1 1 hence set linearly dependent. stated earlier two vectors lie line linearly dependent. let us elaborate here. remark 4.2.2. suppose set two nonzero vectors v1 v2. set linearly dependent α1v1 α2v2 0 least one α1 α2 nonzero. exactly one zero say α1 0 α2 0 α2v2 0. since α2 0 v2 0 contradiction. so case α1 α2 nonzero. thus v1 α2 α1 v2. v1 v2 scalar multiples other lie line. remark clear two nonzero vectors linearly inde pendent scalar multiples other.
4.2. linear dependence independence 119 question 76. said linear independence 3 nonzero vectors argue along similar lines show three nonzero vectors linearly independent none linear combination others. end section example set linearly independent vectors. example 4.2.3. consider set 1 1 2 1 0 1 2 1 2. suppose exist a b c r a1 1 2 b1 0 1 c2 1 1 0 0 0. following homogeneous system linear equations. b 2c 0 c 0 2a b 2c 0 solving this get unique solution b c 0. thus given set linearly independent. 4.2.3 ways check linear independence let us recall check whether set v1 v2 . vn linearly independent not. solve equation α1v1 α2v2 . αnvn 0 arbitrary real numbers α1 α2 . αn. expanding system get homogeneous system equations. thus checking linear independence reduces solving system linear equations coefficients vectors unknowns. know homoge neous system always consistent is homogeneous system always solution. zero vector always solution question is nonzero solutions. α1v1 α2v2 . αnvn 0 reduces system trivial solution set linearly independent. get nonzero solution set linearly dependent. note interested solution interested finding whether system unique solution the trivial solution infinitely many solutions. conclude check whether set v1 v2 . vn linearly independent not need verify whether homogeneous system v x 0 trivial solution not v matrix whose jth column vector vj. note matrix v square thing need verify whether determinant v nonzero not. why example 4.2.4. consider set 1 1 2 0. clearly set vectors linearly independent scalar multiples other. now get
4.2. linear dependence independence 120 2 2 matrix v since 2 vectors 2 components each. need solve equation a1 1 b2 0 0 0. reduces 2b 0 0 matrix v nothing but 1 2 1 0 . v invertible matrix hence vectors linearly independent. example 4.2.5. consider set 1 2 3 2 1 4. form coefficient matrix put vectors columns v . get 3 2 matrix since 2 vectors vector 3 components. homogeneous system get 2b 0 2a b 0 3a 4b 0 using gaussian elimination verify whether system unique solution hence set linearly independent. example 4.2.6. consider set 1 1 1 2 3 4. writing homogeneous system linear equations b 3c 0 2b 4c 0 using gaussian elimination verify system infinitely many solu tions. obvious unknowns number equations. hence set linearly dependent. example 4.2.7. consider set 1 1 1 1 2 1 0 1 0. homogeneous system solved b 0 2b c 0 b 0 system unique solution the trivial solution determinant coefficient matrix nonzero. may verified set linearly independent. examples let us make observation. case 3 vectors r2 infinitely many solutions.
4.2. linear dependence independence 121 question 77. said linear independence set 4 vectors r2 general set n vectors r2 n 3 linearly dependent. reasoning exactly seen example 4.2.6. generally set k vectors rn k n 1 always linearly dependent. case gaussian elimination end independent variables recall independent dependent variables. 4.2.4 exercise question 78. suppose set 2 3 0 0 1 0 v linearly dependent set vector space r3 usual addition scalar multiplication. following vectors possible candidate v . option 1 2 3 1 . option 2 1 2 0 . option 3 1 3 0 . option 4 0 1 1 . option 5 π e 0 question 79. 9 3 1 linear combination vectors 1 1 1 1 1 1 x y z 21 1 1 31 1 1 4x y z 9 3 1 find value x 2z. ans 0 question 80. let v vector space v1 v2 v3 v4 v . v1 linear combination vi 2 3 4 i.e. v1 av2 bv3 cv4 . option 1 v2 linear combination vi 1 3 4. . option 2 v3 linear combination vi 1 4. . option 3 v2 linear combination v2. . option 4 v4 linear combination v4. question 81. let subset r3 linearly dependent. following options true . option 1 1 0 0 must linearly dependent. . option 2 v must linearly dependent v r3.
4.2. linear dependence independence 122 . option 3 v must linearly dependent v r3. . option 4 may exist v s v still linearly dependent. question 82. choose correct statements. . option 1 00 001 1 0 0 implies set 0 0 1 1 linearly independent subset r2. . option 2 21 0 20 1 2 2 implies set 1 0 0 1 2 2 linearly dependent subset r2. . option 3 21 020 1 2 2 implies set 1 0 2 2 linearly dependent subset r2. question 83. choose correct set options. . option 1 union two distinct linearly independent sets may linearly dependent. . option 2 union two distinct linearly independent sets must linearly dependent. . option 3 nonempty intersection two linearly independent sets must linearly independent. . option 4 nonempty intersection two linearly independent sets must linearly dependent. question 84. vectors v1 v2 v3 linearly independent αv1 βv2 γv3 0 find value α β γ. ans 0 question 85. let solution set system homogeneous linear equations 3 variables 3 equations whose matrix representation follows ax 0 33 coefficient matrix x denotes column vector x1 x2 x3 . choose set correct options. . option 1 v1 v2 s linear combination v1 v2 also s.
4.3. spanning sets 123 . option 2 set subspace r3 respect usual addition scalar multiplication r3. . option 3 set v1 v2 v1 v2 linearly dependent subset s. . option 4 set v1 v2 linearly independent subset v1 scalar multiple v2. question 86. many values set 1 2 a a 0 a 1 a2 8 0 1 3a linearly independent answer 0 4.3 spanning sets let us begin section defining span set. span set roughly vectors got using given set vectors. definition 4.3.1. span set s denoted spans set finite linear combinations elements set s. is spans n x i1 αivi vi s αi r note spans vector subspace. verify example 4.3.1. let 1 0 r2. span contain possible finite linear combinations elements s. spans αi1 0 αi r α 0 α r thus span 1 0 xaxis. already know xaxis subspace r2. question 87. span set 1 1 question 88. span x y x y 0 0 question 89. span a b c a b c 0 0 0 example 4.3.2. let 1 1 0 1 0 0. span set spans a1 1 0 b1 0 0 a b r a b a 0 a b r c a 0 c r nothing xyplane r3.
4.3. spanning sets 124 set define spanning set vector space. nothing set vectors generates elements vector space. definition 4.3.2. let v vector space v . said spanning set spans v . example 4.3.3. let 1 0 0 1. spans r2. thus spanning set r2. element r2 generated using elements set s. x y written x1 0 y0 1. example 4.3.4. let 1 0 0 1 1 1. spans r2. clearly every superset spanning set also spanning set. example 4.3.5. 1 1 1 0 spans r2 x y y1 1 x y1 0. question 90. s said relationship spant spans question 91. spanspans 4.3.1 building spanning sets may append vectors set build spanning sets vector space. consider r3. . step 1 start s0 . spans0 0 0 0. why . step 2 since spans0 r3 append vector say 1 1 0 s0. s1 s0 1 1 0 1 1 0. spans1 line r3 still doesnt cover entire space r3. . step 3 choose vector outside spans1. let s2 s1 1 1 1 1 1 0 1 1 1. spans2 plane x y still doesnt cover r3. . step 4 choose vector outside spans2. let s3 s2 1 0 0 1 1 0 1 1 1 1 0 0. now easy verify s3 spans r3. notice stage added vector span previous vectors. question 92. construct spanning set r3 starting set 1 2 3
4.3. spanning sets 125 4.3.2 exercise question 93. choose set correct options. . option 1 spanning set vector space v v must spanning set v v v . . option 2 span empty set zero vector space. . option 3 spanning set vector space v v must spanning set v v s. . option 4 spanning set vector space v v may spanning set v v v . question 94. let set matrices 1 0 0 0 0 1 0 0 0 0 0 1 . choose correct statements . option 1 spans vector space consisting lower triangular square matrices order 2. . option 2 spans vector space consisting upper triangular square matrices order 2. . option 3 spans vector space consisting square matrices order 2. . option 4 spans vector space consisting scalar matrices order 2. question 95. let set matrices 1 0 0 0 0 0 0 1 . choose correct statements . option 1 spans vector space consisting identity matrix order 2. . option 2 spans vector space consisting diagonal matrices order 2. . option 3 spans vector space consisting square matrices order 2. . option 4 spans vector space consisting scalar matrices order 2.
4.4. basis vector space 126 4.4 basis vector space idea spanning set linearly independent set define basis is. laymans terms basis smallest set gives complete information vector space. definition 4.4.1. basis b vector space v linearly independent set also spans v . algorithm used build spanning set note since wanted newly added element span previous elements also building linearly independent set ultimately basis. thus construct basis every vector space. so goal keep necessary vectors get information vector space. example rn set n vectors linearly dependent. basis rn n elements. example 4.4.1. let ei denote vector rn whose ith coordinate 1 coordinates 0. forms basis rn. called standard basis. verify set actually basis recall need check whether set linearly independent spans rn. question 96. show 1 1 1 0 basis r2. clear previous example question may multiple bases vector space. see lot examples little later section. proving next theorem let us define two terms. definition 4.4.2. set said maximal linearly independent set linearly independent superset linearly dependent. words appending vector makes linearly dependent. definition 4.4.3. set said minimal spanning subset vector space v spans v subset span v . words removing vector s new set longer spans v . theorem 4.4.1. following conditions equivalent set b basis vector space v 1. b linearly independent spanb v . 2. b maximal linearly independent set. 3. b minimal spanning set.
4.4. basis vector space 127 conditions equivalent first condition definition basis. let us first see first two statements equivalent. suppose b basis b linearly independent. suppose b b v. since b basis spans v hence v written linear combination elements b hence making b linearly dependent. thus b maximal linearly independent set. prove converse b maximal linearly independent set clearly linearly independent. remains show b spans v . suppose v v v b. set b v linearly independent set contradiction b maximal linearly independent set. similar fashion equivalence first third statements may established. now clearly established basis minimal spanning set well maximal linearly independent set. two ways find basis vector space. 1. start and keep appending vectors till set spans vector space v . 2. start spanning set keep deleting vectors dependent vectors set till set becomes linearly independent. example 4.4.2. let v r2. get basis v let us start and append nonzero vector it say 1 1. now 1 1 span r2 hence shall append vector belong span 1 1 say 1 2. now set 1 1 1 2 linearly independent set by choice vectors also easy verify vector r2 written linear combination vectors. thus 1 1 1 2 spans r2 hence basis r2. example 4.4.3. let v r2. easy check 1 2 2 3 2 4 4 5 spans v . now shall delete vectors written linear combination vectors. clearly 2 4 21 2. so remove one those. let us remove 2 4. set remains 1 2 2 3 4 5. note vectors set written linear combination two vectors. instance 1 2 3 22 3 1 24 5. removing one vectors get linearly independent set also spans v hence basis v . observe irrespective set start method follow always end basis r2 exactly two elements. question 97. obtain basis r3 starting set 1 1 1 2 4 0 0 1 1 2 2 4 3 4 5 general basis rn exactly n elements.
4.5. dimension vector space 128 4.4.1 exercise question 98. let v vector space defined follows v x y z w x z w r4 usual addition scalar multiplication. following set forms basis v . option 1 1 0 0 0 0 1 0 0 0 0 0 1. . option 2 1 1 0 0 0 1 1 0 0 1 0 1. . option 3 1 0 1 0 1 1 0 0 1 0 0 1. . option 4 1 1 0 0 0 1 1 0 0 1 0 1. question 99. v1 v2 v3 forms basis r3 following true . option 1 v1 v2 v1 v3 forms basis r3. . option 2 v1 v1 v2 v1 v3 forms basis r3. . option 3 v1 v1 v2 v1 v3 forms basis r3. . option 4 v1 v1 v2 v1 v3 forms basis r3. 4.5 dimension vector space indeed exciting fact two bases vector space number elements unique number called dimension vector space. let us define formally. definition 4.5.1. dimension vector space v sizecardinality basis b v . denoted dimv . prof. sarang used terminology rank vector space lecture videos. shall use terminology here instead shall stick dimension vector space throughout book. already mentioned basis rn exactly n elements hence dimrn n. let us calculate dimensions certain subspaces. example 4.5.1. let w subspace r3 spanned 1 0 0 0 1 0 3 5 0. calculate dimension w first construct basis w . clearly 1 0 0 0 1 0 3 5 0 spans w hence need check whether linearly inde pendent not. 1 0 0 0 1 0 linearly independent also spans w why. thus 1 0 0 0 1 0 basis w hence dimension w 2.
4.5. dimension vector space 129 note three vectors previous example lie xyplane. know get basis xyplane obtained previous example too. next shall example terms matrices might easier approach follow. steps followed given below . step 1 write vectors rows matrix a. . step 2 apply row reduction obtain row echelon form. . step 3 nonzero rows reduced matrix form basis w . number nonzero rows reduced matrix dimension subspace w . note mentioned method basis vectors need set started with. now propose another method get basis given set vectors. vectors arranged columns matrix matrix say a reduced row echelonreduced row echelon form say r. columns corresponding columns r containing pivots form basis column space a. illustrate examples. example 4.5.2. let w subspace r3 spanned 1 1 2 2 1 4 3 0 6. let us find basis w . first form matrix whose columns given vectors. 1 2 3 1 1 0 2 4 6 . row reduction get row echelon form r 1 2 3 0 1 1 0 0 0 . note pivots present first second columns hence first second columns matrix form basis subspace w . thus basis w given 1 1 2 2 1 4. example 4.5.3. let w subspace r4 spanned set vectors 1 1 0 0 2 1 1 4 3 4 1 4 3 2 1 4. matrix 1 2 3 3 1 1 4 2 0 1 1 1 0 4 4 4 . converting row echelon form get matrix r 1 2 3 3 0 1 1 1 0 0 0 0 0 0 0 0 . pivots present first
4.5. dimension vector space 130 second columns hence first two vectors form basis w . thus 1 1 0 0 2 1 1 4 forms basis w . 4.5.1 exercise question 100. choose set correct options. . option 1 dimension vector space m12r 2. . option 2 dimension vector space m21r 1. . option 3 dimension vector space m33r 3. . option 4 basis m22r set 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 . question 101. consider following sets . v1 a m22r symmetric matrix i.e. . v2 a m22r scalar matrix . v3 a m22r diagonal matrix . v4 a m22r upper triangular matrix . v5 a m22r lower triangular matrix vi 1 2 3 4 5 subspaces vector space m22r. choose set correct options. . option 1 dimension v1 3. . option 2 dimension v2 3. . option 3 dimension v3 1. . option 4 dimension v4 3. . option 5 dimension v5 3. question 102. find dimension vector space v a sum entries row 0 m32r. answer 3 question 103. find dimension vector space v x y z w x z w x w z x y z w r. answer 2
131
5.1. introduction 132 5. rank nullity matrix if theorems find proofs easily enough bernhard riemann 5.1 introduction chapter define rank matrix outline procedure calculate rank matrix. then move define concept null space matrix outline procedure calculate basis dimension namely nullity matrix. end chapter stating important ranknullity theorem. 5.2 rank matrix suppose person x starts moving position a. 10 minutes arrives position b 3km west 4km south a. information helps us identify position x 10 minutes. addition this also say x 5km south west starting point a still arrive location b. hence information redundant. two dimensional plane two directions identify position object completely recall dimension r2 2. role linearly independent vectors highlighted here. sense look find number directions needed completely understand matrix arrive concept rank matrix. definition 5.2.1. let n matrix. rank matrix number linearly independent columns a.
5.2. rank matrix 133 subspace spanned columns called column space dimension column space called column rank a. similarly subspace spanned rows called row space dimension row space called row rank a. important fact row rank column rank matrix always equal called rank matrix. example 5.2.1. let 1 2 3 1 0 1 4 2 2 . column space subspace spanned 1 1 4 2 0 2 3 1 2. note third vector sum first two vectors hence column rank dimension column space 2. row space subspace spanned 1 2 3 1 0 1 4 2 2. clearly 4 2 2 1 2 351 0 1 hence row rank dimension row space also 2. simple procedure calculate rank matrix converting matrix row echelon form reduced row echelon form counting number nonzero rows matrix. performing elementary row operations affect number linearly independent rows hence rank matrix. row echelon form matrix example 1 2 3 0 1 1 0 0 0 . thus rank equal number nonzero rows row echelon form 2. example 5.2.2. let 1 2 1 0 5 3 0 0 1 0 1 2 . row echelon form 1 2 1 0 0 1 5 7 0 0 0 1 7 5 . hence rank the number nonzero rows row echelon form 3. note column space subset r3 row space subset r4. example 5.2.3. rank matrix 2 3 4 0 1 2 1 3 is 2 value a note rank matrix 3 dimension column space 3 hence column vectors linearly independent. happens determinant matrix nonzero. but since given rank 2 two linearly independent vectors one vectors dependent two. forces determinant matrix 0. equating determinant matrix 0 making rank atmost 2. deta 2a 10 rank less 3 deta 0 thus get 5. putting 5 reducing ref verify ranka 2.
5.2. rank matrix 134 explanation rank matrix clear rank cannot exceed number columns number rows matrix. thus . ranka minm n n matrix. . rankab min ranka rankb. 5.2.1 exercise question 104. choose set correct options following. . option 1 row rank column rank matrix always same. . option 2 rank zero matrix always 0. . option 3 rank matrix whose entries nonzero real number must 1. . option 4 rank n n identity matrix 1 n n. question 105. find rank 0 1 3 1 1 0 0 1 3 2 0 3 . answer 3 question 106. consider following upper triangular matrix choose correct options. b c 0 e 0 0 f a b c d e f r. . option 1 f 0 rank matrix must less equal 2. . option 2 f 0 rank matrix must exactly 2. . option 3 a b c d e f nonzero rank matrix must 3. . option 4 a d f nonzero rank matrix must 3. question 107. rank matrix 1 1 1 x z x2 y2 z2 x y z distinct nonzero integers answer 3
5.3. nullity matrix 135 5.3 nullity matrix let n matrix. subspace x rn ax 0 called null space matrix a. nothing solution space homogeneous system linear equations ax 0. one verify quickly null space actually subspace. words null space contains vectors rn killed matrix a i.e. taken zero vector matrix a. dimension null space nullity matrix a. example 5.3.1. let 1 1 0 2 4 1 3 3 1 . null space nothing solutions system ax 0. recall solve system linear equations using gaussian elimination. since homogeneous system last column augmented matrix going affected row operations remain zero throughout hence ignored. since first example shall anyway include last column too. augmented matrix 1 1 0 0 2 4 1 0 3 3 1 0 . reduced row echelon form 1 0 1 6 0 0 1 1 6 0 0 0 0 0 . x3 independent variable pivot third column reduced row echelon form. thus solution set null space a k 6 k 6 k k r. dimension null space 1 nullity 1. note one independent variable reduced row echelon form previous example nullity equal 1. fact true general nullity matrix equal number independent variables reduced row echelon form matrix. got complete solution set null space following . assign arbitrary value ki independent variables. . compute value dependent variables terms kis unique row occur in. . set solutions obtained letting kis vary r. subspace easy get basis null space. substituting ki 1 kj 0 j i varies get basis null space a. example 5.3.2. consider matrix 1 2 3 2 4 6 3 6 9 . find null space a find solutions ax 0. row operations augmented
5.3. nullity matrix 136 matrix but recall last column zeros going affected row operations hence ignored obtain reduced form 1 2 3 0 0 0 0 0 0 . one column pivot. x1 dependent x2 x3 independent. nullity 2. recall procedure given above. assign arbitrary values independent variables x2 x3 say x2 x3 s. thus have reduced system x1 2t 3s. hence null space 2t 3s t s t r. easily get basis putting 1 0 second vector putting 0 1 i.e. 2 1 0 3 0 1 forms basis null space a. example 5.3.3. let 1 2 0 4 2 1 1 0 3 3 1 4 . first find reduced form matrix. 1 0 2 3 4 3 0 1 1 3 8 3 0 0 0 0 . x1 x2 dependent whereas x3 x4 independent. assign x3 x4 s. thus null space 2 3t 4 3s 1 3t 8 3s t t r. clearly nullity 2 since two independent variables. basis got putting 1 0 another vector putting 0 1. 2 3 1 3 1 0 4 3 8 3 0 1 basis null space obtained procedure. 5.3.1 exercise question 108. null space matrix a43 . option 1 subspace w x r4 ax 0 r4. . option 2 subspace w x r3 ax 0 r3. . option 3 subspace w x r2 ax 0 r2. . option 4 first column matrix a43. question 109. nullity matrix a34 . option 1 3 . option 2 4
5.3. nullity matrix 137 . option 3 number independent variables system linear equa tions ax 0 x x1 x2 x3 x4 . . option 4 number dependent variables system linear equations ax 0 x x1 x2 x3 x4 . question 110. choose correct set options following. . option 1 nullity nonzero scalar matrix order 3 must 3. . option 2 nullity nonzero scalar matrix order 3 must 0. . option 3 nullity nonzero diagonal matrix order 3 must 3. . option 4 nullity nonzero diagonal matrix order 3 2. question 111. consider coefficient matrix following system linear equations x1 x2 x4 0 x2 x3 0 x1 x3 x4 0 one following vector spaces represents null space a . option 1 t1 t2 t1 t1 t2 t1 t2 r . option 2 t1 t2 t1 t1 t2 t1 t2 r . option 1 t1 t2 t1 t1 t2 t1 t2 r . option 1 t1 t2 t1 t1 t2 t1 t2 r also find basis null space.
5.4. ranknullity theorem 138 5.4 ranknullity theorem section state one important theorems linear algebra namely ranknullity theorem. here state theorem matrices. similar version linear transformations stated chapter 6. theorem 5.4.1. let n matrix. rankanullitya n. shall prove theorem easily see following observation. rank matrix nothing number nonzero rows reduced row echelon form matrix. also denotes number dependent variables reduced form homogeneous system ax 0 number nonzero rows equal number rows pivot elements. also noticed earlier nullity matrix equal number independent variables reduced form homogeneous system ax 0. thus sum dependent variables rank independent variables nullity gives total number variables which equals number columns matrix. theorem simplifies job finding rank nullity matrix. given matrix know rank get nullity using theorem vice versa. example 5.4.1. let nullity matrix a35 2. rank got using ranknullity theorem. ranka nullitya 5. thus ranka 2. checking linear dependence n vectors vector space rn recall dimension rn n calculated different bases rn. commonly used one standard basis consisting n vectors ith vector 1 ith position zero positions. general set n vectors rn linearly independent must basis rn. recall procedure check whether set vectors linearly independent. try equate linear combination vectors 0 solve coefficients. basically try solve homogeneous system whose coefficient matrix matrix whose columns given vectors. system unique solution that zero solution means coefficients zero hence set vectors independent. case infinitely many solutions set vectors become linearly dependent. note since dealing set n vectors rn coefficient matrix system square matrix. thus homogeneous system ax 0 unique solution deta 0. case set vectors linearly independent hence form basis rn. case deta 0 set vectors linearly dependent form basis rn.
5.4. ranknullity theorem 139 example 5.4.2. consider set vectors 1 2 3 1 0 1 4 5 7. since set 3 vectors forms basis r3 linearly independent in case maximal linearly independent. check forms basis r3 finding determinant matrix whose columns given vectors. 1 1 4 2 0 5 3 1 7 . deta 4 0 hence set linearly independent. enough check basis r3 why. 5.4.1 exercise question 112. following options correct square matrix order n n n natural number . option 1 determinant nonzero nullity must 0. . option 2 determinant nonzero nullity may nonzero. . option 3 nullity nonzero determinant must 0. . option 4 nullity nonzero determinant may nonzero. question 113. choose set correct statements. . option 1 nullity 33 matrix c natural number c 0 c 3 nullity a also c. . option 2 nullitya b nullitya nullityb. . option 3 nullity zero matrix order n n n. . option 4 nullity zero matrix order n n 0. . option 5 exist square matrices b order nn nullity b 0 nullity b n. question 114. 3 4 matrix following options true . option 1 ranka must less equal 3. . option 2 nullitya must greater equal 1. . option 3 2 columns nonzero multiples other remaining columns linear combinations 2 columns nullitya 2.
5.4. ranknullity theorem 140 . option 4 2 columns nonzero multiples other remaining columns linear combinations 2 columns nullitya 1. question 115. let ax 0 homogeneous system linear equations infinitely many solutions n matrix m 1 n 1. following statements possible . option 1 ranka n. . option 2 ranka n. . option 3 ranka n. . option 4 nullitya n. . option 5 nullitya 0. question 116. let ax 0 homogeneous system linear equations unique solution rnn. nullity a answer 0 question 117. suppose x1 0 solves ax 0 r24. minimum number elements linearly independent subset null space also spans set solutions ax 0 answer 2
141
6.1. linear mapping 142 6. linear transformation as everything else mathematical theory beauty perceived explained arthur cayley 6.1 linear mapping previous course mathematics studied function like fx x2 3x e.t.c. functions domain codomain either r subset r. section going study special type function whose domain codomain vector spaces subspaces vector space specific properties. let see example suppose 3 shops locality original shop two shops b c. price rice dal oil shops given table below rice per kg dal per kg oil per kg shop 45 125 150 shop b 40 120 170 shop c 50 130 160 based prices decide shop buy gro ceries
6.1. linear mapping 143 get idea lets write expression total cost buying x1 kg rice x2 kg dal x3 kg oil three shops try compare them. total cost buying x1 kg rice x2 kg dal x3 kg oil shop 45x1 125x2 150x3. thought function ca viewed matrix multiplication. cax1 x2 x3 45x1 125x2 150x3 45 125 150 x1 x2 x3 now observe function ca holds special properties caαx1 x2 x3 αx1 αx2 αx3 45αx1 125αx2 150αx3 α45x1 125x2 150x3 αcax1 x2 x3 cax1 x2 x3 y1 y2 y3 x1 y1 x2 y2 x3 y3 45x1 y1 125x2 y2 150x3 y3 45x1 125x2 150x3 45y1 125y2 150y3 cax1 x2 x3 cay1 y2 y3 properties called linearity property function ca. property verified using matrix multiplication expression ca also. similarly shops b c get function cb cc holds properties function ca whose expression matrix form are cbx1 x2 x3 40x1 120x2 170x3 40 120 170 x1 x2 x3 ccx1 x2 x3 50x1 130x2 160x3 50 130 160 x1 x2 x3 comparing expression clear quantities x1 x2 x3 one would buy i.e. x1 x2 x3 positive third expression yield larger values fist one.
6.1. linear mapping 144 however comparison second expression others depends quantities item bought i.e. x1 x2 x3. natural way make comparison would create vector costs i.e. cax1 x2 x3 cbx1 x2 x3 ccx1 x2 x3. think cost vector function c r3 r3 setting expression coordinates r3 i.e. cx1 x2 x3 cax1 x2 x3 cbx1 x2 x3 ccx1 x2 x3 cx1 x2 x3 45x1 125x2 150x3 40x1 120x2 170x3 50x1 130x2 160x3 use matrix multiplication express cost function c compact form extract properties cx1 x2 x3 45 125 150 40 120 170 50 130 160 x1 x2 x3 seen linearity cost function ca cb cc cost function c r3 r3 also follows linearity property cαx1 x2 x3 y1 y2 y3 cαx1 y1 αx2 y2 αx3 y3 ccaαx1 y1 αx2 y2 αx3 y3 cbαx1 y1 αx2 y2 αx3 y3 ccαx1 y1 αx2 y2 αx3 y3 cαcax1 x2 x3 cay1 y2 y3 αcbx1 x2 x3 cby1 y2 y3 αccx1 x2 x3 ccy1 y2 y3 αccax1 x2 x3 cbx1 x2 x3 ccx1 x2 x3 ccay1 y2 y3 cby1 y2 y3 ccy1 y2 y3 αcx1 x2 x3 cy1 y2 y3 verify using matrix multiplication expression c. say cast function c linear mapping. 6.1.1 linear mapping formal definition definition 6.1.1. linear mapping f r3 r3 defined follows fx1 x2 . . . xn n x j1 a1jxj n x j1 a2jxj . . . n x j1 anjxj
6.1. linear mapping 145 coefficient aij real numbers scalars. linear mapping thought collection linear combinations. write expression rhs matrix form ax a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . am1 am2 . . . amn and x x1 x2 . . . xn using matrix multiplication expression directly expression fx1 x2 . . . xn pn j1 a1jxj pn j1 a2jxj . . . pn j1 anjxj easily check function f holds linearity fαx1 x2 . . . xn y1 y2 . . . yn aαx y a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . am1 am2 . . . amn αx1 αx2 . . . αxn y1 y2 . . . yn a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . am1 am2 . . . amn αx1 αx2 . . . αxn a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . am1 am2 . . . amn y1 y2 . . . yn αfx1 x2 . . . xn fy1 y2 . . . yn 6.1.2 exercises book shop organizing year end sale. price bengali hindi tamil urdu book fixed 200 180 230 250 respectively. let tx y z w denote total price x number bengali books number hindi books z number tamil books w number urdu books. table 6.1 shows numbers books different languages purchased customers.
6.1. linear mapping 146 bengali hindi tamil urdu samprita 3 0 0 2 srinivas 0 1 2 1 anna 0 1 0 3 tiyasha 2 2 0 1 hasan 2 2 1 1 table 6.1 answer questions given data. 1 correct expression tx y z w option 1 tx y z w 200 180 230 250x z w option 2 tx y z w x z w option 3 tx y z w 200x 230y 180z 250w option 4 tx y z w 200x 180y 230z 250w 2 following expressions represents total price books purchased samprita option 1 t3 0 0 2 option 2 t3 0 0 2 2 option 3 t3 2 option 4 t5 3 total price in books purchased tiyasha 4 total price in books purchased hasan 5 following expressions represent total price books pur chased srinivas option 1 2t0 1 1 0 t0 0 0 1 option 2 t0 1 0 0 t0 0 1 1 option 3 t0 1 0 0 2t0 0 1 0 t0 0 0 1 option 4 t0 1 1 0 t0 0 1 1 6 following expressions represents total price books purchased anna option 1 t0 1 0 0 t0 0 0 1
6.2. linear transformation 147 option 2 t0 1 0 0 t0 0 1 0 option 3 t0 1 0 0 3t0 0 0 1 option 4 t0 1 1 0 2t0 0 1 0 7 following expressions represent total price books pur chased srinivas anna together option 1 t0 2 2 4 option 2 t2 2 4 option 3 t0 2 2 1 option 4 t0 1 2 1 t0 1 0 3 8 following expressions represent difference total price books purchased samprita tiyasha option 1 t5 2 0 3 option 2 t1 2 0 1 option 3 t3 0 0 2 t2 2 0 1 option 4 t3 0 0 2 t2 2 0 1 6.2 linear transformation definition 6.2.1. function v w two vector spaces v w said linear transformation two vectors v1 v2 vector space v c r scalar following conditions hold . tv1 v2 tv1 tv2 . tcv1 ctv1 form definition linear transformation clear linear transformation linear mapping. example 6.2.1. consider mapping r2 r2 defined tx y 2x y let v1 x1 y1 v2 x2 y2 tv1 v2 tx1 y1 x2 y2 tx1 x2 y1 y2 tv1 v2 2x1 x2 y1 y2 2x1 2x2 y1 y1
6.2. linear transformation 148 tv1 v2 2x1 y1 2x2 y2 tv1 tv2 let c r tcv1 tcx1 y1 tcx1 cy1 2cx1 cy1 c2x1 y1 ctv1 hence linear transformation. state following proposition quite evident definition linear transformations. proposition 6.2.1. linear transformation v w v w vector spaces t0 0 i.e. image zero vector v zero vector w . proof. since linear transformation write t0 0 t0 t0 t0 t0 t0 t0 0 6.2.1 exercises check following mapping linear transformation 1. consider mapping r2 r2 tx y 2x 0 2. consider mapping r3 r3 tx y z x 2 3y 5z 3. consider mapping r3 r4 tx y z 4y z 3y 11 19z 5x 2z 23y 4. consider mapping r r3 tt t 3t 23 89t 5. consider mapping r2 r tx y x
6.2. linear transformation 149 6.2.2 images vectors basis vector space example 6.2.2. let us choose standard basis 1 0 0 1 r2. define linear transformation follows r2 r2 t1 0 2 0 t0 1 0 1 using information following tx y tx1 0 y0 1 tx1 0 fy0 1 xt1 0 yt0 1 x2 0 y0 1 2x y hence explicit definition given r2 r2 tx y 2x y example 6.2.3. choose different basis r2 get different linear transformation. let us choose 1 0 1 1 basis r2. define linear transformation defined earlier f r2 r2 f1 0 2 0 f1 1 1 1 have fx y fx y1 0 y1 1 fx y1 0 fy1 1 x yf1 0 yf1 1 x y2 0 y1 1 2x 2y 0 y y 2x y y hence explicit definition given r2 r2 tx y 2x y y
6.3. injective surjective linear transformations 150 end section following remark visualised examples. remark 6.2.1. let v w linear transformation. enough know image basis elements v get explicit definition t. 6.2.3 exercises 1 choose set correct options. option 1 let u 3 1 0 v 0 1 7 w 3 0 7. linear transformation r3 r3 tu tv 0 0 0 tw 5 1 0. option 2 let u 2 1 0 v 1 0 1 w 3 5 6. linear transformation r3 r2 tu 1 0 tv 0 1 tw 5 6. option 3 let u 1 0 0 v 1 0 1 w 0 1 0. linear transformation r3 r3 tu 0 0 0 tv 0 0 0 tw 0 0 0 t1 4 2 2 4 1. option 4 let u 1 0 v 0 1. linear transformation r2 r2 tu π 1 tv 1 e. 6.3 injective surjective linear transformations earlier courses seen function called injective two elements domain map image function called surjective every element codomain function preimage. similarly 1. linear transformation v1 w v w vector spaces called monomorphism injective map v w i.e. tv1 tv2 v1 v2. 2. linear transformation v1 w v w vector spaces called epimorphism surjective map v w i.e. every w w exists v v tv w. 3. linear transformation v w isomorphism injective monomorphism surjective epimorphism. example 6.3.1. consider following linear transformation defined follows
6.3. injective surjective linear transformations 151 r2 r2 tx y 2x y checking injectivity tx1 y1 tx2 y2 2x1 y1 2x2 y2. hence x1 x2 y1 y2 i.e. x1 y1 x2 y2. so injective linear transforma tion. checking surjectivity u v r2 u 2 v r2 tu 2 v u v. surjective linear transformation. hence bijective linear transformation hence isomorphism. example 6.3.2. consider following linear transformation defined follows r2 r2 tx y 2x 0 checking injectivity tx1 y1 tx2 y2 implies 2x1 0 2x2 0 hence x1 x2. guarantee y1 y2. example 1 2 1 3 image 2 0. hence injective. checking surjectivity preimage vector u v v nonzero. surjective. 6.3.1 null space range space linear transformation definition 6.3.1. kernel nullspace linear transformation let v w linear transformation. define kernel denoted kert set vectors v v tv 0. kert v v tv 0 definition 6.3.2. image range space linear transformation let v w linear transformation. define image denoted imt follows imt w w v v tv w example 6.3.3. let us consider example considered before. r2 r2 tx y 2x y let v x y tv 0 tx y 0 0 2x y 0 0 x 0 0
6.3. injective surjective linear transformations 152 v x y 0 0 kert 0 0. hence kert singleton set contains zero vector space r2. imt r2. hence imt whole space r2 observed earlier surjective. theorem 6.3.1. kernel nullspace linear transformation v w vector subspace v . proof. let v v kert. tv v tv tv 0 0 0. so v v kert. moreover tcv ctv c0 0 cv kert c r. hence kert vector subspace v . theorem 6.3.2. image range space linear transformation v w vector subspace w . proof. let w w imt. exists v v v that tv w tv w. tv v tvtv ww. hence ww imt. similarly tcv ctv cw c r. hence cw imt c r. so imt vector subspace w . definition 6.3.3. dimension kernel nullspace linear transformation de fined nullityt dimension image range space linear transformation defined rankt. theorem 6.3.3. injective linear transformation kett 0. proof. first assume injective. let v kert. hence tv 0 t0 injective v 0. hence conclude kert 0. conversely let kert 0. let vectors v1 v2 v tv1 tv2. tv1 v2 tv1 tv2 0. hence v1 v2 kert. implies v1 v2 0 i.e. v1 v2. hence injective. remark 6.3.1. linear transformation v w surjective imt w corollary 6.3.4. v w injective resp. surjective linear transformation nullityt 0 resp. rankt dimw . definition 6.3.4. define two vector spaces v w isomorphic iff exists isomorphism v w . point mention important theorem sketch proof it. want readers complete proof.
6.4. matrix representation linear transformation 153 theorem 6.3.5. n dimensional vector space isomorphic rn. proof. let v vector space dimension n. consider basis v1 v2 . . . vn v . define linear transformation v rn follows v rn tvi ei ei 0 0 . . . 0 1 0 . . . 0 i.e. 1 ith coordinate 0 elsewhere. let v v tv 0. v v v written v a1v1 a2v2 . . . anvn ais r. hence tv 0 implies ta1v1 a2v2 . . . anvn 0 a1tv1 a2tv2 . . . antvn 0 a1e1 a2e2 . . . anen 0 a1 a2 . . . an 0 i.e ai 0 1 2 . . . n. hence v 0. kert 0 implies injective. leave checking surjectivity readers. 6.4 matrix representation linear transformation begin section examples. example 6.4.1. let us consider linear transformation r2 r2 tx y 2x y consider basis 1 0 0 1 r2 domain codomain. represent matrix follows t1 0 2 0 21 0 00 1 t0 1 0 1 01 0 10 1 matrix representation linear transformation given by 2 0 0 1 example 6.4.2. transformation t suppose consider different basis 1 0 1 1 r2 domain codomain.
6.4. matrix representation linear transformation 154 represent matrix follows t1 0 2 0 21 0 01 1 t1 1 2 1 11 0 11 1 hence matrix representation linear transformation given by 2 1 0 1 note important note changing basis gives us different matrices corre sponding linear transformation. definition 6.4.1. let v w linear transformation. let β v1 v2 . . . vn basis v γ w1 w2 . . . wm basis w . tvi uniquely written linear combination wjs 1 2 . . . n j 1 2 . . . m. tv1 a11w1 a21w2 . . . am1wm tv2 a12w1 a22w2 . . . am2wm . . . tvn a1nw1 a2nw2 . . . amnwm matrix corresponding linear transformation f respect bases β γ given by a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . am1 am2 . . . amn example 6.4.3. consider following linear transformation r3 r3 tx y z x z 2x 3y z 3y 3z consider standard ordered basis r3 domain codomain. t1 0 0 1 2 0 11 0 0 20 1 0 00 0 1 t0 1 0 0 3 3 01 0 0 30 1 0 30 0 1 t0 0 1 1 1 3 11 0 0 10 1 0 30 0 1 matrix representation respect standard ordered basis r3 domain codomain. 1 0 1 2 3 1 0 3 3
6.4. matrix representation linear transformation 155 example 6.4.4. consider following linear transformation r3 r3 tx y z x z 2x 3y z 3y 3z consider ordered basis 1 1 0 0 1 1 1 0 1 r3 domain codomain. t1 1 0 1 5 3 3 2 1 1 0 7 2 0 1 1 1 2 1 0 1 t0 1 1 1 4 6 3 2 1 1 0 11 2 0 1 1 1 2 1 0 1 t1 0 1 0 3 3 01 1 0 30 1 1 01 0 1 matrix representation respect standard ordered basis r3 domain codomain. 3 2 3 2 0 7 2 11 2 3 1 2 1 2 0 example 6.4.5. consider following linear transformation r3 r3 tx y z x z 2x 3y z 3y 3z consider ordered basis 1 1 0 0 1 1 1 0 1 r3 domain standard ordered basis codomain. t1 1 0 1 5 3 11 0 0 50 1 0 30 0 1 t0 1 1 1 4 6 11 0 0 40 1 0 60 0 1 t1 0 1 0 3 3 01 0 0 30 1 0 30 0 1 matrix representation respect standard ordered basis r3 domain codomain. 1 1 0 5 4 3 3 6 3 6.4.1 exercises 1 suppose r2 r2 linear transformation tx y x 0. following options correct
6.4. matrix representation linear transformation 156 option 1 matrices corresponds respect standard ordered basis r2 i.e. 1 0 0 1 domain codomain 1 0 0 1 . option 2 matrices corresponds respect standard ordered basis r2 i.e. 1 0 0 1 domain codomain 1 0 0 0 . option 3 neither oneone onto. option 4 oneone onto. let w x y z x 2yz subspace r3. let β 2 1 0 1 0 1 basis w . let w r2 linear transformation t2 1 0 1 0 t1 0 1 0 1. answer questions 2 3 4 using given information. 2 following appropriate definition t option 1 tx y z x y option 2 tx y z x z option 3 tx y z y z option 4 tx y z x y z x 2y 3 choose correct options. option 1 one one onto. option 2 onto one one. option 3 neither one one onto. option 4 isomorphism. 4 matrix representation respect basis β w γ 1 1 1 1 r2 option 1 1 2 1 1 1 1 option 2 1 2 1 0 0 1 option 3 1 1 1 1 option 4 1 1 1 1
6.5. finding basis null space range space row reduced echelon form 157 5 option represents kernel image following linear trans formation r2 r2 tx y x 0 option 1 kert span1 0 imt span1 0. option 2 kert span1 0 imt span0 1. option 3 kert span0 1 imt span1 0. option 4 kert span0 1 imt span0 1. 6.5 finding basis null space range space row reduced echelon form let v w linear. follow steps find bases null space range space t. step 1 find matrix corresponding respect standard ordered bases β v1 v2 . . . vn γ w1 w2 . . . wm v w respectively. step 2 use row reduction obtain matrix r reduced row echelon form. step 3 basis solution space rx 0 basis null space matrix obtained finding pivot nonpivot columns dependent independent variables seen earlier. step 4 vectors c11 c12 . . . c1n c21 c22 . . . c2n . . . ck1 ck2 . . . ckn form basis null space precisely vectors v 1 v 2 . . . v k kert v pn j1 cijvj form basis kert. use basis obtained step 3 thus get basis kert. step 5 recall i1 i2 . . . ir columns r containing pivot elements columns form basis column space a.
6.5. finding basis null space range space row reduced echelon form 158 step 6 vectors d11 d12 . . . d1m d21 d22 . . . d2m . . . dr1 dr2 . . . drm form basis column space precisely vectors w 1 w 2 . . . w r imt w pm j1 dijwj form basis imt. use basis obtained step 5 thus get basis imt. using steps find basis nullspace range space linear transformation following example example 6.5.1. consider following linear transformation r4 r3 tx1 x2 x3 x4 2x1 4x2 6x3 8x4 x1 3x2 5x4 x1 x2 6x3 3x4 matrix corresponding standard basis follows 2 4 6 8 1 3 0 5 1 1 6 3 row reduced echelon form matrix is 1 0 9 2 0 1 3 1 0 0 0 0 required null space set vectors following holds 1 0 9 2 0 1 3 1 0 0 0 0 x1 x2 x3 x4 0 gives us x1 9x3 2x4 x2 3x3 x4 hence null space spanned vectors 9 3 1 0 2 1 0 1. moreover first second coloumn row reduced echelon form contains pivot elements. hence range space spanned vectors 2 1 1 4 3 1.
6.6. ranknullity theorem 159 6.6 ranknullity theorem state theorem without proof. theorem 6.6.1. let v w linear transformation. rankt nullityt dimv . end chapter stating immediate corollaries rank nullity theorem. corollary 6.6.2. let v w linear transformation. . injective rankt dimv . . isomorphism dimw dimv . proof. injective nullityt 0. hence rank nullity theorem rankt 0 dimv i.e. rankt dimv . moreover surjective then dimw rankt. hence iso morphism i.e. injective surjective dimw rankt dimv . example 6.6.1. find rank nullity linear transformation t r2 r2 x y 7 0 x. verify rank nullity theorem. solution find rank nullity need find range kernel respectively. definition range space ranget tx y x r 0 x x r x0 1 x r span0 1. since 0 1 basis ranget rankt dimranget 1. similarly definition kernel kert x y r2 tx y 0 0 x y r2 0 x 0 0 x y r2 x 0 0 y r span0 1.
6.6. ranknullity theorem 160 since 0 1 also basis kert nullityt dimkert 1. hence following equality rankt nullityt 1 1 dimv dimr2 2. example 6.6.2. consider linear transformation v w dimv 5 dimw 7 onetoone. find rankt nullityt. solution since onetoone one concludes kert 0. therefore nullityt dimkert 0. ranknullity theorem obtain dimv nullityt rankt 0 rankt rankt dimv 5. example 6.6.3. consider linear transformation v w dimv 5 dimw 3 dimkert 2. show onto. solution rank nullity theorem have dimv nullityt rankt dimkert rankt 2 rankt rankt dimv 2 3. since rankt dimension w range w . therefore onto. example 6.6.4. validate statement linear transformation r4 r3 kernel 0. solution since image subspace r3 rankt dimrange t 3. now rank nullity nullityt rankt dimr4 4 nullityt 4 rankt 1 rankt 3. therefore oneone hence given statement false. 6.6.1 exercises consider following linear transformation r3 r2 tx y z 2x 3z 4y z answer questions 123 4 using information given above.
6.6. ranknullity theorem 161 1 following matrices corresponds given linear transformation respect standard ordered basis r3 standard ordered basis r2 option 1 2 3 0 4 1 0 option 2 2 0 0 4 3 1 option 3 2 0 3 0 4 1 option 4 2 4 3 1 0 0 2 following represents basis kernel t option 1 3 2 0 1 0 1 4 1. option 2 3 2 1 4 1. option 3 3 2 1 4 2. option 4 2 0 3 0 4 1. 3 dimension subspace imt answer 2 4 choose correct option. option 1 isomorphism. option 2 one one onto. option 3 onto one one. option 4 neither one one onto. 5 choose set correct options. option 1 nullity rank identity transformation vector space dimension n 0 n respectively. option 2 nullity rank identity transformation vector space dimension n 1 n 1 respectively. option 3 nullity rank identity transformation vector space dimension n n 0 respectively. option 4 nullity rank isomorphism two vector spaces v w both dimension n n 0 respectively.
6.6. ranknullity theorem 162 option 5 nullity rank isomorphism two vector spaces v w both dimension n 0 n respectively. option 6 cannot exist isomorphism two vector spaces whose dimensions same. 6 choose set correct options. option 1 injective linear transformation two vector spaces dimensions must isomorphism. option 2 surjective linear transformation two vector spaces dimensions must isomorphism. option 3 exist surjective linear transformation r2 r3. option 4 exist injective linear transformation r2 r3. 7 choose set correct options. option 1 exists linear transformation r2 r2 imaget kernelt. option 2 exists linear transformation r3 r3 imaget kernelt. option 3 v v linear transformation v1v2 v linearly independent tv1 tv2 also linearly independent. option 4 v v linear transformation tv1 tv2 linearly independent v1 v2 v also linearly independent.
163
7.1. equivalence matrices 164 7. equivalence similar ity matrices the beauty mathematics shows patient followers. maryam mirzakhani 7.1 equivalence matrices definition 7.1.1. let b two matrices order n. say equivalent b invertible matrix p order n n invertible matrix q order that b qap. let mmnr denote set mn matrices entries r. equivalence matrices mmnr equivalence relation is ab c mmnr . equivalent itself. take q imm identity matrix order p inn identity matrix order n. write imm inn. is equivalent itself. . equivalent b b equivalent a. equivalent b know two invertible matrices p q order n m respectively b qap.
7.1. equivalence matrices 165 rewrite equality as q1bp1. since p q invertible p1 q1 also invertible. therefore b equivalent a. . equivalent b b equivalent c equivalent c. equivalent b b equivalent c write b qap c qbp qq invertible matrices order pp invertible matrices order n. using relation write c c qqapp. note qq pp invertible matrices. therefore equivalent c. example 7.1.1. show 1 2 0 1 7 2 and b 2 1 1 0 2 7 are equivalent. solution take q 1 0 0 0 1 0 0 0 1 and p 0 1 1 0 invertible matrices. write b 2 1 1 0 2 7 1 0 0 0 1 0 0 0 1 1 2 0 1 7 2 0 1 1 0 . therefore b similar. note given b equivalent finding p q challenging cases. later see method find p q particular case. example 7.1.2. suppose know 1 1 0 0 1 1 b 1 1 0 1 2 1 equivalent matrices. given q 0 1 1 0 find number possible choices p.
7.1. equivalence matrices 166 solution let p b c e f g h . using relation b qap obtain 1 1 0 1 2 1 0 1 1 0 1 1 0 0 1 1 b c e f g h 1 1 0 1 2 1 0 1 1 1 1 0 b c e f g h 1 1 0 1 2 1 d g e h f d b e c f . equating component matrices g 1 d 1 by comparing 1st column 1 e h 1 b e 2 by comparing 2nd column 2 f 0 c f 1 by comparing 3rd column 3. see three systems independent each system different variables solve independently. system 1 get 0 1 g 1 solution. system 2 get e 0 b 2 h 1 solution. system 3 get f 0 c 1 0 solution. see p 1 2 1 0 0 0 1 1 0 is invertible satisfies equality b qap. note p unique because infinitely many solutions three systems. finding matrices p q particular case consider linear transformation v w . let β1 v1 v2 vn β2 u1 u2 un two ordered bases v γ1 w1 w2 wm γ2 x1 x2 xm two ordered bases w . . let matrix representation respect bases β1 v γ1 w .
7.1. equivalence matrices 167 . let b matrix representation respect bases β2 v γ2 w . b equivalent satisfy equality b qap p q defined follows . p express elements ordered basis β2 terms ordered basis β1 is u1 a11v1 a21v2 an1vn u2 a12v1 a22v2 an2vn un a1nv1 a2nv2 annvn. matrix p given p a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . an1 an2 . . . ann . . q express elements ordered basis γ1 terms ordered basis γ2 is w1 b11x1 b21x2 bm1xm w2 b12x1 b22x2 am2xm wm b1mx1 b2mx2 bmmxm. matrix q given q b11 b12 . . . b1m b21 b22 . . . b2m . . . . . . . . . . . . bm1 bm2 . . . bmm . satisfy relation b b11 b12 . . . b1m b21 b22 . . . b2m . . . . . . . . . . . . bm1 bm2 . . . bmm a a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . an1 an2 . . . ann .
7.1. equivalence matrices 168 example 7.1.3. let r3 r2 linear transformation defined tx1 x2 x3 x1 x2 2x3 x1. . let matrix representation linear transformation respect ordered bases β1 1 0 0 0 1 0 0 0 1 domain γ1 1 0 0 1 codomain. . let b matrix representation linear transformation respect ordered bases β2 1 0 1 1 1 1 1 0 0 domain γ2 0 1 1 0 codomain. let q p matrices b qap. find matrices abp q. solution first find b. since t1 0 0 1 1 11 0 10 1 t0 1 0 1 0 11 0 00 1 t0 0 1 0 2 01 0 20 1 see matrix relative β1 γ1 1 1 0 1 0 2 . similarly matrix b t1 0 1 1 3 30 1 11 0 t1 1 1 2 1 10 1 21 0 t1 0 0 1 1 10 1 11 0 matrix relative β2 γ2 b 3 1 1 1 2 1 . next compute q p. p express elements ordered basis β2 terms ordered basis β1 1 0 1 11 0 0 00 1 0 10 0 1 1 1 1 11 0 0 10 1 0 10 0 1 1 0 0 11 0 0 00 1 0 00 0 1.
7.1. equivalence matrices 169 therefore matrix p p 1 1 1 0 1 0 1 1 0 . q express elements ordered basis γ1 terms ordered basis γ2 1 0 00 1 11 0 0 1 10 1 01 0 therefore matrix q q 0 1 1 0 . hence relation b qap 3 1 1 1 2 1 0 1 1 0 1 1 0 1 0 2 1 1 1 0 1 0 1 1 0 . relatively easier methods check whether two matrices equivalent not. characterization equivalent matrices 1 two matrices b equivalent transformed b combination elementary row column operations. 2 two matrices b equivalent rankarankb. example 7.1.4. show a 4 0 1 1 1 0 1 1 1 and b 5 1 1 6 5 1 4 2 0 are equivalent. solution use two methods show b equivalent. method1 4 0 1 1 1 0 1 1 1 r1r1r2 5 1 1 1 1 0 1 1 1 r2r22r3 5 1 1 1 3 2 1 1 1 r3r3r1 5 1 1 1 3 2 4 2 0 r2r2r1 5 1 1 4 4 1 4 2 0 r2r2 1 2r3 b 5 1 1 6 5 1 4 2 0
7.1. equivalence matrices 170 obtain b performing elementary operations a. hence b equivalent. method2 compute rank b. row reduce echelon form b 1 0 0 0 1 0 0 0 1 . therefore ranka rankb3 hence b equivalent matrices. example 7.1.5. show a 2 6 3 1 3 2 b 1 3 2 0 0 7 equivalent. solution method1 2 6 3 1 3 2 r1r2 1 3 2 2 6 3 r2r22r1 b 1 3 2 0 0 7 obtain b performing elementary operations a. hence b equiv alent. method2 row reduce echelon form b 1 3 0 0 0 1 . therefore ranka rankb2 hence b equivalent matrices. example 7.1.6. check whether 2 0 0 7 1 0 6 8 3 and b 1 1 2 2 0 1 1 3 5 are equiv alent not. solution row reduced echelon form matrix 1 0 0 0 1 0 0 0 1 and row reduced echelon form b 1 0 1 2 0 1 3 2 0 0 0 . therefore ranka rankb hence b equivalent. example 7.1.7. check whether 2 0 0 7 1 0 6 8 3 and b 1 1 2 0 equivalent not.
7.1. equivalence matrices 171 solution two matrices different order equivalent. therefore b equivalent. example 7.1.8. check whether 2 0 0 4 0 0 b 1 1 0 2 0 0 equivalent not. solution one check ranka 1 rankb 2. therefore b equivalent. question 118. b equivalent matrices order mn investigate whether following true 1 bt equivalent. 2 a2 b2 equivalent. 3 ab ba equivalent. solution. 1 since a b equivalent holds nn invertible matrix p invertible matrix q that b qap. equality have bt qapt pt qt abt bt . since p q invertible pt qt also invertible. therefore bt equivalent. 2 general true a2 b2 always equivalent. consider two matrices 0 0 1 0 b 0 0 0 1 . note ranka rankb same. therefore b equivalent. a2 0 0 0 0 b2 0 0 0 1 . notice ranka2 0 rankb2 1. therefore a2 b2 equivalent. 3 general ab ba always equivalent. take 0 0 1 0 b 0 0 0 1 . notice ab 0 0 0 0 ba 0 0 1 0 . clearly rankab rankba. result ab ba equivalent.
7.2. similar matrices 172 7.2 similar matrices definition 7.2.1. two matrices b order nn said similar exists invertible matrix p b p1ap. note check similarity square matrices order. given b similar finding p difficult higher order matrices. later see method find p particular case. question 119. show similarity defines equivalence relation square matrices order. example 7.2.1. check whether 1 3 1 2 b 10 7 13 9 similar not. solution suppose b similar. definition mean invertible matrix p b p1ap ap pb. let p a b c equality ap pb becomes 1 3 1 2 a b c a b c 10 7 13 9 3c b 3d a 2c b 2d 10a 13b 7a 9b 10c 13d 7c 9d . comparing entries two matrices homogeneous system 11a 13b 3c 0 7a 8b 3d 0 8c 13d 0 b 7c 11d 0. augmented matrix system 11 13 3 0 0 7 8 0 3 0 1 0 8 13 0 0 1 7 11 0 .
7.2. similar matrices 173 row reduced echelon form matrix 1 0 8 13 0 0 1 7 11 0 0 0 0 0 0 0 0 0 0 0 . new system becomes 8c 13d 0 b 7c 11d 0. since c free variables infinitely many solution system. therefore infinitely possible choices p. particular c 1 1 get 5 b 4. result get p 5 4 1 1 . note detp 0 p satisfies condition 10 7 13 9 5 4 1 1 1 1 3 1 2 5 4 1 1 . therefore b similar. note need choose values abc p becomes invertible matrix. lemma 7.2.1. two matrices b similar also equivalent. proof. since a b similar invertible p that b p1ap rewrite equality as b qap q p1. therefore b equivalent. note converse theorem true is two square matrices b equivalent imply b similar. see example. consider matrices 1 0 0 1 b 1 1 0 1 .
7.2. similar matrices 174 note ranka rankb 2. therefore b equivalent. b similar. show using method contradiction. suppose b similar. definition get invertible matrix p b p 1 0 0 1 p1. 1 0 0 1 identity matrix commutes 2 2 real matrices is 1 0 0 1 1 0 0 1 m22r. now b p 1 0 0 1 p1 1 0 0 1 pp1 1 0 0 1 . contradiction b 1 0 0 1 . therefore assumption wrong hence b similar. example 7.2.2. check whether 5 3 3 1 b 1 9 1 9 similar not. solution note ranka rankb. therefore b equivalent. since b equivalent b similar. lemma 7.2.2. 1 b invertible ab similar ba. 2 b similar similar bn. 3 b similar similar bt . proof. 1 first assume invertible hence inverse matrix a1 exists. write ba a1aba p1abp p a. therefore ab ba similar.
7.2. similar matrices 175 similarly b invertible b1 exists. ba babb1 p1abp p b1. hence ab ba similar. 2 b similar invertible matrix p b p1ap. positive integer n bn p1apn p1app1ap p1ap z n times p1anp. therefore bn similar. 3 b similar invertible matrix p b p1ap. equality have bt p1ap t pt p1t abt bt pt pt 1 p1t pt 1 hence bt similar. finding matrix p particular case consider linear transformation v v . let β v1 v2 vn γ u1 u2 un two ordered bases v . . let matrix representation respect basis β domain codomain. . let b matrix representation respect basis γ domain codomain.
7.2. similar matrices 176 b similar satisfy equality b p1ap p p1 defined follows . p express elements ordered basis γ terms ordered basis β is u1 a11v1 a21v2 an1vn u2 a12v1 a22v2 an2vn un a1nv1 a2nv2 annvn. matrix p given p a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . an1 an2 . . . ann . . p1 express elements ordered basis β terms ordered basis γ or one compute p1 directly computing p is v1 b11u1 b21u2 bn1un v2 b12u1 b22u2 an2un vn b1nu1 b2nu2 bnnun. matrix p1 given p1 b11 b12 . . . b1n b21 b22 . . . b2n . . . . . . . . . . . . bn1 bn2 . . . bnn . satisfy relation b p1ap b11 b12 . . . b1n b21 b22 . . . b2n . . . . . . . . . . . . bn1 bn2 . . . bnn a a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . . . . . . an1 an2 . . . ann . example 7.2.3. let r2 r2 linear transformation defined tx1 x2 x2 x1.
7.2. similar matrices 177 . let matrix representation linear transformation respect ordered basis β 1 0 0 1 domain codomain. . let b matrix representation linear transformation respect ordered basis γ 1 2 1 1 domain codomain. let p matrix b p1ap. find matrices abp p1. solution first find b. since t1 0 0 1 01 0 10 1 t0 1 1 0 11 0 00 1 therefore matrix relative β 0 1 1 0 . similarly matrix b t1 2 2 1 1 3 1 2 5 3 1 1 t1 1 1 1 2 31 2 1 31 1 matrix relative γ b 1 3 2 3 5 3 1 3 . next compute p p1. p express elements ordered basis γ terms ordered basis β 1 2 11 0 20 1 1 1 11 0 10 1. therefore matrix p p 1 1 2 1 . p1 express elements ordered basis β terms ordered basis γ 1 0 1 31 2 2 31 1 0 1 1 31 2 1 3 1 1.
7.2. similar matrices 178 therefore matrix p1 p1 1 3 1 3 2 3 1 3 . hence relation b p1ap 1 3 2 3 5 3 1 3 1 3 1 3 2 3 1 3 0 1 1 0 1 1 2 1 . example 7.2.4. let r3 r3 linear transformation defined tx1 x2 x3 3x1 x3 2x1 x2 x1 2x2 4x3. . let matrix representation linear transformation respect ordered basis β 1 0 0 0 1 0 0 0 1 domain codomain. . let b matrix representation linear transformation respect ordered basis γ 1 0 1 1 2 1 2 1 1 domain co domain. let p matrix b p1ap. find matrices abp p1. solution first find b. since t1 0 0 3 2 1 31 0 0 20 1 0 10 0 1 t0 1 0 0 1 2 01 0 0 10 1 0 20 0 1 t0 0 1 1 0 4 11 0 0 00 1 0 40 0 4 therefore matrix relative β 3 0 1 2 1 0 1 2 4 . similarly matrix b t1 0 1 4 2 3 17 4 1 0 1 3 4 1 2 1 1 2 2 1 1 t1 2 1 2 4 9 35 4 1 0 1 15 4 1 2 1 7 2 2 1 1 t2 1 1 7 3 4 11 2 1 0 1 3 2 1 2 1 02 1 1
7.2. similar matrices 179 matrix relative γ b 17 4 35 4 11 2 3 4 15 4 3 2 1 2 7 2 0 . next compute p p1. p express elements ordered basis γ terms ordered basis β 1 0 1 11 0 0 00 1 0 10 0 1 1 2 1 11 0 0 20 1 0 10 0 1 2 1 1 21 0 0 10 1 0 10 0 1 therefore matrix p p 1 1 2 0 2 1 1 1 1 . p1 express elements ordered basis β terms ordered basis γ 1 0 0 1 4 1 0 1 1 4 1 2 1 1 22 1 1 0 1 0 3 4 1 0 1 1 41 2 1 1 22 1 1 0 0 1 5 41 0 1 1 41 2 1 1 2 2 1 1 therefore matrix p1 p1 1 4 3 4 5 4 1 4 1 4 1 4 1 2 1 2 1 2 . hence relation b p1ap 17 4 35 4 11 2 3 4 15 4 3 2 1 2 7 2 0 1 4 3 4 5 4 1 4 1 4 1 4 1 2 1 2 1 2 3 0 1 2 1 0 1 2 4 1 1 2 0 2 1 1 1 1 .
7.3. properties similar matrices 180 7.3 properties similar matrices going properties similar matrices mention facts without proofs. facts . invertible matrix order n rankam rankma ranka arbitrary matrix order n. . b two matrices order n traceab traceba detab detadetb. lemma 7.3.1. two matrices b similar rank. proof. let b similar b p1ap. rankb rankp1ap rankap ranka. lemma 7.3.2. two matrices b similar trace. proof. let b similar b p1ap. traceb tracep1ap traceap1p tracea. lemma 7.3.3. two matrices b similar deter minant. proof. let b similar b p1ap. detb detp1ap detp1detadetp deta.
7.3. properties similar matrices 181 note two matrices similar rank trace determi nant converse true. particular two matrices b rank trace determinant imply similar. take 1 0 0 1 b 1 1 0 1 . note b rank deter minant trace similar for explanation check paragraph lemma 7.2.1. example 7.3.1. check whether 2 1 5 3 b 2 3 1 2 similar not. solution tracea 5 traceb 4. since tracea traceb equal b similar. example 7.3.2. check whether 2 1 0 3 b 1 11 0 4 similar not. solution deta 6 detb 4. since deta detb equal b similar. example 7.3.3. check whether 1 0 1 1 b 1 1 0 1 similar not. solution suppose b similar. definition mean invertible matrix p b p1ap ap pb. let p a b c equality ap pb becomes 1 0 1 1 a b c a b c 1 1 0 1 b c b a b c c . comparing entries two matrices homogeneous system 0 b c 0.
7.4. exercises 182 since c free variables infinitely many solution system. therefore infinitely possible choices p. particular c 1 0 get 0 b 1. result get p 0 1 1 0 . note detp 0 p satisfies condition 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 . therefore b similar. 7.4 exercises 1 choose set correct options. option 1 3 3 matrix similar identity matrix order 3 must identity matrix order 3. option 2 3 3 matrix similar diagonal matrix order 3 must diagonal matrix order 3. option 3 b similar matrices also equivalent matrices. option 4 b equivalent matrices also similar matrices. consider two ordered bases β1 1 0 0 0 1 0 0 0 1 β2 1 0 1 0 1 1 1 1 0 r3 consider standard ordered basis γ 1 0 0 1 r2. let b matrices corresponding linear transformation respect bases β1 β2 domain respectively γ co domain. let q p matrices b qap. answer questions 23 4 using information. 2 following matrices represents a option 1 1 1 0 1 2 1
7.4. exercises 183 option 2 1 1 0 0 1 1 option 3 1 1 1 1 0 1 option 4 1 1 1 2 0 1 3 following matrices p option 1 exist matrix p satisfies property b qap. option 2 1 0 1 0 0 1 1 2 0 option 3 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 option 4 1 0 1 0 1 1 1 1 0 4 following statements true q option 1 q identity matrix order 3. option 2 q identity matrix order 2. option 3 whatever matrix p choose q always unique. option 4 q matrix 0 1 1 0 . 5 choose set correct options. option 1 two square matrices order deter minants must similar other. option 2 similar matrices rank. option 3if b similar matrices ax b unique solution bx b also unique solution. option 4 b similar matrices ax b unique solution bx b also unique solution.
7.4. exercises 184 6 following options correct option 1 two invertible matrices order equivalent. option 2 1 7 6 2 1 1 5 2 3 and b 2 3 2 1 1 4 1 2 6 are equivalent. option 3 2 3 4 6 b 1 3 3 1 equivalent. option 4 consider two linear transformations t1x y z x z x 4y 2z 2x z t2x y z x y x z z. let b denote matrix representation t1 t2 respect standard basis r3. b equivalent.
185
8.1. introduction 186 8. affine subspaces affine mapping in days angel topology devil abstract algebra fight soul individual mathematical domain. hermann weyl 8.1 introduction definition 8.1.1. let v vector space. affine subspace v subset l exists v v vector subspace u v l v u v u u u example 8.1.1. consider vector subspace u x 0 x r vector space v r2 usual addition scalar multiplication. let v 0 1 v . define l v u 0 1 x 0 x r x 1 x r l affine space corresponding vector subspace u. point noted vector v unique representation l v u. consider vector v 2 1. v u 2 1 x 0 x r x 2 1 x r observe x 2 1 x r x 1 x r l. fact representation l v u v vector l. example consider arbitrary vector v a 1 l r. v u a 1 x 0 x r x a 1 x r observe x a 1 x r x 1 x r l.
8.1. introduction 187 proposition 8.1.1. subspace u corresponding affine subspace l v u unique. proof. suppose affine subspace l defined l vu l vu. claim u u. let u u v u l. hence v u v u. exists u u that v u v u. v v 0 l 0 u. v v u1 u1 u i.e. v v u1 u1 u i.e. v v u. hence have v u v u u v v u v v u u u u u. u u. similar argument u u concludes u u. remark 8.1.1. putting discussion example theorem above summarize subspace u corresponding affine subspace unique. however vector v unique and fact vector l. affine subspaces thus translations vector subspace v . v 0 v l 0 u u. implies vector subspace also affine subspace. definition 8.1.2. dimension affine subspace say affine subspace n dimensional corresponding subspace u ndimensional. 8.1.1 two dimensional affine subspaces first let us recall possible subspaces r2 usual addition scalar multiplication. 1 zero subspace i.e. 0 0. 2 straight line passing origin slope m i.e. wm x mx x r. 3 whole vector space r2 usual addition scalar multiplication. translations three types vector subspaces possible affine subspaces r2. 1 point affine subspace i.e. a b 0 0 a b real numbers b .
8.1. introduction 188 2 straight line i.e. real numbers b a b wm a b x mx x r r x a mx b x r r x mx a b x r r x mx b ma x r r x mx c x r c b ma r 3 whole vector space r2 usual addition scalar multiplication. 8.1.2 three dimensional affine subspaces first let us recall possible subspaces r3 usual addition scalar multiplication. 1 zero subspace i.e. 0 0 0. 2 straight line passing origin i.e. spanv1 nonzero vector v1 r3. 3 plane passing origin i.e. spanv1 v2 nonzero vectors v1 v2 r3 set v1 v2 linearly independent. 4 whole vector space r3 usual addition scalar multiplication. translates four types vector subspaces possible affine sub spaces r3. 1 point affine subspace i.e. a b c 0 0 0 a b c real numbers a b c . 2 straight line i.e. v spanv1 nonzero vector v1 r3 vector may zero v r3. 3 plane i.e. v spanv1 v2 nonzero vectors v1 v2 r3 set v1 v2 linearly independent vector may zero v r3. 4 whole vector space r3 usual addition scalar multiplication.
8.1. introduction 189 8.1.3 visual representation . click following link get visual representation affine subspaces r2. diagram get different straight lines different values a b m. fix value b corresponding differ ent values m get different affine subspaces correspond ing vector subspaces affine subspaces x y ax 0 x r. . click following link get visual representation affine subspaces r3. diagram get different planes different values a b c m. fix value a b c corresponding different values m get different affine subspaces corresponding vector subspaces affine subspaces x y z ax cz 0 x y z r. 8.1.4 addition scalar multiplication affine subspaces let l v u affine subspace. let l l two arbitrary elements l. hence l v u l v u vectors u u vector subspace u. define l l v u v u v u u cl cv u v cu example 8.1.2. consider affine subspace l r2 defined 0 1 u x 1 x r u x 0 x r. . addition let l1 0 1 x1 0 x1 1 l2 0 1 x2 0 x2 1. l1 l2 0 1 x1 0 0 1 x2 0 0 1 x1 x2 0 i.e. l1 l2 x1 1 x2 1 x1 x2 1 . scalar multiplication let l 0 1 x 0 x 1. cl c0 1 x 0 0 1 cx 0 0 1 cx 0 i.e. cl cx 1 cx 1
8.1. introduction 190 example 8.1.3. consider affine subspace l r3 defined l x y z x 2y z 5 x y z r x y z x 2y 5 z x y z r x y x 2y 5 x r 0 0 5 x y x 2y x r 0 0 5 x y x 2y x r 0 0 5 x y z x 2y z x y z r 0 0 5 x y z x 2y z 0 x y z r 0 0 5 u u x y z x 2y z 0 x y z r . addition let l1 l2 l. l1 x1 y1 z1 x1 y1 x1 2y1 5 0 0 5 x1 y1 x1 2y1 l2 x2 y2 z2 x2 y2 x2 2y2 5 0 0 5 x2 y2 x2 2y2 l1 l2 x1 y1 z1 x2 y2 z2 x1 y1 x1 2y1 5 x2 y2 x2 2y2 5 0 0 5 x1 y1 x1 2y1 0 0 5 x2 y2 x2 2y2 0 0 5 x1 x2 y1 y2 x1 2y1 x2 2y2 0 0 5 x1 x2 y1 y2 x1 x2 2y1 y2 x1 x2 y1 y2 x1 x2 2y1 y2 5 . scalar multiplication let l l. l x y z x y x 2y 5 0 0 5 x y x 2y cl cx y z cx y x 2y 5 c0 0 5 x y x 2y 0 0 5 cx y x 2y 0 0 5 cx cy cx 2y cx cy cx 2y 5
8.2. solution set system linear equations 191 8.2 solution set system linear equations let ax b system linear equations. case 1 b 0 case homogeneous system seen before solution set subspace rn namely null space a. case 2 b column space a case ax b solution solution set empty set. case 3 b column space a let v solution equation ax b u nullspacea. hence have av u av au b 0 b. v u also solution ax b. hence case solution set l affine subspace rn. specifically described l v nullspacea v solution equation ax b. 8.3 affine mappings affine subspaces let l l affine subspaces v w respectively. let f l l function. consider vector v l unique subspace u v l v u. note fv l hence l fv u u unique subspace w corresponding l. f affine mapping l l function g u u defined gu fv u fv linear transformation. let l1 v u1 l2 v u2 l. fl1 l2 fv u1 v u2 fv u1 u2 fv u1 u2 fv fv gu1 u2 fv using definition g gu1 gu2 fv since g linear transformation fv u1 fv fv u2 fv fv using definition g fl1 fl2 fv hence fl1 fl2 fv fl1 l2.
8.3. affine mappings affine subspaces 192 let l v u l. fcl fcv u fv cu fv cu fv fv gcu fv using definition g cgu fv since g linear transformation cfv u fv fv using definition g cfl fv fv cfl 1 cfv hence cfl c 1fv fcl. 8.3.1 affine mapping corresponding linear transformation linear transformation u u fixed vectors v l v l affine mapping f obtained defining fv u v tu fact every affine mapping obtained way. remark 8.3.1. let f arbitrary affine mapping l v u l vu. f affine mapping exist linear transformation g u u defined gu fv u fv. define fv v. hence affine map f l l obtained by fv u v gu g u u linear transformation. concludes fact every affine mapping obtained way mentioned above. example 8.3.1. let l x 1 x r l x y 5 x r. checking that l affine subspace r3 left reader. l 0 1 x 0 x r 0 1 u l 0 0 5 x y 0 x r 0 0 5 u u x 0 x r u x y 0 x r 0 0 5 u hence according notation used definition affine map v 0 1 define map f l l fx 1 x 2x 5. define g u u gu fv u fv i.e. gx 0 fx 1 f0 1 x 2x 5 0 0 5 x 2x 0. clearly g linear transformation u u hence f affine mapping l l. example 8.3.2. let f r3 r2 map defined by fx y z 2x 3y 2 4x 5y 3. define r3 r2 tx y z 2x 3y 4x 5y. clearly linear transformation fx y z 2 3tx y z. hence f affine mapping r3 r2.
8.3. affine mappings affine subspaces 193 8.3.2 exercises 1 let l affine subspace r3 defined l x y z x 2z 6 following subspaces r3 corresponds affine subspace l option 1 x y z x z 5 option 2 x y z x 2z 0 option 3 x y z x 2y z 0 option 4 x y z 2x z 0 2 following subsets r2 represent affine subspace r2 option 1 x y x2 y2 0 option 2 x y x2 y2 4 option 3 x y x5 1 y option 4 x y 3x 1 option 5 x y x 1 option 6 x y 5x2 option 7 x y x3 2 option 8 12 3 let u subspace vector space r3 basis u given 0 1 3 1 0 1. following subsets r3 affine subspace r3 corresponding vector subspace u option 1 l x y z x 3y z 3 option 2 l x y z x 3y 2z 0 option 3 l x y z x 3y z 0 option 4 l x y z x 3y z 8 4 consider system linear equations x z 1 x y z 1 x z 0 following affine subspace l r3 v l v solution system linear equations
8.3. affine mappings affine subspaces 194 option 1 t 1 t r option 2 t 1 t r option 3 t 1 t r option 4 t 1 t r 5 suppose f r2 r2 affine mapping f2 3 3 1f2 1 1 2 f1 0 0 1. f2 5 a b value b ans 9.
195
9.1. dot product two vectors euclidean space dimension 2 196 9. inner product space mathematics knows races geographical boundaries mathematics cultural world one country. david hilbert previous chapter studied notion vector space. also know rn finite n n forms vector space usual addition scalar multiplication fixed n. section going impose condition inner product vector space forms another space call inner product space. already know r2 r3 forms vector space usual addition scalar multiplication. mathematics data science 1 know find distance two points distance point origin. going study similar kind things inner product space. study inner product lets see notion 9.1 dot product two vectors euclidean space dimension 2 let v1 x1 y1 v2 x2 y2 two vectors form vector space r2 dot product two vectors scalar computed follows x1 y1 x2 y2 x1x2 y1y2 dot product sign two vectors denoted . example 9.1.1. find dot product two vectors 2 4 3 5 vector space r2.
9.1. dot product two vectors euclidean space dimension 2 197 2 4 3 5 2 3 4 5 6 20 26 9.1.1 length vector euclidean space dimension 2 let 3 4 vector r2 3 4 x y figure 9.1 using pythagoras theorem length vector 34 32 42 5 unit 9.1.2 relation length dot product euclidean space dimension 2 let 3 4 vector r2 found length vector 32 42 5 unit get using dot product follows length vector 3 4 p 3 4 3 4 32 42 5 generally length vector x y r2 p x2 y2 p x y x y
9.1. dot product two vectors euclidean space dimension 2 198 9.1.3 dot product angle two vectors euclidean space dimension 2 angle vectors u v measures far direction v u or vice versa. e.g. θ angle u 3 4 v 1 5 measured degree between 0 360 radians between 0 2π. θ 3 4 1 5 figure 9.2 let u v two vectors r2. compute angle θ vectors u u1 u2 v v1 v2 using dot products as cos θ u v p v v u u u1v1 u2v2 q v2 1 v2 2 u2 1 u2 2 i.e. θ cos1 u v p v v u u cos1 u1v1 u2v2 q v2 1 v2 2 u2 1 u2 2
9.2. dot product two vectors euclidean space dimension 3 199 9.2 dot product two vectors euclidean space dimension 3 consider vectors 1 2 3 2 0 1 r3. dot product two vectors gives us scalar follows 1 2 3 2 0 1 1 2 2 0 3 1 2 0 3 5 two general vectors x1 y1 z1 x2 y2 z2 r3 dot product two vectors scalar computed follows x1 y1 z1 x2 y2 z2 x1x2 y1y2 z1z2 9.2.1 length vector euclidean space dimension 3 let us find length vector 4 3 3 r3. z x 4 3 3 figure 9.3 using pythagoras theorem length 4 3 3 42 32 32 34 unit.
9.2. dot product two vectors euclidean space dimension 3 200 9.2.2 length dot product euclidean space dimension 3 done r2 similarly r3 dot product 4 3 3 4 3 3 42 33 32 hence length 4 3 3 expressed square root dot product vector itself. length vector 4 3 3 p 4 3 3 4 3 3 p 42 33 32 34 observe 4 3 3 4 3 3 42 32 32 hence lenght 433 expressed square root dot product vector self. length vector 4 3 3 p 4 3 3 4 3 3 42 32 32 34 generally length vector x y z r3 p x2 y2 z2 p x y z x y z 9.2.3 angle two vectors euclidean space dimension 3 dot product angle vectors u v r3 angle computed passing plane them. measures far direction v uor vice versa plane. let u v twp vectors r3. compute angle θ vectors u v using dot product as cos θ u v p u c v v θ cos1 u v p u c v v
9.3. dot product euclidean space dimension n length angle 201 example 9.2.1. find angle vectors 1 00 1 0 1. first find dot product 1 0 0 1 0 1 1 1 0 0 1 00 1 1 0 1 1 0 1 2 θ cos1 1 1 2 cos1 1 2 π 4 radian 45 9.3 dot product euclidean space dimension n length angle . definitions vector space rn n n i dot product let u u1 u2 . . . un v v1 v2 . . . vn rn dot product two vectors rn defined uv u1v1u2v2. . .unvn scalar. ii length vector let u u1 u2 . . . un rn length vector u u q u2 1 u2 2 . . . u2 n. iii relation dot product length vector u u1 u2 . . . un rn length vector u u u q u2 1 u2 2 . . . u2 n. iv angle two vectors rn let u u1 u2 . . . un v v1 v2 . . . vn rn angle θ two vectors rn given θ cos1 uv uuvv cos1 u1v1u2v2.unvn u2 1u2 2.u2 nv2 1 v2 2 .v2 n exercise question 120. u 1 0 1 v 0 1 1 two vectors vector space r3. angle u v . π 2 . π . 1 2 . π 3
9.3. dot product euclidean space dimension n length angle 202 question 121. consider vector u v w r3 dot products uv 1 uw 1 us 0 v 1 2 3 w 1 1 1 1 0 1. u a b c find value b c question 122. a 3 b 2 2 angle b 45 value a.b ans 6 question 123. given 3 b 5 b 7.5 two vectors b rn. find angle in degrees two vectors a b. note cos 60 12 cos 30 32. . option 1 30 . option 2 60 . option 3 120 . option 4 150 question 124. consider two vectors 3 4 1 b 2 1 2 r3. choose correct options. . option 1 lengthnorm 26 b 3. . option 2 lengthnorm 5 b 3. . option 3 lengthnorm 24 b 7. . option 4 angle two vectors 90 degrees. . option 5 angle two vectors approximately 0 degrees. question 125. dot product two vectors zero choose correct option. . . option 1 two vectors perpendicular other. . option 2 two vectors parallel other. . option 3 two vectors exactly same. . option 4 length two vectors must same. question 126. 6 1 3 b 4 c 2 a b r3 b perpendicular other find value c answer 30
9.3. dot product euclidean space dimension n length angle 203 question 127. consider two vectors 1 2 b 2 2 θ angle them. sum two vectors given c b. choose correct options. . option 1 lengthnorm c 5. . option 2 lengthnorm c 25. . option 3 lengthnorm c 4. . option 4 cos θ 3 10. . option 5 cos θ 3 5. question 128. consider 3 vectors a b c r3 scalar λ r. choose set correct options. note . represents dot product. . . option 1 λa b λa b . option 2 λa b λb . option 3 λa b λa λb . option 4 a c b b c b . option 5 a c b c c b . option 6 0 b b 0 a b null vectors. question 129. consider two vectors 1 3 5 7 9 b 2 4 6 8 10. choose set correct options. . option 1 length b a. . option 2 length b. . option 3 length a b 5. . option 4 length a b 50.
9.4. inner product vector space 204 9.4 inner product vector space seen sections geometrical concepts like angle be tween two vectors length vector e.t.c. vector spaces r2 r3 also help dot product connect geometrical concepts algebraically i.e. numerically trying find angle two vectors length vector e.t.c. . extend concepts general form i.e. extend concepts different vector spaces fact vector spaces using inner product. already aware concept functions vector spaces previous sections like linear transformation. inner product also function defined cartesian product vector space properties. lets see definition itself definition 9.4.1. inner product inner product vector space v function v v r satisfying following conditions let u v w v i v v 0 v v 0 v v 0 v 0. ii u v w u w v w. iii u v v u. iv cu v cu v note vector space together inner product is called inner product space. example 9.4.1. dot product example inner product recall dot product vector space rn let u u1 u2 . . . un v v1 v2 . . . vn rn dot product two vectors rn defined u v u1v1 u2v2 . . . unvn scalar. think dot product function . rn rn r defined u v u v lets verify conditions inner product.
9.4. inner product vector space 205 i v v 0 v v 0 v v 0 v 0. first assume v 0 v v v v v1v1 v2v2 . . . vnvn v2 1 v2 2 . . . v2 n since term v2 1 v2 2 . . . v2 n square non zero real number always grater 0. hence v v 0 assume v vector rn v v 0 v2 1 v2 2 . . . v2 n 0 v1 v2 . . . vn 0 v 0 . . . 0 i.e. v zero vector rn. conversely v zero vector rn i.e. v 0 . . . 0 0 . . . 0 0 . . . 0 0 0 . . . 0 0 first condition satisfied. ii u v w u w v w. let u u1 u2 . . . un v v1 v2 . . . vn w w1 w2 . . . wn rn u v w u v w u1 v1 u2 v2 . . . un vn w1 w2 . . . wn u1 v1w1 u2 v2w2 . . . un vnwn u1w1 v1w1 u2w2 v2w2 . . . unwn vnwn u1w1 u2w2 . . . unwn v1w1 v2w2 . . . vnwn u w v w second condition satisfied. iii u v v u. u v u v v u v u third condition satisfied.
9.5. norm vector space 206 iv cu v cu v cu v cu v cu v fourth condition satisfied. dot product inner product vector space rn. note inner product also called standard inner product rn. example 9.4.2. consider following function . . r2 r2 r u v u1v1 u1v2 u2v1 2u2v2 u u1 u2 v v1 v2 vectors r2. observe u v u1v1 u1v2 u2v1 2u2v2 u1 u2 1 1 1 2 v1 v2 easily check as checked dot product conditions inner product follow given function. given function inner product vector r2. hence r2 inner product space respect given inner product. 9.5 norm vector space coordinate system much familiar term distance two points. norm also function vector space usually used find length vector vector space i.e. distance vector form origin. definition 9.5.1. norm vector space v function v r x x satisfying following conditions i x yxy x v .
9.5. norm vector space 207 ii cx cx c r x v . iii x0 x v x0 x 0. example 9.5.1. length example norm vector space rn let u u1 u2 . . . un rn length vector u u q u2 1 u2 2 . . . u2 n. now check length function rn r actually norm rn. i u vuv u v rn. u u1 u2 . . . un v v1 v2 . . . vn u v u1 v1 u2 v2 . . . un vn therefore u v p u1 v12 u2 v22 . . . un vn2 q u2 1 v2 1 2u1v1 u2 2 v2 2 2u2v2 . . . u2 n v2 n 2unvn q u2 1 u2 2 . . . u2 n v2 1 v2 2 . . . v2 n 2u1v1 u2v2 . . . unvn q u2 1 u2 2 . . . u2 n q v2 1 v2 2 . . . v2 n using cauchyschwartz inequality uv ii cu cu c r u rn. have cu cu1 cu2 . . . cun q cu12 cu22 . . . cun2 c q u2 1 u2 2 . . . u2 n cu
9.6. norm induced inner product 208 iii u 0 u rn u 0 u 0. obvious length function positive square root u2 1 u2 2 . . .u2 n length vector zero vector vice versa. example 9.5.2. consider following function . rn r defined u u1u2 . . . un norm. 9.6 norm induced inner product theorem 9.6.1. let v inner product space inner product . .. function . v r defined v p v vis norm v . proof. let u vw v lets check first condition norm u v2 u v u v u v u u v v u u v u u v v v u u 2u v v v using cauchyschwartz inequality u22uvv2 uv2 hence u vuv lets check second condition norm cu p cu cu q c2u u c p u u hence cu cu lets check third condition
9.6. norm induced inner product 209 let u 0 u p u u which positive square root non zero real number always positive. let u 0 p u u 0 u u 0 v 0 hence proved. exercise question 130. consider function f v v r v r2 defined fv w 2v1w1 5v2w2 v v1 v2 w w1 w2. choose set correct options. . option 1 f satisfies symmetry condition inner product. . option 2 f satisfies bilinearity condition inner product. . option 3 f satisfies positivity condition inner product. . option 4 f inner product. . option 5 f inner product. question 131. consider function f v v r v r2 defined fv w v1w1 v1w2 v2w1 4v2w2 v v1 v2 w w1 w2. choose set correct options. . option 1 f satisfies symmetry condition inner product. . option 2 f satisfies bilinearity condition inner product. . option 3 f satisfies positivity condition inner product. . option 4 f inner product. . option 5 f inner product. question 132. consider two vectors 0.4 1.3 2.2 b 2 3 5 r3. choose set correct options. .
9.6. norm induced inner product 210 . option 1 two vectors satisfy triangle inequality given b b . . option 2 two vectors satisfy triangle inequality. . option 3 two vectors satisfy cauchyschwarz inequality given a b b . . option 4 two vectors satisfy cauchyschwarz inequality. question 133. consider function f v v r v r2 defined fv w v2 1w2 1 v1w2 2 v2 2w1 v v1 v2 w w1 w2. choose set correct options. . . option 1 f satisfies symmetry condition inner product. . option 2 f satisfies positivity condition inner product. . option 3 f inner product. . option 4 f inner product. question 134. consider vector a1 b1 c1 r3. are possible candidates norm . option 1 q a2 1 b2 1 c2 1 . option 2 a1 b1 . option 3 a1 b1 c1 . option 4 maxa1 b1 c1 . option 5 mina1 b1 c1 . option 5 maxa1 b1 c1 question 135. choose set correct statements. . option 1 v r2 function . . v v r defined x1 x2 y1 y2 5x1y1 8x2y2 6x1y2 6x2y1 inner product v . . option 2 v r2 function . . v v r defined x1 x2 y1 y2 x12y2 inner product v .
9.6. norm induced inner product 211 . option 3 v r2 function . . v v r defined x1 x2 y1 y2 3x1y1 5x2y2 inner product v . . option 4 v r2 function . . v v r defined x1 x2 y1 y2 3x1y1 5x2y2 inner product v .
9.7. orthogonality linear independence 212 9.7 orthogonality linear independence already studied find angle two vectors using dot product vector space rn also know dot product length vector special cases inner product norm rn. definition 9.7.1. orthogonal vector two vectors u v inner product space v said orthogonal u v 0. example 9.7.1. consider inner product space r2 inner product u v u1v1 u1v2 u2v1 2u2v2 u u1 u2 v v1 v2 vectors r2. lets check vectors 1 1 10 orthogonal each. 1 1 1 0 1 0 1 0 0 vectors 1 1 1 0 orthogonal other. definition 9.7.2. orthogonal set vectors orthogonal set vectors inner product space v set vectors whose elements mutually orthogonal. explicitly v1 v1 . . . vk v orthogonal set vectors vi vj 0 i j 1 2 . . . k j. example 9.7.2. consider set vectors 4 3 2 3 2 3 5 18 17. set orthogonal set vectors inner product space r3 respect dot product. 4 3 2 3 2 3 4 3 3 2 2 3 0 similarly 3 2 3 5 18 17 0 4 3 2 5 18 17 0 theorem 9.7.1. let v1 v2 . . . vk orthogonal set vectors inner product space v . v1 v2 . . . vk linearly independent set vectors proof. lets assume pk i1 civi 0 ci r 1 2 . . . k then pk i1 civi v1 0 v1 0
9.7. orthogonality linear independence 213 pk i1 civi v1 0 pk i1 civ1 v1 0 c1vi v1 0 since set orthogonal set c1 0 similarly find ci 0 2 . . . k hence orthogonal set linearly independent set. definition 9.7.3. orthogonal basis let v inner product space. basis con sisting mutually orthogonal vectors called orthogonal basis. know basis maximal linearly independent set orthogonal set vectors already linearly independent orthogonal set basis precisely maximal orthogonal set i.e. orthogonal set strictly contains set. note dimv n orthogonal basis nothing orthogonal set n vectors lets see example orthogonal basis i standard basis inner product space rn respect dot product. ii seen set 4 3 2 3 2 3 5 18 17 orthogonal subset r3 respect dot product dimr3 3 set orthogonal basis inner product r3. iii seen vectors 1 1 1 0 orthogonal vectors inner product space r2 inner product u v u1v1 u1v2 u2v1 2u2v2 u u1 u2 v v1 v2 vectors r2. set 1 1 1 0 orthogonal basis inner product space r3. definition 9.7.4. orthonormal set orthonormal set vectors inner product space v orthogonal set vectors norm vector set 1.
9.7. orthogonality linear independence 214 explicitly v1 v1 . . . vk v orthonormal set vectors vi vj 0 i j 1 2 . . . k j vi 1 1 2 . . . k definition 9.7.5. orthonormal basis orthonormal basis orthonormal set vectors form basis. note orthonormal basis orthogonal basis norm vector 1. note orthonormal basis maximal orthonormal set inner product space. example standard basis rn respect dot product forms or thonormal basis. example 9.7.3. consider r3 usual inner product i.e. dot product set 1 31 2 2 1 32 1 2 1 32 2 1. forms orthonormal basis r3. 1 31 2 2 1 31 2 2 1 31 2 2 1 similarly others vectors norm 1. now lets check orthogonality 1 31 2 2 1 32 1 2 0 vectors 1 31 2 2 1 32 1 2 orthogonal vectors. similarly check others. hence set orthonormal set dimr3 3 orthonormal basis r3. 9.7.1 obtaining orthonormal set orthogonal set γ v1 v2 . . . vn orthogonal set vectors obtain or thonormal set vectors β γ dividing vector norm i.e. β v1 v1 v2 v2 . . . vn vn .
9.7. orthogonality linear independence 215 example 9.7.4. consider r2 usual inner product orthogonal basis 1 3 3 1. s1 1 101 3 1 103 1 orthonormal basis r2. 9.7.2 importance orthonormal basis suppose v1 v2 . . . vn orthonormal basis inner product space v let v v . v written v c1v1 c2v2 . . . cnvn find c1 c2 . . . cn basis means writing system linear equation solving it. since orthonormal use inner product compute ci v vi 1 2 3 . . . n v vi c1v1 c2v2 . . . cnvn vi c1v1 vi c2v2 vi . . . cnvn vi civi vi civi2 ci exercise question 136. consider two vectors 2 0 3 0 8 b 3 2 2 4 0 r5. choose set correct options. . option 1 b orthogonal. . option 2 b orthogonal. . option 3 a b 0. . option 4 a b 77. question 137. choose set correct statements. . option 1 orthogonal set norms vectors equal. . option 2 orthogonal set vectors linearly independent.
9.7. orthogonality linear independence 216 . option 3 orthogonal set vectors linearly dependent. . option 4 columns n n coefficient matrix comprises indi vidual vectors orthogonal set rn must unique solution system ax b x b n 1 vectors. . option 5 columns nn coefficient matrix comprises individual vectors orthogonal set rn solutions system ax b x b n 1 vectors. . option 6 determinant square matrix formed set orthogonal vectors rn zero. . option 7 set n vectors never form orthogonal basis rn1. question 138. following orthogonal basis given vector spaces respect standard inner product dot product . option 1 1 0 0 1 orthogonal basis r2. . option 2 1 0 0 0 1 0 0 0 1 orthogonal basis r3. . option 3 3 4 4 3 2 3 orthogonal basis r2. . option 4 2 1 1 1 1 1 3 3 3 orthogonal basis r3. question 139. find vector r4 orthogonal subspace spanned 1 1 0 0 0 1 1 0 respect dot product inner product. . option 1 1 1 1 0 . option 2 2 3 4 5 . option 3 1 1 1 1 . option 4 1 1 1 0 question 140. following orthogonal vector 1 2 r2 respect inner product v w v1w1 v1w2 v2w1 4v2w2 . option 1 7 1 . option 2 6 1 . option 3 7 1 . option 4 9 1
9.7. orthogonality linear independence 217 . option 5 14 2 question 141. consider system linear equations x1 2x2 3x3 1 2x1 x2 5 3x1 6x2 5x3 9. number solution system ans 1 question 142. let coefficient matrix system linear equations x1 2x2 3x3 1 2x1 x2 5 3x1 6x2 5x3 9. one following true matrix aat . option 1 scalar matrix. . option 2 identity matrix. . option 3 diagonal matrix. . option 4 lower triangular matrix. . option 5 upper triangular matrix. . option 6 none above. question 143. sets form orthonormal basis r3 respect dot product inner product r3 . option 1 1 0 0 0 1 0 . option 2 2 0 0 0 2 0 0 0 2 . option 3 1 6 1 6 2 6 3 11 1 11 1 11 1 66 7 66 4 66 . option 4 2 1 1 1 1 1 1 2 2 . option 5 1 2 0 1 2 0 1 0 1 2 0 1 2 question 144. consider two orthogonal vectors a b r2. b b orthogonal choose correct option.
9.7. orthogonality linear independence 218 . option 1 b 1 . option 2 b . option 3 2 b . option 4 2 b question 145. consider 1 1 b 1 1. let v spana b. choose correct options considering standard inner product dot product. . . option 1 vectors a b form orthogonal basis v . . option 2 vectors a b form orthonormal basis v . . option 3 exist scalar multiples a b form orthonormal basis . option 4 exist scalar multiples a b form orthonormal basis question 146. choose set correct statements. . option 1 determinant matrix formed 3 orthonormal vectors r3 1. . option 2 determinant matrix formed 3 orthonormal vectors r3 0. . option 3 determinant matrix formed 2 orthonormal vectors r2 1. . option 4 determinant matrix formed 2 orthonormal vectors r2 0. question 147. consider system linear equations 2x1 2x2 7x3 b1 2x1 x2 10x3 b2 3x1 2x2 2x3 b3. let coefficient matrix given system linear equations. let matrix b contain column vectors a normalized respective norms columns i.e. first column vector normalized norm first column b. following statements true
9.7. orthogonality linear independence 219 . option 1 determinant bbt 1. . option 2 bbt identity matrix. . option 3 bbt scalar matrix. . option 4 bbt diagonal matrix. question 148. choose correct options. . option 1 vectors orthonormal set linearly independent. . option 2 set linearly dependent vectors orthonormal. . option 3 set linearly independent vectors always orthonormal. . option 4 set linearly independent vectors always orthogonal orthonormal. . option 5 vectors orthogonal set linearly independent. consider inner product u v 3u1v12u2v2 u u1 u2 v v1 v2 vectors r2. answer following questions based information. question 149. following orthonormal basis respect inner product defined above . option 1 2 3 4 4 . option 2 1 302 3 1 804 4 . option 3 1 132 3 1 324 4 . option 4 2 3 3 2 . option 5 1 132 3 1 133 2 question 150. use orthonormal basis u v obtained question 7 respect defined inner product. express vector 4 0 linear combination basis vectors u v 4 0 c1u c2v. following gives coefficients linear combination . option 1 c1 24 30 c2 48 80 . option 2 c1 24 13 c2 48 32 . option 3 c1 8 13 c2 16 32 . option 4 c1 24 30 c2 48 80
9.8. projections using inner products 220 9.8 projections using inner products section shall discuss projections. 9.8.1 projection vector along another vector let b two points r2. suppose want find point nearest b line l passing origin. figure 9.4 already know nearest point foot perpendicular drawn point b line l. section shall try solve problem perspective vectors using concept inner products. this shall draw vectors corresponding points b. figure 9.5 want find length vector v foot perpendicular b line l. know length v determine vector v. because v αa since v lies along vector a. hence v αa. get value α v a. thus v v aa. note also written
9.8. projections using inner products 221 v v a is find unit vector direction multiply length vector v. since vector know remains find a. know v right angled triangle. v bcos θ θ angle two vectors b know cos θ earlier section a b a b. thus v b a b a b a a b a2 a b a aa. so know vectors b find point along vector nearest vector b. vector v called projection vector b along vector a. similarly talk shortest distances r3. suppose b point whose shortest distance plane generated 1 0 0 0 1 0 needs calculated. figure 9.6 use exact idea case r2. instead single vector a plane namely xyplane r3. shall see projections general vector space case r3 particular case general vector space. 9.8.2 projection vector onto subspace let v inner product space v v w v subspace. projection v onto w vector w denoted projw v computed follows . find orthonormal basis v1 v2 . . . vn w . . define projw v pn i1v vivi. note vi 1 i hence dividing norm. observe v1 v2 . vn randomly chosen orthonormal basis w hence projw v independent chosen orthonormal basis. projection vector onto subspace change choice basis as expected. projection vector v onto subspace w nearest vector v w v satisfies v vv w w w .
9.8. projections using inner products 222 example 9.8.1. let v r2 w line x. projection 1 2 onto w 1 2 1 1 1 1 1 11 1 3 2 3 2. done taking vector along line x instead 1 1. also solve problem finding orthonormal basis w using formula projection. orthonormal basis w 1 21 1. note w onedimensional subspace like one given example projwv projw v w subspace generated vector w. projection vector v along another vector w also visualised projection v onto subspace generated w. example 9.8.2. let v r3 w x y z xy 0. clearly 1 1 0 0 0 1 basis w . also orthogonal basis. convert or thonormal basis dividing vector norm. thus 1 21 1 0 0 0 1 orthonormal basis w . projection 2 2 0 think projection 2 1 3 this use orthonormal basis w obtained earlier. projw 2 1 3 2 1 3 1 2 1 1 01 2 1 1 0 2 1 3 0 0 10 0 1 1 2 1 2 1 1 0 30 0 1 1 2 1 2 3. observe projw v lies w for obvious reasons. suppose project vector v onto subspace w orthogonal basis given instead orthonormal basis convert orthogonal basis orthonormal one illustrated previous example. 9.8.3 projection linear transformation let v inner product space w subspace. define v v defined tv projw v v v . question 151. . verify linear transformation. . range t . rank t
9.8. projections using inner products 223 . kernel t . tv v w linear transformation called projection map v w denoted pw . note pw sends vectors v onto w hence range pw w . also rank pw dimension w since rpw w . now coming kernel pw . suppose v1 v2 . vn orthonormal basis w . kernel pw vectors satisfy pw v projw v pn i1v vivi 0. realise looking vectors v orthogonal vis. set w v v v w 0 w w called orthogonal complement w kernel pw precisely w . suppose v w pw v v why. this easy conclude p2 w pw . also pw vv why. projection vector cannot longer vector itself. example 9.8.3. consider inner product a b a1b1 a1b2 a2b1 4a2b2 a1 a2 b b1 b2 vectors r2. 1. let x 1 2. find projection x direction 3 4 using inner product defined above. proj341 2 1 2 3 4 3 4 3 43 4 31 32 41 442 33 34 43 4443 43 4 25 493 4 2. let x 1 2 find projection x direction perpendicular 3 4. direction v1 v2 perpendicular 3 4 got solving v1 v2 3 4 0. thus 3v14v13v216v2 0 13v2 v1. direction perpendicular 3 4 13 1. proj1311 2 got similar fashion. orthogonal basis find orthonormal basis. given basis orthogonal basis procedure convert given basis orthonormal basis called gramschmidt orthonormalization process discussed next section.
9.8. projections using inner products 224 9.8.4 exercises question 152. consider orthonormal basis 1 0 0 0 1 0 subspace w r3. x 1 2 3 vector r3 following represents vector w whose distance x least consider dot product standard inner product. . option 1 2 4 0 . option 2 3 4 0 . option 3 4 5 0 . option 4 1 2 0 question 153. suppose w1 w2 subspaces vector space v . let pw1 pw2 denote projection v w1 v w2 respectively consider fol lowing statements . statement p pw1 pw2 projection v w1 w2 pw1 pw2 pw2 pw1 0. . statement q p2 w1 pw1 pw1 projection v w1. . statement r matrix representation pw2 symmetric matrix. . statement s pw1 pw2 projection v w1 w2. find number correct statements ans 1 question 154. consider orthogonal basis 1 2 1 2 0 2 subspace w inner product space r3 respect dot product. 1 2 3 r3 find projw y. . option 1 1 3 8 3 7 3 . option 2 18 6 32 8 36 8 18 6 32 8 . option 3 1 3 8 3 7 3 . option 4 18 6 32 8 36 8 18 6 32 8
9.9. gramschmidt orthonormalization 225 9.9 gramschmidt orthonormalization already mentioned inner product space gramschmidt process con verts basis orthonormal one. let us begin example understand idea behind process. example 9.9.1. consider basis u1 u2 u3 1 2 2 1 0 2 0 0 1 r3 usual inner product. note basis orthonormal basis 1 2 2 1 0 2 3 0 thus set even orthogonal. first convert basis orthogonal basis divide vector norm get orthonormal basis. first let v1 u11 2 2. now want consider vectors orthogonal v1 vectors v1. this use projection pv1 v2. define v2 u2 pv1u2. thus get v2 1 0 2 pv11 0 2 1 0 2 1 0 2 1 2 2 1 2 2 1 2 2 1 2 2 1 0 2 3 91 2 2 4 3 2 3 4 3 note that stage v1 v2 0. note way defined v2 made sure v1 v2 0. v1 v2 v1 u2 pv1u2 v1 u2 v1 u2 v1 v1 v1 v1 u2v1 u2 v1 v1v1 v1 v1 u2v1 u2 0 already stated w is null space pw . here using vector w namely i pw v. clearly pw i pw v 0 v. next step want v3 orthogonal complement span v1 v2 set span v1 v2. is want v3 orthogonal v1
9.9. gramschmidt orthonormalization 226 v2. define v3 u3 pv1u3 pv2u3. v3 0 0 1 pv10 0 1 pv20 0 1 0 0 1 0 0 1 1 2 2 1 2 2 1 2 21 2 2 0 0 1 4 3 2 3 4 3 4 3 2 3 4 3 4 3 2 3 4 34 3 2 3 4 3 0 0 1 2 91 2 2 4 3 36 9 4 3 2 3 4 3 0 0 1 2 9 4 9 4 9 4 9 2 9 4 9 2 9 2 9 1 9 verify v1 v2 v3 forms orthogonal set hence linearly independent hence basis r3. this divide vectors norm construct orthonormal basis. 9.9.1 gramschmidt process let γ v1 v2 . . . vn given ordered basis vector space. steps process described yields orthonormal basis β u1 u2 . . . un. step1 w1 v1 u1 w1 w1. step2 w2 v2 v2 u1u1 u2 w2 w2. step3 w3 v3 v3 u1u1 v3 u2u2 u3 w3 w3. . . . stepi wi vi pi1 j1vi ujuj ui wi wi. . . . stepn wn vn pn1 j1 vn ujuj un wn wn. note w1 w2 . wn orthogonal basis. stage span v1 v2 . vi span u1 u2 . ui span w1 w2 . wi. process conclude section important theorem finite dimensional inner product spaces. theorem 9.9.1. finite dimensional inner product space orthonormal basis.
9.10. orthogonal transformations rotations 227 9.9.2 exercises question 155. let w subspace inner product space r4 respect dot product v1 v2 v3 ordered basis w v1 1 1 1 1 v2 1 1 1 0 v3 1 1 0 0. orthonormal basis w obtained using gramschmidt process . option 1 1 21 1 1 1 1 2 31 1 1 3 1 61 1 2 0 . option 2 1 21 1 1 1 1 2 31 1 1 3 1 613 13 23 0 . option 3 1 21 1 1 1 1 2 31 1 1 3 1 63 1 2 0 . option 4 1 21 1 1 1 1 2 31 1 1 3 1 63 1 2 2 question 156. let 2 3 2 3 1 3 vector inner product space r3 respect dot product w x y z r3 x y z 2 3 2 3 1 3 0 subspace r3. following are basis w . option 1 1 0 2 0 1 2 . option 2 1 0 2 0 1 2 . option 3 1 0 2 0 1 2 . option 4 1 0 2 0 1 2 question 157. consider set w 1 1 1 inner product space r3. find dimension subspace w w is collection vectors orthogonal vector 111. ans 2 9.10 orthogonal transformations rotations 9.10.1 orthogonal transformations let v inner product space linear transformation v v . said orthogonal transformation tv tw v w v w v . is orthogonal transformation preserves inner product. inner product space consideration rn dot product inner product orthogonal transformation preserves lengths angles. why
9.10. orthogonal transformations rotations 228 example 9.10.1. consider inner product space r2 dot product. let r2 r2 defined tx y xy 2 xy 2 . shall show orthogonal linear transformation. clear linear transformation is it. show orthogonal need show tx1 y1 tx2 y2 x1 y1 x2 y2x1 y1 x2 y2 r2. tx1 y1 tx2 y2 x1 y1 2 x1 y1 2 x2 y2 2 x2 y2 2 x1 y1 2 x2 y2 2 x1 y1 2 x2 y2 2 1 2x1x2 x1y2 y1x2 y1y2 x1x2 x1y2 y1x2 y1y2 x1x2 y1y2 x1 y1 x2 y2 thus orthogonal transformation. example 9.10.2. consider inner product spaces r3 r2 dot product. let r3 r2 defined tx y z x z x y. shown orthogonal transformation. t1 0 0 t0 1 0 1 1 1 1 2 1 0 0 0 1 0 0 since t1 0 0 t0 1 0 1 0 0 0 1 0 orthogonal transforma tion. 9.10.2 rotation matrices rotation matrices rotate vector angle θ. let us calculate rotation matrix r2. consider standard basis r2 1 0 0 1. rotate plane angle θ. matrix linear transformation got vectors obtained rotation.
9.10. orthogonal transformations rotations 229 figure 9.7 see vector 1 0 gets rotated vector makes angle θ positive xaxis vector cos θ sin θ. similarly vector 0 1 gets rotated vector makes angle π 2 θ positive xaxis vector cosπ 2 θ sin π 2 θ nothing sin θ cos θ. easy get matrix linear transformation information hand. t1 0 cos θ sin θ cos θ1 0 sin θ0 1 t0 1 sin θ cos θ sin θ1 0 cos θ0 1 thus matrix linear transformation cos θ sin θ sin θ cos θ . transfor mation denoted rθ matrix rθ rotation matrix rotation angle θ r2. observe rt θ rθ rt θ rθ rθrt θ i. also rotated vectors also length one orthogonal hence form orthonormal basis r2. r3 get different rotation matrices based axis rotation. consider rotations axes r3. since preserve lengths angles orthogonal transformations. again respect standard ordered basis rotation matrix corre sponding rotation zaxis angle θ got keeping 0 0 1 exactly rotate 1 0 0 0 1 0 fashion similar one r2. thus matrix obtained denoted t3θ cos θ sin θ 0 sin θ cos θ 0 0 0 1 . similarly get matrix corresponding rotation xaxis rotation happens yzplane. t1θ 1 0 0 0 cos θ sin θ 0 sin θ cos θ . again get matrix corresponding rotation yaxis
9.10. orthogonal transformations rotations 230 rotation happens xzplane. t2θ cos θ 0 sin θ 0 1 0 sin θ 0 cos θ . 9.10.3 orthogonal matrices suppose matrix orthogonal linear transformation v v respect orthonormal basis v . satisfies aat i. matrix satisfying aat called orthogonal matrix. note rows matrix orthonormal aat columns matrix orthonormal i. thus orthogonal matrix rows form orthonormal set columns form orthonormal set. 9.10.4 exercises question 158. choose correct options. . option 1 let r2 r2 linear transformation r2 inner product space respect dot product. tu tv u v. . option 2 let r2 r2 orthogonal linear transformation r2 inner product space respect dot product. tu tv u v. . option 3 let r2 r2 linear transformation r2 inner product space respect inner product given a b 2a1b1 5a2b2. tu tv u v. . option 4 let r2 r2 orthogonal linear transformation r2 inner product space respect inner product given a b 2a1b1 5a2b2. tu tv u v. question 159. following options isare true . option 1 let b orthogonal matrices order 3. b equivalent matrices. . option 2 let b orthogonal matrices. b similar matrices. . option 3 let rotation matrix corresponding anticlock wise rotation xyplane zaxis angle θ. nullity matrix 0.
9.10. orthogonal transformations rotations 231 . option 4 let rotation matrix corresponding anticlock wise rotation xyplane zaxis angle θ b rotation matrix corresponding anti clock wise rotation yzplane x axis angle β. b equivalent matrices. question 160. matrix representation orthogonal transformation rn rn choose set correct statements. . option 1 column vectors orthogonal orthonormal. . option 2 column vectors orthonormal. . option 3 row vectors orthogonal orthonormal. . option 4 row vectors orthonormal. question 161. following matrices are orthogonal . option 1 1 0 0 1 . . option 2 1 2 1 1 1 1 . . option 3 1 0 1 2 1 2 1 2 1 . . option 4 1 2 1 0 1 0 2 0 1 0 1 .
mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong

contents foreword 1 part mathematical foundations 9 1 introduction motivation 11 1.1 finding words intuitions 12 1.2 two ways read book 13 1.3 exercises feedback 16 2 linear algebra 17 2.1 systems linear equations 19 2.2 matrices 22 2.3 solving systems linear equations 27 2.4 vector spaces 35 2.5 linear independence 40 2.6 basis rank 44 2.7 linear mappings 48 2.8 affine spaces 61 2.9 reading 63 exercises 64 3 analytic geometry 70 3.1 norms 71 3.2 inner products 72 3.3 lengths distances 75 3.4 angles orthogonality 76 3.5 orthonormal basis 78 3.6 orthogonal complement 79 3.7 inner product functions 80 3.8 orthogonal projections 81 3.9 rotations 91 3.10 reading 94 exercises 96 4 matrix decompositions 98 4.1 determinant trace 99 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.
ii contents 4.2 eigenvalues eigenvectors 105 4.3 cholesky decomposition 114 4.4 eigendecomposition diagonalization 115 4.5 singular value decomposition 119 4.6 matrix approximation 129 4.7 matrix phylogeny 134 4.8 reading 135 exercises 137 5 vector calculus 139 5.1 differentiation univariate functions 141 5.2 partial differentiation gradients 146 5.3 gradients vectorvalued functions 149 5.4 gradients matrices 155 5.5 useful identities computing gradients 158 5.6 backpropagation automatic differentiation 159 5.7 higherorder derivatives 164 5.8 linearization multivariate taylor series 165 5.9 reading 170 exercises 170 6 probability distributions 172 6.1 construction probability space 172 6.2 discrete continuous probabilities 178 6.3 sum rule product rule bayes theorem 183 6.4 summary statistics independence 186 6.5 gaussian distribution 197 6.6 conjugacy exponential family 205 6.7 change variablesinverse transform 214 6.8 reading 221 exercises 221 7 continuous optimization 225 7.1 optimization using gradient descent 227 7.2 constrained optimization lagrange multipliers 233 7.3 convex optimization 236 7.4 reading 246 exercises 247 part ii central machine learning problems 249 8 models meet data 251 8.1 data models learning 251 8.2 empirical risk minimization 258 8.3 parameter estimation 265 8.4 probabilistic modeling inference 272 8.5 directed graphical models 278 draft 20230215 mathematics machine learning. feedback
contents iii 8.6 model selection 283 9 linear regression 289 9.1 problem formulation 291 9.2 parameter estimation 292 9.3 bayesian linear regression 303 9.4 maximum likelihood orthogonal projection 313 9.5 reading 315 10 dimensionality reduction principal component analysis 317 10.1 problem setting 318 10.2 maximum variance perspective 320 10.3 projection perspective 325 10.4 eigenvector computation lowrank approximations 333 10.5 pca high dimensions 335 10.6 key steps pca practice 336 10.7 latent variable perspective 339 10.8 reading 343 11 density estimation gaussian mixture models 348 11.1 gaussian mixture model 349 11.2 parameter learning via maximum likelihood 350 11.3 em algorithm 360 11.4 latentvariable perspective 363 11.5 reading 368 12 classification support vector machines 370 12.1 separating hyperplanes 372 12.2 primal support vector machine 374 12.3 dual support vector machine 383 12.4 kernels 388 12.5 numerical solution 390 12.6 reading 392 references 395 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.

foreword machine learning latest long line attempts distill human knowledge reasoning form suitable constructing ma chines engineering automated systems. machine learning becomes ubiquitous software packages become easier use nat ural desirable lowlevel technical details abstracted away hidden practitioner. however brings danger practitioner becomes unaware design decisions and hence limits machine learning algorithms. enthusiastic practitioner interested learn magic behind successful machine learning algorithms currently faces daunting set prerequisite knowledge programming languages data analysis tools largescale computation associated frameworks mathematics statistics machine learning builds universities introductory courses machine learning tend spend early parts course covering prerequisites. histori cal reasons courses machine learning tend taught computer science department students often trained first two areas knowledge much mathematics statistics. current machine learning textbooks primarily focus machine learn ing algorithms methodologies assume reader com petent mathematics statistics. therefore books spend one two chapters background mathematics either beginning book appendices. found many people want delve foundations basic machine learning methods strug gle mathematical knowledge required read machine learning textbook. taught undergraduate graduate courses universi ties find gap high school mathematics math ematics level required read standard machine learning textbook big many people. book brings mathematical foundations basic machine learn ing concepts fore collects information single place skills gap narrowed even closed. 1 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.
2 foreword another book machine learning machine learning builds upon language mathematics express concepts seem intuitively obvious surprisingly difficult formalize. formalized properly gain insights task want solve. one common complaint students mathematics around globe topics covered seem little relevance practical problems. believe machine learning obvious direct motivation people learn mathematics. book intended guidebook vast mathematical lit erature forms foundations modern machine learning. mo math linked popular mind phobia anxiety. youd think were discussing spiders. strogatz 2014 page 281 tivate need mathematical concepts directly pointing usefulness context fundamental machine learning problems. interest keeping book short many details advanced concepts left out. equipped basic concepts presented here fit larger context machine learning reader find numerous resources study provide end respective chapters. readers mathematical back ground book provides brief precisely stated glimpse machine learning. contrast books focus methods models machine learning mackay 2003 bishop 2006 alpaydin 2010 bar ber 2012 murphy 2012 shalevshwartz bendavid 2014 rogers girolami 2016 programmatic aspects machine learning m uller guido 2016 raschka mirjalili 2017 chollet allaire 2018 provide four representative examples machine learning algo rithms. instead focus mathematical concepts behind models themselves. hope readers able gain deeper understand ing basic questions machine learning connect practical ques tions arising use machine learning fundamental choices mathematical model. aim write classical machine learning book. instead intention provide mathematical background applied four cen tral machine learning problems make easier read machine learning textbooks. target audience applications machine learning become widespread society believe everybody understanding underlying principles. book written academic mathematical style enables us precise concepts behind machine learning. encourage readers unfamiliar seemingly terse style persevere keep goals topic mind. sprinkle comments remarks throughout text hope provides useful guidance respect big picture. book assumes reader mathematical knowledge commonly draft 20230215 mathematics machine learning. feedback
foreword 3 covered high school mathematics physics. example reader seen derivatives integrals before geometric vectors two three dimensions. starting there generalize con cepts. therefore target audience book includes undergraduate university students evening learners learners participating online machine learning courses. analogy music three types interaction people machine learning astute listener democratization machine learning pro vision opensource software online tutorials cloudbased tools al lows users worry specifics pipelines. users focus extracting insights data using offtheshelf tools. enables non techsavvy domain experts benefit machine learning. sim ilar listening music user able choose discern different types machine learning benefits it. experi enced users like music critics asking important questions application machine learning society ethics fairness pri vacy individual. hope book provides foundation thinking certification risk management machine learning systems allows use domain expertise build better machine learning systems. experienced artist skilled practitioners machine learning plug play different tools libraries analysis pipeline. stereo typical practitioner would data scientist engineer understands machine learning interfaces use cases able perform wonderful feats prediction data. similar virtuoso play ing music highly skilled practitioners bring existing instru ments life bring enjoyment audience. using mathe matics presented primer practitioners would able under stand benefits limits favorite method extend generalize existing machine learning algorithms. hope book provides impetus rigorous principled development machine learning methods. fledgling composer machine learning applied new domains developers machine learning need develop new methods extend existing algorithms. often researchers need understand mathematical basis machine learning uncover relationships be tween different tasks. similar composers music who within rules structure musical theory create new amazing pieces. hope book provides highlevel overview technical books people want become composers machine learning. great need society new researchers able propose explore novel approaches attacking many challenges learning data. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
4 foreword acknowledgments grateful many people looked early drafts book suffered painful expositions concepts. tried imple ment ideas vehemently disagree with. would like especially acknowledge christfried webers careful reading many parts book detailed suggestions structure presentation. many friends colleagues also kind enough provide time energy different versions chapter. lucky benefit generosity online commu nity suggested improvements via greatly improved book. following people found bugs proposed clarifications sug gested relevant literature either via personal communication. names sorted alphabetically. abdulganiy usman adam gaier adele jackson aditya menon alasdair tran aleksandar krnjaic alexander makrigiorgos alfredo canziani ali shafti amr khalifa andrew tanggara angus gruen antal a. buss antoine toisoul le cann areg sarvazyan artem artemev artyom stepanov bill kromydas bob williamson boon ping lim chao qu cheng li chris sherlock christopher gray daniel mcnamara daniel wood darren siegel david johnston dawei chen ellen broad fengkuangtian zhu fiona condon georgios theodorou xin irene raissa kameni jakub nabaglo james hensman jamie liu jean kaddour jeanpaul ebejer jerry qiang jitesh sindhare john lloyd jonas ngnawe jon martin justin hsi kai arulkumaran kamil dreczkowski lily wang lionel tondji ngoupeyou lydia kn ufing mahmoud aslan mark hartenstein mark van der wilk markus hegland martin hewing matthew alger matthew lee draft 20230215 mathematics machine learning. feedback
foreword 5 maximus mccann mengyan zhang michael bennett michael pedersen minjeong shin mohammad malekzadeh naveen kumar nico montali oscar armas patrick henriksen patrick wieschollek pattarawat chormai paul kelly petros christodoulou piotr januszewski pranav subramani quyu kong ragib zaman rui zhang ryanrhys griffiths salomon kabongo samuel ogunmola sandeep mavadia sarvesh nikumbh sebastian raschka senanayak sesh kumar karri seungheon baek shahbaz chaudhary shakir mohamed shawn berry sheikh abdul raheem ali sheng xue sridhar thiagarajan syed nouman hasany szymon brych thomas b uhler timur sharapov tom melamed vincent adam vincent dutordoir vu minh wasim aftab wen zhi wojciech stokowiec xiaonan chong xiaowei zhang yazhou hao yicheng luo young lee yu lu yun cheng yuxiao huang zac cranko zijian cao zoe nolan contributors github whose real names listed github profile are samdatamad bumptiousmonkey idoamihai deepakiim insad horizonp csmaillist kudo23 empet victorbigand 17skye jessjing1995 also grateful parameswaran raman many anony mous reviewers organized cambridge university press read one chapters earlier versions manuscript provided con structive criticism led considerable improvements. special men tion goes dinesh singh negi l ex support detailed prompt advice l exrelated issues. last least grateful editor lauren cowles patiently guiding us gestation process book. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
6 foreword table symbols symbol typical meaning a b c α β γ scalars lowercase x y z vectors bold lowercase a b c matrices bold uppercase x a transpose vector matrix a1 inverse matrix x y inner product x xy dot product x b b1 b2 b3 ordered tuple b b1 b2 b3 matrix column vectors stacked horizontally b b1 b2 b3 set vectors unordered z n integers natural numbers respectively r c real complex numbers respectively rn ndimensional vector space real numbers x universal quantifier x x existential quantifier exists x b defined b b b defined b proportional b i.e. constant b g f function composition g f implies a c sets a element set empty set ab without b set elements b number dimensions indexed 1 . . . n number data points indexed n 1 . . . n im identity matrix size 0mn matrix zeros size n 1mn matrix ones size n ei standardcanonical vector where component 1 dim dimensionality vector space rka rank matrix imφ image linear mapping φ kerφ kernel null space linear mapping φ spanb1 span generating set b1 tra trace deta determinant absolute value determinant depending context norm euclidean unless specified λ eigenvalue lagrange multiplier eλ eigenspace corresponding eigenvalue λ draft 20230215 mathematics machine learning. feedback
foreword 7 symbol typical meaning x y vectors x orthogonal v vector space v orthogonal complement vector space v pn n1 xn sum xn x1 . . . xn qn n1 xn product xn x1 . . . xn θ parameter vector f x partial derivative f respect x df dx total derivative f respect x gradient f minx fx smallest function value f xarg minx fx value xthat minimizes f note arg min returns set values l lagrangian l negative loglikelihood n k binomial coefficient n choose k vxx variance x respect random variable x exx expectation x respect random variable x covxy x y covariance x y. x y z x conditionally independent given z x p random variable x distributed according p n µ σ gaussian distribution mean µ covariance σ berµ bernoulli distribution parameter µ binn µ binomial distribution parameters n µ betaα β beta distribution parameters α β table abbreviations acronyms acronym meaning e.g. exempli gratia latin example gmm gaussian mixture model i.e. id est latin means i.i.d. independent identically distributed map maximum posteriori mle maximum likelihood estimationestimator onb orthonormal basis pca principal component analysis ppca probabilistic principal component analysis ref rowechelon form spd symmetric positive definite svm support vector machine 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.

part mathematical foundations 9 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.

1 introduction motivation machine learning designing algorithms automatically extract valuable information data. emphasis automatic i.e. machine learning concerned generalpurpose methodologies applied many datasets producing something mean ingful. three concepts core machine learning data model learning. since machine learning inherently data driven data core data machine learning. goal machine learning design general purpose methodologies extract valuable patterns data ideally without much domainspecific expertise. example given large corpus documents e.g. books many libraries machine learning methods used automatically find relevant topics shared across documents hoffman et al. 2010. achieve goal design mod els typically related process generates data similar model dataset given. example regression setting model would describe function maps inputs realvalued outputs. paraphrase mitchell 1997 model said learn data per formance given task improves data taken account. goal find good models generalize well yet unseen data may care future. learning understood learning way automatically find patterns structure data optimizing parameters model. machine learning seen many success stories software readily available design train rich flexible machine learning systems believe mathematical foundations machine learn ing important order understand fundamental principles upon complicated machine learning systems built. understand ing principles facilitate creating new machine learning solutions understanding debugging existing approaches learning inherent assumptions limitations methodologies work ing with. 11 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.
12 introduction motivation 1.1 finding words intuitions challenge face regularly machine learning concepts words slippery particular component machine learning system abstracted different mathematical concepts. example word algorithm used least two different senses con text machine learning. first sense use phrase machine learning algorithm mean system makes predictions based in put data. refer algorithms predictors. second sense predictor use exact phrase machine learning algorithm mean system adapts internal parameters predictor performs well future unseen input data. refer adapta tion training system. training book resolve issue ambiguity want high light upfront that depending context expressions mean different things. however attempt make context suffi ciently clear reduce level ambiguity. first part book introduces mathematical concepts foundations needed talk three main components machine learning system data models learning. briefly outline components here revisit chapter 8 discussed necessary mathematical concepts. data numerical often useful consider data number format. book assume data already appropriately converted numerical representation suitable read ing computer program. therefore think data vectors. data vectors another illustration subtle words are at least three different ways think vectors vector array numbers a computer science view vector arrow direction magni tude a physics view vector object obeys addition scaling a mathematical view. model typically used describe process generating data sim model ilar dataset hand. therefore good models also thought simplified versions real unknown datagenerating process capturing aspects relevant modeling data extracting hidden patterns it. good model used predict would happen real world without performing realworld experi ments. come crux matter learning component learning machine learning. assume given dataset suitable model. training model means use data available optimize pa rameters model respect utility function evaluates well model predicts training data. training methods thought approach analogous climbing hill reach peak. analogy peak hill corresponds maximum draft 20230215 mathematics machine learning. feedback
1.2 two ways read book 13 desired performance measure. however practice interested model perform well unseen data. performing well data already seen training data may mean found good way memorize data. however may generalize well unseen data and practical applications often need expose machine learning system situations encountered before. let us summarize main concepts machine learning cover book represent data vectors. choose appropriate model either using probabilistic opti mization view. learn available data using numerical optimization methods aim model performs well data used training. 1.2 two ways read book consider two strategies understanding mathematics machine learning bottomup building concepts foundational ad vanced. often preferred approach technical fields mathematics. strategy advantage reader times able rely previously learned concepts. unfor tunately practitioner many foundational concepts particularly interesting themselves lack motivation means foundational definitions quickly forgotten. topdown drilling practical needs basic require ments. goaldriven approach advantage readers know times need work particular concept clear path required knowledge. downside strat egy knowledge built potentially shaky foundations readers remember set words way understanding. decided write book modular way separate foundational mathematical concepts applications book read ways. book split two parts part lays math ematical foundations part ii applies concepts part set fundamental machine learning problems form four pillars machine learning illustrated figure 1.2 regression dimensionality reduction density estimation classification. chapters part mostly build upon previous ones possible skip chapter work backward necessary. chapters part ii loosely coupled read order. many pointers forward backward 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
14 introduction motivation figure 1.2 foundations four pillars machine learning. classiﬁcation density estimation regression dimensionality reduction machine learning vector calculus probability distributions optimization analytic geometry matrix decomposition linear algebra two parts book link mathematical concepts machine learning algorithms. course two ways read book. readers learn using combination topdown bottomup approaches some times building basic mathematical skills attempting com plex concepts also choosing topics based applications machine learning. part mathematics four pillars machine learning cover book see figure 1.2 require solid mathematical foundation laid part i. represent numerical data vectors represent table data matrix. study vectors matrices called linear algebra introduce chapter 2. collection vectors matrix linear algebra also described there. given two vectors representing two objects real world want make statements similarity. idea vectors similar predicted similar outputs machine learning algorithm our predictor. formalize idea similarity be tween vectors need introduce operations take two vectors input return numerical value representing similarity. con struction similarity distances central analytic geometry analytic geometry discussed chapter 3. chapter 4 introduce fundamental concepts matri ces matrix decomposition. operations matrices extremely matrix decomposition useful machine learning allow intuitive interpretation data efficient learning. often consider data noisy observations true underly ing signal. hope applying machine learning identify signal noise. requires us language quantify ing noise means. often would also like predictors draft 20230215 mathematics machine learning. feedback
1.2 two ways read book 15 allow us express sort uncertainty e.g. quantify confi dence value prediction particular test data point. quantification uncertainty realm probability theory probability theory covered chapter 6. train machine learning models typically find parameters maximize performance measure. many optimization techniques re quire concept gradient tells us direction search solution. chapter 5 vector calculus details vector calculus concept gradients subsequently use chapter 7 talk optimization find maximaminima functions. optimization part ii machine learning second part book introduces four pillars machine learning shown figure 1.2. illustrate mathematical concepts in troduced first part book foundation pillar. broadly speaking chapters ordered difficulty in ascending order. chapter 8 restate three components machine learning data models parameter estimation mathematical fashion. addition provide guidelines building experimental setups guard overly optimistic evaluations machine learning sys tems. recall goal build predictor performs well unseen data. chapter 9 close look linear regression linear regression objective find functions map inputs x rd corresponding ob served function values r interpret labels respective inputs. discuss classical model fitting parameter esti mation via maximum likelihood maximum posteriori estimation well bayesian linear regression integrate parameters instead optimizing them. chapter 10 focuses dimensionality reduction second pillar fig dimensionality reduction ure 1.2 using principal component analysis. key objective dimen sionality reduction find compact lowerdimensional representation highdimensional data x rd often easier analyze original data. unlike regression dimensionality reduction con cerned modeling data labels associated data point x. chapter 11 move third pillar density estimation. density estimation objective density estimation find probability distribution de scribes given dataset. focus gaussian mixture models purpose discuss iterative scheme find parameters model. dimensionality reduction labels associated data points x rd. however seek lowdimensional representation data. instead interested density model describes data. chapter 12 concludes book indepth discussion fourth 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
16 introduction motivation pillar classification. discuss classification context support classification vector machines. similar regression chapter 9 inputs x corresponding labels y. however unlike regression labels realvalued labels classification integers requires special care. 1.3 exercises feedback provide exercises part i done mostly pen paper. part ii provide programming tutorials jupyter notebooks explore properties machine learning algorithms discuss book. appreciate cambridge university press strongly supports aim democratize education learning making book freely available download tutorials errata additional materials found. mistakes reported feedback provided using preceding url. draft 20230215 mathematics machine learning. feedback
2 linear algebra formalizing intuitive concepts common approach construct set objects symbols set rules manipulate objects. known algebra. linear algebra study vectors certain algebra rules manipulate vectors. vectors many us know school called geometric vectors usually denoted small arrow letter e.g. x . book discuss general concepts vectors use bold letter represent them e.g. x y. general vectors special objects added together multiplied scalars produce another object kind. abstract mathematical viewpoint object satisfies two properties considered vector. examples vector objects 1. geometric vectors. example vector may familiar high school mathematics physics. geometric vectors see figure 2.1a directed segments drawn at least two dimen sions. two geometric vectors x added x z another geometric vector. furthermore multiplication scalar λ x λ r also geometric vector. fact original vector scaled λ. therefore geometric vectors instances vector concepts introduced previously. interpreting vectors geometric vec tors enables us use intuitions direction magnitude reason mathematical operations. 2. polynomials also vectors see figure 2.1b two polynomials figure 2.1 different types vectors. vectors surprising objects including a geometric vectors b polynomials. x x a geometric vectors. 2 0 2 x 6 4 2 0 2 4 b polynomials. 17 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.
18 linear algebra added together results another polynomial multiplied scalar λ r result polynomial well. therefore polynomials rather unusual instances vectors. note polynomials different geometric vectors. geometric vectors concrete drawings polynomials abstract concepts. however vectors sense previously de scribed. 3. audio signals vectors. audio signals represented series numbers. add audio signals together sum new audio signal. scale audio signal also obtain audio signal. therefore audio signals type vector too. 4. elements rn tuples n real numbers vectors. rn abstract polynomials concept focus book. instance 1 2 3 r3 2.1 example triplet numbers. adding two vectors a b rn componentwise results another vector b c rn. moreover multiplying rn λ r results scaled vector λa rn. considering vectors elements rn additional benefit careful check whether array operations actually perform vector operations implementing computer. loosely corresponds arrays real numbers computer. many programming languages support array operations allow con venient implementation algorithms involve vector operations. linear algebra focuses similarities vector concepts. add together multiply scalars. largely pavel grinfelds series linear algebra comnahclwm gilbert strangs course linear algebra com29p5q8j 3blue1brown series linear algebra comh5g4kps focus vectors rn since algorithms linear algebra for mulated rn. see chapter 8 often consider data represented vectors rn. book focus finite dimensional vector spaces case 11 correspondence kind vector rn. convenient use intuitions geometric vectors consider arraybased algorithms. one major idea mathematics idea closure. ques tion set things result proposed oper ations case vectors set vectors result starting small set vectors adding scaling them results vector space section 2.4. concept vector space properties underlie much machine learning. concepts introduced chapter summarized figure 2.2. chapter mostly based lecture notes books drumm weil 2001 strang 2003 hogben 2013 liesen mehrmann 2015 well pavel grinfelds linear algebra series. excellent draft 20230215 mathematics machine learning. feedback
2.1 systems linear equations 19 figure 2.2 mind map concepts introduced chapter along used parts book. vector vector space matrix chapter 5 vector calculus group system linear equations matrix inverse gaussian elimination linearaffine mapping linear independence basis chapter 10 dimensionality reduction chapter 12 classification chapter 3 analytic geometry composes closure abelian represents represents solved solves property maximal set resources gilbert strangs linear algebra course mit linear algebra series 3blue1brown. linear algebra plays important role machine learning gen eral mathematics. concepts introduced chapter ex panded include idea geometry chapter 3. chapter 5 discuss vector calculus principled knowledge matrix op erations essential. chapter 10 use projections to intro duced section 3.8 dimensionality reduction principal compo nent analysis pca. chapter 9 discuss linear regression linear algebra plays central role solving leastsquares problems. 2.1 systems linear equations systems linear equations play central part linear algebra. many problems formulated systems linear equations linear algebra gives us tools solving them. example 2.1 company produces products n1 . . . nn resources r1 . . . rm required. produce unit product nj aij units resource ri needed 1 . . . j 1 . . . n. objective find optimal production plan i.e. plan many units xj product nj produced total bi units resource ri available ideally resources left over. produce x1 . . . xn units corresponding products need 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
20 linear algebra total ai1x1 ainxn 2.2 many units resource ri. optimal production plan x1 . . . xn rn therefore satisfy following system equations a11x1 a1nxn b1 . . . am1x1 amnxn bm 2.3 aij r bi r. equation 2.3 general form system linear equations system linear equations x1 . . . xn unknowns system. every ntuple x1 . . . xn rn satisfies 2.3 solution linear equation system. solution example 2.2 system linear equations x1 x2 x3 3 1 x1 x2 2x3 2 2 2x1 3x3 1 3 2.4 solution adding first two equations yields 2x13x3 5 contradicts third equation 3. let us look system linear equations x1 x2 x3 3 1 x1 x2 2x3 2 2 x2 x3 2 3 . 2.5 first third equation follows x1 1. 12 get 2x1 3x3 5 i.e. x3 1. 3 get x2 1. therefore 1 1 1 possible unique solution verify 1 1 1 solution plugging in. third example consider x1 x2 x3 3 1 x1 x2 2x3 2 2 2x1 3x3 5 3 . 2.6 since 123 omit third equation redundancy. 1 2 get 2x1 53x3 2x2 1x3. define x3 r free variable triplet 5 2 3 2a 1 2 1 2a r 2.7 draft 20230215 mathematics machine learning. feedback
2.1 systems linear equations 21 figure 2.1 solution space system two linear equations two variables geometrically interpreted intersection two lines. every linear equation represents line. 2x1 4x2 1 4x1 4x2 5 x1 x2 solution system linear equations i.e. obtain solution set contains infinitely many solutions. general realvalued system linear equations obtain either no exactly one infinitely many solutions. linear regression chapter 9 solves version example 2.1 cannot solve system linear equations. remark geometric interpretation systems linear equations. system linear equations two variables x1 x2 linear equation defines line x1x2plane. since solution system linear equations must satisfy equations simultaneously solution set intersection lines. intersection set line if linear equations describe line point empty when lines parallel. illustration given figure 2.1 system 4x1 4x2 5 2x1 4x2 1 2.8 solution space point x1 x2 1 1 4. similarly three variables linear equation determines plane threedimensional space. intersect planes i.e. satisfy linear equations time obtain solution set plane line point empty when planes common intersection. systematic approach solving systems linear equations introduce useful compact notation. collect coefficients aij vectors collect vectors matrices. words write system 2.3 following form a11 . . . am1 x1 a12 . . . am2 x2 a1n . . . amn xn b1 . . . bm 2.9 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
22 linear algebra a11 a1n . . . . . . am1 amn x1 . . . xn b1 . . . bm . 2.10 following close look matrices de fine computation rules. return solving linear equations sec tion 2.3. 2.2 matrices matrices play central role linear algebra. used com pactly represent systems linear equations also represent linear functions linear mappings see later section 2.7. discuss interesting topics let us first define matrix kind operations matrices. see properties matrices chapter 4. definition 2.1 matrix. m n n realvalued m n matrix matrix mntuple elements aij 1 . . . m j 1 . . . n ordered according rectangular scheme consisting rows n columns a11 a12 a1n a21 a22 a2n . . . . . . . . . am1 am2 amn aij r . 2.11 convention 1 nmatrices called rows m 1matrices called row columns. special matrices also called rowcolumn vectors. column row vector column vector figure 2.2 stacking columns matrix represented long vector a. reshape r42 r8 rmn set realvalued m nmatrices. rmn equivalently represented rmn stacking n columns matrix long vector see figure 2.2. 2.2.1 matrix addition multiplication sum two matrices rmn b rmn defined element wise sum i.e. b a11 b11 a1n b1n . . . . . . am1 bm1 amn bmn rmn . 2.12 matrices rmn b rnk elements cij product note size matrices. c ab rmk computed c np.einsumil lj a b cij n x l1 ailblj 1 . . . m j 1 . . . k. 2.13 draft 20230215 mathematics machine learning. feedback
2.2 matrices 23 means compute element cij multiply elements ith n columns n rows b compute ailblj l 1 . . . n. commonly dot product two vectors a b denoted ab a b. row jth column b sum up. later section 3.2 call dot product corresponding row column. cases need explicit performing multiplication use notation b denote multiplication explicitly showing . remark. matrices multiplied neighboring dimensions match. instance n kmatrix multiplied k m matrix b left side z nk b z km c z nm 2.14 product ba defined n since neighboring dimensions match. remark. matrix multiplication defined elementwise operation matrix elements i.e. cij aijbij even size a b cho sen appropriately. kind elementwise multiplication often appears programming languages multiply multidimensional arrays other called hadamard product. hadamard product example 2.3 1 2 3 3 2 1 r23 b 0 2 1 1 0 1 r32 obtain ab 1 2 3 3 2 1 0 2 1 1 0 1 2 3 2 5 r22 2.15 ba 0 2 1 1 0 1 1 2 3 3 2 1 6 4 2 2 0 2 3 2 1 r33 . 2.16 figure 2.3 even matrix multiplications ab ba defined dimensions results different. example already see matrix multiplication commutative i.e. ab ba see also figure 2.3 illustration. definition 2.2 identity matrix. rnn define identity matrix identity matrix 1 0 0 0 0 1 0 0 . . . . . . . . . . . . . . 0 0 1 0 . . . . . . . . . . . . . . 0 0 0 1 rnn 2.17 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
24 linear algebra n nmatrix containing 1 diagonal 0 everywhere else. defined matrix multiplication matrix addition identity matrix let us look properties matrices associativity associativity a rmn b rnp c rpq abc abc 2.18 distributivity distributivity a b rmn c rnp a bc ac bc 2.19a ac d ac ad 2.19b multiplication identity matrix a rmn ima 2.20 note im n. 2.2.2 inverse transpose definition 2.3 inverse. consider square matrix rnn. let matrix square matrix possesses number columns rows. b rnn property ab ba. b called inverse denoted a1. inverse unfortunately every matrix possesses inverse a1. inverse exist called regularinvertiblenonsingular otherwise regular invertible nonsingular singularnoninvertible. matrix inverse exists unique. sec singular noninvertible tion 2.3 discuss general way compute inverse matrix solving system linear equations. remark existence inverse 2 2matrix. consider matrix a11 a12 a21 a22 r22 . 2.21 multiply a a22 a12 a21 a11 2.22 obtain aa a11a22 a12a21 0 0 a11a22 a12a21 a11a22 a12a21i . 2.23 therefore a1 1 a11a22 a12a21 a22 a12 a21 a11 2.24 a11a22 a12a21 0. section 4.1 see a11a22 draft 20230215 mathematics machine learning. feedback
2.2 matrices 25 a12a21 determinant 22matrix. furthermore generally use determinant check whether matrix invertible. example 2.4 inverse matrix matrices 1 2 1 4 4 5 6 7 7 b 7 7 6 2 1 1 4 5 4 2.25 inverse since ab ba. definition 2.4 transpose. rmn matrix b rnm bij aji called transpose a. write b a. transpose main diagonal sometimes called principal diagonal primary diagonal leading diagonal major diagonal matrix collection entries aij j. general acan obtained writing columns rows a. following important properties inverses transposes scalar case 2.28 1 24 1 6 1 2 1 4 . aa1 a1a 2.26 ab1 b1a1 2.27 a b1 a1 b1 2.28 a 2.29 a b a b 2.30 ab ba 2.31 definition 2.5 symmetric matrix. matrix rnn symmetric symmetric matrix a. note n nmatrices symmetric. generally call n nmatrices also square matrices possess num square matrix ber rows columns. moreover invertible a a1 a1 a. remark sum product symmetric matrices. sum symmet ric matrices a b rnn always symmetric. however although product always defined generally symmetric 1 0 0 0 1 1 1 1 1 1 0 0 . 2.32 2.2.3 multiplication scalar let us look happens matrices multiplied scalar λ r. let rmn λ r. λa k kij λ aij. practically λ scales element a. λ ψ r following holds 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
26 linear algebra associativity associativity λψc λψc c rmn λbc λbc bλc bcλ b rmn c rnk. note allows us move scalar values around. λc cλ cλ λcsince λ λfor λ r. distributivity distributivity λ ψc λc ψc c rmn λb c λb λc b c rmn example 2.5 distributivity define c 1 2 3 4 2.33 λ ψ r obtain λ ψc λ ψ1 λ ψ2 λ ψ3 λ ψ4 λ ψ 2λ 2ψ 3λ 3ψ 4λ 4ψ 2.34a λ 2λ 3λ 4λ ψ 2ψ 3ψ 4ψ λc ψc . 2.34b 2.2.4 compact representations systems linear equations consider system linear equations 2x1 3x2 5x3 1 4x1 2x2 7x3 8 9x1 5x2 3x3 2 2.35 use rules matrix multiplication write equation system compact form 2 3 5 4 2 7 9 5 3 x1 x2 x3 1 8 2 . 2.36 note x1 scales first column x2 second one x3 third one. generally system linear equations compactly represented matrix form ax b see 2.3 product ax linear combination columns a. discuss linear combinations detail section 2.5. draft 20230215 mathematics machine learning. feedback
2.3 solving systems linear equations 27 2.3 solving systems linear equations 2.3 introduced general form equation system i.e. a11x1 a1nxn b1 . . . am1x1 amnxn bm 2.37 aij r bi r known constants xj unknowns 1 . . . m j 1 . . . n. thus far saw matrices used compact way formulating systems linear equations write ax b see 2.10. moreover defined basic matrix operations addition multiplication matrices. following focus solving systems linear equations provide algorithm finding inverse matrix. 2.3.1 particular general solution discussing generally solve systems linear equations let us look example. consider system equations 1 0 8 4 0 1 2 12 x1 x2 x3 x4 42 8 . 2.38 system two equations four unknowns. therefore general would expect infinitely many solutions. system equations particularly easy form first two columns consist 1 0. remember want find scalars x1 . . . x4 p4 i1 xici b define ci ith column matrix b righthandside 2.38. solution problem 2.38 found immediately taking 42 times first column 8 times second column b 42 8 42 1 0 8 0 1 . 2.39 therefore solution 42 8 0 0. solution called particular particular solution solution special solution. however solution special solution system linear equations. capture solutions need creative generating 0 nontrivial way using columns matrix adding 0 special solution change special solution. so express third column using first two columns which simple form 8 2 8 1 0 2 0 1 2.40 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
28 linear algebra 0 8c1 2c2 1c3 0c4 x1 x2 x3 x4 8 2 1 0. fact scaling solution λ1 r produces 0 vector i.e. 1 0 8 4 0 1 2 12 λ1 8 2 1 0 λ18c1 2c2 c3 0 . 2.41 following line reasoning express fourth column matrix 2.38 using first two columns generate another set nontrivial versions 0 1 0 8 4 0 1 2 12 λ2 4 12 0 1 λ24c1 12c2 c4 0 2.42 λ2 r. putting everything together obtain solutions equation system 2.38 called general solution set general solution x r4 x 42 8 0 0 λ1 8 2 1 0 λ2 4 12 0 1 λ1 λ2 r . 2.43 remark. general approach followed consisted following three steps 1. find particular solution ax b. 2. find solutions ax 0. 3. combine solutions steps 1. 2. general solution. neither general particular solution unique. system linear equations preceding example easy solve matrix 2.38 particularly convenient form allowed us find particular general solution in spection. however general equation systems simple form. fortunately exists constructive algorithmic way transforming system linear equations particularly simple form gaussian elimination. key gaussian elimination elementary transformations systems linear equations transform equation system simple form. then apply three steps simple form discussed context example 2.38. 2.3.2 elementary transformations key solving system linear equations elementary transformations elementary transformations keep solution set same transform equation system simpler form draft 20230215 mathematics machine learning. feedback
2.3 solving systems linear equations 29 exchange two equations rows matrix representing system equations multiplication equation row constant λ r0 addition two equations rows example 2.6 r seek solutions following system equations 2x1 4x2 2x3 x4 4x5 3 4x1 8x2 3x3 3x4 x5 2 x1 2x2 x3 x4 x5 0 x1 2x2 3x4 4x5 . 2.44 start converting system equations compact matrix notation ax b. longer mention variables x explicitly build augmented matrix in form a b augmented matrix 2 4 2 1 4 3 4 8 3 3 1 2 1 2 1 1 1 0 1 2 0 3 4 swap r3 swap r1 used vertical line separate lefthand side righthand side 2.44. use to indicate transformation augmented matrix using elementary transformations. augmented matrix b compactly represents system linear equations ax b. swapping rows 1 3 leads 1 2 1 1 1 0 4 8 3 3 1 2 2 4 2 1 4 3 1 2 0 3 4 4r1 2r1 r1 apply indicated transformations e.g. subtract row 1 four times row 2 obtain 1 2 1 1 1 0 0 0 1 1 3 2 0 0 0 3 6 3 0 0 1 2 3 r2 r3 1 2 1 1 1 0 0 0 1 1 3 2 0 0 0 3 6 3 0 0 0 0 0 a1 1 1 3 1 2 1 1 1 0 0 0 1 1 3 2 0 0 0 1 2 1 0 0 0 0 0 a1 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
30 linear algebra augmented matrix convenient form rowechelon form rowechelon form ref. reverting compact notation back explicit notation variables seek obtain x1 2x2 x3 x4 x5 0 x3 x4 3x5 2 x4 2x5 1 0 1 . 2.45 1 system solved. particular solution particular solution x1 x2 x3 x4 x5 2 0 1 1 0 . 2.46 general solution captures set possible solutions general solution x r5 x 2 0 1 1 0 λ1 2 1 0 0 0 λ2 2 0 1 2 1 λ1 λ2 r . 2.47 following detail constructive way obtain particular general solution system linear equations. remark pivots staircase structure. leading coefficient row first nonzero number left called pivot always pivot strictly right pivot row it. therefore equa tion system rowechelon form always staircase structure. definition 2.6 rowechelon form. matrix rowechelon form rowechelon form rows contain zeros bottom matrix corre spondingly rows contain least one nonzero element top rows contain zeros. looking nonzero rows only first nonzero number left also called pivot leading coefficient always strictly pivot leading coefficient right pivot row it. texts sometimes required pivot 1. remark basic free variables. variables corresponding pivots rowechelon form called basic variables basic variable variables free variables. example 2.45 x1 x3 x4 basic free variable variables whereas x2 x5 free variables. remark obtaining particular solution. rowechelon form makes draft 20230215 mathematics machine learning. feedback
2.3 solving systems linear equations 31 lives easier need determine particular solution. this express righthand side equation system using pivot columns b pp i1 λipi pi 1 . . . p pivot columns. λi determined easiest start rightmost pivot column work way left. previous example would try find λ1 λ2 λ3 λ1 1 0 0 0 λ2 1 1 0 0 λ3 1 1 1 0 0 2 1 0 . 2.48 here find relatively directly λ3 1 λ2 1 λ1 2. put everything together must forget nonpivot columns set coefficients implicitly 0. therefore get particular solution x 2 0 1 1 0. remark reduced row echelon form. equation system reduced reduced rowechelon form rowechelon form also rowreduced echelon form row canonical form rowechelon form. every pivot 1. pivot nonzero entry column. reduced rowechelon form play important role later sec tion 2.3.3 allows us determine general solution sys tem linear equations straightforward way. gaussian elimination remark gaussian elimination. gaussian elimination algorithm performs elementary transformations bring system linear equations reduced rowechelon form. example 2.7 reduced row echelon form verify following matrix reduced rowechelon form the pivots bold 1 3 0 0 3 0 0 1 0 9 0 0 0 1 4 . 2.49 key idea finding solutions ax 0 look non pivot columns need express linear combination pivot columns. reduced row echelon form makes relatively straightforward express nonpivot columns terms sums multiples pivot columns left second col umn 3 times first column we ignore pivot columns right second column. therefore obtain 0 need subtract 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
32 linear algebra second column three times first column. now look fifth column second nonpivot column. fifth column expressed 3 times first pivot column 9 times second pivot column 4 times third pivot column. need keep track indices pivot columns translate 3 times first col umn 0 times second column which nonpivot column 9 times third column which second pivot column 4 times fourth column which third pivot column. need subtract fifth column obtain 0. end still solving homogeneous equation system. summarize solutions ax 0 x r5 given x r5 x λ1 3 1 0 0 0 λ2 3 0 9 4 1 λ1 λ2 r . 2.50 2.3.3 minus1 trick following introduce practical trick reading solu tions x homogeneous system linear equations ax 0 rkn x rn. start assume reduced rowechelon form without rows contain zeros i.e. 0 0 1 0 0 . . . . . . 0 0 0 1 . . . . . . . . . . . . . . . . . . . . . . . . 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 0 . . . . . . 0 0 0 0 0 0 0 0 1 2.51 can arbitrary real number constraints first nonzero entry per row must 1 entries corresponding column must 0. columns j1 . . . jk pivots marked bold standard unit vectors e1 . . . ek rk. extend matrix n nmatrix adding n k rows form 0 0 1 0 0 2.52 diagonal augmented matrix contains either 1 1. then columns contain 1 pivots solutions draft 20230215 mathematics machine learning. feedback
2.3 solving systems linear equations 33 homogeneous equation system ax 0. precise columns form basis section 2.6.1 solution space ax 0 later call kernel null space see section 2.7.3. kernel null space example 2.8 minus1 trick let us revisit matrix 2.49 already reduced ref 1 3 0 0 3 0 0 1 0 9 0 0 0 1 4 . 2.53 augment matrix 5 5 matrix adding rows form 2.52 places pivots diagonal missing obtain 1 3 0 0 3 0 1 0 0 0 0 0 1 0 9 0 0 0 1 4 0 0 0 0 1 . 2.54 form immediately read solutions ax 0 taking columns a contain 1 diagonal x r5 x λ1 3 1 0 0 0 λ2 3 0 9 4 1 λ1 λ2 r 2.55 identical solution 2.50 obtained insight. calculating inverse compute inverse a1 rnn need find matrix x satisfies ax in. then x a1. write set simultaneous linear equations ax in solve x x1 xn. use augmented matrix notation compact representation set systems linear equations obtain ain ina1 . 2.56 means bring augmented equation system reduced rowechelon form read inverse righthand side equation system. hence determining inverse matrix equiv alent solving systems linear equations. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
34 linear algebra example 2.9 calculating inverse matrix gaussian elimination determine inverse 1 0 2 0 1 1 0 0 1 2 0 1 1 1 1 1 2.57 write augmented matrix 1 0 2 0 1 0 0 0 1 1 0 0 0 1 0 0 1 2 0 1 0 0 1 0 1 1 1 1 0 0 0 1 use gaussian elimination bring reduced rowechelon form 1 0 0 0 1 2 2 2 0 1 0 0 1 1 2 2 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 2 desired inverse given righthand side a1 1 2 2 2 1 1 2 2 1 1 1 1 1 0 1 2 . 2.58 verify 2.58 indeed inverse performing multi plication aa1 observing recover i4. 2.3.4 algorithms solving system linear equations following briefly discuss approaches solving system lin ear equations form ax b. make assumption solu tion exists. solution need resort approximate solutions cover chapter. one way solve ap proximate problem using approach linear regression discuss detail chapter 9. special cases may able determine inverse a1 solution ax b given x a1b. however possible square matrix invertible often case. otherwise mild assumptions i.e. needs linearly independent columns use transformation ax b aax ab x aa1ab 2.59 draft 20230215 mathematics machine learning. feedback
2.4 vector spaces 35 use moorepenrose pseudoinverse aa1ato determine moorepenrose pseudoinverse solution 2.59 solves ax b also corresponds mini mum norm leastsquares solution. disadvantage approach requires many computations matrixmatrix product comput ing inverse aa. moreover reasons numerical precision generally recommended compute inverse pseudoinverse. following therefore briefly discuss alternative approaches solving systems linear equations. gaussian elimination plays important role computing deter minants section 4.1 checking whether set vectors linearly inde pendent section 2.5 computing inverse matrix section 2.2.2 computing rank matrix section 2.6.2 determining basis vector space section 2.6.1. gaussian elimination intuitive constructive way solve system linear equations thousands variables. however systems millions variables impracti cal required number arithmetic operations scales cubically number simultaneous equations. practice systems many linear equations solved indirectly ei ther stationary iterative methods richardson method ja cobi method gaußseidel method successive overrelaxation method krylov subspace methods conjugate gradients gener alized minimal residual biconjugate gradients. refer books stoer burlirsch 2002 strang 2003 liesen mehrmann 2015 details. let xbe solution ax b. key idea iterative methods set iteration form xk1 cxk 2.60 suitable c reduces residual error xk1 xin every iteration converges x. introduce norms allow us compute similarities vectors section 3.1. 2.4 vector spaces thus far looked systems linear equations solve section 2.3. saw systems linear equations com pactly represented using matrixvector notation 2.10. following closer look vector spaces i.e. structured space vectors live. beginning chapter informally characterized vectors objects added together multiplied scalar remain objects type. now ready formalize this start introducing concept group set elements operation defined elements keeps structure set intact. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
36 linear algebra 2.4.1 groups groups play important role computer science. besides providing fundamental framework operations sets heavily used cryptography coding theory graphics. definition 2.7 group. consider set g operation gg g defined g. g g called group following hold group closure 1. closure g x g x y g associativity 2. associativity x y z g x y z x y z neutral element 3. neutral element e g x g x e x e x x inverse element 4. inverse element x g y g x y e x e e neutral element. often write x1 denote inverse element x. remark. inverse element defined respect operation necessarily mean 1 x. additionally x g x y x g g abelian abelian group group commutative. example 2.10 groups let us look examples sets associated operations see whether groups z abelian group. n0 group although n0 possesses neutral element n0 n 0 0 inverse elements missing. z group although z contains neutral element 1 inverse elements z z z 1 missing. r group since 0 possess inverse element. r0 abelian. rn zn n n abelian defined componentwise i.e. x1 xn y1 yn x1 y1 xn yn. 2.61 then x1 xn1 x1 xn inverse element e 0 0 neutral element. rmn set nmatrices abelian with componentwise addition defined 2.61. let us closer look rnn i.e. set nnmatrices matrix multiplication defined 2.13. closure associativity follow directly definition matrix multiplication. neutral element identity matrix neutral element respect matrix multiplication rnn . draft 20230215 mathematics machine learning. feedback
2.4 vector spaces 37 inverse element inverse exists a regular a1 inverse element rnn exactly case rnn group called general linear group. definition 2.8 general linear group. set regular invertible matrices rnn group respect matrix multiplication defined 2.13 called general linear group gln r. however general linear group since matrix multiplication commutative group abelian. 2.4.2 vector spaces discussed groups looked sets g inner operations g i.e. mappings g g g operate elements g. following consider sets addition inner operation also contain outer operation multiplication vector x g scalar λ r. think inner operation form addition outer operation form scaling. note innerouter operations nothing innerouter products. definition 2.9 vector space. realvalued vector space v v vector space set v two operations v v v 2.62 r v v 2.63 1. v abelian group 2. distributivity 1. λ r x v λ x y λ x λ 2. λ ψ r x v λ ψ x λ x ψ x 3. associativity outer operation λ ψ r x v λψx λψx 4. neutral element respect outer operation x v 1x x elements x v called vectors. neutral element v vector zero vector 0 0 . . . 0 inner operation called vector vector addition addition. elements λ r called scalars outer operation scalar multiplication scalars. note scalar product something multiplication scalars different get section 3.2. remark. vector multiplication ab a b rn defined. theoret ically could define elementwise multiplication c ab cj ajbj. array multiplication common many program ming languages makes mathematically limited sense using stan dard rules matrix multiplication treating vectors n 1 matrices 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
38 linear algebra which usually do use matrix multiplication defined 2.13. however dimensions vectors match. following multiplications vectors defined abrnn outer outer product product ab r innerscalardot product. example 2.11 vector spaces let us look important examples v rn n n vector space operations defined follows addition xy x1 . . . xny1 . . . yn x1y1 . . . xnyn x rn multiplication scalars λx λx1 . . . xn λx1 . . . λxn λ r x rn v rmn m n n vector space addition b a11 b11 a1n b1n . . . . . . am1 bm1 amn bmn is defined ele mentwise a b v multiplication scalars λa λa11 λa1n . . . . . . λam1 λamn as defined section 2.2. remember rmn equivalent rmn. v c standard definition addition complex numbers. remark. following denote vector space v v standard vector addition scalar multiplication. moreover use notation x v vectors v simplify notation. remark. vector spaces rn rn1 r1n different way write vectors. following make distinction rn rn1 allows us write ntuples column vectors column vector x x1 . . . xn . 2.64 simplifies notation regarding vector space operations. however distinguish rn1 r1n the row vectors avoid con row vector fusion matrix multiplication. default write x denote col umn vector row vector denoted x transpose x. transpose draft 20230215 mathematics machine learning. feedback
2.4 vector spaces 39 2.4.3 vector subspaces following introduce vector subspaces. intuitively sets contained original vector space property perform vector space operations elements within subspace never leave it. sense closed. vector subspaces key idea machine learning. example chapter 10 demonstrates use vector subspaces dimensionality reduction. definition 2.10 vector subspace. let v v vector space u v u . u u called vector subspace v or vector subspace linear subspace u vector space vector space operations linear subspace restricted u u ru. write u v denote subspace u v . u v v vector space u naturally inherits many prop erties directly v hold x v particular x u v. includes abelian group properties distribu tivity associativity neutral element. determine whether u subspace v still need show 1. u particular 0 u 2. closure u a. respect outer operation λ r x u λx u. b. respect inner operation x u x u. example 2.12 vector subspaces let us look examples every vector space v trivial subspaces v 0. example figure 2.1 subspace r2 with usual inner outer operations. c closure property violated b contain 0. solution set homogeneous system linear equations ax 0 n unknowns x x1 . . . xnis subspace rn. solution inhomogeneous system linear equations ax b b 0 subspace rn. intersection arbitrarily many subspaces subspace itself. figure 2.1 subsets r2 subspaces. c closure property violated b contain 0. subspace. 0 0 0 0 b c 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
40 linear algebra remark. every subspace u rn solution space homo geneous system linear equations ax 0 x rn. 2.5 linear independence following close look vectors elements vector space. particular add vectors together multiply scalars. closure property guarantees end another vector vector space. possible find set vectors represent every vector vector space adding together scaling them. set vectors basis discuss section 2.6.1. get there need introduce concepts linear combinations linear independence. definition 2.11 linear combination. consider vector space v finite number vectors x1 . . . xk v . then every v v form v λ1x1 λkxk k x i1 λixi v 2.65 λ1 . . . λk r linear combination vectors x1 . . . xk. linear combination 0vector always written linear combination k vec tors x1 . . . xk 0 pk i1 0xi always true. following interested nontrivial linear combinations set vectors represent 0 i.e. linear combinations vectors x1 . . . xk coefficients λi 2.65 0. definition 2.12 linear independence. let us consider vector space v k n x1 . . . xk v . nontrivial linear com bination 0 pk i1 λixi least one λi 0 vectors x1 . . . xk linearly dependent. trivial solution exists i.e. linearly dependent λ1 . . . λk 0 vectors x1 . . . xk linearly independent. linearly independent linear independence one important concepts linear algebra. intuitively set linearly independent vectors consists vectors redundancy i.e. remove vectors set lose something. throughout next sections formalize intuition more. example 2.13 linearly dependent vectors geographic example may help clarify concept linear indepen dence. person nairobi kenya describing kigali rwanda might say you get kigali first going 506 km northwest kam pala uganda 374 km southwest.. sufficient information draft 20230215 mathematics machine learning. feedback
2.5 linear independence 41 describe location kigali geographic coordinate sys tem may considered twodimensional vector space ignoring altitude earths curved surface. person may add it 751 km west here. although last statement true necessary find kigali given previous information see figure 2.2 illus tration. example 506 km northwest vector blue 374 km southwest vector purple linearly independent. means southwest vector cannot described terms northwest vec tor vice versa. however third 751 km west vector black linear combination two vectors makes set vec tors linearly dependent. equivalently given 751 km west 374 km southwest linearly combined obtain 506 km northwest. figure 2.2 geographic example with crude approximations cardinal directions linearly dependent vectors twodimensional space plane. 506 km northwest 751 km west 374 km southwest 374 km southwest kampala nairobi kigali remark. following properties useful find whether vectors linearly independent k vectors either linearly dependent linearly independent. third option. least one vectors x1 . . . xk 0 linearly de pendent. holds two vectors identical. vectors x1 . . . xk xi 0 1 . . . k k 2 linearly dependent at least one linear combination others. particular one vector multiple another vector i.e. xi λxj λ r set x1 . . . xk xi 0 1 . . . k linearly dependent. practical way checking whether vectors x1 . . . xk v linearly independent use gaussian elimination write vectors columns matrix perform gaussian elimination matrix row echelon form the reduced rowechelon form unnecessary here 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
42 linear algebra pivot columns indicate vectors linearly indepen dent vectors left. note ordering vec tors matrix built. nonpivot columns expressed linear combinations pivot columns left. instance rowechelon form 1 3 0 0 0 2 2.66 tells us first third columns pivot columns. sec ond column nonpivot column three times first column. column vectors linearly independent columns pivot columns. least one nonpivot column columns and therefore corresponding vectors linearly dependent. example 2.14 consider r4 x1 1 2 3 4 x2 1 1 0 2 x3 1 2 1 1 . 2.67 check whether linearly dependent follow general ap proach solve λ1x1 λ2x2 λ3x3 λ1 1 2 3 4 λ2 1 1 0 2 λ3 1 2 1 1 0 2.68 λ1 . . . λ3. write vectors xi 1 2 3 columns matrix apply elementary row operations identify pivot columns 1 1 1 2 1 2 3 0 1 4 2 1 1 1 1 0 1 0 0 0 1 0 0 0 . 2.69 here every column matrix pivot column. therefore nontrivial solution require λ1 0 λ2 0 λ3 0 solve equation system. hence vectors x1 x2 x3 linearly independent. draft 20230215 mathematics machine learning. feedback
2.5 linear independence 43 remark. consider vector space v k linearly independent vectors b1 . . . bk linear combinations x1 k x i1 λi1bi . . . xm k x i1 λimbi . 2.70 defining b b1 . . . bk matrix whose columns linearly independent vectors b1 . . . bk write xj bλj λj λ1j . . . λkj j 1 . . . 2.71 compact form. want test whether x1 . . . xm linearly independent. purpose follow general approach testing pm j1 ψjxj 0. 2.71 obtain x j1 ψjxj x j1 ψjbλj b x j1 ψjλj . 2.72 means x1 . . . xm linearly independent column vectors λ1 . . . λm linearly independent. remark. vector space v linear combinations k vectors x1 . . . xk linearly dependent k. example 2.15 consider set linearly independent vectors b1 b2 b3 b4 rn x1 b1 2b2 b3 b4 x2 4b1 2b2 4b4 x3 2b1 3b2 b3 3b4 x4 17b1 10b2 11b3 b4 . 2.73 vectors x1 . . . x4 rn linearly independent answer question investigate whether column vectors 1 2 1 1 4 2 0 4 2 3 1 3 17 10 11 1 2.74 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
44 linear algebra linearly independent. reduced rowechelon form corre sponding linear equation system coefficient matrix 1 4 2 17 2 2 3 10 1 0 1 11 1 4 3 1 2.75 given 1 0 0 7 0 1 0 15 0 0 1 18 0 0 0 0 . 2.76 see corresponding linear equation system nontrivially solv able last column pivot column x4 7x115x218x3. therefore x1 . . . x4 linearly dependent x4 expressed linear combination x1 . . . x3. 2.6 basis rank vector space v particularly interested sets vectors possess property vector v v obtained linear combination vectors a. vectors special vectors following characterize them. 2.6.1 generating set basis definition 2.13 generating set span. consider vector space v v set vectors x1 . . . xk v. every vector v v expressed linear combination x1 . . . xk called generating set v . set linear combinations vectors generating set called span a. spans vector space v write v spana span v spanx1 . . . xk. generating sets sets vectors span vector subspaces i.e. every vector represented linear combination vectors generating set. now specific characterize smallest generating set spans vector subspace. definition 2.14 basis. consider vector space v v v. generating set v called minimal exists smaller set minimal a v spans v . every linearly independent generating set v minimal called basis v . basis draft 20230215 mathematics machine learning. feedback
2.6 basis rank 45 let v v vector space b v b . then following statements equivalent basis minimal generating set maximal linearly independent set vectors. b basis v . b minimal generating set. b maximal linearly independent set vectors v i.e. adding vector set make linearly dependent. every vector x v linear combination vectors b every linear combination unique i.e. x k x i1 λibi k x i1 ψibi 2.77 λi ψi r bi b follows λi ψi 1 . . . k. example 2.16 r3 canonicalstandard basis canonical basis b 1 0 0 0 1 0 0 0 1 . 2.78 different bases r3 b1 1 0 0 1 1 0 1 1 1 b2 0.5 0.8 0.4 1.8 0.3 0.3 2.2 1.3 3.5 . 2.79 set 1 2 3 4 2 1 0 2 1 1 0 4 2.80 linearly independent generating set and basis r4 instance vector 1 0 0 0cannot obtained linear com bination elements a. remark. every vector space v possesses basis b. preceding exam ples show many bases vector space v i.e. unique basis. however bases possess number elements basis vectors. basis vector consider finitedimensional vector spaces v . case dimension v number basis vectors v write dimv . dimension u v subspace v dimu dimv dimu 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
46 linear algebra dimv u v . intuitively dimension vector space thought number independent directions vector space. dimension vector space corresponds number basis vectors. remark. dimension vector space necessarily number elements vector. instance vector space v span 0 1 onedimensional although basis vector possesses two elements. remark. basis subspace u spanx1 . . . xm rn found executing following steps 1. write spanning vectors columns matrix 2. determine rowechelon form a. 3. spanning vectors associated pivot columns basis u. example 2.17 determining basis vector subspace u r5 spanned vectors x1 1 2 1 1 1 x2 2 1 1 2 2 x3 3 4 3 5 3 x4 1 8 5 6 1 r5 2.81 interested finding vectors x1 . . . x4 basis u. this need check whether x1 . . . x4 linearly independent. therefore need solve 4 x i1 λixi 0 2.82 leads homogeneous system equations matrix x1 x2 x3 x4 1 2 3 1 2 1 4 8 1 1 3 5 1 2 5 6 1 2 3 1 . 2.83 basic transformation rules systems linear equations obtain rowechelon form 1 2 3 1 2 1 4 8 1 1 3 5 1 2 5 6 1 2 3 1 1 2 3 1 0 1 2 2 0 0 0 1 0 0 0 0 0 0 0 0 . draft 20230215 mathematics machine learning. feedback
2.6 basis rank 47 since pivot columns indicate set vectors linearly indepen dent see rowechelon form x1 x2 x4 linearly inde pendent because system linear equations λ1x1 λ2x2 λ4x4 0 solved λ1 λ2 λ4 0. therefore x1 x2 x4 basis u. 2.6.2 rank number linearly independent columns matrix rmn equals number linearly independent rows called rank rank denoted rka. remark. rank matrix important properties rka rka i.e. column rank equals row rank. columns rmn span subspace u rm dimu rka. later call subspace image range. basis u found applying gaussian elimination identify pivot columns. rows rmn span subspace w rn dimw rka. basis w found applying gaussian elimination a. rnn holds regular invertible rka n. rmn b rm holds linear equation system ax b solved rka rkab ab denotes augmented system. rmn subspace solutions ax 0 possesses dimen sion n rka. later call subspace kernel null kernel null space space. matrix rmn full rank rank equals largest possible full rank rank matrix dimensions. means rank fullrank matrix lesser number rows columns i.e. rka minm n. matrix said rank deficient rank deficient full rank. example 2.18 rank 1 0 1 0 1 1 0 0 0 . two linearly independent rowscolumns rka 2. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
48 linear algebra 1 2 1 2 3 1 3 5 0 . use gaussian elimination determine rank 1 2 1 2 3 1 3 5 0 1 2 1 0 1 3 0 0 0 . 2.84 here see number linearly independent rows columns 2 rka 2. 2.7 linear mappings following study mappings vector spaces preserve structure allow us define concept coordinate. beginning chapter said vectors objects added together multiplied scalar resulting object still vector. wish preserve property applying mapping consider two real vector spaces v w. mapping φ v w preserves structure vector space φx y φx φy 2.85 φλx λφx 2.86 x v λ r. summarize following definition definition 2.15 linear mapping. vector spaces v w mapping φ v w called linear mapping or vector space homomorphism linear mapping vector space homomorphism linear transformation linear transformation x v λ ψ r φλx ψy λφx ψφy . 2.87 turns represent linear mappings matrices sec tion 2.7.1. recall also collect set vectors columns matrix. working matrices keep mind matrix represents linear mapping collection vectors. see linear mappings chapter 4. continue briefly introduce special mappings. definition 2.16 injective surjective bijective. consider mapping φ v w v w arbitrary sets. φ called injective injective x v φx φy x y. surjective surjective φv w. bijective bijective injective surjective. draft 20230215 mathematics machine learning. feedback
2.7 linear mappings 49 φ surjective every element w reached v using φ. bijective φ undone i.e. exists mapping ψ w v ψ φx x. mapping ψ called inverse φ normally denoted φ1. definitions introduce following special cases linear mappings vector spaces v w isomorphism isomorphism φ v w linear bijective endomorphism endomorphism φ v v linear automorphism automorphism φ v v linear bijective define idv v v x 7x identity mapping identity identity mapping identity automorphism automorphism v . example 2.19 homomorphism mapping φ r2 c φx x1 ix2 homomorphism φ x1 x2 y1 y2 x1 y1 ix2 y2 x1 ix2 y1 iy2 φ x1 x2 φ y1 y2 φ λ x1 x2 λx1 λix2 λx1 ix2 λφ x1 x2 . 2.88 also justifies complex numbers represented tuples r2 bijective linear mapping converts elementwise addi tion tuples r2 set complex numbers correspond ing addition. note showed linearity bijection. theorem 2.17 theorem 3.59 axler 2015. finitedimensional vector spaces v w isomorphic dimv dimw. theorem 2.17 states exists linear bijective mapping be tween two vector spaces dimension. intuitively means vector spaces dimension kind thing transformed without incurring loss. theorem 2.17 also gives us justification treat rmn the vector space nmatrices rmn the vector space vectors length mn same dimensions mn exists linear bi jective mapping transforms one other. remark. consider vector spaces v w x. then linear mappings φ v w ψ w x mapping ψ φ v x also linear. φ v w isomorphism φ1 w v isomor phism too. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
50 linear algebra figure 2.1 two different coordinate systems defined two sets basis vectors. vector x different coordinate representations depending coordinate system chosen. x x e1 e2 b1 b2 φ v w ψ v w linear φ ψ λφ λ r linear too. 2.7.1 matrix representation linear mappings ndimensional vector space isomorphic rn theorem 2.17. consider basis b1 . . . bn ndimensional vector space v . following order basis vectors important. therefore write b b1 . . . bn 2.89 call ntuple ordered basis v . ordered basis remark notation. point notation gets bit tricky. therefore summarize parts here. b b1 . . . bn ordered basis b b1 . . . bn unordered basis b b1 . . . bn matrix whose columns vectors b1 . . . bn. definition 2.18 coordinates. consider vector space v ordered basis b b1 . . . bn v . x v obtain unique represen tation linear combination x α1b1 . . . αnbn 2.90 x respect b. α1 . . . αn coordinates x coordinate respect b vector α α1 . . . αn rn 2.91 coordinate vectorcoordinate representation x respect coordinate vector coordinate representation ordered basis b. draft 20230215 mathematics machine learning. feedback
2.7 linear mappings 51 basis effectively defines coordinate system. familiar cartesian coordinate system two dimensions spanned canonical basis vectors e1 e2. coordinate system vector x r2 representation tells us linearly combine e1 e2 obtain x. however basis r2 defines valid coordinate system vector x may different coordinate rep resentation b1 b2 basis. figure 2.1 coordinates x respect standard basis e1 e2 2 2. however respect basis b1 b2 vector x represented 1.09 0.72 i.e. x 1.09b1 0.72b2. following sections discover obtain representation. example 2.20 let us look geometric vector x r2 coordinates 2 3 figure 2.2 different coordinate representations vector x depending choice basis. e1 e2 b2 b1 x 1 2b1 5 2b2 x 2e1 3e2 respect standard basis e1 e2 r2. means write x 2e1 3e2. however choose standard basis represent vector. use basis vectors b1 1 1 b2 1 1 obtain coordinates 1 21 5to represent vector respect b1 b2 see figure 2.2. remark. ndimensional vector space v ordered basis b v mapping φ rn v φei bi 1 . . . n linear and theorem 2.17 isomorphism e1 . . . en standard basis rn. ready make explicit connection matrices linear mappings finitedimensional vector spaces. definition 2.19 transformation matrix. consider vector spaces v w corresponding ordered bases b b1 . . . bn c c1 . . . cm. moreover consider linear mapping φ v w. j 1 . . . n φbj α1jc1 αmjcm x i1 αijci 2.92 unique representation φbj respect c. then call nmatrix aφ whose elements given aφi j αij 2.93 transformation matrix φ with respect ordered bases b v transformation matrix c w. coordinates φbj respect ordered basis c w jth column aφ. consider finitedimensional vector spaces v w ordered bases b c linear mapping φ v w 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
52 linear algebra transformation matrix aφ. ˆ x coordinate vector x v respect b ˆ coordinate vector φx w respect c ˆ aφˆ x . 2.94 means transformation matrix used map coordinates respect ordered basis v coordinates respect ordered basis w. example 2.21 transformation matrix consider homomorphism φ v w ordered bases b b1 . . . b3 v c c1 . . . c4 w. φb1 c1 c2 3c3 c4 φb2 2c1 c2 7c3 2c4 φb3 3c2 c3 4c4 2.95 transformation matrix aφ respect b c satisfies φbk p4 i1 αikci k 1 . . . 3 given aφ α1 α2 α3 1 2 0 1 1 3 3 7 1 1 2 4 2.96 αj j 1 2 3 coordinate vectors φbj respect c. example 2.22 linear transformations vectors figure 2.3 three examples linear transformations vectors shown dots a b rotation 45 c stretching horizontal coordinates 2 d combination reflection rotation stretching. a original data. b rotation 45. c stretch along horizontal axis. d general linear mapping. consider three linear transformations set vectors r2 transformation matrices a1 cos π 4 sin π 4 sin π 4 cos π 4 a2 2 0 0 1 a3 1 2 3 1 1 1 . 2.97 draft 20230215 mathematics machine learning. feedback
2.7 linear mappings 53 figure 2.3 gives three examples linear transformations set vec tors. figure 2.3a shows 400 vectors r2 represented dot corresponding x1 x2coordinates. vectors ar ranged square. use matrix a1 2.97 linearly transform vectors obtain rotated square figure 2.3b. apply linear mapping represented a2 obtain rectangle figure 2.3c x1coordinate stretched 2. figure 2.3d shows original square figure 2.3a linearly transformed using a3 combination reflection rotation stretch. 2.7.2 basis change following closer look transformation matrices linear mapping φ v w change change bases v w. consider two ordered bases b b1 . . . bn b b1 . . . bn 2.98 v two ordered bases c c1 . . . cm c c1 . . . cm 2.99 w. moreover aφ rmn transformation matrix linear mapping φ v w respect bases b c aφ rmn corresponding transformation mapping respect b c. following investigate related i.e. how whether transform aφ aφ choose perform basis change b c b c. remark. effectively get different coordinate representations identity mapping idv . context figure 2.2 would mean map coordinates respect e1 e2 onto coordinates respect b1 b2 without changing vector x. changing basis corre spondingly representation vectors transformation matrix respect new basis particularly simple form allows straightforward computation. example 2.23 basis change consider transformation matrix 2 1 1 2 2.100 respect canonical basis r2. define new basis b 1 1 1 1 2.101 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
54 linear algebra obtain diagonal transformation matrix 3 0 0 1 2.102 respect b easier work a. following look mappings transform coordinate vectors respect one basis coordinate vectors respect different basis. state main result first provide explanation. theorem 2.20 basis change. linear mapping φ v w ordered bases b b1 . . . bn b b1 . . . bn 2.103 v c c1 . . . cm c c1 . . . cm 2.104 w transformation matrix aφ φ respect b c corresponding transformation matrix aφ respect bases b c given aφ 1aφs . 2.105 here rnn transformation matrix idv maps coordinates respect b onto coordinates respect b rmm transformation matrix idw maps coordinates respect c onto coordinates respect c. proof following drumm weil 2001 write vectors new basis b v linear combination basis vectors b bj s1jb1 snjbn n x i1 sijbi j 1 . . . n . 2.106 similarly write new basis vectors c w linear combination basis vectors c yields ck t1kc1 tmkcm x l1 tlkcl k 1 . . . . 2.107 define sij rnn transformation matrix maps coordinates respect b onto coordinates respect b tlk rmm transformation matrix maps coordinates respect c onto coordinates respect c. particular jth column coordinate representation bj respect b draft 20230215 mathematics machine learning. feedback
2.7 linear mappings 55 kth column coordinate representation ck respect c. note regular. going look φ bj two perspectives. first applying mapping φ get j 1 . . . n φ bj x k1 akj ck z w 2.107 x k1 akj x l1 tlkcl x l1 x k1 tlk akj cl 2.108 first expressed new basis vectors ck w linear com binations basis vectors cl w swapped order summation. alternatively express bj v linear combinations bj v arrive φ bj 2.106 φ n x i1 sijbi n x i1 sijφbi n x i1 sij x l1 alicl 2.109a x l1 n x i1 alisij cl j 1 . . . n 2.109b exploited linearity φ. comparing 2.108 2.109b follows j 1 . . . n l 1 . . . x k1 tlk akj n x i1 alisij 2.110 and therefore aφ aφs rmn 2.111 aφ 1aφs 2.112 proves theorem 2.20. theorem 2.20 tells us basis change v b replaced b w c replaced c transformation matrix aφ linear mapping φ v w replaced equivalent matrix aφ aφ 1aφs. 2.113 figure 2.2 illustrates relation consider homomorphism φ v w ordered bases b b v c c w. mapping φcb in stantiation φ maps basis vectors b onto linear combinations basis vectors c. assume know transformation matrix aφ φcb respect ordered bases b c. perform basis change b b v c c w determine 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
56 linear algebra figure 2.2 homomorphism φ v w ordered bases b b v c c w marked blue express mapping φ c b respect bases b c equivalently composition homomorphisms φ c b ξ cc φcb ψb b respect bases subscripts. corresponding transformation matrices red. v w b b c c φ φcb φ c b ψb b ξc c aφ aφ v w b b c c φ φcb φ c b ψb b ξ cc ξ1 c c 1 aφ aφ vector spaces ordered bases corresponding transformation matrix aφ follows first find ma trix representation linear mapping ψb b v v maps coordi nates respect new basis b onto unique coordinates respect old basis b in v . then use transformation ma trix aφ φcb v w map coordinates onto coordinates respect c w. finally use linear mapping ξ cc w w map coordinates respect c onto coordinates respect c. therefore express linear mapping φ c b composition linear mappings involve old basis φ c b ξ cc φcb ψb b ξ1 c c φcb ψb b . 2.114 concretely use ψb b idv ξc c idw i.e. identity mappings map vectors onto themselves respect different basis. definition 2.21 equivalence. two matrices a rmn equivalent equivalent exist regular matrices rnn rmm 1as. definition 2.22 similarity. two matrices a rnn similar similar exists regular matrix rnn s1as remark. similar matrices always equivalent. however equivalent ma trices necessarily similar. remark. consider vector spaces v w x. remark follows theorem 2.17 already know linear mappings φ v w ψ w x mapping ψ φ v x also linear. transformation matrices aφ aψ corresponding mappings overall transformation matrix aψφ aψaφ. light remark look basis changes perspec tive composing linear mappings aφ transformation matrix linear mapping φcb v w respect bases b c. aφ transformation matrix linear mapping φ c b v w respect bases b c. transformation matrix linear mapping ψb b v v automorphism represents b terms b. normally ψ idv identity mapping v . draft 20230215 mathematics machine learning. feedback
2.7 linear mappings 57 transformation matrix linear mapping ξc c w w automorphism represents c terms c. normally ξ idw identity mapping w. informally write transformations terms bases aφ b c aφ b c b b c c 1 c c b c b bc c 2.115 aφ 1aφs . 2.116 note execution order 2.116 right left vec tors multiplied righthand side x 7sx 7aφsx 7 1aφsx aφx. example 2.24 basis change consider linear mapping φ r3 r4 whose transformation matrix aφ 1 2 0 1 1 3 3 7 1 1 2 4 2.117 respect standard bases b 1 0 0 0 1 0 0 0 1 c 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 . 2.118 seek transformation matrix aφ φ respect new bases b 1 1 0 0 1 1 1 0 1 r3 c 1 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 . 2.119 then 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 0 0 0 0 1 2.120 ith column coordinate representation bi terms basis vectors b. since b standard basis co ordinate representation straightforward find. general basis b would need solve linear equation system find λi 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
58 linear algebra p3 i1 λibi bj j 1 . . . 3. similarly jth column coordi nate representation cj terms basis vectors c. therefore obtain aφ 1aφs 1 2 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 2 3 2 1 0 4 2 10 8 4 1 6 3 2.121a 4 4 2 6 0 0 4 8 4 1 6 3 . 2.121b chapter 4 able exploit concept basis change find basis respect transformation matrix en domorphism particularly simple diagonal form. chapter 10 look data compression problem find convenient basis onto project data minimizing compression loss. 2.7.3 image kernel image kernel linear mapping vector subspaces cer tain important properties. following characterize carefully. definition 2.23 image kernel. φ v w define kernelnull space kernel null space kerφ φ10w v v φv 0w 2.122 imagerange image range imφ φv w wv v φv w . 2.123 also call v w also domain codomain φ respectively. domain codomain intuitively kernel set vectors v v φ maps onto neutral element 0w w. image set vectors w w reached φ vector v . illustration given figure 2.2. remark. consider linear mapping φ v w v w vector spaces. always holds φ0v 0w and therefore 0v kerφ. particular null space never empty. imφ w subspace w kerφ v subspace v . draft 20230215 mathematics machine learning. feedback
2.7 linear mappings 59 figure 2.2 kernel image linear mapping φ v w. imφ 0w kerφ 0v φ v w v w φ injective onetoone kerφ 0. remark null space column space. let us consider rmn linear mapping φ rn rm x 7ax. a1 . . . an ai columns a obtain imφ ax x rn n x i1 xiai x1 . . . xn r 2.124a spana1 . . . an rm 2.124b i.e. image span columns a also called column column space space. therefore column space image subspace rm height matrix. rka dimimφ. kernelnull space kerφ general solution homoge neous system linear equations ax 0 captures possible linear combinations elements rn produce 0 rm. kernel subspace rn n width matrix. kernel focuses relationship among columns use determine whetherhow express column linear combination columns. example 2.25 image kernel linear mapping mapping φ r4 r2 x1 x2 x3 x4 7 1 2 1 0 1 0 0 1 x1 x2 x3 x4 x1 2x2 x3 x1 x4 2.125a 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
60 linear algebra x1 1 1 x2 2 0 x3 1 0 x4 0 1 2.125b linear. determine imφ take span columns transformation matrix obtain imφ span 1 1 2 0 1 0 0 1 . 2.126 compute kernel null space φ need solve ax 0 i.e. need solve homogeneous equation system. this use gaussian elimination transform reduced rowechelon form 1 2 1 0 1 0 0 1 1 0 0 1 0 1 1 2 1 2 . 2.127 matrix reduced rowechelon form use minus 1 trick compute basis kernel see section 2.3.3. alternatively express nonpivot columns columns 3 4 linear com binations pivot columns columns 1 2. third column a3 equivalent 1 2 times second column a2. therefore 0 a3 1 2a2. way see a4 a11 2a2 and therefore 0 a11 2a2a4. overall gives us kernel null space kerφ span 0 1 2 1 0 1 1 2 0 1 . 2.128 ranknullity theorem theorem 2.24 ranknullity theorem. vector spaces v w lin ear mapping φ v w holds dimkerφ dimimφ dimv . 2.129 ranknullity theorem also referred fundamental theorem fundamental theorem linear mappings linear mappings axler 2015 theorem 3.22. following direct consequences theorem 2.24 dimimφ dimv kerφ nontrivial i.e. kernel contains 0v dimkerφ 1. aφ transformation matrix φ respect ordered basis dimimφ dimv system linear equations aφx 0 infinitely many solutions. dimv dimw following threeway equivalence holds φ injective φ surjective φ bijective since imφ w. draft 20230215 mathematics machine learning. feedback
2.8 affine spaces 61 2.8 affine spaces following closer look spaces offset origin i.e. spaces longer vector subspaces. moreover briefly discuss properties mappings affine spaces resemble linear mappings. remark. machine learning literature distinction linear affine sometimes clear find references affine spacesmappings linear spacesmappings. 2.8.1 affine subspaces definition 2.25 affine subspace. let v vector space x0 v u v subspace. subset l x0 u x0 u u u 2.130a v v u u v x0 u v 2.130b called affine subspace linear manifold v . u called direction affine subspace linear manifold direction direction space x0 called support point. chapter 12 refer direction space support point subspace hyperplane. hyperplane note definition affine subspace excludes 0 x0 u. therefore affine subspace linear subspace vector subspace v x0 u. examples affine subspaces points lines planes r3 necessarily go origin. remark. consider two affine subspaces l x0 u l x0 u vector space v . then l l u u x0 x0 u. affine subspaces often described parameters consider kdimen sional affine space l x0 u v . b1 . . . bk ordered basis u every element x l uniquely described x x0 λ1b1 . . . λkbk 2.131 λ1 . . . λk r. representation called parametric equation parametric equation l directional vectors b1 . . . bk parameters λ1 . . . λk. parameters example 2.26 affine subspaces onedimensional affine subspaces called lines written line x0 λb1 λ r u spanb1 rn one dimensional subspace rn. means line defined sup port point x0 vector b1 defines direction. see figure 2.2 illustration. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
62 linear algebra twodimensional affine subspaces rn called planes. para plane metric equation planes x0 λ1b1 λ2b2 λ1 λ2 r u spanb1 b2 rn. means plane defined support point x0 two linearly independent vectors b1 b2 span direction space. rn n 1dimensional affine subspaces called hyperplanes hyperplane corresponding parametric equation x0 pn1 i1 λibi b1 . . . bn1 form basis n 1dimensional subspace u rn. means hyperplane defined support point x0 n 1 linearly independent vectors b1 . . . bn1 span direction space. r2 line also hyperplane. r3 plane also hyperplane. figure 2.2 lines affine subspaces. vectors line x0 λb1 lie affine subspace l support point x0 direction b1. 0 x0 b1 l x0 λb1 remark inhomogeneous systems linear equations affine subspaces. rmn x rm solution system linear equa tions aλ x either empty set affine subspace rn dimension n rka. particular solution linear equation λ1b1 . . . λnbn x λ1 . . . λn 0 . . . 0 hyperplane rn. rn every kdimensional affine subspace solution inho mogeneous system linear equations ax b rmn b rm rka n k. recall homogeneous equation systems ax 0 solution vector subspace also think special affine space support point x0 0. 2.8.2 affine mappings similar linear mappings vector spaces discussed section 2.7 define affine mappings two affine spaces. linear affine mappings closely related. therefore many properties already know linear mappings e.g. composition linear mappings linear mapping also hold affine mappings. definition 2.26 affine mapping. two vector spaces v w linear draft 20230215 mathematics machine learning. feedback
2.9 reading 63 mapping φ v w w mapping ϕ v w 2.132 x 7a φx 2.133 affine mapping v w. vector called translation affine mapping translation vector vector ϕ. every affine mapping ϕ v w also composition linear mapping φ v w translation τ w w w ϕ τ φ. mappings φ τ uniquely determined. composition ϕ ϕ affine mappings ϕ v w ϕ w x affine. affine mappings keep geometric structure invariant. also pre serve dimension parallelism. 2.9 reading many resources learning linear algebra including text books strang 2003 golan 2007 axler 2015 liesen mehrmann 2015. also several online resources men tioned introduction chapter. covered gaussian elim ination here many approaches solving systems linear equations refer numerical linear algebra textbooks stoer burlirsch 2002 golub van loan 2012 horn johnson 2013 indepth discussion. book distinguish topics linear algebra e.g. vectors matrices linear independence basis topics related geometry vector space. chapter 3 introduce inner product induces norm. concepts allow us define angles lengths distances use orthogonal projections. pro jections turn key many machine learning algorithms linear regression principal component analysis cover chapters 9 10 respectively. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
64 linear algebra exercises 2.1 consider r1 b ab b a b r1 2.134 a. show r1 abelian group. b. solve 3 x x 15 abelian group r1 is defined 2.134. 2.2 let n n0. let k x z. define congruence class k integer k set k x z x k 0 modn x z a z x k n a . define znz sometimes written zn set congruence classes modulo n. euclidean division implies set finite set con taining n elements zn 0 1 . . . n 1 a b zn define b b a. show zn group. abelian b. define another operation for b zn b b 2.135 b represents usual multiplication z. let n 5. draw times table elements z50 i.e. calculate products b b z50. hence show z50 closed and possesses neutral element . display inverse elements z50 . conclude z50 abelian group. c. show z80 group. d. recall b ezout theorem states two integers b relatively prime i.e. gcda b 1 exist two integers u v au bv 1. show zn0 group n n0 prime. 2.3 consider set g 3 3 matrices defined follows g 1 x z 0 1 0 0 1 r33 x y z r define standard matrix multiplication. g group yes abelian justify answer. 2.4 compute following matrix products possible draft 20230215 mathematics machine learning. feedback
exercises 65 a. 1 2 4 5 7 8 1 1 0 0 1 1 1 0 1 b. 1 2 3 4 5 6 7 8 9 1 1 0 0 1 1 1 0 1 c. 1 1 0 0 1 1 1 0 1 1 2 3 4 5 6 7 8 9 d. 1 2 1 2 4 1 1 4 0 3 1 1 2 1 5 2 e. 0 3 1 1 2 1 5 2 1 2 1 2 4 1 1 4 2.5 find set solutions x following inhomogeneous linear systems ax b b defined follows a. 1 1 1 1 2 5 7 5 2 1 1 3 5 2 4 2 b 1 2 4 6 b. 1 1 0 0 1 1 1 0 3 0 2 1 0 1 1 1 2 0 2 1 b 3 6 5 1 2.6 using gaussian elimination find solutions inhomogeneous equa tion system ax b 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 b 2 1 1 . 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
66 linear algebra 2.7 find solutions x x1 x2 x3 r3 equation system ax 12x 6 4 3 6 0 9 0 8 0 p3 i1 xi 1. 2.8 determine inverses following matrices possible a. 2 3 4 3 4 5 4 5 6 b. 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 2.9 following sets subspaces r3 a. λ λ µ3 λ µ3 λ µ r b. b λ2 λ2 0 λ r c. let γ r. c ξ1 ξ2 ξ3 r3 ξ1 2ξ2 3ξ3 γ d. ξ1 ξ2 ξ3 r3 ξ2 z 2.10 following sets vectors linearly independent a. x1 2 1 3 x2 1 1 2 x3 3 3 8 b. x1 1 2 1 0 0 x2 1 1 0 1 1 x3 1 0 0 1 1 2.11 write 1 2 5 linear combination x1 1 1 1 x2 1 2 3 x3 2 1 1 draft 20230215 mathematics machine learning. feedback
exercises 67 2.12 consider two subspaces r4 u1 span 1 1 3 1 2 1 0 1 1 1 1 1 u2 span 1 2 2 1 2 2 0 0 3 6 2 1 . determine basis u1 u2. 2.13 consider two subspaces u1 u2 u1 solution space homogeneous equation system a1x 0 u2 solution space homogeneous equation system a2x 0 a1 1 0 1 1 2 1 2 1 3 1 0 1 a2 3 3 0 1 2 3 7 5 2 3 1 2 . a. determine dimension u1 u2. b. determine bases u1 u2. c. determine basis u1 u2. 2.14 consider two subspaces u1 u2 u1 spanned columns a1 u2 spanned columns a2 a1 1 0 1 1 2 1 2 1 3 1 0 1 a2 3 3 0 1 2 3 7 5 2 3 1 2 . a. determine dimension u1 u2 b. determine bases u1 u2 c. determine basis u1 u2 2.15 let f x y z r3 xyz 0 g ab ab a3b a b r. a. show f g subspaces r3. b. calculate f g without resorting basis vector. c. find one basis f one g calculate fg using basis vectors previously found check result previous question. 2.16 following mappings linear a. let a b r. φ l1a b r f 7φf z b fxdx l1a b denotes set integrable functions a b. b. φ c1 c0 f 7φf f k 1 ck denotes set k times continuously differen tiable functions c0 denotes set continuous functions. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
68 linear algebra c. φ r r x 7φx cosx d. φ r3 r2 x 7 1 2 3 1 4 3 x e. let θ 0 2π φ r2 r2 x 7 cosθ sinθ sinθ cosθ x 2.17 consider linear mapping φ r3 r4 φ x1 x2 x3 3x1 2x2 x3 x1 x2 x3 x1 3x2 2x1 3x2 x3 find transformation matrix aφ. determine rkaφ. compute kernel image φ. dimkerφ dimimφ 2.18 let e vector space. let f g two automorphisms e f g ide i.e. f g identity mapping ide. show kerf kerg f img img f kerf img 0e. 2.19 consider endomorphism φ r3 r3 whose transformation matrix with respect standard basis r3 aφ 1 1 0 1 1 0 1 1 1 . a. determine kerφ imφ. b. determine transformation matrix aφ respect basis b 1 1 1 1 2 1 1 0 0 i.e. perform basis change toward new basis b. 2.20 let us consider b1 b2 b 1 b 2 4 vectors r2 expressed standard basis r2 b1 2 1 b2 1 1 b 1 2 2 b 2 1 1 let us define two ordered bases b b1 b2 b b 1 b 2 r2. draft 20230215 mathematics machine learning. feedback
exercises 69 a. show b b two bases r2 draw basis vectors. b. compute matrix p 1 performs basis change b b. c. consider c1 c2 c3 three vectors r3 defined standard basis r3 c1 1 2 1 c2 0 1 2 c3 1 0 1 define c c1 c2 c3. i show c basis r3 e.g. using determinants see section 4.1. ii let us call c c 1 c 2 c 3 standard basis r3. determine matrix p 2 performs basis change c c. d. consider homomorphism φ r2 r3 φb1 b2 c2 c3 φb1 b2 2c1 c2 3c3 b b1 b2 c c1 c2 c3 ordered bases r2 r3 respectively. determine transformation matrix aφ φ respect or dered bases b c. e. determine a transformation matrix φ respect bases b c. f. let us consider vector x r2 whose coordinates b 2 3. words x 2b 1 3b 2. i calculate coordinates x b. ii based that compute coordinates φx expressed c. iii then write φx terms c 1 c 2 c 3. iv use representation x b matrix a find result directly. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
3 analytic geometry chapter 2 studied vectors vector spaces linear mappings general abstract level. chapter add geomet ric interpretation intuition concepts. particular look geometric vectors compute lengths distances angles two vectors. able this equip vec tor space inner product induces geometry vector space. inner products corresponding norms metrics capture intuitive notions similarity distances use develop support vector machine chapter 12. use concepts lengths angles vectors discuss orthogonal projections play central role discuss principal component anal ysis chapter 10 regression via maximum likelihood estimation chapter 9. figure 3.1 gives overview concepts chapter related connected chapters book. figure 3.1 mind map concepts introduced chapter along used parts book. inner product norm lengths orthogonal projection angles rotations chapter 4 matrix decomposition chapter 10 dimensionality reduction chapter 9 regression chapter 12 classification induces 70 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.
3.1 norms 71 figure 3.1 different norms red lines indicate set vectors norm 1. left manhattan norm right euclidean distance. 1 1 1 1 x1 1 x2 1 3.1 norms think geometric vectors i.e. directed line segments start origin intuitively length vector distance end directed line segment origin. following discuss notion length vectors using concept norm. definition 3.1 norm. norm vector space v function norm v r 3.1 x 7x 3.2 assigns vector x length xr λ r length x v following hold absolutely homogeneous absolutely homogeneous λx λx triangle inequality triangle inequality x yx y positive definite positive definite x0 x 0 x 0 figure 3.2 triangle inequality. b c a b geometric terms triangle inequality states triangle sum lengths two sides must greater equal length remaining side see figure 3.2 illustration. definition 3.1 terms general vector space v section 2.4 book consider finitedimensional vector space rn. recall vector x rn denote elements vector using subscript is xi ith element vector x. example 3.1 manhattan norm manhattan norm rn defined x rn manhattan norm x1 n x i1 xi 3.3 absolute value. left panel figure 3.1 shows vectors x r2 x1 1. manhattan norm also called ℓ1 ℓ1 norm norm. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
72 analytic geometry example 3.2 euclidean norm euclidean norm x rn defined euclidean norm x2 v u u n x i1 x2 xx 3.4 computes euclidean distance x origin. right panel euclidean distance figure 3.1 shows vectors x r2 x2 1. euclidean norm also called ℓ2 norm. ℓ2 norm remark. throughout book use euclidean norm 3.4 default stated otherwise. 3.2 inner products inner products allow introduction intuitive geometrical con cepts length vector angle distance two vectors. major purpose inner products determine whether vectors orthogonal other. 3.2.1 dot product may already familiar particular type inner product scalar productdot product rn given scalar product dot product xy n x i1 xiyi . 3.5 refer particular inner product dot product book. however inner products general concepts specific properties introduce. 3.2.2 general inner products recall linear mapping section 2.7 rearrange mapping respect addition multiplication scalar. bi bilinear mapping linear mapping ωis mapping two arguments linear argument i.e. look vector space v holds x y z v λ ψ r ωλx ψy z λωx z ψωy z 3.6 ωx λy ψz λωx y ψωx z . 3.7 here 3.6 asserts ωis linear first argument 3.7 asserts ωis linear second argument see also 2.87. draft 20230215 mathematics machine learning. feedback
3.2 inner products 73 definition 3.2. let v vector space ω v v r bilinear mapping takes two vectors maps onto real number. ωis called symmetric ωx y ωy x x v i.e. symmetric order arguments matter. ωis called positive definite positive definite x v 0 ωx x 0 ω0 0 0 . 3.8 definition 3.3. let v vector space ω v v r bilinear mapping takes two vectors maps onto real number. positive definite symmetric bilinear mapping ω v v r called inner product v . typically write x yinstead ωx y. inner product pair v called inner product space real vector space inner product space vector space inner product inner product. use dot product defined 3.5 call v euclidean vector space. euclidean vector space refer spaces inner product spaces book. example 3.3 inner product dot product consider v r2. define x y x1y1 x1y2 x2y1 2x2y2 3.9 is inner product different dot product. proof exercise. 3.2.3 symmetric positive definite matrices symmetric positive definite matrices play important role machine learning defined via inner product. section 4.3 return symmetric positive definite matrices context matrix decompositions. idea symmetric positive semidefinite matrices key definition kernels section 12.4. consider ndimensional vector space v inner product v v r see definition 3.3 ordered basis b b1 . . . bn v . recall section 2.6.1 vectors x v written linear combinations basis vectors x pn i1 ψibi v pn j1 λjbj v suitable ψi λj r. due bilinearity inner product holds x v x y n x i1 ψibi n x j1 λjbj n x i1 n x j1 ψi bi bjλj ˆ xaˆ 3.10 aij bi bjand ˆ x ˆ coordinates x respect basis b. implies inner product is uniquely deter mined a. symmetry inner product also means 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
74 analytic geometry symmetric. furthermore positive definiteness inner product implies x v 0 xax 0 . 3.11 definition 3.4 symmetric positive definite matrix. symmetric matrix rnn satisfies 3.11 called symmetric positive definite symmetric positive definite positive definite. holds 3.11 called symmetric positive definite symmetric positive semidefinite positive semidefinite. example 3.4 symmetric positive definite matrices consider matrices a1 9 6 6 5 a2 9 6 6 3 . 3.12 a1 positive definite symmetric xa1x x1 x2 9 6 6 5 x1 x2 3.13a 9x2 1 12x1x2 5x2 2 3x1 2x22 x2 2 0 3.13b x v 0. contrast a2 symmetric positive definite xa2x 9x2 1 12x1x2 3x2 2 3x1 2x22 x2 2 less 0 e.g. x 2 3. rnn symmetric positive definite x y ˆ xaˆ 3.14 defines inner product respect ordered basis b ˆ x ˆ coordinate representations x v respect b. theorem 3.5. realvalued finitedimensional vector space v ordered basis b v holds v v r inner product exists symmetric positive definite matrix rnn x y ˆ xaˆ . 3.15 following properties hold rnn symmetric positive definite null space kernel consists 0 xax 0 x 0. implies ax 0 x 0. diagonal elements aii positive aii e aei 0 ei ith vector standard basis rn. draft 20230215 mathematics machine learning. feedback
3.3 lengths distances 75 3.3 lengths distances section 3.1 already discussed norms use compute length vector. inner products norms closely related sense inner product induces norm inner products induce norms. x q x x 3.16 natural way compute lengths vectors using in ner product. however every norm induced inner product. manhattan norm 3.3 example norm without corresponding inner product. following focus norms induced inner products introduce geometric concepts lengths dis tances angles. remark cauchyschwarz inequality. inner product vector space v induced norm satisfies cauchyschwarz inequality cauchyschwarz inequality x y xy. 3.17 example 3.5 lengths vectors using inner products geometry often interested lengths vectors. use inner product compute using 3.16. let us take x 1 1 r2. use dot product inner product 3.16 obtain x xx 12 12 2 3.18 length x. let us choose different inner product x y x 1 1 2 1 2 1 x1y1 1 2x1y2 x2y1 x2y2 . 3.19 compute norm vector inner product returns smaller values dot product x1 x2 sign and x1x2 0 otherwise returns greater values dot product. inner product obtain x x x2 1 x1x2 x2 2 1 1 1 1 x 1 1 3.20 x shorter inner product dot product. definition 3.6 distance metric. consider inner product space v . dx y x y q x y x y 3.21 called distance x x v . use dot distance product inner product distance called euclidean distance. euclidean distance 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
76 analytic geometry mapping v v r 3.22 x y 7dx y 3.23 called metric. metric remark. similar length vector distance vectors require inner product norm sufficient. norm induced inner product distance may vary depending choice inner product. metric satisfies following 1. positive definite i.e. dx y 0 x v dx y positive definite 0 x . 2. symmetric i.e. dx y dy x x v . symmetric triangle inequality 3. triangle inequality dx z dx y dy z x y z v . remark. first glance lists properties inner products met rics look similar. however comparing definition 3.3 defini tion 3.6 observe x yand dx y behave opposite directions. similar x result large value inner product small value metric. 3.4 angles orthogonality figure 3.2 restricted 0 π fω cosω returns unique number interval 1 1. 0 π2 π ω 1 0 1 cosω addition enabling definition lengths vectors well distance two vectors inner products also capture geometry vector space defining angle ω two vectors. use cauchyschwarz inequality 3.17 define angles ω inner prod uct spaces two vectors x y notion coincides intuition r2 r3. assume x 0 0. 1 x y xy1 . 3.24 therefore exists unique ω 0 π illustrated figure 3.2 cos ω x y xy. 3.25 number ω angle vectors x y. intuitively angle angle two vectors tells us similar orientations are. example using dot product angle x 4x i.e. scaled version x 0 orientation same. draft 20230215 mathematics machine learning. feedback
3.4 angles orthogonality 77 example 3.6 angle vectors let us compute angle x 1 1r2 1 2r2 figure 3.3 angle ω two vectors x computed using inner product. x 1 0 1 ω see figure 3.3 use dot product inner product. get cos ω x y p x xy y xy p xxyy 3 10 3.26 angle two vectors arccos 3 10 0.32 rad corresponds 18. key feature inner product also allows us characterize vectors orthogonal. definition 3.7 orthogonality. two vectors x orthogonal orthogonal x y 0 write x y. additionally x 1 y i.e. vectors unit vectors x orthonormal. orthonormal implication definition 0vector orthogonal every vector vector space. remark. orthogonality generalization concept perpendic ularity bilinear forms dot product. context geometrically think orthogonal vectors right angle respect specific inner product. example 3.7 orthogonal vectors figure 3.1 angle ω two vectors x change depending inner product. x 1 1 0 1 ω consider two vectors x 1 1 1 1r2 see figure 3.1. interested determining angle ω using two different inner products. using dot product inner product yields angle ω x 90 x y. however choose inner product x y x 2 0 0 1 3.27 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
78 analytic geometry get angle ω x given cos ω x y xy 1 3 ω 1.91 rad 109.5 3.28 x orthogonal. therefore vectors orthogonal respect one inner product orthogonal re spect different inner product. definition 3.8 orthogonal matrix. square matrix rnn orthogonal matrix columns orthonormal orthogonal matrix aa aa 3.29 implies a1 a 3.30 i.e. inverse obtained simply transposing matrix. convention call matrices orthogonal precise description would orthonormal. transformations orthogonal matrices special length vector x changed transforming using orthogonal matrix a. dot product obtain transformations orthogonal matrices preserve distances angles. ax 2 axax xaax xix xx x 2 . 3.31 moreover angle two vectors x y measured inner product also unchanged transforming using orthogonal matrix a. assuming dot product inner product angle images ax ay given cos ω axay axay xaay q xaaxyaay xy xy 3.32 gives exactly angle x y. means orthog onal matrices a a1 preserve angles distances. turns orthogonal matrices define transformations rota tions with possibility flips. section 3.9 discuss details rotations. 3.5 orthonormal basis section 2.6.1 characterized properties basis vectors found ndimensional vector space need n basis vectors i.e. n vectors linearly independent. sections 3.3 3.4 used inner products compute length vectors angle vectors. following discuss special case basis vectors orthogonal length basis vector 1. call basis orthonormal basis. draft 20230215 mathematics machine learning. feedback
3.6 orthogonal complement 79 let us introduce formally. definition 3.9 orthonormal basis. consider ndimensional vector space v basis b1 . . . bn v . bi bj 0 j 3.33 bi bi 1 3.34 i j 1 . . . n basis called orthonormal basis onb. orthonormal basis onb 3.33 satisfied basis called orthogonal basis. note orthogonal basis 3.34 implies every basis vector lengthnorm 1. recall section 2.6.1 use gaussian elimination find basis vector space spanned set vectors. assume given set b1 . . . bn nonorthogonal unnormalized basis vectors. concatenate matrix b b1 . . . bn apply gaussian elim ination augmented matrix section 2.3.2 b b b obtain orthonormal basis. constructive way iteratively build orthonor mal basis b1 . . . bn called gramschmidt process strang 2003. example 3.8 orthonormal basis canonicalstandard basis euclidean vector space rn or thonormal basis inner product dot product vectors. r2 vectors b1 1 2 1 1 b2 1 2 1 1 3.35 form orthonormal basis since b 1 b2 0 b1 1 b2. exploit concept orthonormal basis chapter 12 chapter 10 discuss support vector machines principal com ponent analysis. 3.6 orthogonal complement defined orthogonality look vector spaces orthogonal other. play important role chapter 10 discuss linear dimensionality reduction geometric per spective. consider ddimensional vector space v mdimensional sub space u v . orthogonal complement u is dmdimensional orthogonal complement subspace v contains vectors v orthogonal every vector u. furthermore u u 0 vector x v 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
80 analytic geometry figure 3.1 plane u threedimensional vector space described normal vector spans orthogonal complement u. e3 e1 e2 w u uniquely decomposed x x m1 λmbm dm x j1 ψjb j λm ψj r 3.36 b1 . . . bm basis u b 1 . . . b dm basis u . therefore orthogonal complement also used describe plane u twodimensional subspace threedimensional vector space. specifically vector w w 1 orthogonal plane u basis vector u . figure 3.1 illustrates setting. vectors orthogonal w must by construction lie plane u. vector w called normal vector u. normal vector generally orthogonal complements used describe hyperplanes ndimensional vector affine spaces. 3.7 inner product functions thus far looked properties inner products compute lengths angles distances. focused inner products finitedimensional vectors. following look example inner products different type vectors inner products functions. inner products discussed far defined vectors finite number entries. think vector x rn function n function values. concept inner product generalized vectors infinite number entries countably infinite also continuousvalued functions uncountably infinite. sum individual components vectors see equation 3.5 example turns integral. inner product two functions u r r v r r defined definite integral u v z b uxvxdx 3.37 draft 20230215 mathematics machine learning. feedback
3.8 orthogonal projections 81 lower upper limits a b respectively. usual inner product define norms orthogonality looking inner product. 3.37 evaluates 0 functions u v orthogonal. make preceding inner product mathematically precise need take care measures definition integrals leading definition hilbert space. furthermore unlike inner products finitedimensional vectors inner products functions may diverge have infinite value. requires diving intricate details real functional analysis cover book. example 3.9 inner product functions choose u sinx v cosx integrand fx uxvx figure 3.2 fx sinx cosx. 2.5 0.0 2.5 x 0.5 0.0 0.5 sinx cosx 3.37 shown figure 3.2. see function odd i.e. fx fx. therefore integral limits π b π product evaluates 0. therefore sin cos orthogonal functions. remark. also holds collection functions 1 cosx cos2x cos3x . . . 3.38 orthogonal integrate π π i.e. pair functions orthogonal other. collection functions 3.38 spans large subspace functions even periodic π π projecting functions onto subspace fundamental idea behind fourier series. section 6.4.6 look second type unconventional inner products inner product random variables. 3.8 orthogonal projections projections important class linear transformations besides rota tions reflections play important role graphics coding the ory statistics machine learning. machine learning often deal data highdimensional. highdimensional data often hard analyze visualize. however highdimensional data quite often pos sesses property dimensions contain information dimensions essential describe key properties data. compress visualize highdimensional data lose information. minimize compression loss ideally find informative dimensions data. discussed chapter 1 feature common expression data representation. data represented vectors chapter discuss fundamental tools data compression. specifically project original highdimensional data onto lowerdimensional feature space work lowerdimensional space learn dataset extract relevant patterns. example machine 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
82 analytic geometry figure 3.1 orthogonal projection orange dots twodimensional dataset blue dots onto onedimensional subspace straight line. 4 2 0 2 4 x1 2 1 0 1 2 x2 learning algorithms principal component analysis pca pear son 1901 hotelling 1933 deep neural networks e.g. deep autoencoders deng et al. 2010 heavily exploit idea dimension ality reduction. following focus orthogonal projections use chapter 10 linear dimensionality reduction chapter 12 classification. even linear regression discuss chapter 9 interpreted using orthogonal projections. given lowerdimensional subspace orthogonal projections highdimensional data retain much information possible minimize difference error original data corresponding projection. il lustration orthogonal projection given figure 3.1. detail obtain projections let us define projection actually is. definition 3.10 projection. let v vector space u v subspace v . linear mapping π v u called projection projection π2 π π π. since linear mappings expressed transformation matrices see section 2.7 preceding definition applies equally special kind transformation matrices projection matrices p π exhibit projection matrix property p 2 π p π. following derive orthogonal projections vectors inner product space rn onto subspaces. start one dimensional subspaces also called lines. mentioned oth line erwise assume dot product x y xy inner product. 3.8.1 projection onto onedimensional subspaces lines assume given line onedimensional subspace ori gin basis vector b rn. line onedimensional subspace u rn spanned b. project x rn onto u seek vector πux u closest x. using geometric arguments let us draft 20230215 mathematics machine learning. feedback
3.8 orthogonal projections 83 figure 3.2 examples projections onto onedimensional subspaces. b x πux ω a projection x r2 onto subspace u basis vector b. cos ω ω sin ω b x b projection twodimensional vector x x 1 onto onedimensional subspace spanned b. characterize properties projection πux figure 3.2a serves illustration projection πux closest x closest implies distance xπuxis minimal. follows segment πuxx πux x orthogonal u therefore basis vector b u. orthogonality condition yields πux x b 0 since angles vectors defined via inner product. λ coordinate πux respect b. projection πux x onto u must element u and there fore multiple basis vector b spans u. hence πux λb λ r. following three steps determine coordinate λ projection πux u projection matrix p π maps x rn onto u 1. finding coordinate λ. orthogonality condition yields x πux b 0 πuxλb x λb b 0 . 3.39 exploit bilinearity inner product arrive general inner product get λ x bif b 1. x bλ b b 0 λ x b b b b x b2 . 3.40 last step exploited fact inner products symmet ric. choose to dot product obtain λ bx bb bx b2 . 3.41 b 1 coordinate λ projection given bx. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
84 analytic geometry 2. finding projection point πux u. since πux λb imme diately obtain 3.40 πux λb x b b2 b bx b2 b 3.42 last equality holds dot product only. also compute length πux means definition 3.1 πux λb λ b. 3.43 hence projection length λ times length b. also adds intuition λ coordinate πux respect basis vector b spans onedimensional subspace u. use dot product inner product get πux 3.42 bx b2 b 3.25 cos ω xbb b2 cos ω x. 3.44 here ω angle x b. equation familiar trigonometry x 1 x lies unit circle. follows projection onto horizontal axis spanned b exactly horizontal axis onedimensional subspace. cos ω length corresponding vector πux cos ω. illustration given figure 3.2b. 3. finding projection matrix p π. know projection lin ear mapping see definition 3.10. therefore exists projection matrix p π πux p πx. dot product inner product πux λb bλ b bx b2 bb b2 x 3.45 immediately see p π bb b2 . 3.46 note bband consequently p π symmetric matrix of rank projection matrices always symmetric. 1 b2 b bis scalar. projection matrix p π projects vector x rn onto line origin direction b equivalently subspace u spanned b. remark. projection πux rn still ndimensional vector scalar. however longer require n coordinates represent projection single one want express respect basis vector b spans subspace u λ. draft 20230215 mathematics machine learning. feedback
3.8 orthogonal projections 85 figure 3.1 projection onto twodimensional subspace u basis b1 b2. projection πux x r3 onto u expressed linear combination b1 b2 displacement vector x πux orthogonal b1 b2. 0 x b1 b2 u πux x πux example 3.10 projection onto line find projection matrix p π onto line origin spanned b 1 2 2 . b direction basis onedimensional subspace line origin. 3.46 obtain p π bb bb 1 9 1 2 2 1 2 2 1 9 1 2 2 2 4 4 2 4 4 . 3.47 let us choose particular x see whether lies subspace spanned b. x 1 1 1 projection πux p πx 1 9 1 2 2 2 4 4 2 4 4 1 1 1 1 9 5 10 10 span 1 2 2 . 3.48 note application p π πux change anything i.e. p ππux πux. expected according definition 3.10 know projection matrix p π satisfies p 2 πx p πx x. remark. results chapter 4 show πux eigenvector p π corresponding eigenvalue 1. 3.8.2 projection onto general subspaces u given set spanning vectors basis make sure determine basis b1 . . . bm proceeding. following look orthogonal projections vectors x rn onto lowerdimensional subspaces u rn dimu 1. illustration given figure 3.1. assume b1 . . . bm ordered basis u. projection πux onto u necessarily element u. therefore represented 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
86 analytic geometry linear combinations basis vectors b1 . . . bm u πux pm i1 λibi. basis vectors form columns b rnm b b1 . . . bm. 1d case follow threestep procedure find projec tion πux projection matrix p π 1. find coordinates λ1 . . . λm projection with respect basis u linear combination πux x i1 λibi bλ 3.49 b b1 . . . bm rnm λ λ1 . . . λmrm 3.50 closest x rn. 1d case closest means minimum distance implies vector connecting πux u x rn must orthogonal basis vectors u. therefore obtain simultaneous conditions assuming dot product inner product b1 x πux b 1 x πux 0 3.51 . . . bm x πux b mx πux 0 3.52 which πux bλ written b 1 x bλ 0 3.53 . . . b mx bλ 0 3.54 obtain homogeneous linear equation system b 1 . . . b x bλ 0 bx bλ 0 3.55 bbλ bx . 3.56 last expression called normal equation. since b1 . . . bm normal equation basis u and therefore linearly independent bb rmm reg ular inverted. allows us solve coefficients coordinates λ bb1bx . 3.57 matrix bb1bis also called pseudoinverse b pseudoinverse computed nonsquare matrices b. requires bb positive definite case b full rank. practical ap plications e.g. linear regression often add jitter term ϵi draft 20230215 mathematics machine learning. feedback
3.8 orthogonal projections 87 bb guarantee increased numerical stability positive definite ness. ridge rigorously derived using bayesian inference. see chapter 9 details. 2. find projection πux u. already established πux bλ. therefore 3.57 πux bbb1bx . 3.58 3. find projection matrix p π. 3.58 immediately see projection matrix solves p πx πux must p π bbb1b. 3.59 remark. solution projecting onto general subspaces includes 1d case special case dimu 1 bb r scalar rewrite projection matrix 3.59 p π bbb1bas p π bb bb exactly projection matrix 3.46. example 3.11 projection onto twodimensional subspace subspace u span 1 1 1 0 1 2 r3 x 6 0 0 r3 find coordinates λ x terms subspace u projection point πux projection matrix p π. first see generating set u basis linear indepen dence write basis vectors u matrix b 1 0 1 1 1 2 . second compute matrix bb vector bx bb 1 1 1 0 1 2 1 0 1 1 1 2 3 3 3 5 bx 1 1 1 0 1 2 6 0 0 6 0 . 3.60 third solve normal equation bbλ bx find λ 3 3 3 5 λ1 λ2 6 0 λ 5 3 . 3.61 fourth projection πux x onto u i.e. column space b directly computed via πux bλ 5 2 1 . 3.62 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
88 analytic geometry corresponding projection error norm difference vector projection error original vector projection onto u i.e. projection error also called reconstruction error. x πux 1 2 1 6 . 3.63 fifth projection matrix for x r3 given p π bbb1b 1 6 5 2 1 2 2 2 1 2 5 . 3.64 verify results a check whether displacement vector πux x orthogonal basis vectors u b verify p π p 2 π see definition 3.10. remark. projections πux still vectors rn although lie mdimensional subspace u rn. however represent projected vector need coordinates λ1 . . . λm respect basis vectors b1 . . . bm u. remark. vector spaces general inner products pay attention computing angles distances defined means inner product. find approximate solutions unsolvable linear equation systems using projections. projections allow us look situations linear system ax b without solution. recall means b lie span a i.e. vector b lie subspace spanned columns a. given linear equation cannot solved exactly find approximate solution. idea find vector subspace spanned columns closest b i.e. compute orthogonal projection b onto subspace spanned columns a. problem arises often practice solution called leastsquares solution assuming dot product inner product leastsquares solution overdetermined system. discussed section 9.4. using reconstruction errors 3.63 one possible approach derive principal component analysis section 10.3. remark. looked projections vectors x onto subspace u basis vectors b1 . . . bk. basis onb i.e. 3.33 3.34 satisfied projection equation 3.58 simplifies greatly πux bbx 3.65 since bb coordinates λ bx . 3.66 means longer compute inverse 3.58 saves computation time. draft 20230215 mathematics machine learning. feedback
3.8 orthogonal projections 89 3.8.3 gramschmidt orthogonalization projections core gramschmidt method allows us constructively transform basis b1 . . . bn ndimensional vector space v orthogonalorthonormal basis u1 . . . un v . basis always exists liesen mehrmann 2015 spanb1 . . . bn spanu1 . . . un. gramschmidt orthogonalization method iteratively gramschmidt orthogonalization constructs orthogonal basis u1 . . . un basis b1 . . . bn v follows u1 b1 3.67 uk bk πspanu1.uk1bk k 2 . . . n . 3.68 3.68 kth basis vector bk projected onto subspace spanned first k 1 constructed orthogonal vectors u1 . . . uk1 see sec tion 3.8.2. projection subtracted bk yields vector uk orthogonal k 1dimensional subspace spanned u1 . . . uk1. repeating procedure n basis vectors b1 . . . bn yields orthogonal basis u1 . . . un v . normalize uk obtain onb uk 1 k 1 . . . n. example 3.12 gramschmidt orthogonalization figure 3.2 gramschmidt orthogonalization. a nonorthogonal basis b1 b2 r2 b first constructed basis vector u1 orthogonal projection b2 onto spanu1 c orthogonal basis u1 u2 r2. b1 b2 0 a original nonorthogonal basis vectors b1 b2. u1 b2 0 πspanu1b2 b first new basis vector u1 b1 projection b2 onto subspace spanned u1. u1 b2 0 πspanu1b2 u2 c orthogonal basis vectors u1 u2 b2 πspanu1b2. consider basis b1 b2 r2 b1 2 0 b2 1 1 3.69 see also figure 3.2a. using gramschmidt method construct orthogonal basis u1 u2 r2 follows assuming dot product inner product u1 b1 2 0 3.70 u2 b2 πspanu1b2 3.45 b2 u1u 1 u1 2 b2 1 1 1 0 0 0 1 1 0 1 . 3.71 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
90 analytic geometry figure 3.3 projection onto affine space. a original setting b setting shifted x0 x x0 projected onto direction space u c projection translated back x0 πux x0 gives final orthogonal projection πlx. l x0 x b2 b1 0 a setting. b1 0 x x0 u l x0 πux x0 b2 b reduce problem pro jection πu onto vector sub space. l x0 x b2 b1 0 πlx c add support point back get affine projection πl. steps illustrated figures 3.2b c. immediately see u1 u2 orthogonal i.e. u 1 u2 0. 3.8.4 projection onto affine subspaces thus far discussed project vector onto lowerdimensional subspace u. following provide solution projecting vector onto affine subspace. consider setting figure 3.3a. given affine space l x0 u b1 b2 basis vectors u. determine orthogonal projection πlx x onto l transform problem problem know solve projection onto vector subspace. order get there subtract support point x0 x l l x0 u exactly vector subspace u. use orthogonal projections onto subspace discussed section 3.8.2 obtain projection πux x0 illustrated figure 3.3b. projection translated back l adding x0 obtain orthogonal projection onto affine space l πlx x0 πux x0 3.72 πu orthogonal projection onto subspace u i.e. direction space l see figure 3.3c. figure 3.3 also evident distance x affine space l identical distance x x0 u i.e. dx l x πlx x x0 πux x0 3.73a dx x0 πux x0 dx x0 u . 3.73b use projections onto affine subspace derive concept separating hyperplane section 12.1. draft 20230215 mathematics machine learning. feedback
3.9 rotations 91 figure 3.2 rotation rotates objects plane origin. rotation angle positive rotate counterclockwise. original rotated 112.5 figure 3.1 robotic arm needs rotate joints order pick objects place correctly. figure taken deisenroth et al. 2015. 3.9 rotations length angle preservation discussed section 3.4 two characteristics linear mappings orthogonal transformation matri ces. following closer look specific orthogonal transformation matrices describe rotations. rotation linear mapping more specifically automorphism rotation euclidean vector space rotates plane angle θ origin i.e. origin fixed point. positive angle θ 0 com mon convention rotate counterclockwise direction. example shown figure 3.2 transformation matrix r 0.38 0.92 0.92 0.38 . 3.74 important application areas rotations include computer graphics robotics. example robotics often important know rotate joints robotic arm order pick place object see figure 3.1. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
92 analytic geometry figure 3.2 rotation standard basis r2 angle θ. e1 e2 θ θ φe2 sin θ cos θ φe1 cos θ sin θ cos θ sin θ sin θ cos θ 3.9.1 rotations r2 consider standard basis e1 1 0 e2 0 1 r2 defines standard coordinate system r2. aim rotate coordinate system angle θ illustrated figure 3.2. note rotated vectors still linearly independent and therefore basis r2. means rotation performs basis change. rotations φ linear mappings express rotation matrix rθ. trigonometry see figure 3.2 allows us deter rotation matrix mine coordinates rotated axes the image φ respect standard basis r2. obtain φe1 cos θ sin θ φe2 sin θ cos θ . 3.75 therefore rotation matrix performs basis change rotated coordinates rθ given rθ φe1 φe2 cos θ sin θ sin θ cos θ . 3.76 3.9.2 rotations r3 contrast r2 case r3 rotate twodimensional plane onedimensional axis. easiest way specify general rota tion matrix specify images standard basis e1 e2 e3 supposed rotated making sure images re1 re2 re3 orthonormal other. obtain general rotation matrix r combining images standard basis. meaningful rotation angle define coun terclockwise means operate two dimensions. use convention counterclockwise planar rotation axis refers rotation axis look axis head on end toward origin. r3 therefore three planar rotations three standard basis vectors see figure 3.2 draft 20230215 mathematics machine learning. feedback
3.9 rotations 93 figure 3.2 rotation vector gray r3 angle θ e3axis. rotated vector shown blue. e1 e2 e3 θ rotation e1axis r1θ φe1 φe2 φe3 1 0 0 0 cos θ sin θ 0 sin θ cos θ . 3.77 here e1 coordinate fixed counterclockwise rotation performed e2e3 plane. rotation e2axis r2θ cos θ 0 sin θ 0 1 0 sin θ 0 cos θ . 3.78 rotate e1e3 plane e2 axis need look e2 axis tip toward origin. rotation e3axis r3θ cos θ sin θ 0 sin θ cos θ 0 0 0 1 . 3.79 figure 3.2 illustrates this. 3.9.3 rotations n dimensions generalization rotations 2d 3d ndimensional eu clidean vector spaces intuitively described fixing n 2 dimen sions restrict rotation twodimensional plane ndimen sional space. threedimensional case rotate plane twodimensional subspace rn. definition 3.11 givens rotation. let v ndimensional euclidean vector space φ v v automorphism transformation ma 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
94 analytic geometry trix rijθ ii1 0 0 0 cos θ 0 sin θ 0 0 0 iji1 0 0 0 sin θ 0 cos θ 0 0 0 inj rnn 3.80 1 i j n θ r. rijθ called givens rotation. givens rotation essentially rijθ identity matrix rii cos θ rij sin θ rji sin θ rjj cos θ . 3.81 two dimensions i.e. n 2 obtain 3.76 special case. 3.9.4 properties rotations rotations exhibit number useful properties derived considering orthogonal matrices definition 3.8 rotations preserve distances i.e. xy rθxrθy. words rotations leave distance two points unchanged transformation. rotations preserve angles i.e. angle rθx rθy equals angle x y. rotations three or more dimensions generally commuta tive. therefore order rotations applied important even rotate point. two dimensions vector rotations commutative rϕrθ rθrϕ ϕ θ 0 2π. form abelian group with multiplication rotate point e.g. origin. 3.10 reading chapter gave brief overview important concepts analytic geometry use later chapters book. broader indepth overview concepts presented refer following excellent books axler 2015 boyd vandenberghe 2018. inner products allow us determine specific bases vector subspaces vector orthogonal others orthogonal bases using gramschmidt method. bases important optimization numerical algorithms solving linear equation systems. instance krylov subspace methods conjugate gradients generalized minimal residual method gmres minimize residual errors or thogonal stoer burlirsch 2002. machine learning inner products important context draft 20230215 mathematics machine learning. feedback
3.10 reading 95 kernel methods sch olkopf smola 2002. kernel methods exploit fact many linear algorithms expressed purely inner prod uct computations. then kernel trick allows us compute inner products implicitly potentially infinitedimensional feature space without even knowing feature space explicitly. allowed nonlinearization many algorithms used machine learning kernelpca sch olkopf et al. 1997 dimensionality reduction. gaus sian processes rasmussen williams 2006 also fall category kernel methods current state art probabilistic re gression fitting curves data points. idea kernels explored chapter 12. projections often used computer graphics e.g. generate shad ows. optimization orthogonal projections often used iteratively minimize residual errors. also applications machine learning e.g. linear regression want find linear function minimizes residual errors i.e. lengths orthogonal projec tions data onto linear function bishop 2006. investi gate chapter 9. pca pearson 1901 hotelling 1933 also uses projections reduce dimensionality highdimensional data. discuss detail chapter 10. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
96 analytic geometry exercises 3.1 show defined x x1 x2r2 y1 y2r2 x y x1y1 x1y2 x2y1 2x2y2 inner product. 3.2 consider r2 defined x r2 x y x 2 0 1 2 z a . an inner product 3.3 compute distance x 1 2 3 1 1 0 using a. x y xy b. x y xay 2 1 0 1 3 1 0 1 2 3.4 compute angle x 1 2 1 1 using a. x y xy b. x y xby b 2 1 1 3 3.5 consider euclidean vector space r5 dot product. subspace u r5 x r5 given u span 0 1 2 0 2 1 3 1 1 2 3 4 1 2 1 1 3 5 0 7 x 1 9 1 4 1 . a. determine orthogonal projection πux x onto u b. determine distance dx u 3.6 consider r3 inner product x y x 2 1 0 1 2 1 0 1 2 y . furthermore define e1 e2 e3 standardcanonical basis r3. draft 20230215 mathematics machine learning. feedback
exercises 97 a. determine orthogonal projection πue2 e2 onto u spane1 e3 . hint orthogonality defined inner product. b. compute distance de2 u. c. draw scenario standard basis vectors πue2 3.7 let v vector space π endomorphism v . a. prove π projection idv π projection idv identity endomorphism v . b. assume π projection. calculate imidv π keridv π function imπ kerπ. 3.8 using gramschmidt method turn basis b b1 b2 two dimensional subspace u r3 onb c c1 c2 u b1 1 1 1 b2 1 2 0 . 3.9 let n n let x1 . . . xn 0 n positive real numbers x1 . . . xn 1. use cauchyschwarz inequality show a. pn i1 x2 1 n b. pn i1 1 xi n2 hint think dot product rn. then choose specific vectors x rn apply cauchyschwarz inequality. 3.10 rotate vectors x1 2 3 x2 0 1 30. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
4 matrix decompositions chapters 2 3 studied ways manipulate measure vectors projections vectors linear mappings. mappings transforma tions vectors conveniently described operations performed matrices. moreover data often represented matrix form well e.g. rows matrix represent different people columns describe different features people weight height socio economic status. chapter present three aspects matrices summarize matrices matrices decomposed decompositions used matrix approximations. first consider methods allow us describe matrices numbers characterize overall properties matrices. sections determinants section 4.1 eigenval ues section 4.2 important special case square matrices. characteristic numbers important mathematical consequences allow us quickly grasp useful properties matrix has. proceed matrix decomposition methods analogy ma trix decomposition factoring numbers factoring 21 prime numbers 7 3. reason matrix decomposition also often referred matrix factorization. matrix decompositions used matrix factorization describe matrix means different representation using factors interpretable matrices. first cover squarerootlike operation symmetric positive definite matrices cholesky decomposition section 4.3. look two related methods factorizing matrices canoni cal forms. first one known matrix diagonalization section 4.4 allows us represent linear mapping using diagonal trans formation matrix choose appropriate basis. second method singular value decomposition section 4.5 extends factorization nonsquare matrices considered one fundamental concepts linear algebra. decompositions helpful matrices represent ing numerical data often large hard analyze. conclude chapter systematic overview types matrices characteristic properties distinguish form matrix tax onomy section 4.7. methods cover chapter become important 98 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.
4.1 determinant trace 99 figure 4.2 mind map concepts introduced chapter along used parts book. determinant invertibility cholesky eigenvalues eigenvectors orthogonal matrix diagonalization svd chapter 6 probability distributions chapter 10 dimensionality reduction tests used used used determines used used used constructs used used used subsequent mathematical chapters chapter 6 also applied chapters dimensionality reduction chapters 10 den sity estimation chapter 11. chapters overall structure depicted mind map figure 4.2. 4.1 determinant trace determinant notation a must confused absolute value. determinants important concepts linear algebra. determinant mathematical object analysis solution systems linear equations. determinants defined square matrices rnn i.e. matrices number rows columns. book write determinant deta sometimes a deta a11 a12 . . . a1n a21 a22 . . . a2n . . . . . . . an1 an2 . . . ann . 4.1 determinant square matrix rnn function maps determinant 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
100 matrix decompositions onto real number. providing definition determinant general n n matrices let us look motivating examples define determinants special matrices. example 4.1 testing matrix invertibility let us begin exploring square matrix invertible see sec tion 2.2.2. smallest cases already know matrix invertible. 1 1 matrix i.e. scalar number a1 1 a. thus 1 1 holds 0. 2 2 matrices definition inverse definition 2.3 know aa1 i. then 2.24 inverse a1 1 a11a22 a12a21 a22 a12 a21 a11 . 4.2 hence invertible a11a22 a12a21 0 . 4.3 quantity determinant r22 i.e. deta a11 a12 a21 a22 a11a22 a12a21 . 4.4 example 4.1 points already relationship determinants existence inverse matrices. next theorem states result n n matrices. theorem 4.1. square matrix rnn holds invertible deta 0. explicit closedform expressions determinants small matrices terms elements matrix. n 1 deta deta11 a11 . 4.5 n 2 deta a11 a12 a21 a22 a11a22 a12a21 4.6 observed preceding example. n 3 known sarrus rule a11 a12 a13 a21 a22 a23 a31 a32 a33 a11a22a33 a21a32a13 a31a12a23 4.7 a31a22a13 a11a32a23 a21a12a33 . draft 20230215 mathematics machine learning. feedback
4.1 determinant trace 101 memory aid product terms sarrus rule try tracing elements triple products matrix. call square matrix uppertriangular matrix tij 0 uppertriangular matrix j i.e. matrix zero diagonal. analogously define lowertriangular matrix matrix zeros diagonal. tri lowertriangular matrix angular matrix rnn determinant product diagonal elements i.e. dett n i1 tii . 4.8 determinant signed volume parallelepiped formed columns matrix. figure 4.3 area parallelogram shaded region spanned vectors b g detb g. b g figure 4.4 volume parallelepiped shaded volume spanned vectors r b g detr b g. b g r example 4.2 determinants measures volume notion determinant natural consider mapping set n vectors spanning object rn. turns de terminant deta signed volume ndimensional parallelepiped formed columns matrix a. n 2 columns matrix form parallelogram see fig ure 4.3. angle vectors gets smaller area parallel ogram shrinks too. consider two vectors b g form columns matrix b g. then absolute value determinant area parallelogram vertices 0 b g b g. particular b g linearly dependent b λg λ r longer form twodimensional parallelogram. therefore corresponding area 0. contrary b g linearly independent multiples canonical basis vectors e1 e2 written b b 0 g 0 g determinant b 0 0 g bg 0 bg. sign determinant indicates orientation spanning vectors b g respect standard basis e1 e2. figure flip ping order g b swaps columns reverses orientation shaded area. becomes familiar formula area height length. intuition extends higher dimensions. r3 consider three vectors r b g r3 spanning edges parallelepiped i.e. solid faces parallel parallelograms see figure 4.4. ab sign determinant indicates orientation spanning vectors. solute value determinant 3 3 matrix r b g volume solid. thus determinant acts function measures signed volume formed column vectors composed matrix. consider three linearly independent vectors r g b r3 given r 2 0 8 g 6 1 0 b 1 4 1 . 4.9 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
102 matrix decompositions writing vectors columns matrix r g b 2 6 1 0 1 4 8 0 1 4.10 allows us compute desired volume v deta 186 . 4.11 computing determinant n n matrix requires general algo rithm solve cases n 3 going explore fol lowing. theorem 4.2 reduces problem computing deter minant nn matrix computing determinant n1n1 matrices. recursively applying laplace expansion theorem 4.2 therefore compute determinants n n matrices ultimately computing determinants 2 2 matrices. laplace expansion theorem 4.2 laplace expansion. consider matrix rnn. then j 1 . . . n 1. expansion along column j detakj called minor 1kj detakj cofactor. deta n x k1 1kjakj detakj . 4.12 2. expansion along row j deta n x k1 1kjajk detajk . 4.13 akj rn1n1 submatrix obtain delet ing row k column j. example 4.3 laplace expansion let us compute determinant 1 2 3 3 1 2 0 0 1 4.14 using laplace expansion along first row. applying 4.13 yields 1 2 3 3 1 2 0 0 1 111 1 1 2 0 1 112 2 3 2 0 1 113 3 3 1 0 0 . 4.15 draft 20230215 mathematics machine learning. feedback
4.1 determinant trace 103 use 4.6 compute determinants 22 matrices obtain deta 11 0 23 0 30 0 5 . 4.16 completeness compare result computing determi nant using sarrus rule 4.7 deta 111303022013102321 16 5 . 4.17 rnn determinant exhibits following properties determinant matrix product product corresponding determinants detab detadetb. determinants invariant transposition i.e. deta deta. regular invertible deta1 1 deta. similar matrices definition 2.22 possess determinant. there fore linear mapping φ v v transformation matrices aφ φ determinant. thus determinant invariant choice basis linear mapping. adding multiple columnrow another one change deta. multiplication columnrow λ r scales deta λ. particular detλa λn deta. swapping two rowscolumns changes sign deta. last three properties use gaussian elimination see section 2.1 compute deta bringing rowechelon form. stop gaussian elimination triangular form elements diagonal 0. recall 4.8 determinant triangular matrix product diagonal elements. theorem 4.3. square matrix rnn deta 0 rka n. words invertible full rank. mathematics mainly performed hand determinant calculation considered essential way analyze matrix invertibil ity. however contemporary approaches machine learning use direct numerical methods superseded explicit calculation deter minant. example chapter 2 learned inverse matrices computed gaussian elimination. gaussian elimination thus used compute determinant matrix. determinants play important theoretical role following sections especially learn eigenvalues eigenvectors section 4.2 characteristic polynomial. definition 4.4. trace square matrix rnn defined trace 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
104 matrix decompositions tra n x i1 aii 4.18 i.e. trace sum diagonal elements a. trace satisfies following properties tra b tra trb a b rnn trαa αtra α r rnn trin n trab trba rnk b rkn shown one function satisfies four properties to gether trace gohberg et al. 2012. properties trace matrix products general. specif ically trace invariant cyclic permutations i.e. trace invariant cyclic permutations. trakl trkla 4.19 matrices rak k rkl l rla. property generalizes products arbitrary number matrices. special case 4.19 follows two vectors x rn trxy tryx yx r . 4.20 given linear mapping φ v v v vector space define trace map using trace matrix representation φ. given basis v describe φ means transfor mation matrix a. trace φ trace a. different basis v holds corresponding transformation matrix b φ obtained basis change form s1as suitable see section 2.7.2. corresponding trace φ means trb trs1as 4.19 trass1 tra . 4.21 hence matrix representations linear mappings basis depen dent trace linear mapping φ independent basis. section covered determinants traces functions char acterizing square matrix. taking together understanding determi nants traces define important equation describing matrix terms polynomial use extensively following sections. definition 4.5 characteristic polynomial. λ r square ma trix rnn paλ deta λi 4.22a c0 c1λ c2λ2 cn1λn1 1nλn 4.22b c0 . . . cn1 r characteristic polynomial a. particular characteristic polynomial draft 20230215 mathematics machine learning. feedback
4.2 eigenvalues eigenvectors 105 c0 deta 4.23 cn1 1n1tra . 4.24 characteristic polynomial 4.22a allow us compute eigen values eigenvectors covered next section. 4.2 eigenvalues eigenvectors get know new way characterize matrix associ ated linear mapping. recall section 2.7.1 every linear mapping unique transformation matrix given ordered basis. in terpret linear mappings associated transformation matrices performing eigen analysis. see eigenvalues lin eigen german word meaning characteristic self own. ear mapping tell us special set vectors eigenvectors transformed linear mapping. definition 4.6. let rnn square matrix. λ r eigenvalue x rn0 corresponding eigenvector eigenvalue eigenvector ax λx . 4.25 call 4.25 eigenvalue equation. eigenvalue equation remark. linear algebra literature software often conven tion eigenvalues sorted descending order largest eigenvalue associated eigenvector called first eigenvalue associated eigenvector second largest called second eigen value associated eigenvector on. however textbooks publications may different notion orderings. want presume ordering book stated explicitly. following statements equivalent λ eigenvalue rnn. exists x rn0 ax λx equivalently a λinx 0 solved nontrivially i.e. x 0. rka λin n. deta λin 0. definition 4.7 collinearity codirection. two vectors point direction called codirected. two vectors collinear codirected collinear point opposite direction. remark nonuniqueness eigenvectors. x eigenvector associated eigenvalue λ c r0 holds cx eigenvector eigenvalue since acx cax cλx λcx . 4.26 thus vectors collinear x also eigenvectors a. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
106 matrix decompositions theorem 4.8. λ r eigenvalue rnn λ root characteristic polynomial paλ a. definition 4.9. let square matrix eigenvalue λi. algebraic algebraic multiplicity multiplicity λi number times root appears character istic polynomial. definition 4.10 eigenspace eigenspectrum. rnn set eigenvectors associated eigenvalue λ spans subspace rn called eigenspace respect λ denoted eigenspace eλ. set eigenvalues called eigenspectrum eigenspectrum spectrum a. spectrum λ eigenvalue rnn corresponding eigenspace eλ solution space homogeneous system linear equations aλix 0. geometrically eigenvector corresponding nonzero eigenvalue points direction stretched linear mapping. eigenvalue factor stretched. eigenvalue negative direction stretching flipped. example 4.4 the case identity matrix identity matrix rnn characteristic polynomial piλ deti λi 1λn 0 one eigenvalue λ 1 oc curs n times. moreover ix λx 1x holds vectors x rn0. this sole eigenspace e1 identity matrix spans n di mensions n standard basis vectors rn eigenvectors i. useful properties regarding eigenvalues eigenvectors include following matrix transpose apossess eigenvalues necessarily eigenvectors. eigenspace eλ null space λi since ax λx ax λx 0 4.27a a λix 0 x kera λi. 4.27b similar matrices see definition 2.22 possess eigenvalues. therefore linear mapping φ eigenvalues independent choice basis transformation matrix. makes eigenvalues together determinant trace key characteristic param eters linear mapping invariant basis change. symmetric positive definite matrices always positive real eigen values. draft 20230215 mathematics machine learning. feedback
4.2 eigenvalues eigenvectors 107 example 4.5 computing eigenvalues eigenvectors eigenspaces let us find eigenvalues eigenvectors 2 2 matrix 4 2 1 3 . 4.28 step 1 characteristic polynomial. definition eigen vector x 0 eigenvalue λ a vector ax λx i.e. a λix 0. since x 0 requires kernel null space λi contains elements 0. means λi invertible therefore deta λi 0. hence need compute roots characteristic polynomial 4.22a find eigenvalues. step 2 eigenvalues. characteristic polynomial paλ deta λi 4.29a det 4 2 1 3 λ 0 0 λ 4 λ 2 1 3 λ 4.29b 4 λ3 λ 2 1 . 4.29c factorize characteristic polynomial obtain pλ 4 λ3 λ 2 1 10 7λ λ2 2 λ5 λ 4.30 giving roots λ1 2 λ2 5. step 3 eigenvectors eigenspaces. find eigenvectors correspond eigenvalues looking vectors x 4 λ 2 1 3 λ x 0 . 4.31 λ 5 obtain 4 5 2 1 3 5 x1 x2 1 2 1 2 x1 x2 0 . 4.32 solve homogeneous system obtain solution space e5 span 2 1 . 4.33 eigenspace onedimensional possesses single basis vector. analogously find eigenvector λ 2 solving homoge neous system equations 4 2 2 1 3 2 x 2 2 1 1 x 0 . 4.34 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
108 matrix decompositions means vector x x1 x2 x2 x1 1 1 eigenvector eigenvalue 2. corresponding eigenspace given e2 span 1 1 . 4.35 two eigenspaces e5 e2 example 4.5 onedimensional spanned single vector. however cases may multiple identical eigenvalues see definition 4.9 eigenspace may one dimension. definition 4.11. let λi eigenvalue square matrix a. geometric multiplicity λi number linearly independent eigen geometric multiplicity vectors associated λi. words dimensionality eigenspace spanned eigenvectors associated λi. remark. specific eigenvalues geometric multiplicity must least one every eigenvalue least one associated eigenvector. eigenvalues geometric multiplicity cannot exceed algebraic multiplic ity may lower. example 4.6 matrix 2 1 0 2 two repeated eigenvalues λ1 λ2 2 algebraic multiplicity 2. eigenvalue has however one distinct unit eigenvector x1 1 0 and thus geometric multiplicity 1. graphical intuition two dimensions let us gain intuition determinants eigenvectors eigenval ues using different linear mappings. figure 4.2 depicts five transformation matrices a1 . . . a5 impact square grid points centered origin geometry areapreserving properties type shearing parallel axis also known cavalieris principle equal areas parallelograms katz 2004. a1 1 2 0 0 2 . direction two eigenvectors correspond canonical basis vectors r2 i.e. two cardinal axes. vertical axis extended factor 2 eigenvalue λ1 2 horizontal axis compressed factor 1 2 eigenvalue λ2 1 2. mapping area preserving deta1 1 2 1 2. a2 1 1 2 0 1 corresponds shearing mapping i.e. shears points along horizontal axis right positive draft 20230215 mathematics machine learning. feedback
4.2 eigenvalues eigenvectors 109 figure 4.2 determinants eigenspaces. overview five linear mappings associated transformation matrices ai r22 projecting 400 colorcoded points x r2 left column onto target points aix right column. central column depicts first eigenvector stretched associated eigenvalue λ1 second eigenvector stretched eigenvalue λ2. row depicts effect one five transformation matrices ai respect standard basis. deta 1.0 λ1 2.0 λ2 0.5 deta 1.0 λ1 1.0 λ2 1.0 deta 1.0 λ1 0.870.5j λ2 0.870.5j deta 0.0 λ1 0.0 λ2 2.0 deta 0.75 λ1 0.5 λ2 1.5 half vertical axis left vice versa. mapping area preserving deta2 1. eigenvalue λ1 1 λ2 repeated eigenvectors collinear drawn emphasis two opposite directions. indicates mapping acts along one direction the horizontal axis. a3 cos π 6 sin π 6 sin π 6 cos π 6 1 2 3 1 1 3 matrix a3 rotates points π 6 rad 30counterclockwise complex eigen values reflecting mapping rotation hence eigenvectors drawn. rotation volume preserving deter minant 1. details rotations refer section 3.9. a4 1 1 1 1 represents mapping standard basis col lapses twodimensional domain onto one dimension. since one eigen 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
110 matrix decompositions value 0 space direction blue eigenvector corresponding λ1 0 collapses orthogonal red eigenvector stretches space factor λ2 2. therefore area image 0. a5 1 1 2 1 2 1 shearandstretch mapping scales space 75 since deta5 3 4. stretches space along red eigenvector λ2 factor 1.5 compresses along orthogonal blue eigenvector factor 0.5. example 4.7 eigenspectrum biological neural network figure 4.3 caenorhabditis elegans neural network kaiser hilgetag 2006.a sym metrized connectivity matrix b eigenspectrum. 0 50 100 150 200 250 neuron index 0 50 100 150 200 250 neuron index a connectivity matrix. 0 100 200 index sorted eigenvalue 10 5 0 5 10 15 20 25 eigenvalue b eigenspectrum. methods analyze learn network data essential com ponent machine learning methods. key understanding networks connectivity network nodes especially two nodes connected not. data science applications often useful study matrix captures connectivity data. build connectivityadjacency matrix r277277 complete neural network worm c.elegans. rowcolumn represents one 277 neurons worms brain. connectivity matrix value aij 1 neuron talks neuron j synapse aij 0 otherwise. connectivity matrix symmetric im plies eigenvalues may real valued. therefore compute symmetrized version connectivity matrix asym a. new matrix asym shown figure 4.3a nonzero value aij two neurons connected white pixels irrespective direction connection. figure 4.3b show correspond ing eigenspectrum asym. horizontal axis shows index eigenvalues sorted descending order. vertical axis shows corre sponding eigenvalue. slike shape eigenspectrum typical many biological neural networks. underlying mechanism responsible area active neuroscience research. draft 20230215 mathematics machine learning. feedback
4.2 eigenvalues eigenvectors 111 theorem 4.12. eigenvectors x1 . . . xn matrix rnn n distinct eigenvalues λ1 . . . λn linearly independent. theorem states eigenvectors matrix n distinct eigen values form basis rn. definition 4.13. square matrix rnn defective possesses defective fewer n linearly independent eigenvectors. nondefective matrix rnn necessarily require n dis tinct eigenvalues require eigenvectors form basis rn. looking eigenspaces defective matrix follows sum dimensions eigenspaces less n. specifically de fective matrix least one eigenvalue λi algebraic multiplicity 1 geometric multiplicity less m. remark. defective matrix cannot n distinct eigenvalues distinct eigenvalues linearly independent eigenvectors theorem 4.12. theorem 4.14. given matrix rmn always obtain sym metric positive semidefinite matrix rnn defining aa . 4.36 remark. rka n aa symmetric positive definite. understanding theorem 4.14 holds insightful use symmetrized matrices symmetry requires s insert ing 4.36 obtain aa aa aa s. more over positive semidefiniteness section 3.2.3 requires xsx 0 inserting 4.36 obtain xsx xaax xaax axax 0 dot product computes sum squares which nonnegative. spectral theorem theorem 4.15 spectral theorem. rnn symmetric ex ists orthonormal basis corresponding vector space v consisting eigenvectors a eigenvalue real. direct implication spectral theorem eigendecompo sition symmetric matrix exists with real eigenvalues find onb eigenvectors p dp diagonal columns p contain eigenvectors. example 4.8 consider matrix 3 2 2 2 3 2 2 2 3 . 4.37 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
112 matrix decompositions characteristic polynomial paλ λ 12λ 7 4.38 obtain eigenvalues λ1 1 λ2 7 λ1 repeated eigenvalue. following standard procedure computing eigenvectors obtain eigenspaces e1 span 1 1 0 z x1 1 0 1 z x2 e7 span 1 1 1 z x3 . 4.39 see x3 orthogonal x1 x2. however since x 1 x2 1 0 orthogonal. spectral theorem theorem 4.15 states exists orthogonal basis one orthogonal. however construct one. construct basis exploit fact x1 x2 eigenvec tors associated eigenvalue λ. therefore α β r holds aαx1 βx2 ax1α ax2β λαx1 βx2 4.40 i.e. linear combination x1 x2 also eigenvector as sociated λ. gramschmidt algorithm section 3.8.3 method iteratively constructing orthogonalorthonormal basis set basis vectors using linear combinations. therefore even x1 x2 orthogonal apply gramschmidt algorithm find eigenvectors associated λ1 1 orthogonal and x3. example obtain x 1 1 1 0 x 2 1 2 1 1 2 4.41 orthogonal other orthogonal x3 eigenvectors associated λ1 1. conclude considerations eigenvalues eigenvectors useful tie matrix characteristics together concepts determinant trace. theorem 4.16. determinant matrix rnn product eigenvalues i.e. deta n i1 λi 4.42 λi c possibly repeated eigenvalues a. draft 20230215 mathematics machine learning. feedback
4.2 eigenvalues eigenvectors 113 figure 4.1 geometric interpretation eigenvalues. eigenvectors get stretched corresponding eigenvalues. area unit square changes λ1λ2 perimeter changes factor 1 2 λ1 λ2. x1 x2 v1 v2 theorem 4.17. trace matrix rnn sum eigenval ues i.e. tra n x i1 λi 4.43 λi c possibly repeated eigenvalues a. let us provide geometric intuition two theorems. consider matrix r22 possesses two linearly independent eigenvectors x1 x2. example assume x1 x2 onb r2 orthogonal area square span 1 see figure 4.1. section 4.1 know determinant computes change area unit square transformation a. example compute change area explicitly mapping eigenvectors using gives us vectors v1 ax1 λ1x1 v2 ax2 λ2x2 i.e. new vectors vi scaled versions eigenvectors xi scaling factors corresponding eigenvalues λi. v1 v2 still orthogonal area rectangle span λ1λ2. given x1 x2 in example orthonormal directly compute perimeter unit square 21 1. mapping eigen vectors using creates rectangle whose perimeter 2λ1 λ2. therefore sum absolute values eigenvalues tells us perimeter unit square changes transformation matrix a. example 4.9 googles pagerank webpages eigenvectors google uses eigenvector corresponding maximal eigenvalue matrix determine rank page search. idea pagerank algorithm developed stanford university larry page sergey brin 1996 importance web page ap proximated importance pages link it. this write web sites huge directed graph shows page links which. pagerank computes weight importance xi 0 web site ai counting number pages pointing ai. moreover pager ank takes account importance web sites link ai. navigation behavior user modeled transition matrix graph tells us click probability somebody end 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
114 matrix decompositions different web site. matrix property ini tial rankimportance vector x web site sequence x ax a2x . . . converges vector x. vector called pagerank satisfies pagerank ax x i.e. eigenvector with corresponding eigenvalue 1 a. normalizing x x 1 interpret entries probabilities. details different perspectives pagerank found original technical report page et al. 1999. 4.3 cholesky decomposition many ways factorize special types matrices en counter often machine learning. positive real numbers squareroot operation gives us decomposition number identical components e.g. 9 3 3. matrices need careful compute squarerootlike operation positive quanti ties. symmetric positive definite matrices see section 3.2.3 choose number squareroot equivalent operations. cholesky cholesky decomposition decompositioncholesky factorization provides squareroot equivalent op cholesky factorization eration symmetric positive definite matrices useful practice. theorem 4.18 cholesky decomposition. symmetric positive definite matrix factorized product ll l lower triangular matrix positive diagonal elements a11 a1n . . . . . . . an1 ann l11 0 . . . . . . . ln1 lnn l11 ln1 . . . . . . . 0 lnn . 4.44 l called cholesky factor a l unique. cholesky factor example 4.10 cholesky factorization consider symmetric positive definite matrix r33. inter ested finding cholesky factorization ll i.e. a11 a21 a31 a21 a22 a32 a31 a32 a33 ll l11 0 0 l21 l22 0 l31 l32 l33 l11 l21 l31 0 l22 l32 0 0 l33 . 4.45 multiplying righthand side yields l2 11 l21l11 l31l11 l21l11 l2 21 l2 22 l31l21 l32l22 l31l11 l31l21 l32l22 l2 31 l2 32 l2 33 . 4.46 draft 20230215 mathematics machine learning. feedback
4.4 eigendecomposition diagonalization 115 comparing lefthand side 4.45 righthand side 4.46 shows simple pattern diagonal elements lii l11 a11 l22 q a22 l2 21 l33 q a33 l2 31 l2 32 . 4.47 similarly elements diagonal lij j also repeating pattern l21 1 l11 a21 l31 1 l11 a31 l32 1 l22 a32 l31l21 . 4.48 thus constructed cholesky decomposition symmetric pos itive definite 3 3 matrix. key realization backward calculate components lij l be given values aij previously computed values lij. cholesky decomposition important tool numerical computations underlying machine learning. here symmetric positive def inite matrices require frequent manipulation e.g. covariance matrix multivariate gaussian variable see section 6.5 symmetric positive definite. cholesky factorization covariance matrix allows us generate samples gaussian distribution. also allows us perform linear transformation random variables heavily exploited computing gradients deep stochastic models varia tional autoencoder jimenez rezende et al. 2014 kingma welling 2014. cholesky decomposition also allows us compute determi nants efficiently. given cholesky decomposition ll know deta detl detl detl2. since l triangular matrix determinant simply product diagonal entries deta q l2 ii. thus many numerical software packages use cholesky decomposition make computations efficient. 4.4 eigendecomposition diagonalization diagonal matrix matrix value zero offdiagonal ele diagonal matrix ments i.e. form c1 0 . . . . . . . 0 cn . 4.49 allow fast computation determinants powers inverses. determinant product diagonal entries matrix power dk given diagonal element raised power k inverse d1 reciprocal diagonal elements nonzero. section discuss transform matrices diagonal 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
116 matrix decompositions form. important application basis change discussed section 2.7.2 eigenvalues section 4.2. recall two matrices a similar definition 2.22 ex ists invertible matrix p p 1ap . specifically look matrices similar diagonal matrices con tain eigenvalues diagonal. definition 4.19 diagonalizable. matrix rnn diagonalizable diagonalizable similar diagonal matrix i.e. exists invertible matrix p rnn p 1ap . following see diagonalizing matrix rnn way expressing linear mapping another basis see section 2.6.1 turn basis consists eigen vectors a. let rnn let λ1 . . . λn set scalars let p1 . . . pn set vectors rn. define p p1 . . . pn let rnn diagonal matrix diagonal entries λ1 . . . λn. show ap p 4.50 λ1 . . . λn eigenvalues p1 . . . pn cor responding eigenvectors a. see statement holds ap ap1 . . . pn ap1 . . . apn 4.51 p p1 . . . pn λ1 0 . 0 λn λ1p1 . . . λnpn . 4.52 thus 4.50 implies ap1 λ1p1 4.53 . . . apn λnpn . 4.54 therefore columns p must eigenvectors a. definition diagonalization requires p rnn invertible i.e. p full rank theorem 4.3. requires us n linearly independent eigenvectors p1 . . . pn i.e. pi form basis rn. theorem 4.20 eigendecomposition. square matrix rnn factored p dp 1 4.55 p rnn diagonal matrix whose diagonal entries eigenvalues a eigenvectors form basis rn. draft 20230215 mathematics machine learning. feedback
4.4 eigendecomposition diagonalization 117 figure 4.1 intuition behind eigendecomposition sequential transformations. topleft bottomleft p 1 performs basis change here drawn r2 depicted rotationlike operation standard basis eigenbasis. bottomleft bottomright performs scaling along remapped orthogonal eigenvectors depicted circle stretched ellipse. bottomright topright p undoes basis change depicted reverse rotation restores original coordinate frame. e1 e2 p1 p2 p1 p2 e1 e2 p1 p2 λ1p1 λ2p2 e1 e2 ae1 ae2 p 1 p theorem 4.20 implies nondefective matrices diagonal ized columns p n eigenvectors a. symmetric matrices obtain even stronger outcomes eigenvalue decom position. theorem 4.21. symmetric matrix rnn always diagonalized. theorem 4.21 follows directly spectral theorem 4.15. more over spectral theorem states find onb eigenvectors rn. makes p orthogonal matrix p ap . remark. jordan normal form matrix offers decomposition works defective matrices lang 1987 beyond scope book. geometric intuition eigendecomposition interpret eigendecomposition matrix follows see also figure 4.1 let transformation matrix linear mapping respect standard basis ei blue arrows. p 1 performs basis change standard basis eigenbasis. then diagonal scales vectors along axes eigenvalues λi. finally p transforms scaled vectors back standardcanonical coordi nates yielding λipi. example 4.11 eigendecomposition let us compute eigendecomposition 1 2 5 2 2 5 . step 1 compute eigenvalues eigenvectors. characteristic 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
118 matrix decompositions polynomial deta λi det 5 2 λ 1 1 5 2 λ 4.56a 5 2 λ2 1 λ2 5λ 21 4 λ 7 2λ 3 2 . 4.56b therefore eigenvalues λ1 7 2 λ2 3 2 the roots characteristic polynomial associated normalized eigenvectors obtained via ap1 7 2p1 ap2 3 2p2 . 4.57 yields p1 1 2 1 1 p2 1 2 1 1 . 4.58 step 2 check existence. eigenvectors p1 p2 form basis r2. therefore diagonalized. step 3 construct matrix p diagonalize a. collect eigen vectors p p p1 p2 1 2 1 1 1 1 . 4.59 obtain p 1ap 7 2 0 0 3 2 . 4.60 equivalently get exploiting p 1 p since eigenvectors figure 4.1 visualizes eigendecomposition 5 2 2 5 sequence linear transformations. p1 p2 example form onb 1 2 5 2 2 5 z 1 2 1 1 1 1 z p 7 2 0 0 3 2 z 1 2 1 1 1 1 z p 1 . 4.61 diagonal matrices efficiently raised power. therefore find matrix power matrix rnn via eigenvalue decomposition if exists ak p dp 1k p dkp 1 . 4.62 computing dk efficient apply operation individually diagonal element. assume eigendecomposition p dp 1 exists. then deta detp dp 1 detp detd detp 1 4.63a draft 20230215 mathematics machine learning. feedback
4.5 singular value decomposition 119 detd dii 4.63b allows efficient computation determinant a. eigenvalue decomposition requires square matrices. would useful perform decomposition general matrices. next sec tion introduce general matrix decomposition technique singular value decomposition. 4.5 singular value decomposition singular value decomposition svd matrix central matrix decomposition method linear algebra. referred fundamental theorem linear algebra strang 1993 applied matrices square matrices always exists. moreover explore following svd matrix a represents linear mapping φ v w quantifies change underlying geometry two vector spaces. recom mend work kalman 1996 roy banerjee 2014 deeper overview mathematics svd. svd theorem theorem 4.22 svd theorem. let rmn rectangular matrix rank r 0 minm n. svd decomposition form svd singular value decomposition u v σ n n n n 4.64 orthogonal matrix u rmm column vectors ui 1 . . . m orthogonal matrix v rnn column vectors vj j 1 . . . n. moreover σ n matrix σii σi 0 σij 0 j. diagonal entries σi 1 . . . r σ called singular values singular values ui called leftsingular vectors vj called rightsingular leftsingular vectors rightsingular vectors vectors. convention singular values ordered i.e. σ1 σ2 σr 0. singular value matrix σ unique requires attention. singular value matrix observe σ rmn rectangular. particular σ size a. means σ diagonal submatrix contains singular values needs additional zero padding. specifically n matrix σ diagonal structure row n consists 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
120 matrix decompositions figure 4.1 intuition behind svd matrix r32 sequential transformations. topleft bottomleft v performs basis change r2. bottomleft bottomright σ scales maps r2 r3. ellipse bottomright lives r3. third dimension orthogonal surface elliptical disk. bottomright topright u performs basis change within r3. v2 v1 σ2u2 σ1u1 e2 e1 σ2e2 σ1e1 v σ u 0row vectors n 1 σ σ1 0 0 0 . 0 0 0 σn 0 . . . 0 . . . . . . 0 . . . 0 . 4.65 n matrix σ diagonal structure column columns consist 0 1 n σ σ1 0 0 0 . . . 0 0 . 0 . . . . . . 0 0 σm 0 . . . 0 . 4.66 remark. svd exists matrix rmn. 4.5.1 geometric intuitions svd svd offers geometric intuitions describe transformation matrix a. following discuss svd sequential linear trans formations performed bases. example 4.12 apply transformation matrices svd set vectors r2 allows us visualize effect transformation clearly. svd matrix interpreted decomposition corre sponding linear mapping recall section 2.7.1 φ rn rm three operations see figure 4.1. svd intuition follows superficially simi lar structure eigendecomposition intuition see figure 4.1 broadly speaking svd performs basis change via v followed scal ing augmentation or reduction dimensionality via singular draft 20230215 mathematics machine learning. feedback
4.5 singular value decomposition 121 value matrix σ. finally performs second basis change via u. svd entails number important details caveats review intuition detail. useful review basis changes section 2.7.2 orthogonal matrices definition 3.8 orthonormal bases section 3.5. assume given transformation matrix linear mapping φ rn rm respect standard bases b c rn rm respectively. moreover assume second basis b rn c rm. 1. matrix v performs basis change domain rn b rep resented red orange vectors v1 v2 topleft fig ure 4.1 standard basis b. v v 1 performs basis change b b. red orange vectors aligned canonical basis bottomleft figure 4.1. 2. changed coordinate system b σ scales new coordi nates singular values σi and adds deletes dimensions i.e. σ transformation matrix φ respect b c rep resented red orange vectors stretched lying e1e2 plane embedded third dimension bottomright figure 4.1. 3. u performs basis change codomain rm c canoni cal basis rm represented rotation red orange vectors e1e2 plane. shown topright figure 4.1. svd expresses change basis domain codomain. contrast eigendecomposition operates within vector space basis change applied un done. makes svd special two different bases simultaneously linked singular value matrix σ. example 4.12 vectors svd consider mapping square grid vectors x r2 fit box size 2 2 centered origin. using standard basis map vectors using 1 0.8 0 1 1 0 uσv 4.67a 0.79 0 0.62 0.38 0.78 0.49 0.48 0.62 0.62 1.62 0 0 1.0 0 0 0.78 0.62 0.62 0.78 . 4.67b start set vectors x colored dots see topleft panel fig ure 4.2 arranged grid. apply v r22 rotates x . rotated vectors shown bottomleft panel figure 4.2. map vectors using singular value matrix σ codomain r3 see bottomright panel figure 4.2. note vectors lie 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
122 matrix decompositions x1x2 plane. third coordinate always 0. vectors x1x2 plane stretched singular values. direct mapping vectors x codomain r3 equals transformation x uσv u performs rotation within codomain r3 mapped vectors longer restricted x1x2 plane still plane shown topright panel figure 4.2. figure 4.2 svd mapping vectors represented discs. panels follow anticlockwise structure figure 4.1. 1.5 1.0 0.5 0.0 0.5 1.0 1.5 x1 1.5 1.0 0.5 0.0 0.5 1.0 1.5 x2 x1 1.5 0.5 0.5 1.5 x2 1.5 0.5 0.5 1.5 x3 1.0 0.5 0.0 0.5 1.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 x1 1.5 1.0 0.5 0.0 0.5 1.0 1.5 x2 x1 1.5 0.5 0.5 1.5 x2 1.5 0.5 0.5 1.5 x3 0 4.5.2 construction svd next discuss svd exists show compute detail. svd general matrix shares similarities eigendecomposition square matrix. remark. compare eigendecomposition spd matrix s p dp 4.68 draft 20230215 mathematics machine learning. feedback
4.5 singular value decomposition 123 corresponding svd uσv . 4.69 set u p v σ 4.70 see svd spd matrices eigendecomposition. following explore theorem 4.22 holds svd constructed. computing svd rmn equivalent finding two sets orthonormal bases u u1 . . . um v v1 . . . vn codomain rm domain rn respectively. ordered bases construct matrices u v . plan start constructing orthonormal set right singular vectors v1 . . . vn rn. construct orthonormal set leftsingular vectors u1 . . . um rm. thereafter link two require orthogonality vi preserved trans formation a. important know images avi form set orthogonal vectors. normalize images scalar factors turn singular values. let us begin constructing rightsingular vectors. spectral theorem theorem 4.15 tells us eigenvectors symmetric matrix form onb also means diagonalized. more over theorem 4.14 always construct symmetric positive semidefinite matrix aa rnn rectangular matrix rmn. thus always diagonalize aa obtain aa p dp p λ1 0 . . . . . . . 0 λn p 4.71 p orthogonal matrix composed orthonormal eigenbasis. λi 0 eigenvalues aa. let us assume svd exists inject 4.64 4.71. yields aa uσv uσv v σu uσv 4.72 u v orthogonal matrices. therefore u u ob tain aa v σσv v σ2 1 0 0 0 . 0 0 0 σ2 n v . 4.73 comparing 4.71 4.73 identify v p 4.74 σ2 λi . 4.75 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
124 matrix decompositions therefore eigenvectors aa compose p rightsingular vectors v see 4.74. eigenvalues aa squared singular values σ see 4.75. obtain leftsingular vectors u follow similar procedure. start computing svd symmetric matrix aarmm instead previous aa rnn. svd yields aa uσv uσv uσv v σu 4.76a u σ2 1 0 0 0 . 0 0 0 σ2 u . 4.76b spectral theorem tells us aa sdscan diagonalized find onb eigenvectors aa collected s. orthonormal eigenvectors aaare leftsingular vectors u form orthonormal basis codomain svd. leaves question structure matrix σ. since aa aa nonzero eigenvalues see page 106 nonzero entries σ matrices svd cases same. last step link parts touched upon far. orthonormal set rightsingular vectors v . finish construc tion svd connect orthonormal vectors u. reach goal use fact images vi orthogonal too. show using results section 3.4. require inner product avi avj must 0 j. two orthogonal eigenvectors vi vj j holds aviavj v aavj v λjvj λjv vj 0 . 4.77 case r holds av1 . . . avr basis r dimensional subspace rm. complete svd construction need leftsingular vectors orthonormal normalize images rightsingular vectors avi obtain ui avi avi 1 λi avi 1 σi avi 4.78 last equality obtained 4.75 4.76b showing us eigenvalues aaare σ2 λi. therefore eigenvectors aa know right singular vectors vi normalized images a leftsingular vectors ui form two selfconsistent onbs connected singular value matrix σ. let us rearrange 4.78 obtain singular value equation singular value equation avi σiui 1 . . . r . 4.79 draft 20230215 mathematics machine learning. feedback
4.5 singular value decomposition 125 equation closely resembles eigenvalue equation 4.25 vectors left righthand sides same. n m 4.79 holds n 4.79 says nothing ui n. however know construction or thonormal. conversely n 4.79 holds m. m avi 0 still know vi form orthonormal set. means svd also supplies orthonormal basis kernel null space a set vectors x ax 0 see section 2.7.3. concatenating vi columns v ui columns u yields av uς 4.80 σ dimensions diagonal structure rows 1 . . . r. hence rightmultiplying v yields uσv svd a. example 4.13 computing svd let us find singular value decomposition 1 0 1 2 1 0 . 4.81 svd requires us compute rightsingular vectors vj singular values σk leftsingular vectors ui. step 1 rightsingular vectors eigenbasis aa. start computing aa 1 2 0 1 1 0 1 0 1 2 1 0 5 2 1 2 1 0 1 0 1 . 4.82 compute singular values rightsingular vectors vj eigenvalue decomposition aa given aa 5 30 0 1 6 2 30 1 5 2 6 1 30 2 5 1 6 6 0 0 0 1 0 0 0 0 5 30 2 30 1 30 0 1 5 2 5 1 6 2 6 1 6 p dp 4.83 obtain rightsingular vectors columns p v p 5 30 0 1 6 2 30 1 5 2 6 1 30 2 5 1 6 . 4.84 step 2 singularvalue matrix. singular values σi square roots eigenvalues 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
126 matrix decompositions aa obtain straight d. since rka 2 two nonzero singular values σ1 6 σ2 1. singular value matrix must size a obtain σ 6 0 0 0 1 0 . 4.85 step 3 leftsingular vectors normalized image right singular vectors. find leftsingular vectors computing image right singular vectors normalizing dividing corresponding singular value. obtain u1 1 σ1 av1 1 6 1 0 1 2 1 0 5 30 2 30 1 30 1 5 2 5 4.86 u2 1 σ2 av2 1 1 1 0 1 2 1 0 0 1 5 2 5 2 5 1 5 4.87 u u1 u2 1 5 1 2 2 1 . 4.88 note computer approach illustrated poor numerical behavior svd normally computed without resorting eigenvalue decomposition aa. 4.5.3 eigenvalue decomposition vs. singular value decomposition let us consider eigendecomposition p dp 1 svd uσv and review core elements past sections. svd always exists matrix rmn. eigendecomposition defined square matrices rnn exists find basis eigenvectors rn. vectors eigendecomposition matrix p necessarily orthogonal i.e. change basis simple rotation scaling. hand vectors matrices u v svd orthonormal represent rotations. eigendecomposition svd compositions three linear mappings 1. change basis domain 2. independent scaling new basis vector mapping do main codomain 3. change basis codomain draft 20230215 mathematics machine learning. feedback
4.5 singular value decomposition 127 figure 4.1 movie ratings three people four movies svd decomposition. 5 4 1 5 5 0 0 0 5 1 0 4 ali beatrix chandra star wars blade runner amelie delicatessen 0.6710 0.0236 0.4647 0.5774 0.7197 0.2054 0.4759 0.4619 0.0939 0.7705 0.5268 0.3464 0.1515 0.6030 0.5293 0.5774 9.6438 0 0 0 6.3639 0 0 0 0.7056 0 0 0 0.7367 0.6515 0.1811 0.0852 0.1762 0.9807 0.6708 0.7379 0.0743 key difference eigendecomposition svd svd domain codomain vector spaces different dimensions. svd left rightsingular vector matrices u v generally inverse they perform basis changes dif ferent vector spaces. eigendecomposition basis change ma trices p p 1 inverses other. svd entries diagonal matrix σ real non negative generally true diagonal matrix eigendecomposition. svd eigendecomposition closely related projections leftsingular vectors eigenvectors aa rightsingular vectors eigenvectors aa. nonzero singular values square roots nonzero eigenvalues aaand aa. symmetric matrices rnn eigenvalue decomposition svd one same follows spectral theo rem 4.15. example 4.14 finding structure movie ratings consumers let us add practical interpretation svd analyzing data people preferred movies. consider three viewers ali beatrix chandra rating four different movies star wars blade runner amelie delicatessen. ratings values 0 worst 5 best encoded data matrix r43 shown figure 4.1. row represents movie column user. thus column vectors movie ratings one viewer xali xbeatrix xchandra. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
128 matrix decompositions factoring using svd offers us way capture relationships people rate movies especially structure linking people like movies. applying svd data matrix makes number assumptions 1. viewers rate movies consistently using linear mapping. 2. errors noise ratings. 3. interpret leftsingular vectors ui stereotypical movies rightsingular vectors vj stereotypical viewers. make assumption viewers specific movie preferences expressed linear combination vj. similarly movies likeability expressed linear combination ui. therefore vector domain svd interpreted viewer space stereotypical viewers vector codomain svd correspondingly movie space stereotypical movies. let us two spaces meaningfully spanned respective viewer movie data data covers sufficient diversity viewers movies. inspect svd movieuser matrix. first leftsingular vector u1 large absolute values two science fiction movies large first singular value red shading figure 4.1. thus groups type users specific set movies science fiction theme. similarly first rightsingular v1 shows large absolute values ali beatrix give high ratings science fiction movies green shading figure 4.1. suggests v1 reflects notion science fiction lover. similarly u2 seems capture french art house film theme v2 in dicates chandra close idealized lover movies. ide alized science fiction lover purist loves science fiction movies science fiction lover v1 gives rating zero everything science fiction themedthis logic implied diagonal substructure singular value matrix σ. specific movie therefore represented decomposes linearly stereotypical movies. likewise person would represented decompose via linear combination movie themes. worth briefly discuss svd terminology conventions different versions used literature. differences confusing mathematics remains invariant them. convenience notation abstraction use svd notation svd described two square left rightsingular vector matrices nonsquare singular value matrix. defini tion 4.64 svd sometimes called full svd. full svd authors define svd bit differently focus square sin gular matrices. then rmn n mn u mn σ nn v nn . 4.89 draft 20230215 mathematics machine learning. feedback
4.6 matrix approximation 129 sometimes formulation called reduced svd e.g. datta 2010 reduced svd svd e.g. press et al. 2007. alternative format changes merely matrices constructed leaves mathematical structure svd unchanged. convenience alternative formulation σ diagonal eigenvalue decomposition. section 4.6 learn matrix approximation techniques using svd also called truncated svd. truncated svd possible define svd rankr matrix u r matrix σ diagonal matrix r r v r n matrix. construction similar definition ensures diagonal matrix σ nonzero entries along diagonal. main convenience alternative notation σ diagonal eigenvalue decomposition. restriction svd applies n matrices n practically unnecessary. n svd decomposition yield σ zero columns rows and consequently singular values σm1 . . . σn 0. svd used variety applications machine learning leastsquares problems curve fitting solving systems linear equa tions. applications harness various important properties svd relation rank matrix ability approximate matrices given rank lowerrank matrices. substituting matrix svd often advantage making calculation robust nu merical rounding errors. explore next section svds ability approximate matrices simpler matrices principled manner opens machine learning applications ranging dimension ality reduction topic modeling data compression clustering. 4.6 matrix approximation considered svd way factorize uσv rmn product three matrices u rmm v rnn or thogonal σ contains singular values main diagonal. instead full svd factorization investigate svd allows us represent matrix sum simpler lowrank matrices ai lends matrix approximation scheme cheaper compute full svd. construct rank1 matrix ai rmn ai uiv 4.90 formed outer product ith orthogonal column vector u v . figure 4.2 shows image stonehenge represented matrix r14321910 outer products ai defined 4.90. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
130 matrix decompositions figure 4.2 image processing svd. a original grayscale image 1 432 1 910 matrix values 0 black 1 white. bf rank1 matrices a1 . . . a5 corresponding singular values σ1 . . . σ5. gridlike structure rank1 matrix imposed outerproduct left rightsingular vectors. a original image a. b a1 σ1 228 052. c a2 σ2 40 647. d a3 σ3 26 125. e a4 σ4 20 232. f a5 σ5 15 436. matrix rmn rank r written sum rank1 matrices ai r x i1 σiuiv r x i1 σiai 4.91 outerproduct matrices ai weighted ith singular value σi. see 4.91 holds diagonal structure singular value matrix σ multiplies matching left rightsingular vectors uiv scales corresponding singular value σi. terms σijuiv j vanish j σ diagonal matrix. terms r vanish corresponding singular values 0. 4.90 introduced rank1 matrices ai. summed r in dividual rank1 matrices obtain rankr matrix a see 4.91. sum run matrices ai 1 . . . r intermediate value k r obtain rankk approximation rankk approximation b ak k x i1 σiuiv k x i1 σiai 4.92 rk b ak k. figure 4.3 shows lowrank approximations b ak original image stonehenge. shape rocks becomes increasingly visible clearly recognizable rank5 approximation. original image requires 1 432 1 910 2 735 120 numbers rank5 approximation requires us store five singular values five left rightsingular vectors 1 432 1 910dimensional each total 51 4321 9101 16 715 numbers 0.6 original. measure difference error rankk approxima tion b ak need notion norm. section 3.1 already used draft 20230215 mathematics machine learning. feedback
4.6 matrix approximation 131 figure 4.3 image reconstruction svd. a original image. bf image reconstruction using lowrank approximation svd rankk approximation given b ak pk i1 σiai. a original image a. b rank1 approximation b a1.c rank2 approximation b a2. d rank3 approximation b a3.e rank4 approximation b a4.f rank5 approximation b a5. norms vectors measure length vector. analogy also define norms matrices. definition 4.23 spectral norm matrix. x rn0 spectral spectral norm norm matrix rmn defined a2 max x ax2 x2 . 4.93 introduce notation subscript matrix norm lefthand side similar euclidean norm vectors righthand side subscript 2. spectral norm 4.93 determines long vector x become multiplied a. theorem 4.24. spectral norm largest singular value σ1. leave proof theorem exercise. eckartyoung theorem theorem 4.25 eckartyoung theorem eckart young 1936. con sider matrix rmn rank r let b rmn matrix rank k. k r b ak pk i1 σiuiv holds b ak argminrkbk a b2 4.94 b ak 2 σk1 . 4.95 eckartyoung theorem states explicitly much error intro duce approximating using rankk approximation. inter pret rankk approximation obtained svd projection fullrank matrix onto lowerdimensional space rankatmostk matrices. possible projections svd minimizes error with respect spectral norm rankk approximation. retrace steps understand 4.95 hold. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
132 matrix decompositions observe difference b ak matrix containing sum remaining rank1 matrices b ak r x ik1 σiuiv . 4.96 theorem 4.24 immediately obtain σk1 spectral norm difference matrix. let us closer look 4.94. assume another matrix b rkb k a b2 b ak 2 4.97 exists least n kdimensional null space z rn x z implies bx 0. follows ax2 a bx2 4.98 using version cauchyschwartz inequality 3.17 en compasses norms matrices obtain ax2 a b2 x2 σk1 x2 . 4.99 however exists k 1dimensional subspace ax2 σk1 x2 spanned rightsingular vectors vj j k 1 a. adding dimensions two spaces yields number greater n must nonzero vector spaces. contradiction ranknullity theorem theorem 2.24 section 2.7.3. eckartyoung theorem implies use svd reduce rankr matrix rankk matrix b principled optimal in spectral norm sense manner. interpret approximation rankk matrix form lossy compression. therefore lowrank approximation matrix appears many machine learning applications e.g. image processing noise filtering regularization illposed prob lems. furthermore plays key role dimensionality reduction principal component analysis see chapter 10. example 4.15 finding structure movie ratings consumers continued coming back movierating example apply con cept lowrank approximations approximate original data matrix. recall first singular value captures notion science fiction theme movies science fiction lovers. thus using first singular value term rank1 decomposition movierating matrix obtain predicted ratings a1 u1v 1 0.6710 0.7197 0.0939 0.1515 0.7367 0.6515 0.1811 4.100a draft 20230215 mathematics machine learning. feedback
4.7 matrix phylogeny 133 0.4943 0.4372 0.1215 0.5302 0.4689 0.1303 0.0692 0.0612 0.0170 0.1116 0.0987 0.0274 . 4.100b first rank1 approximation a1 insightful tells us ali beatrix like science fiction movies star wars bladerunner entries values 0.4 fails capture ratings movies chandra. surprising chandras type movies captured first singular value. second singular value gives us better rank1 approximation movietheme lovers a2 u2v 2 0.0236 0.2054 0.7705 0.6030 0.0852 0.1762 0.9807 4.101a 0.0020 0.0042 0.0231 0.0175 0.0362 0.2014 0.0656 0.1358 0.7556 0.0514 0.1063 0.5914 . 4.101b second rank1 approximation a2 capture chandras ratings movie types well science fiction movies. leads us consider rank2 approximation b a2 combine first two rank1 approximations b a2 σ1a1 σ2a2 4.7801 4.2419 1.0244 5.2252 4.7522 0.0250 0.2493 0.2743 4.9724 0.7495 0.2756 4.0278 . 4.102 b a2 similar original movie ratings table 5 4 1 5 5 0 0 0 5 1 0 4 4.103 suggests ignore contribution a3. in terpret data table evidence third movie thememovielovers category. also means entire space moviethemesmovielovers example twodimensional space spanned science fiction french art house movies lovers. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
134 matrix decompositions figure 4.2 functional phylogeny matrices encountered machine learning. real matrices pseudoinverse svd square determinant trace nonsquare defective singular nondefective diagonalizable singular normal nonnormal symmetric eigenvalues r positive definite cholesky eigenvalues 0 diagonal identity matrix inverse matrix regular invertible orthogonal rotation rnn rnm basis eigenvectors basis eigenvectors aa aa aa aa columns orthogonal eigenvectors aa aa det 0 det 0 det 0 4.7 matrix phylogeny word phylogenetic describes capture relationships among individuals groups derived greek words tribe source. chapters 2 3 covered basics linear algebra analytic geometry. chapter looked fundamental characteristics ma trices linear mappings. figure 4.2 depicts phylogenetic tree relationships different types matrices black arrows indicating is subset of covered operations perform in blue. consider real matrices rnm. nonsquare matrices where n m svd always exists saw chapter. focus ing square matrices rnn determinant informs us whether square matrix possesses inverse matrix i.e. whether belongs class regular invertible matrices. square n n matrix possesses n linearly independent eigenvectors matrix nondefective eigendecomposition exists theorem 4.12. know repeated eigen values may result defective matrices cannot diagonalized. nonsingular nondefective matrices same. exam ple rotation matrix invertible determinant nonzero diagonalizable real numbers eigenvalues guaranteed real numbers. draft 20230215 mathematics machine learning. feedback
4.8 reading 135 dive branch nondefective square n n matrices. normal condition aa aaholds. moreover restrictive condition holds aa aa i called or thogonal see definition 3.8. set orthogonal matrices subset regular invertible matrices satisfies a a1. normal matrices frequently encountered subset symmetric matrices rnn satisfy s. symmetric matrices real eigenvalues. subset symmetric matrices consists pos itive definite matrices p satisfy condition xp x 0 x rn0. case unique cholesky decomposition exists theo rem 4.18. positive definite matrices positive eigenvalues always invertible i.e. nonzero determinant. another subset symmetric matrices consists diagonal matrices d. diagonal matrices closed multiplication addition necessarily form group this case diagonal entries nonzero matrix invertible. special diagonal matrix identity matrix i. 4.8 reading content chapter establishes underlying mathematics connects methods studying mappings many heart machine learning level underpinning software so lutions building blocks almost machine learning theory. matrix characterization using determinants eigenspectra eigenspaces pro vides fundamental features conditions categorizing analyzing matrices. extends forms representations data map pings involving data well judging numerical stability compu tational operations matrices press et al. 2007. determinants fundamental tools order invert matrices compute eigenvalues by hand. however almost smallest instances numerical computation gaussian elimination outperforms determinants press et al. 2007. determinants remain nevertheless powerful theoretical concept e.g. gain intuition orientation basis based sign determinant. eigenvectors used perform basis changes transform data coordinates mean ingful orthogonal feature vectors. similarly matrix decomposition meth ods cholesky decomposition reappear often com pute simulate random events rubinstein kroese 2016. therefore cholesky decomposition enables us compute reparametrization trick want perform continuous differentiation random variables e.g. variational autoencoders jimenez rezende et al. 2014 kingma welling 2014. eigendecomposition fundamental enabling us extract mean ingful interpretable information characterizes linear mappings. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
136 matrix decompositions therefore eigendecomposition underlies general class machine learning algorithms called spectral methods perform eigendecomposi tion positivedefinite kernel. spectral decomposition methods encompass classical approaches statistical data analysis following principal component analysis principal component analysis pca pearson 1901 see also chapter 10 lowdimensional subspace explains vari ability data sought. fisher discriminant analysis fisher discriminant analysis aims determine separating hy perplane data classification mika et al. 1999. multidimensional scaling multidimensional scaling mds carroll chang 1970. computational efficiency methods typically comes find ing best rankk approximation symmetric positive semidefinite matrix. contemporary examples spectral methods different origins requires computation eigenvectors eigenvalues positivedefinite kernel isomap tenenbaum isomap et al. 2000 laplacian eigenmaps belkin niyogi 2003 hessian laplacian eigenmaps hessian eigenmaps eigenmaps donoho grimes 2003 spectral clustering shi spectral clustering malik 2000. core computations generally underpinned lowrank matrix approximation techniques belabbas wolfe 2009 encountered via svd. svd allows us discover kind information eigendecomposition. however svd generally applicable nonsquare matrices data tables. matrix factorization meth ods become relevant whenever want identify heterogeneity data want perform data compression approximation e.g. in stead storing nm values storing nmk values want perform data preprocessing e.g. decorrelate predictor variables design matrix ormoneit et al. 2001. svd operates matrices interpret rectangular arrays two indices rows columns. extension matrixlike structure higherdimensional arrays called tensors. turns svd special case general family decompositions operate tensors kolda bader 2009. svdlike operations lowrank approxima tions tensors are example tucker decomposition tucker 1966 tucker decomposition cp decomposition carroll chang 1970. cp decomposition svd lowrank approximation frequently used machine learn ing computational efficiency reasons. reduces amount memory operations nonzero multiplications need perform potentially large matrices data trefethen bau iii 1997. moreover lowrank approximations used operate ma trices may contain missing values well purposes lossy compression dimensionality reduction moonen de moor 1995 markovsky 2011. draft 20230215 mathematics machine learning. feedback
exercises 137 exercises 4.1 compute determinant using laplace expansion using first row sarrus rule 1 3 5 2 4 6 0 2 4 . 4.2 compute following determinant efficiently 2 0 1 2 0 2 1 0 1 1 0 1 2 1 2 2 0 2 1 2 2 0 0 1 1 . 4.3 compute eigenspaces a. 1 0 1 1 b. b 2 2 2 1 4.4 compute eigenspaces 0 1 1 1 1 1 2 3 2 1 0 0 1 1 1 0 . 4.5 diagonalizability matrix unrelated invertibility. determine following four matrices whether diagonalizable andor invert ible 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 0 . 4.6 compute eigenspaces following transformation matrices. diagonalizable a. 2 3 0 1 4 3 0 0 1 b. 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
138 matrix decompositions 4.7 following matrices diagonalizable yes determine diagonal form basis respect transformation matrices di agonal. no give reasons diagonalizable. a. 0 1 8 4 b. 1 1 1 1 1 1 1 1 1 c. 5 4 2 1 0 1 1 1 1 1 3 0 1 1 1 2 d. 5 6 6 1 4 2 3 6 4 4.8 find svd matrix 3 2 2 2 3 2 . 4.9 find singular value decomposition 2 2 1 1 . 4.10 find rank1 approximation 3 2 2 2 3 2 4.11 show rmn matrices aa aapossess nonzero eigenvalues. 4.12 show x 0 theorem 4.24 holds i.e. show max x ax2 x2 σ1 σ1 largest singular value rmn. draft 20230215 mathematics machine learning. feedback
5 vector calculus many algorithms machine learning optimize objective function respect set desired model parameters control well model explains data finding good parameters phrased opti mization problem see sections 8.2 8.3. examples include i lin ear regression see chapter 9 look curvefitting problems optimize linear weight parameters maximize likelihood ii neuralnetwork autoencoders dimensionality reduction data com pression parameters weights biases layer minimize reconstruction error repeated application chain rule iii gaussian mixture models see chapter 11 modeling data distributions optimize location shape parameters mixture component maximize likelihood model. figure 5.1 illustrates problems typically solve using optimization algorithms exploit gradient information section 7.1. figure 5.2 gives overview concepts chap ter related connected chapters book. central chapter concept function. function f quantity relates two quantities other. book quantities typically inputs x rd targets function values fx assume realvalued stated otherwise. rd domain f function values fx imagecodomain f. domain imagecodomain figure 5.1 vector calculus plays central role a regression curve fitting b density estimation i.e. modeling data distributions. 4 2 0 2 4 x 4 2 0 2 4 training data mle a regression problem find parameters curve explains observations crosses well. 10 5 0 5 10 x1 10 5 0 5 10 x2 b density estimation gaussian mixture model find means covariances data dots explained well. 139 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.
140 vector calculus figure 5.2 mind map concepts introduced chapter along used parts book. difference quotient partial derivatives jacobian hessian taylor series chapter 7 optimization chapter 6 probability chapter 9 regression chapter 10 dimensionality reduction chapter 11 density estimation chapter 12 classification defines collected used used used used used used used section 2.7.3 provides much detailed discussion context linear functions. often write f rd r 5.1a x 7fx 5.1b specify function 5.1a specifies f mapping rd r 5.1b specifies explicit assignment input x function value fx. function f assigns every input x exactly one function value fx. example 5.1 recall dot product special case inner product section 3.2. previous notation function fx xx x r2 would specified f r2 r 5.2a x 7x2 1 x2 2 . 5.2b chapter discuss compute gradients functions often essential facilitate learning machine learning models since gradient points direction steepest ascent. therefore draft 20230215 mathematics machine learning. feedback
5.1 differentiation univariate functions 141 figure 5.1 average incline function f x0 x0 δx incline secant blue fx0 fx0 δx given δyδx. δy δx fx x fx0 fx0 δx vector calculus one fundamental mathematical tools need machine learning. throughout book assume functions differentiable. additional technical definitions cover here many approaches presented extended subdifferentials functions continuous differentiable certain points. look extension case functions constraints chapter 7. 5.1 differentiation univariate functions following briefly revisit differentiation univariate function may familiar high school mathematics. start difference quotient univariate function fx x r subsequently use define derivatives. definition 5.1 difference quotient. difference quotient difference quotient δy δx fx δx fx δx 5.3 computes slope secant line two points graph f. figure 5.1 points xcoordinates x0 x0 δx. difference quotient also considered average slope f x x δx assume f linear function. limit δx 0 obtain tangent f x f differentiable. tangent derivative f x. definition 5.2 derivative. formally h 0 derivative f derivative x defined limit df dx lim h0 fx h fx h 5.4 secant figure 5.1 becomes tangent. derivative f points direction steepest ascent f. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
142 vector calculus example 5.2 derivative polynomial want compute derivative fx xn n n. may already know answer nxn1 want derive result using definition derivative limit difference quotient. using definition derivative 5.4 obtain df dx lim h0 fx h fx h 5.5a lim h0 x hn xn h 5.5b lim h0 pn i0 n xnihi xn h . 5.5c see xn n 0 xn0h0. starting sum 1 xnterm cancels obtain df dx lim h0 pn i1 n xnihi h 5.6a lim h0 n x i1 n xnihi1 5.6b lim h0 n 1 xn1 n x i2 n xnihi1 z 0 h0 5.6c n 1n 1xn1 nxn1 . 5.6d 5.1.1 taylor series taylor series representation function f infinite sum terms. terms determined using derivatives f evaluated x0. definition 5.3 taylor polynomial. taylor polynomial degree n taylor polynomial f r r x0 defined define t0 1 r. tnx n x k0 f kx0 k x x0k 5.7 f kx0 kth derivative f x0 which assume exists f kx0 k coefficients polynomial. definition 5.4 taylor series. smooth function f c f r r taylor series f x0 defined taylor series draft 20230215 mathematics machine learning. feedback
5.1 differentiation univariate functions 143 tx x k0 f kx0 k x x0k . 5.8 x0 0 obtain maclaurin series special instance f cmeans f continuously differentiable infinitely many times. maclaurin series taylor series. fx tx f called analytic. analytic remark. general taylor polynomial degree n approximation function need polynomial. taylor poly nomial similar f neighborhood around x0. however taylor polynomial degree n exact representation polynomial f degree k n since derivatives f i k vanish. example 5.3 taylor polynomial consider polynomial fx x4 5.9 seek taylor polynomial t6 evaluated x0 1. start com puting coefficients f k1 k 0 . . . 6 f1 1 5.10 f 1 4 5.11 f 1 12 5.12 f 31 24 5.13 f 41 24 5.14 f 51 0 5.15 f 61 0 5.16 therefore desired taylor polynomial t6x 6 x k0 f kx0 k x x0k 5.17a 1 4x 1 6x 12 4x 13 x 14 0 . 5.17b multiplying rearranging yields t6x 1 4 6 4 1 x4 12 12 4 x26 12 6 x34 4 x4 5.18a x4 fx 5.18b i.e. obtain exact representation original function. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
144 vector calculus figure 5.2 taylor polynomials. original function fx sinx cosx black solid approximated taylor polynomials dashed around x0 0. higherorder taylor polynomials approximate function f better globally. t10 already similar f 4 4. 4 2 0 2 4 x 2 0 2 4 f t0 t1 t5 t10 example 5.4 taylor series consider function figure 5.2 given fx sinx cosx c. 5.19 seek taylor series expansion f x0 0 maclaurin series expansion f. obtain following derivatives f0 sin0 cos0 1 5.20 f 0 cos0 sin0 1 5.21 f 0 sin0 cos0 1 5.22 f 30 cos0 sin0 1 5.23 f 40 sin0 cos0 f0 1 5.24 . . . see pattern here coefficients taylor series 1 since sin0 0 occurs twice switching one. furthermore f k40 f k0. therefore full taylor series expansion f x0 0 given tx x k0 f kx0 k x x0k 5.25a 1 x 1 2x2 1 3x3 1 4x4 1 5x5 5.25b 1 1 2x2 1 4x4 x 1 3x3 1 5x5 5.25c x k0 1k 1 2kx2k x k0 1k 1 2k 1x2k1 5.25d cosx sinx 5.25e draft 20230215 mathematics machine learning. feedback
5.1 differentiation univariate functions 145 used power series representations power series representation cosx x k0 1k 1 2kx2k 5.26 sinx x k0 1k 1 2k 1x2k1 . 5.27 figure 5.2 shows corresponding first taylor polynomials tn n 0 1 5 10. remark. taylor series special case power series fx x k0 akx ck 5.28 ak coefficients c constant special form definition 5.4. 5.1.2 differentiation rules following briefly state basic differentiation rules denote derivative f f . product rule fxgx f xgx fxgx 5.29 quotient rule fx gx f xgx fxgx gx2 5.30 sum rule fx gx f x gx 5.31 chain rule gfx g fx gfxf x 5.32 here g f denotes function composition x 7fx 7gfx. example 5.5 chain rule let us compute derivative function hx 2x 14 using chain rule. hx 2x 14 gfx 5.33 fx 2x 1 5.34 gf f 4 5.35 obtain derivatives f g f x 2 5.36 gf 4f 3 5.37 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
146 vector calculus derivative h given hx gff x 4f 3 2 5.34 42x 13 2 82x 13 5.38 used chain rule 5.32 substituted definition f 5.34 gf. 5.2 partial differentiation gradients differentiation discussed section 5.1 applies functions f scalar variable x r. following consider general case function f depends one variables x rn e.g. fx fx1 x2. generalization derivative functions sev eral variables gradient. find gradient function f respect x varying one variable time keeping others constant. gradient collection partial derivatives. definition 5.5 partial derivative. function f rn r x 7 fx x rn n variables x1 . . . xn define partial derivatives partial derivative f x1 lim h0 fx1 h x2 . . . xn fx h . . . f xn lim h0 fx1 . . . xn1 xn h fx h 5.39 collect row vector xf gradf df dx fx x1 fx x2 fx xn r1n 5.40 n number variables 1 dimension image rangecodomain f. here defined column vector x x1 . . . xn rn. row vector 5.40 called gradient f jacobian gradient jacobian generalization derivative section 5.1. remark. definition jacobian special case general definition jacobian vectorvalued functions collection partial derivatives. get back section 5.3. use results scalar differentiation partial derivative derivative respect scalar. example 5.6 partial derivatives using chain rule fx y x 2y32 obtain partial derivatives fx y x 2x 2y3 xx 2y3 2x 2y3 5.41 draft 20230215 mathematics machine learning. feedback
5.2 partial differentiation gradients 147 fx y y 2x 2y3 yx 2y3 12x 2y3y2 . 5.42 used chain rule 5.32 compute partial derivatives. remark gradient row vector. uncommon literature define gradient vector column vector following conven tion vectors generally column vectors. reason define gradient vector row vector twofold first consistently generalize gradient vectorvalued functions f rn rm then gradient becomes matrix. second immediately apply multivariate chain rule without paying attention dimension gradient. discuss points section 5.3. example 5.7 gradient fx1 x2 x2 1x2 x1x3 2 r partial derivatives i.e. deriva tives f respect x1 x2 fx1 x2 x1 2x1x2 x3 2 5.43 fx1 x2 x2 x2 1 3x1x2 2 5.44 gradient df dx fx1 x2 x1 fx1 x2 x2 2x1x2 x3 2 x2 1 3x1x2 2 r12 . 5.45 5.2.1 basic rules partial differentiation product rule fg fg fg sum rule f g f g chain rule gf gff multivariate case x rn basic differentiation rules know school e.g. sum rule product rule chain rule see also section 5.1.2 still apply. however compute derivatives re spect vectors x rn need pay attention gradients involve vectors matrices matrix multiplication commuta tive section 2.2.1 i.e. order matters. general product rule sum rule chain rule product rule x fxgx f xgx fx g x 5.46 sum rule x fx gx f x g x 5.47 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
148 vector calculus chain rule xg fx x gfx g f f x 5.48 let us closer look chain rule. chain rule 5.48 resem intuition mathematically correct since partial derivative fraction. bles degree rules matrix multiplication said neighboring dimensions match matrix multiplication de fined see section 2.2.1. go left right chain rule exhibits similar properties f shows denominator first factor numerator second factor. multiply factors to gether multiplication defined i.e. dimensions f match f cancels gx remains. 5.2.2 chain rule consider function f r2 r two variables x1 x2. furthermore x1t x2t functions t. compute gradient f respect t need apply chain rule 5.48 multivariate functions df dt h f x1 f x2 x1t t x2t t f x1 x1 t f x2 x2 t 5.49 denotes gradient partial derivatives. example 5.8 consider fx1 x2 x2 1 2x2 x1 sin x2 cos t df dt f x1 x1 t f x2 x2 t 5.50a 2 sin tsin t 2cos t 5.50b 2 sin cos 2 sin 2 sin tcos 1 5.50c corresponding derivative f respect t. fx1 x2 function x1 x2 x1s t x2s t functions two variables t chain rule yields partial derivatives f s f x1 x1 s f x2 x2 s 5.51 f t f x1 x1 t f x2 x2 t 5.52 draft 20230215 mathematics machine learning. feedback
5.3 gradients vectorvalued functions 149 gradient obtained matrix multiplication df ds t f x x s t h f x1 f x2 z f x x1 s x1 t x2 s x2 t z x s t . 5.53 compact way writing chain rule matrix multiplication chain rule written matrix multiplication. makes sense gradient defined row vector. otherwise need start transposing gradients matrix dimensions match. may still straightforward long gradient vector matrix however gradient becomes tensor we discuss following transpose longer triviality. remark verifying correctness gradient implementation. definition partial derivatives limit corresponding dif ference quotient see 5.39 exploited numerically checking correctness gradients computer programs compute gradient checking gradients implement them use finite differences numer ically test computation implementation choose value h small e.g. h 104 compare finitedifference approxima tion 5.39 analytic implementation gradient. error small gradient implementation probably correct. small could mean q p idhid fi2 p idhid fi2 106 dhi finitedifference approximation fi analytic gradient f respect ith variable xi. 5.3 gradients vectorvalued functions thus far discussed partial derivatives gradients functions f rn r mapping real numbers. following generalize concept gradient vectorvalued functions vector fields f rn rm n 1 1. function f rn rm vector x x1 . . . xnrn corresponding vector function values given fx f1x . . . fmx rm . 5.54 writing vectorvalued function way allows us view vector valued function f rn rm vector functions f1 . . . fm fi rn r map onto r. differentiation rules every fi exactly ones discussed section 5.2. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
150 vector calculus therefore partial derivative vectorvalued function f rn rm respect xi r 1 . . . n given vector f xi f1 xi . . . fm xi limh0 f1x1.xi1xihxi1.xnf1x h . . . limh0 fmx1.xi1xihxi1.xnfmx h rm . 5.55 5.40 know gradient f respect vector row vector partial derivatives. 5.55 every partial derivative fxi column vector. therefore obtain gradient f rn rm respect x rn collecting partial derivatives dfx dx fx x1 fx xn 5.56a f1x x1 f1x xn . . . . . . fmx x1 fmx xn rmn . 5.56b definition 5.6 jacobian. collection firstorder partial deriva tives vectorvalued function f rn rm called jacobian. jacobian jacobian j n matrix define arrange follows gradient function f rn rm matrix size n. j xf dfx dx fx x1 fx xn 5.57 f1x x1 f1x xn . . . . . . fmx x1 fmx xn 5.58 x x1 . . . xn ji j fi xj . 5.59 special case 5.58 function f rn r1 maps vector x rn onto scalar e.g. fx pn i1 xi possesses jacobian row vector matrix dimension 1 n see 5.40. remark. book use numerator layout derivative i.e. numerator layout derivative dfdx f rm respect x rn n matrix elements f define rows elements x define columns corresponding jacobian see 5.58. draft 20230215 mathematics machine learning. feedback
5.3 gradients vectorvalued functions 151 figure 5.1 determinant jacobian f used compute magnifier blue orange area. b1 b2 c1 c2 f exists also denominator layout transpose numerator denominator layout layout. book use numerator layout. see jacobian used changeofvariable method probability distributions section 6.7. amount scaling due transformation variable provided determinant. section 4.1 saw determinant used compute area parallelogram. given two vectors b1 1 0 b2 0 1as sides unit square blue see figure 5.1 area square det 1 0 0 1 1 . 5.60 take parallelogram sides c1 2 1 c2 1 1 orange figure 5.1 area given absolute value deter minant see section 4.1 det 2 1 1 1 3 3 5.61 i.e. area exactly three times area unit square. find scaling factor finding mapping transforms unit square square. linear algebra terms effectively perform variable transformation b1 b2 c1 c2. case mapping linear absolute value determinant mapping gives us exactly scaling factor looking for. describe two approaches identify mapping. first ex ploit mapping linear use tools chapter 2 identify mapping. second find mapping using partial derivatives using tools discussing chapter. approach 1 get started linear algebra approach identify b1 b2 c1 c2 bases r2 see section 2.6.1 recap. effectively perform change basis b1 b2 c1 c2 looking transformation matrix implements basis change. using results section 2.7.2 identify desired basis change matrix j 2 1 1 1 5.62 jb1 c1 jb2 c2. absolute value determi 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
152 vector calculus nant j yields scaling factor looking for given detj 3 i.e. area square spanned c1 c2 three times greater area spanned b1 b2. approach 2 linear algebra approach works linear trans formations nonlinear transformations which become relevant sec tion 6.7 follow general approach using partial derivatives. approach consider function f r2 r2 performs variable transformation. example f maps coordinate represen tation vector x r2 respect b1 b2 onto coordinate representation r2 respect c1 c2. want identify mapping compute area or volume changes transformed f. this need find fx changes modify x bit. question exactly answered jacobian matrix df dx r22. since write y1 2x1 x2 5.63 y2 x1 x2 5.64 obtain functional relationship x y allows us get partial derivatives y1 x1 2 y1 x2 1 y2 x1 1 y2 x2 1 5.65 compose jacobian j y1 x1 y1 x2 y2 x1 y2 x2 2 1 1 1 . 5.66 jacobian represents coordinate transformation looking geometrically jacobian determinant gives magnification scaling factor transform area volume. for. exact coordinate transformation linear as case 5.66 recovers exactly basis change matrix 5.62. co ordinate transformation nonlinear jacobian approximates non linear transformation locally linear one. absolute value jacobian determinant detj factor areas volumes jacobian determinant scaled coordinates transformed. case yields detj 3. jacobian determinant variable transformations become relevant section 6.7 transform random variables prob ability distributions. transformations extremely relevant ma figure 5.2 dimensionality partial derivatives. fx x f x chine learning context training deep neural networks using reparametrization trick also called infinite perturbation analysis. chapter encountered derivatives functions. figure 5.2 sum marizes dimensions derivatives. f r r gradient simply scalar topleft entry. f rd r gradient 1 row vector topright entry. f r re gradient e 1 column vector f rd re gradient e matrix. draft 20230215 mathematics machine learning. feedback
5.3 gradients vectorvalued functions 153 example 5.9 gradient vectorvalued function given fx ax fx rm rmn x rn . compute gradient dfdx first determine dimension dfdx since f rn rm follows dfdx rmn. second compute gradient determine partial derivatives f respect every xj fix n x j1 aijxj fi xj aij 5.67 collect partial derivatives jacobian obtain gradient df dx f1 x1 f1 xn . . . . . . fm x1 fm xn a11 a1n . . . . . . am1 amn rmn . 5.68 example 5.10 chain rule consider function h r r ht f gt f r2 r 5.69 g r r2 5.70 fx expx1x2 2 5.71 x x1 x2 gt t cos sin 5.72 compute gradient h respect t. since f r2 r g r r2 note f x r12 g t r21 . 5.73 desired gradient computed applying chain rule dh dt f x x t f x1 f x2 x1 t x2 t 5.74a expx1x2 2x2 2 2 expx1x2 2x1x2 cos t sin sin cos 5.74b expx1x2 2 x2 2cos t sin t 2x1x2sin cos t 5.74c x1 cos x2 sin t see 5.72. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
154 vector calculus example 5.11 gradient leastsquares loss linear model let us consider linear model discuss model much detail chapter 9 context linear regression need derivatives leastsquares loss l respect parameters θ. φθ 5.75 θ rd parameter vector φ rnd input features rn corresponding observations. define functions le e2 5.76 eθ φθ . 5.77 seek l θ use chain rule purpose. l called leastsquares loss function. leastsquares loss start calculation determine dimensionality gradient l θ r1d . 5.78 chain rule allows us compute gradient l θ l e e θ 5.79 dth element given dldtheta np.einsum nnd dldededtheta l θ 1 d n x n1 l e ne θn d . 5.80 know e2 ee see section 3.2 determine l e 2er1n . 5.81 furthermore obtain e θ φ rnd 5.82 desired derivative l θ 2eφ 5.77 2yθφ z 1n φ z nd r1d . 5.83 remark. would obtained result without using chain rule immediately looking function l2θ y φθ2 y φθy φθ . 5.84 approach still practical simple functions like l2 becomes impractical deep function compositions. draft 20230215 mathematics machine learning. feedback
5.4 gradients matrices 155 figure 5.3 visualization gradient computation matrix respect vector. interested computing gradient r42 respect vector x r3. know gradient da dx r423. follow two equivalent approaches arrive there a collating partial derivatives jacobian tensor b flattening matrix vector computing jacobian matrix reshaping jacobian tensor. r42 x r3 a x1 r42 a x2 r42 a x3 r42 x1 x2 x3 da dx r423 4 2 3 partial derivatives collate a approach 1 compute partial derivative a x1 a x2 a x3 4 2 matrix col late 4 2 3 tensor. r42 x r3 x1 x2 x3 da dx r423 reshape reshape gradient r42 r8 dx r83 b approach 2 reshape flatten r42 vec tor r8. then compute gradient dx r83. obtain gradient tensor reshaping gradient illustrated above. 5.4 gradients matrices think tensor multidimensional array. encounter situations need take gradients matrices respect vectors or matrices results multidimen sional tensor. think tensor multidimensional array 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
156 vector calculus collects partial derivatives. example compute gradient n matrix respect p q matrix b resulting jacobian would mnpq i.e. fourdimensional tensor j whose entries given jijkl aijbkl. since matrices represent linear mappings exploit fact vectorspace isomorphism linear invertible mapping space rmn n matrices space rmn mn vectors. therefore reshape matrices vectors lengths mn pq respectively. gradient using mn vectors results jacobian size mn pq. figure 5.3 visualizes approaches. practical ap matrices transformed vectors stacking columns matrix flattening. plications often desirable reshape matrix vector continue working jacobian matrix chain rule 5.48 boils simple matrix multiplication whereas case jacobian tensor need pay attention dimensions need sum out. example 5.12 gradient vectors respect matrices let us consider following example f ax f rm rmn x rn 5.85 seek gradient dfda. let us start determining dimension gradient df da rmmn . 5.86 definition gradient collection partial derivatives df da f1 a . . . fm a fi a r1mn . 5.87 compute partial derivatives helpful explicitly write matrix vector multiplication fi n x j1 aijxj 1 . . . 5.88 partial derivatives given fi aiq xq . 5.89 allows us compute partial derivatives fi respect row a given fi ai xr11n 5.90 draft 20230215 mathematics machine learning. feedback
5.4 gradients matrices 157 fi aki 0r11n 5.91 pay attention correct dimensionality. since fi maps onto r row size 1 n obtain 1 1 n sized tensor partial derivative fi respect row a. stack partial derivatives 5.91 get desired gradient 5.87 via fi a 0 . . . 0 x 0 . . . 0 r1mn . 5.92 example 5.13 gradient matrices respect matrices consider matrix r rmn f rmn rnn fr rr k rnn 5.93 seek gradient dkdr. solve hard problem let us first write already know gradient dimensions dk dr rnnmn 5.94 tensor. moreover dkpq dr r1mn 5.95 p q 1 . . . n kpq p qth entry k fr. de noting ith column r ri every entry k given dot product two columns r i.e. kpq r p rq x m1 rmprmq . 5.96 compute partial derivative kpq rij obtain kpq rij x m1 rij rmprmq pqij 5.97 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
158 vector calculus pqij riq j p p q rip j q p q 2riq j p p q 0 otherwise . 5.98 5.94 know desired gradient dimension n n m n every single entry tensor given pqij 5.98 p q j 1 . . . n 1 . . . m. 5.5 useful identities computing gradients following list useful gradients frequently required machine learning context petersen pedersen 2012. here use tr trace see definition 4.4 det determinant see section 4.1 fx1 inverse fx assuming exists. x fx fx x 5.99 x trfx tr fx x 5.100 x detfx detfxtr fx1 fx x 5.101 x fx1 fx1 fx x fx1 5.102 ax1b x x1abx1 5.103 xa x a 5.104 ax x a 5.105 axb x ab 5.106 xbx x xb b 5.107 sx asw x as 2x asw symmetric w 5.108 remark. book cover traces transposes matrices. however seen derivatives higherdimensional ten sors case usual trace transpose defined. cases trace ddef tensor would efdimensional matrix. special case tensor contraction. similarly draft 20230215 mathematics machine learning. feedback
5.6 backpropagation automatic differentiation 159 transpose tensor mean swapping first two dimensions. specif ically 5.99 5.102 require tensorrelated computations work multivariate functions f compute derivatives respect matrices and choose vectorize discussed section 5.4. 5.6 backpropagation automatic differentiation good discussion backpropagation chain rule available blog tim vieira comycfm2yrw. many machine learning applications find good model parameters performing gradient descent section 7.1 relies fact compute gradient learning objective respect parameters model. given objective function obtain gradient respect model parameters using calculus applying chain rule see section 5.2.2. already taste section 5.3 looked gradient squared loss respect parameters linear regression model. consider function fx q x2 expx2 cos x2 expx2 . 5.109 application chain rule noting differentiation linear compute gradient df dx 2x 2x expx2 2 p x2 expx2 sin x2 expx2 2x 2x expx2 2x 1 2 p x2 expx2 sin x2 expx2 1 expx2 . 5.110 writing gradient explicit way often impractical since often results lengthy expression derivative. practice means that careful implementation gradient could significantly expensive computing function imposes unnecessary overhead. training deep neural network mod els backpropagation algorithm kelley 1960 bryson 1961 dreyfus backpropagation 1962 rumelhart et al. 1986 efficient way compute gradient error function respect parameters model. 5.6.1 gradients deep network area chain rule used extreme deep learning function value computed manylevel function composition fk fk1 f1x fkfk1 f1x 5.111 x inputs e.g. images observations e.g. class labels every function fi 1 . . . k possesses parameters. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
160 vector calculus figure 5.2 forward pass multilayer neural network compute loss l function inputs x parameters ai bi. x f k a0 b0 ak1 bk1 l f k1 ak2 bk2 f 1 a1 b1 neural networks multiple layers functions fixi1 discuss case activation functions identical layer unclutter notation. σai1xi1 bi1 ith layer. xi1 output layer 1 σ activation function logistic sigmoid 1 1ex tanh rectified linear unit relu. order train models require gradient loss function l respect model parameters aj bj j 1 . . . k. also requires us compute gradient l respect inputs layer. example inputs x observations network structure defined f 0 x 5.112 f σiai1f i1 bi1 1 . . . k 5.113 see also figure 5.2 visualization may interested finding aj bj j 0 . . . k 1 squared loss lθ y f kθ x2 5.114 minimized θ a0 b0 . . . ak1 bk1. obtain gradients respect parameter set θ require partial derivatives l respect parameters θj aj bj layer j 0 . . . k 1. chain rule allows us determine partial derivatives indepth discussion gradients neural networks found justin domkes lecture notes comyalcxgtv. l θk1 l f k f k θk1 5.115 l θk2 l f k f k f k1 f k1 θk2 5.116 l θk3 l f k f k f k1 f k1 f k2 f k2 θk3 5.117 l θi l f k f k f k1 f i2 f i1 f i1 θi 5.118 orange terms partial derivatives output layer respect inputs whereas blue terms partial derivatives output layer respect parameters. assuming already computed partial derivatives lθi1 com putation reused compute lθi. additional terms draft 20230215 mathematics machine learning. feedback
5.6 backpropagation automatic differentiation 161 figure 5.2 backward pass multilayer neural network compute gradients loss function. x f k a0 b0 ak1 bk1 l f k1 ak2 bk2 f 1 a1 b1 figure 5.1 simple graph illustrating flow data x via intermediate variables a b. x b need compute indicated boxes. figure 5.2 visualizes gradients passed backward network. 5.6.2 automatic differentiation turns backpropagation special case general technique numerical analysis called automatic differentiation. think au automatic differentiation tomatic differentation set techniques numerically in contrast symbolically evaluate exact up machine precision gradient function working intermediate variables applying chain rule. automatic differentiation applies series elementary arithmetic automatic differentiation different symbolic differentiation numerical approximations gradient e.g. using finite differences. operations e.g. addition multiplication elementary functions e.g. sin cos exp log. applying chain rule operations gradient quite complicated functions computed automatically. automatic differentiation applies general computer programs forward reverse modes. baydin et al. 2018 give great overview automatic differentiation machine learning. figure 5.1 shows simple graph representing data flow inputs x outputs via intermediate variables a b. compute derivative dydx would apply chain rule obtain dy dx dy db db da da dx . 5.119 intuitively forward reverse mode differ order multipli general case work jacobians vectors matrices tensors. cation. due associativity matrix multiplication choose dy dx dy db db da da dx 5.120 dy dx dy db db da da dx . 5.121 equation 5.120 would reverse mode gradients prop reverse mode agated backward graph i.e. reverse data flow. equa tion 5.121 would forward mode gradients flow forward mode data left right graph. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
162 vector calculus following focus reverse mode automatic differentia tion backpropagation. context neural networks input dimensionality often much higher dimensionality labels reverse mode computationally significantly cheaper forward mode. let us start instructive example. example 5.14 consider function fx q x2 expx2 cos x2 expx2 5.122 5.109. implement function f computer would able save computation using intermediate variables intermediate variables x2 5.123 b expa 5.124 c b 5.125 c 5.126 e cosc 5.127 f e . 5.128 figure 5.1 computation graph inputs x function values f intermediate variables a b c d e. x 2 exp b c cos e f kind thinking process occurs applying chain rule. note preceding set equations requires fewer operations direct implementation function fx defined 5.109. corresponding computation graph figure 5.1 shows flow data computations required obtain function value f. set equations include intermediate variables thought computation graph representation widely used imple mentations neural network software libraries. directly compute derivatives intermediate variables respect corre sponding inputs recalling definition derivative elementary functions. obtain following a x 2x 5.129 b a expa 5.130 c a 1 c b 5.131 draft 20230215 mathematics machine learning. feedback
5.6 backpropagation automatic differentiation 163 d c 1 2c 5.132 e c sinc 5.133 f d 1 f e . 5.134 looking computation graph figure 5.1 compute fx working backward output obtain f c f d d c f e e c 5.135 f b f c c b 5.136 f a f b b a f c c a 5.137 f x f a a x . 5.138 note implicitly applied chain rule obtain fx. substi tuting results derivatives elementary functions get f c 1 1 2c 1 sinc 5.139 f b f c 1 5.140 f a f b expa f c 1 5.141 f x f a 2x . 5.142 thinking derivatives variable observe computation required calculating derivative similar complexity computation function itself. quite counter intuitive since mathematical expression derivative f x 5.110 significantly complicated mathematical expression function fx 5.109. automatic differentiation formalization example 5.14. let x1 . . . xd input variables function xd1 . . . xd1 intermediate variables xd output variable. computation graph expressed follows 1 . . . xi gixpaxi 5.143 gi elementary functions xpaxi parent nodes variable xi graph. given function defined way 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
164 vector calculus use chain rule compute derivative function step bystep fashion. recall definition f xd hence f xd 1 . 5.144 variables xi apply chain rule f xi x xjxipaxj f xj xj xi x xjxipaxj f xj gj xi 5.145 paxj set parent nodes xj computation graph. equation 5.143 forward propagation function whereas 5.145 autodifferentiation reverse mode requires parse tree. backpropagation gradient computation graph. neural network training backpropagate error prediction respect label. automatic differentiation approach works whenever function expressed computation graph ele mentary functions differentiable. fact function may even mathematical function computer program. however com puter programs automatically differentiated e.g. cannot find differential elementary functions. programming structures loops statements require care well. 5.7 higherorder derivatives far discussed gradients i.e. firstorder derivatives. some times interested derivatives higher order e.g. want use newtons method optimization requires secondorder derivatives nocedal wright 2006. section 5.1.1 discussed taylor series approximate functions using polynomials. mul tivariate case exactly same. following exactly this. let us start notation. consider function f r2 r two variables x y. use following notation higherorder partial derivatives and gradients 2f x2 second partial derivative f respect x. nf xn nth partial derivative f respect x. 2f yx y f x partial derivative obtained first partial differ entiating respect x respect y. 2f xy partial derivative obtained first partial differentiating x. hessian collection secondorder partial derivatives. hessian fx y twice continuously differentiable function 2f xy 2f yx 5.146 draft 20230215 mathematics machine learning. feedback
5.8 linearization multivariate taylor series 165 figure 5.1 linear approximation function. original function f linearized x0 2 using firstorder taylor series expansion. 4 2 0 2 4 x 2 1 0 1 fx fx fx0 fx0 f x0x x0 i.e. order differentiation matter corresponding hessian matrix hessian matrix h 2f x2 2f xy 2f xy 2f y2 5.147 symmetric. hessian denoted 2 xyfx y. generally x rn f rn r hessian n n matrix. hessian measures curvature function locally around x y. remark hessian vector field. f rn rm vector field hessian m n ntensor. 5.8 linearization multivariate taylor series gradient f function f often used locally linear approxi mation f around x0 fx fx0 xfx0x x0 . 5.148 xfx0 gradient f respect x evaluated x0. figure 5.1 illustrates linear approximation function f input x0. original function approximated straight line. approx imation locally accurate farther move away x0 worse approximation gets. equation 5.148 special case mul tivariate taylor series expansion f x0 consider first two terms. discuss general case following allow better approximations. definition 5.7 multivariate taylor series. consider function f rd r 5.149 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
166 vector calculus figure 5.1 visualizing outer products. outer products vectors increase dimensionality array 1 per term. a outer product two vectors results matrix b outer product three vectors yields thirdorder tensor. a given vector δ r4 obtain outer product δ2 δ δ δδ r44 matrix. b outer product δ3 δ δ δ r444 results thirdorder tensor three dimensional matrix i.e. array three indexes. x 7fx x rd 5.150 smooth x0. define difference vector δ x x0 multivariate taylor series f x0 defined multivariate taylor series fx x k0 dk xfx0 k δk 5.151 dk xfx0 kth total derivative f respect x eval uated x0. definition 5.8 taylor polynomial. taylor polynomial degree n taylor polynomial f x0 contains first n 1 components series 5.151 defined tnx n x k0 dk xfx0 k δk . 5.152 5.151 5.152 used slightly sloppy notation δk defined vectors x rd 1 k 1. note dk xf δk kth order tensors i.e. kdimensional arrays. vector implemented onedimensional array matrix twodimensional array. kthorder tensor δk r k times z dd.d obtained kfold outer product denoted vector δ rd. example δ2 δ δ δδ δ2i j δiδj 5.153 δ3 δ δ δ δ3i j k δiδjδk . 5.154 figure 5.1 visualizes two outer products. general obtain draft 20230215 mathematics machine learning. feedback
5.8 linearization multivariate taylor series 167 terms dk xfx0δk x i11 x ik1 dk xfx0i1 . . . ikδi1 δik 5.155 taylor series dk xfx0δk contains kth order polynomials. defined taylor series vector fields let us explicitly write first terms dk xfx0δk taylor series expansion k 0 . . . 3 δ x x0 np.einsum iidf1d np.einsum ijij df2dd np.einsum ijkijk df3ddd k 0 d0 xfx0δ0 fx0 r 5.156 k 1 d1 xfx0δ1 xfx0 z 1d δ z d1 x i1 xfx0iδi r 5.157 k 2 d2 xfx0δ2 tr hx0 z dd δ z d1 δ z 1d δhx0δ 5.158 x i1 x j1 hi jδiδj r 5.159 k 3 d3 xfx0δ3 x i1 x j1 x k1 d3 xfx0i j kδiδjδk r 5.160 here hx0 hessian f evaluated x0. example 5.15 taylor series expansion function two vari ables consider function fx y x2 2xy y3 . 5.161 want compute taylor series expansion f x0 y0 1 2. start let us discuss expect function 5.161 polynomial degree 3. looking taylor series expansion linear combination polynomials. therefore expect taylor series expansion contain terms fourth higher order express thirdorder polynomial. means sufficient determine first four terms 5.151 exact alterna tive representation 5.161. determine taylor series expansion start constant term firstorder derivatives given f1 2 13 5.162 f x 2x 2y f x1 2 6 5.163 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
168 vector calculus f y 2x 3y2 f y 1 2 14 . 5.164 therefore obtain d1 xyf1 2 xyf1 2 h f x1 2 f y 1 2 6 14 r12 5.165 d1 xyf1 2 1 δ 6 14 x 1 2 6x 1 14y 2 . 5.166 note d1 xyf1 2δ contains linear terms i.e. firstorder polyno mials. secondorder partial derivatives given 2f x2 2 2f x2 1 2 2 5.167 2f y2 6y 2f y2 1 2 12 5.168 2f yx 2 2f yx1 2 2 5.169 2f xy 2 2f xy1 2 2 . 5.170 collect secondorder partial derivatives obtain hes sian h 2f x2 2f xy 2f yx 2f y2 2 2 2 6y 5.171 h1 2 2 2 2 12 r22 . 5.172 therefore next term taylorseries expansion given d2 xyf1 2 2 δ2 1 2δh1 2δ 5.173a 1 2 x 1 2 2 2 2 12 x 1 2 5.173b x 12 2x 1y 2 6y 22 . 5.173c here d2 xyf1 2δ2 contains quadratic terms i.e. secondorder poly nomials. draft 20230215 mathematics machine learning. feedback
5.8 linearization multivariate taylor series 169 thirdorder derivatives obtained d3 xyf h h x h y r222 5.174 d3 xyf 1 h x 3f x3 3f x2y 3f xyx 3f xy2 5.175 d3 xyf 2 h y 3f yx2 3f yxy 3f y2x 3f y3 . 5.176 since secondorder partial derivatives hessian 5.171 constant nonzero thirdorder partial derivative 3f y3 6 3f y3 1 2 6 . 5.177 higherorder derivatives mixed derivatives degree 3 e.g. f 3 x2y vanish d3 xyf 1 0 0 0 0 d3 xyf 2 0 0 0 6 5.178 d3 xyf1 2 3 δ3 y 23 5.179 collects cubic terms taylor series. overall exact taylor series expansion f x0 y0 1 2 fx f1 2 d1 xyf1 2δ d2 xyf1 2 2 δ2 d3 xyf1 2 3 δ3 5.180a f1 2 f1 2 x x 1 f1 2 y y 2 1 2 2f1 2 x2 x 12 2f1 2 y2 y 22 22f1 2 xy x 1y 2 1 6 3f1 2 y3 y 23 5.180b 13 6x 1 14y 2 x 12 6y 22 2x 1y 2 y 23 . 5.180c case obtained exact taylor series expansion polyno mial 5.161 i.e. polynomial 5.180c identical original polynomial 5.161. particular example result sur prising since original function thirdorder polynomial expressed linear combination constant terms firstorder secondorder thirdorder polynomials 5.180c. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
170 vector calculus 5.9 reading details matrix differentials along short review required linear algebra found magnus neudecker 2007. automatic differentiation long history refer griewank walther 2003 griewank walther 2008 elliott 2009 references therein. machine learning and disciplines often need compute expectations i.e. need solve integrals form exfx z fxpxdx . 5.181 even px convenient form e.g. gaussian integral gen erally cannot solved analytically. taylor series expansion f one way finding approximate solution assuming px n µ σ gaussian firstorder taylor series expansion around µ locally linearizes nonlinear function f. linear functions compute mean and covariance exactly px gaussian distributed see section 6.5. property heavily exploited extended kalman extended kalman filter filter maybeck 1979 online state estimation nonlinear dynami cal systems also called statespace models. deterministic ways approximate integral 5.181 unscented transform julier unscented transform uhlmann 1997 require gradients laplace laplace approximation approximation mackay 2003 bishop 2006 murphy 2012 uses secondorder taylor series expansion requiring hessian local gaussian approximation px around mode. exercises 5.1 compute derivative fx fx logx4 sinx3 . 5.2 compute derivative fx logistic sigmoid fx 1 1 expx . 5.3 compute derivative fx function fx exp1 2σ2 x µ2 µ σ r constants. 5.4 compute taylor polynomials tn n 0 . . . 5 fx sinx cosx x0 0. 5.5 consider following functions f1x sinx1 cosx2 x r2 f2x y xy x rn f3x xx x rn draft 20230215 mathematics machine learning. feedback
exercises 171 a. dimensions fi x b. compute jacobians. 5.6 differentiate f respect g respect x ft sinlogtt rd gx traxb rde x ref b rf d tr denotes trace. 5.7 compute derivatives dfdx following functions using chain rule. provide dimensions every single partial derivative. describe steps detail. a. fz log1 z z xx x rd b. fz sinz z ax b red x rd b re sin applied every element z. 5.8 compute derivatives dfdx following functions. describe steps detail. a. use chain rule. provide dimensions every single partial deriva tive. fz exp1 2z z gy ys1y hx x µ x µ rd rdd. b. fx trxx σ2i x rd tra trace a i.e. sum diagonal elements aii. hint explicitly write outer product. c. use chain rule. provide dimensions every single partial deriva tive. need compute product partial derivatives explicitly. f tanhz rm z ax b x rn rmn b rm. here tanh applied every component z. 5.9 define gx z ν log px z log qz ν z tϵ ν differentiable functions p q x rd z re ν rf ϵ rg. using chain rule compute gradient dν gx z ν . 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
6 probability distributions probability loosely speaking concerns study uncertainty. probabil ity thought fraction times event occurs degree belief event. would like use probability mea sure chance something occurring experiment. mentioned chapter 1 often quantify uncertainty data uncertainty machine learning model uncertainty predictions produced model. quantifying uncertainty requires idea random variable random variable function maps outcomes random experiments set properties interested in. associated random variable function measures probability particular outcome or set outcomes occur called probability distribution. probability distribution probability distributions used building block con cepts probabilistic modeling section 8.4 graphical models sec tion 8.5 model selection section 8.6. next section present three concepts define probability space the sample space events probability event related fourth concept called random variable. presentation deliber ately slightly hand wavy since rigorous presentation may occlude intuition behind concepts. outline concepts presented chapter shown figure 6.2. 6.1 construction probability space theory probability aims defining mathematical structure describe random outcomes experiments. example tossing single coin cannot determine outcome large num ber coin tosses observe regularity average outcome. using mathematical structure probability goal perform automated reasoning sense probability generalizes logical reasoning jaynes 2003. 6.1.1 philosophical issues constructing automated reasoning systems classical boolean logic allow us express certain forms plausible reasoning. consider 172 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.
6.1 construction probability space 173 figure 6.2 mind map concepts related random variables probability distributions described chapter. random variable distribution sum rule product rule bayes theorem summary statistics mean variance transformations independence inner product gaussian bernoulli beta sufficient statistics exponential family chapter 9 regression chapter 10 dimensionality reduction chapter 11 density estimation property similarity example example conjugate property finite following scenario observe false. find b becomes less plausible although conclusion drawn classical logic. observe b true. seems becomes plausible. use form reasoning daily. waiting friend consider three possibilities h1 time h2 delayed traffic h3 abducted aliens. observe friend late must logically rule h1. also tend consider h2 likely though logically required so. finally may consider h3 possible continue consider quite unlikely. conclude h2 plausible answer seen way for plausible reasoning necessary extend discrete true false values truth continuous plausibilities jaynes 2003. probability theory considered generalization boolean logic. context machine learning often applied way formalize design automated reasoning systems. arguments probability theory foundation reasoning systems found pearl 1988. philosophical basis probability somehow related think true in logical sense studied cox jaynes 2003. another way think precise common sense end constructing probabilities. e. t. jaynes 19221998 identified three mathematical criteria must apply plausibilities 1. degrees plausibility represented real numbers. 2. numbers must based rules common sense. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
174 probability distributions 3. resulting reasoning must consistent three following meanings word consistent a consistency noncontradiction result reached different means plausibility value must found cases. b honesty available data must taken account. c reproducibility state knowledge two problems same must assign degree plausibility them. coxjaynes theorem proves plausibilities sufficient define universal mathematical rules apply plausibility p transformation arbitrary monotonic function. crucially rules rules probability. remark. machine learning statistics two major interpre tations probability bayesian frequentist interpretations bishop 2006 efron hastie 2016. bayesian interpretation uses probabil ity specify degree uncertainty user event. sometimes referred subjective probability degree belief. frequentist interpretation considers relative frequencies events interest total number events occurred. probability event defined relative frequency event limit one infinite data. machine learning texts probabilistic models use lazy notation jargon confusing. text exception. multiple distinct concepts referred probability distribution reader often disentangle meaning context. one trick help make sense probability distributions check whether trying model something categorical a discrete random variable some thing continuous a continuous random variable. kinds questions tackle machine learning closely related whether con sidering categorical continuous models. 6.1.2 probability random variables three distinct ideas often confused discussing probabilities. first idea probability space allows us quantify idea probability. however mostly work directly basic probability space. instead work random variables the second idea transfers probability convenient often numerical space. third idea idea distribution law associated random variable. introduce first two ideas section expand third idea section 6.2. modern probability based set axioms proposed kolmogorov draft 20230215 mathematics machine learning. feedback
6.1 construction probability space 175 grinstead snell 1997 jaynes 2003 introduce three con cepts sample space event space probability measure. prob ability space models realworld process referred experiment random outcomes. sample space ω sample space set possible outcomes experiment sample space usually denoted ω. example two successive coin tosses sample space hh tt ht th h denotes heads t denotes tails. event space event space space potential results experiment. event space subset sample space ωis event space end experiment observe whether particular outcome ω ω a. event space obtained considering collection subsets ω discrete probability distributions section 6.2.1 often power set ω. probability p event a associate number pa measures probability degree belief event occur. pa called probability a. probability probability single event must lie interval 0 1 total probability outcomes sample space ωmust 1 i.e. pω 1. given probability space ω a p want use model realworld phenomenon. machine learning often avoid explic itly referring probability space instead refer probabilities quantities interest denote . book refer target space refer elements states. introduce target space function x ωt takes element ωan outcome returns particular quantity interest x value . associationmapping ωto called random variable. example case tossing random variable two coins counting number heads random variable x maps three possible outcomes xhh 2 xht 1 xth 1 xtt 0. particular case 0 1 2 probabilities elements interested in. finite sample space ωand name random variable great source misunderstanding neither random variable. function. finite function corresponding random variable essentially lookup table. subset t associate pxs 0 1 the probability particular event occurring corresponding random variable x. example 6.1 provides concrete illustration terminol ogy. remark. aforementioned sample space ωunfortunately referred different names different books. another common name ω state space jacod protter 2004 state space sometimes reserved referring states dynamical system hasselblatt 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
176 probability distributions katok 2003. names sometimes used describe ωare sample description space possibility space event space. example 6.1 assume reader already familiar computing probabilities toy example essentially biased coin flip example. intersections unions sets events. gentler introduction probability many examples found chapter 2 walpole et al. 2011. consider statistical experiment model funfair game con sisting drawing two coins bag with replacement. coins usa denoted uk denoted bag since draw two coins bag four outcomes total. state space sample space ωof experiment . let us assume composition bag coins draw returns random probability 0.3. event interested total number times repeated draw returns . let us define random variable x maps sample space ωto denotes number times draw bag. see preceding sample space get zero one two s therefore 0 1 2. random variable x a function lookup table represented table like following x 2 6.1 x 1 6.2 x 1 6.3 x 0 . 6.4 since return first coin draw drawing second implies two draws independent other discuss section 6.4.5. note two experimental outcomes map event one draws returns . therefore probability mass function section 6.2.1 x given px 2 p p p 0.3 0.3 0.09 6.5 px 1 p p p 0.3 1 0.3 1 0.3 0.3 0.42 6.6 px 0 p p p 1 0.3 1 0.3 0.49 . 6.7 draft 20230215 mathematics machine learning. feedback
6.1 construction probability space 177 calculation equated two different concepts probability output x probability samples ω. example 6.7 say px 0 p . consider random variable x ωt subset t for example single element outcome one head obtained tossing two coins. let x1s preimage x i.e. set elements ωthat map x ω ω xω s. one way understand transformation probability events ωvia random variable x associate probability preimage jacod protter 2004. t notation pxs px s px1s pω ω xω s . 6.8 lefthand side 6.8 probability set possible outcomes e.g. number 1 interested in. via random variable x maps states outcomes see righthand side 6.8 probability set states in ω property e.g. . say random variable x distributed according particular probability distribution px defines probability mapping event probability outcome random variable. words function px equivalently p x1 law distribution random variable x. law distribution remark. target space is range random variable x used indicate kind probability space i.e. random variable. finite countably infinite called discrete random variable section 6.2.1. continuous random variables section 6.2.2 consider r rd. 6.1.3 statistics probability theory statistics often presented together con cern different aspects uncertainty. one way contrasting kinds problems considered. using probability consider model process underlying uncertainty captured random variables use rules probability derive happens. statistics observe something happened try figure underlying process explains observations. sense machine learning close statistics goals construct model adequately represents process generated data. use rules probability obtain bestfitting model data. another aspect machine learning systems interested generalization error see chapter 8. means actually interested performance system instances observe future identical instances 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
178 probability distributions seen far. analysis future performance relies probability statistics beyond presented chapter. interested reader encouraged look books boucheron et al. 2013 shalevshwartz bendavid 2014. see statistics chapter 8. 6.2 discrete continuous probabilities let us focus attention ways describe probability event introduced section 6.1. depending whether target space dis crete continuous natural way refer distributions different. target space discrete specify probability random variable x takes particular value x t denoted px x. expression px x discrete random variable x known probability mass function. target space continuous e.g. probability mass function real line r natural specify probability random variable x interval denoted pa x b b. con vention specify probability random variable x less particular value x denoted px x. expression px x continuous random variable x known cumulative distribution cumulative distribution function function. discuss continuous random variables section 6.2.2. revisit nomenclature contrast discrete continuous random variables section 6.2.3. remark. use phrase univariate distribution refer distribu univariate tions single random variable whose states denoted nonbold x. refer distributions one random variable multivariate distributions usually consider vector random multivariate variables whose states denoted bold x. 6.2.1 discrete probabilities target space discrete imagine probability distri bution multiple random variables filling multidimensional array numbers. figure 6.1 shows example. target space joint probability cartesian product target spaces random variables. define joint probability entry joint probability values jointly px xi yj nij n 6.9 nij number events state xi yj n total number events. joint probability probability intersec tion events is px xi yj px xi y yj. figure 6.1 illustrates probability mass function pmf discrete prob probability mass function ability distribution. two random variables x probability draft 20230215 mathematics machine learning. feedback
6.2 discrete continuous probabilities 179 figure 6.1 visualization discrete bivariate probability mass function random variables x . diagram adapted bishop 2006. x x1 x2 x3 x4 x5 y3 y2 y1 nij rj ci z x x lazily written px y called joint probability. one think probability function takes state x returns real number reason write px y. marginal probability x takes value x irrespective value marginal probability random variable lazily written px. write x px denote random variable x distributed according px. consider instances x x fraction instances the conditional probability written lazily py x. conditional probability example 6.2 consider two random variables x x five possible states three possible states shown figure 6.1. denote nij number events state x xi yj denote n total number events. value ci sum individual frequencies ith column is ci p3 j1 nij. similarly value rj row sum is rj p5 i1 nij. using definitions compactly express distribution x . probability distribution random variable marginal probability seen sum row column px xi ci n p3 j1 nij n 6.10 py yj rj n p5 i1 nij n 6.11 ci rj ith column jth row probability table respectively. convention discrete random variables finite number events assume probabilties sum one is 5 x i1 px xi 1 3 x j1 py yj 1 . 6.12 conditional probability fraction row column par 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
180 probability distributions ticular cell. example conditional probability given x py yj x xi nij ci 6.13 conditional probability x given px xi yj nij rj . 6.14 machine learning use discrete probability distributions model categorical variables i.e. variables take finite set unordered val categorical variable ues. could categorical features degree taken uni versity used predicting salary person categorical la bels letters alphabet handwriting recognition. discrete distributions also often used construct probabilistic models combine finite number continuous distributions chapter 11. 6.2.2 continuous probabilities consider realvalued random variables section i.e. consider target spaces intervals real line r. book pretend perform operations real random variables dis crete probability spaces finite states. however simplification precise two situations repeat something infinitely often want draw point interval. first situation arises discuss generalization errors machine learning chap ter 8. second situation arises want discuss continuous distributions gaussian section 6.5. purposes lack precision allows briefer introduction probability. remark. continuous spaces two additional technicalities counterintuitive. first set subsets used define event space section 6.1 well behaved enough. needs restricted behave well set complements set intersections set unions. second size set which discrete spaces obtained counting elements turns tricky. size set called measure. example cardinality discrete sets measure length interval r volume region rd mea sures. sets behave well set operations additionally topology called borel σalgebra. betancourt details careful con borel σalgebra struction probability spaces set theory without bogged technicalities see pre cise construction refer billingsley 1995 jacod protter 2004. book consider realvalued random variables cor draft 20230215 mathematics machine learning. feedback
6.2 discrete continuous probabilities 181 responding borel σalgebra. consider random variables values rd vector realvalued random variables. definition 6.1 probability density function. function f rd r called probability density function pdf probability density function pdf 1. x rd fx 0 2. integral exists z rd fxdx 1 . 6.15 probability mass functions pmf discrete random variables integral 6.15 replaced sum 6.12. observe probability density function function f nonnegative integrates one. associate random variable x function f pa x b z b fxdx 6.16 a b r x r outcomes continuous random vari able x. states x rd defined analogously considering vector x r. association 6.16 called law distribution law random variable x. px x set measure zero. remark. contrast discrete random variables probability con tinuous random variable x taking particular value px x zero. like trying specify interval 6.16 b. definition 6.2 cumulative distribution function. cumulative distribu cumulative distribution function tion function cdf multivariate realvalued random variable x states x rd given fxx px1 x1 . . . xd xd 6.17 x x1 . . . xd x x1 . . . xd righthand side represents probability random variable xi takes value smaller equal xi. cdfs corresponding pdfs. cdf expressed also integral probability density function fx fxx z x1 z xd fz1 . . . zddz1 dzd . 6.18 remark. reiterate fact two distinct concepts talking distributions. first idea pdf denoted fx nonnegative function sums one. second law random variable x is association random variable x pdf fx. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
182 probability distributions figure 6.2 examples a discrete b continuous uniform distributions. see example 6.3 details distributions. 1 0 1 2 z 0.0 0.5 1.0 1.5 2.0 pz z a discrete distribution 1 0 1 2 x 0.0 0.5 1.0 1.5 2.0 px b continuous distribution book use notation fx fxx mostly need distinguish pdf cdf. however need careful pdfs cdfs section 6.7. 6.2.3 contrasting discrete continuous distributions recall section 6.1.2 probabilities positive total prob ability sums one. discrete random variables see 6.12 implies probability state must lie interval 0 1. however continuous random variables normalization see 6.15 imply value density less equal 1 values. illustrate figure 6.2 using uniform distribution uniform distribution discrete continuous random variables. example 6.3 consider two examples uniform distribution state equally likely occur. example illustrates differences discrete continuous probability distributions. let z discrete uniform random variable three states z 1.1 z 0.3 z 1.5. probability mass function represented actual values states meaningful here deliberately chose numbers drive home point want use and ignore ordering states. table probability values z pz z 1.1 1 3 0.3 1 3 1.5 1 3 alternatively think graph figure 6.2a use fact states located xaxis yaxis represents probability particular state. yaxis figure 6.2a deliberately extended figure 6.2b. let x continuous random variable taking values range 0.9 x 1.6 represented figure 6.2b. observe height draft 20230215 mathematics machine learning. feedback
6.3 sum rule product rule bayes theorem 183 table 6.1 nomenclature probability distributions. type point probability interval probability discrete px x applicable probability mass function continuous px px x probability density function cumulative distribution function density greater 1. however needs hold z 1.6 0.9 pxdx 1 . 6.19 remark. additional subtlety regards discrete prob ability distributions. states z1 . . . zd principle structure i.e. usually way compare them example z1 red z2 green z3 blue. however many machine learning applications discrete states take numerical values e.g. z1 1.1 z2 0.3 z3 1.5 could say z1 z2 z3. discrete states as sume numerical values particularly useful often consider expected values section 6.4.1 random variables. unfortunately machine learning literature uses notation nomen clature hides distinction sample space ω target space random variable x. value x set possible outcomes random variable x i.e. x t px denotes prob think outcome x argument results probability px. ability random variable x outcome x. discrete random variables written px x known probabil ity mass function. pmf often referred distribution. continuous variables px called probability density function often referred density. muddy things even further cumulative distribution function px x often also referred distribu tion. chapter use notation x refer univariate multivariate random variables denote states x x re spectively. summarize nomenclature table 6.1. remark. using expression probability distribution discrete probability mass functions also continuous proba bility density functions although technically incorrect. line machine learning literature also rely context distinguish different uses phrase probability distribution. 6.3 sum rule product rule bayes theorem think probability theory extension logical reasoning. discussed section 6.1.1 rules probability presented follow 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
184 probability distributions naturally fulfilling desiderata jaynes 2003 chapter 2. prob abilistic modeling section 8.4 provides principled foundation de signing machine learning methods. defined probability dis tributions section 6.2 corresponding uncertainties data problem turns two fundamental rules sum rule product rule. recall 6.9 px y joint distribution two ran dom variables x y. distributions px py correspond ing marginal distributions py x conditional distribution given x. given definitions marginal conditional probability discrete continuous random variables section 6.2 present two fundamental rules probability theory. two rules arise naturally jaynes 2003 requirements discussed section 6.1.1. first rule sum rule states sum rule px x yy px y discrete z px ydy continuous 6.20 states target space random variable . means sum or integrate out set states random variable . sum rule also known marginalization property. marginalization property sum rule relates joint distribution marginal distribution. general joint distribution contains two random vari ables sum rule applied subset random variables resulting marginal distribution potentially one random variable. concretely x x1 . . . xd obtain marginal pxi z px1 . . . xddxi 6.21 repeated application sum rule integratesum random variables except xi indicated i reads all except i. remark. many computational challenges probabilistic modeling due application sum rule. many variables discrete variables many states sum rule boils per forming highdimensional sum integral. performing highdimensional sums integrals generally computationally hard sense known polynomialtime algorithm calculate exactly. second rule known product rule relates joint distribution product rule conditional distribution via px y py xpx . 6.22 product rule interpreted fact every joint distribu tion two random variables factorized written product draft 20230215 mathematics machine learning. feedback
6.3 sum rule product rule bayes theorem 185 two distributions. two factors marginal distribu tion first random variable px conditional distribution second random variable given first py x. since ordering random variables arbitrary px y product rule also implies px y px ypy. precise 6.22 expressed terms probability mass functions discrete random variables. continuous random variables product rule expressed terms probability density functions section 6.2.3. machine learning bayesian statistics often interested making inferences unobserved latent random variables given observed random variables. let us assume prior knowledge px unobserved random variable x rela tionship py x x second random variable y observe. observe y use bayes theorem draw conclusions x given observed values y. bayes theorem also bayes theorem bayes rule bayes law bayes rule bayes law px y z posterior likelihood z py x prior z px py z evidence 6.23 direct consequence product rule 6.22 since px y px ypy 6.24 px y py xpx 6.25 px ypy py xpx px y py xpx py . 6.26 6.23 px prior encapsulates subjective prior prior knowledge unobserved latent variable x observing data. choose prior makes sense us critical ensure prior nonzero pdf or pmf plausible x even rare. likelihood py x describes x related likelihood likelihood sometimes also called measurement model. case discrete probability distributions probability data know latent variable x. note likelihood distribution x y. call py x either likelihood x given y probability given x never likelihood mackay 2003. posterior px y quantity interest bayesian statistics posterior expresses exactly interested in i.e. know x observed y. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
186 probability distributions quantity py z py xpxdx expy x 6.27 marginal likelihoodevidence. righthand side 6.27 uses marginal likelihood evidence expectation operator define section 6.4.1. definition marginal likelihood integrates numerator 6.23 respect latent variable x. therefore marginal likelihood independent x ensures posterior px y normalized. marginal likelihood also interpreted expected likelihood take expectation respect prior px. beyond normalization posterior marginal likelihood also plays important role bayesian model selection discuss section 8.6. due integration 8.44 evidence often hard compute. bayes theorem also called probabilistic inverse. bayes theorem 6.23 allows us invert relationship x given likelihood. therefore bayes theorem sometimes called probabilistic inverse. discuss bayes theorem probabilistic inverse section 8.4. remark. bayesian statistics posterior distribution quantity interest encapsulates available information prior data. instead carrying posterior around possible focus statistic posterior maximum posterior discuss section 8.3. however focusing statistic posterior leads loss information. think bigger con text posterior used within decisionmaking system full posterior extremely useful lead decisions robust disturbances. example context modelbased re inforcement learning deisenroth et al. 2015 show using full posterior distribution plausible transition functions leads fast datasample efficient learning whereas focusing maximum posterior leads consistent failures. therefore full pos terior useful downstream task. chapter 9 continue discussion context linear regression. 6.4 summary statistics independence often interested summarizing sets random variables com paring pairs random variables. statistic random variable de terministic function random variable. summary statistics distribution provide one useful view random variable behaves name suggests provide numbers summarize charac terize distribution. describe mean variance two well known summary statistics. discuss two ways compare pair random variables first say two random variables inde pendent second compute inner product them. draft 20230215 mathematics machine learning. feedback
6.4 summary statistics independence 187 6.4.1 means covariances mean covariance often useful describe properties probabil ity distributions expected values spread. see section 6.6 useful family distributions called exponential fam ily statistics random variable capture possible infor mation. concept expected value central machine learning foundational concepts probability derived expected value whittle 2000. definition 6.3 expected value. expected value function g r expected value r univariate continuous random variable x px given exgx z x gxpxdx . 6.28 correspondingly expected value function g discrete random variable x px given exgx x xx gxpx 6.29 x set possible outcomes the target space random variable x. section consider discrete random variables numerical outcomes. seen observing function g takes real numbers inputs. expected value function random variable sometimes referred law unconscious statistician casella berger 2002 section 2.2. remark. consider multivariate random variables x finite vector univariate random variables x1 . . . xd. multivariate random variables define expected value element wise exgx ex1gx1 . . . exdgxd rd 6.30 subscript exd indicates taking expected value respect dth element vector x. definition 6.3 defines meaning notation ex operator indicating take integral respect probabil ity density for continuous distributions sum states for discrete distributions. definition mean definition 6.4 special case expected value obtained choosing g iden tity function. definition 6.4 mean. mean random variable x states mean 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
188 probability distributions x rd average defined exx ex1x1 . . . exdxd rd 6.31 exdxd z x xdpxddxd x continuous random variable x xix xipxd xi x discrete random variable 6.32 1 . . . d subscript indicates corresponding di mension x. integral sum states x target space random variable x. one dimension two intuitive notions average median mode. median middle value median sort values i.e. 50 values greater median 50 smaller median. idea generalized contin uous values considering value cdf definition 6.2 0.5. distributions asymmetric long tails median provides estimate typical value closer human intuition mean value. furthermore median robust outliers mean. generalization median higher dimensions nontrivial obvious way sort one dimen sion hallin et al. 2010 kong mizera 2012. mode mode frequently occurring value. discrete random variable mode defined value x highest frequency occurrence. continuous random variable mode defined peak density px. particular density px may one mode fur thermore may large number modes highdimensional distributions. therefore finding modes distribution computationally challenging. example 6.4 consider twodimensional distribution illustrated figure 6.2 px 0.4 n x 10 2 1 0 0 1 0.6 n x 0 0 8.4 2.0 2.0 1.7 . 6.33 define gaussian distribution n µ σ2 section 6.5. also shown corresponding marginal distribution dimension. ob serve distribution bimodal has two modes one draft 20230215 mathematics machine learning. feedback
6.4 summary statistics independence 189 marginal distributions unimodal has one mode. horizontal bi modal univariate distribution illustrates mean median different other. tempting define two dimensional median concatenation medians di mension fact cannot define ordering twodimensional points makes difficult. say cannot define ordering mean one way define relation 3 0 2 3 . figure 6.2 illustration mean mode median twodimensional dataset well marginal densities. mean modes median remark. expected value definition 6.3 linear operator. ex ample given realvalued function fx agxbhx a b r x rd obtain exfx z fxpxdx 6.34a z agx bhxpxdx 6.34b z gxpxdx b z hxpxdx 6.34c aexgx bexhx . 6.34d two random variables may wish characterize correspon 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
190 probability distributions dence other. covariance intuitively represents notion dependent random variables one another. definition 6.5 covariance univariate. covariance two covariance univariate random variables x r given expected product deviations respective means i.e. covxy x y exy x exxy ey y . 6.35 terminology covariance multivariate random variables covx y sometimes referred crosscovariance covariance referring covx x. remark. random variable associated expectation covariance clear arguments subscript often suppressed for example exx often written ex. using linearity expectations expression definition 6.5 rewritten expected value product minus product expected values i.e. covx y exy exey . 6.36 covariance variable covx x called variance variance denoted vxx. square root variance called standard standard deviation deviation often denoted σx. notion covariance generalized multivariate random variables. definition 6.6 covariance multivariate. consider two multivari ate random variables x states x rd re respec tively covariance x defined covariance covx y exy exey covy xrde . 6.37 definition 6.6 applied multivariate random vari able arguments results useful concept intuitively captures spread random variable. multivariate random variable variance describes relation individual dimen sions random variable. definition 6.7 variance. variance random variable x variance states x rd mean vector µ rd defined vxx covxx x 6.38a exx µx µ exxx exxexx 6.38b covx1 x1 covx1 x2 . . . covx1 xd covx2 x1 covx2 x2 . . . covx2 xd . . . . . . . . . . covxd x1 . . . . . . covxd xd . 6.38c matrix 6.38c called covariance matrix mul covariance matrix tivariate random variable x. covariance matrix symmetric pos itive semidefinite tells us something spread data. diagonal covariance matrix contains variances marginals marginal draft 20230215 mathematics machine learning. feedback
6.4 summary statistics independence 191 figure 6.3 twodimensional datasets identical means variances along axis colored lines different covariances. 5 0 5 x 2 0 2 4 6 a x negatively correlated. 5 0 5 x 2 0 2 4 6 b x positively correlated. pxi z px1 . . . xddxi 6.39 i denotes all variables i. offdiagonal entries crosscovariance terms covxi xj i j 1 . . . d j. crosscovariance remark. book generally assume covariance matrices positive definite enable better intuition. therefore discuss corner cases result positive semidefinite lowrank covariance ma trices. want compare covariances different pairs random variables turns variance random variable affects value covariance. normalized version covariance called correlation. definition 6.8 correlation. correlation two random vari correlation ables x given corrx y covx y p vxvy 1 1 . 6.40 correlation matrix covariance matrix standardized random variables xσx. words random variable divided standard deviation the square root variance correlation matrix. covariance and correlation indicate two random variables related see figure 6.3. positive correlation corrx y means x grows also expected grow. negative correlation means x increases decreases. 6.4.2 empirical means covariances definitions section 6.4.1 often also called population mean population mean covariance covariance refers true statistics population. ma chine learning need learn empirical observations data. con sider random variable x. two conceptual steps go 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
192 probability distributions population statistics realization empirical statistics. first use fact finite dataset of size n construct empirical statistic function finite number identical random variables x1 . . . xn. second observe data is look realiza tion x1 . . . xn random variables apply empirical statistic. specifically mean definition 6.4 given particular dataset obtain estimate mean called empirical mean empirical mean sample mean. holds empirical covariance. sample mean definition 6.9 empirical mean covariance. empirical mean vec empirical mean tor arithmetic average observations variable defined x 1 n n x n1 xn 6.41 xn rd. similar empirical mean empirical covariance matrix dd empirical covariance matrix σ 1 n n x n1 xn xxn x. 6.42 throughout book use empirical covariance biased estimate. unbiased sometimes called corrected covariance factor n 1 denominator instead n. compute statistics particular dataset would use realizations observations x1 . . . xn use 6.41 6.42. em pirical covariance matrices symmetric positive semidefinite see sec tion 3.2.3. 6.4.3 three expressions variance focus single random variable x use preceding em pirical formulas derive three possible expressions variance. derivations exercises end chapter. following derivation population variance except need take care integrals. standard definition variance cor responding definition covariance definition 6.5 expec tation squared deviation random variable x expected value µ i.e. vxx exx µ2 . 6.43 expectation 6.43 mean µ exx computed us ing 6.32 depending whether x discrete continuous random variable. variance expressed 6.43 mean new random variable z x µ2. estimating variance 6.43 empirically need resort twopass algorithm one pass data calculate mean µ using 6.41 second pass using estimate ˆ µ calculate draft 20230215 mathematics machine learning. feedback
6.4 summary statistics independence 193 variance. turns avoid two passes rearranging terms. formula 6.43 converted socalled rawscore rawscore formula variance formula variance vxx exx2 exx 2 . 6.44 expression 6.44 remembered the mean square minus square mean. calculated empirically one pass data since accumulate xi to calculate mean x2 simultaneously xi ith observation. unfortunately imple two terms 6.44 huge approximately equal may suffer unnecessary loss numerical precision floatingpoint arithmetic. mented way numerically unstable. rawscore version variance useful machine learning e.g. deriving biasvariance decomposition bishop 2006. third way understand variance sum pairwise dif ferences pairs observations. consider sample x1 . . . xn realizations random variable x compute squared differ ence pairs xi xj. expanding square show sum n 2 pairwise differences empirical variance observations 1 n 2 n x ij1 xi xj2 2 1 n n x i1 x2 1 n n x i1 xi 2 . 6.45 see 6.45 twice rawscore expression 6.44. means express sum pairwise distances of n 2 them sum deviations mean of n. ge ometrically means equivalence pairwise distances distances center set points. computational perspective means computing mean n terms summation computing variance again n terms summation obtain expression lefthand side 6.45 n 2 terms. 6.4.4 sums transformations random variables may want model phenomenon cannot well explained textbook distributions we introduce sections 6.5 6.6 hence may perform simple manipulations random variables such adding two random variables. consider two random variables x states x rd. then ex y ex ey 6.46 ex y ex ey 6.47 vx y vx vy covx y covy x 6.48 vx y vx vy covx y covy x . 6.49 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
194 probability distributions mean covariance exhibit useful properties comes affine transformation random variables. consider random variable x mean µ covariance matrix σ deterministic affine transformation ax b x. random variable whose mean vector covariance matrix given ey y exax b aexx b aµ b 6.50 vy y vxax b vxax avxxa aσa 6.51 respectively. furthermore shown directly using definition mean covariance. covx y exax b exeax b 6.52a exb exxaµbµµa 6.52b µbµb exx µµa 6.52c 6.38b σa 6.52d σ exx µµis covariance x. 6.4.5 statistical independence definition 6.10 independence. two random variables x statis statistical independence tically independent px y pxpy . 6.53 intuitively two random variables x independent value once known add additional information x and vice versa. x statistically independent py x py px y px vxy x y vxx vy y covxy x y 0 last point may hold converse i.e. two random variables covariance zero statistically independent. understand why recall covariance measures linear dependence. therefore random variables nonlinearly dependent could covariance zero. example 6.5 consider random variable x zero mean exx 0 also exx3 0. let x2 hence dependent x consider covariance 6.36 x . gives covx y exy exey ex3 0 . 6.54 draft 20230215 mathematics machine learning. feedback
6.4 summary statistics independence 195 machine learning often consider problems mod eled independent identically distributed i.i.d. random variables independent identically distributed i.i.d. x1 . . . xn. two random variables word indepen dent definition 6.10 usually refers mutually independent random variables subsets independent see pollard 2002 chap ter 4 jacod protter 2004 chapter 3. phrase identically distributed means random variables distri bution. another concept important machine learning conditional independence. definition 6.11 conditional independence. two random variables x conditionally independent given z conditionally independent px z px zpy z z z 6.55 z set states random variable z. write x y z denote x conditionally independent given z. definition 6.11 requires relation 6.55 must hold true every value z. interpretation 6.55 understood given knowledge z distribution x factorizes. independence cast special case conditional independence write x . using product rule probability 6.22 expand lefthand side 6.55 obtain px z px y zpy z . 6.56 comparing righthand side 6.55 6.56 see py z appears px y z px z . 6.57 equation 6.57 provides alternative definition conditional indepen dence i.e. x y z. alternative presentation provides inter pretation given know z knowledge change knowledge x. 6.4.6 inner products random variables recall definition inner products section 3.2. define inner products multivariate random variables treated similar fashion inner product random variables briefly describe section. two uncorrelated random variables x vx y vx vy . 6.58 since variances measured squared units looks much like pythagorean theorem right triangles c2 a2 b2. following see whether find geometric interpreta tion variance relation uncorrelated random variables 6.58. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
196 probability distributions figure 6.1 geometry random variables. random variables x uncorrelated orthogonal vectors corresponding vector space pythagorean theorem applies. p vary p varx p varx y p varx vary c b random variables considered vectors vector space define inner products obtain geometric properties random vari ables eaton 2007. define x covx y 6.59 zero mean random variables x obtain inner product. see covariance symmetric positive definite linear either covx x 0 x 0 argument. length random variable covαx z y α covx y covz y α r. x q covx x q vx σx 6.60 i.e. standard deviation. longer random variable uncertain is random variable length 0 deterministic. look angle θ two random variables x get cos θ x xy covx y p vxvy 6.61 correlation definition 6.8 two random vari ables. means think correlation cosine angle two random variables consider geometri cally. know definition 3.7 x y x 0. case means x orthogonal covx y 0 i.e. uncorrelated. figure 6.1 illustrates relationship. remark. tempting use euclidean distance constructed draft 20230215 mathematics machine learning. feedback
6.5 gaussian distribution 197 figure 6.1 gaussian distribution two random variables x1 x2. x1 1 0 1 x2 5.0 2.5 0.0 2.5 5.0 7.5 px1 x2 0.00 0.05 0.10 0.15 0.20 preceding definition inner products compare probability distributions unfortunately best way obtain distances be tween distributions. recall probability mass or density posi tive needs add 1. constraints mean distributions live something called statistical manifold. study space probability distributions called information geometry. computing dis tances distributions often done using kullbackleibler diver gence generalization distances account properties statistical manifold. like euclidean distance special case metric section 3.3 kullbackleibler divergence special case two general classes divergences called bregman divergences fdivergences. study divergences beyond scope book refer details recent book amari 2016 one founders field information geometry. 6.5 gaussian distribution gaussian distribution wellstudied probability distribution continuousvalued random variables. also referred normal normal distribution distribution. importance originates fact many com gaussian distribution arises naturally consider sums independent identically distributed random variables. known central limit theorem grinstead snell 1997. putationally convenient properties discussing fol lowing. particular use define likelihood prior linear regression chapter 9 consider mixture gaussians density estimation chapter 11. many areas machine learning also benefit using gaussian distribution example gaussian processes variational inference reinforcement learning. also widely used ap plication areas signal processing e.g. kalman filter control e.g. linear quadratic regulator statistics e.g. hypothesis testing. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
198 probability distributions figure 6.2 gaussian distributions overlaid 100 samples. a one dimensional case b twodimensional case. 5.0 2.5 0.0 2.5 5.0 7.5 x 0.00 0.05 0.10 0.15 0.20 px mean sample 2σ a univariate onedimensional gaussian red cross shows mean red line shows extent variance. 1 0 1 x1 4 2 0 2 4 6 8 x2 mean sample b multivariate twodimensional gaus sian viewed top. red cross shows mean colored lines show con tour lines density. univariate random variable gaussian distribution den sity given px µ σ2 1 2πσ2 exp x µ2 2σ2 . 6.62 multivariate gaussian distribution fully characterized mean multivariate gaussian distribution mean vector vector µ covariance matrix σ defined covariance matrix px µ σ 2πd 2 σ1 2 exp 1 2x µσ1x µ 6.63 x rd. write px n x µ σ x n µ σ . fig also known multivariate normal distribution. ure 6.1 shows bivariate gaussian mesh corresponding con tour plot. figure 6.2 shows univariate gaussian bivariate gaussian corresponding samples. special case gaussian zero mean identity covariance is µ 0 σ i referred standard normal distribution. standard normal distribution gaussians widely used statistical estimation machine learn ing closedform expressions marginal conditional dis tributions. chapter 9 use closedform expressions extensively linear regression. major advantage modeling gaussian ran dom variables variable transformations section 6.7 often needed. since gaussian distribution fully specified mean covariance often obtain transformed distribution applying transformation mean covariance random variable. 6.5.1 marginals conditionals gaussians gaussians following present marginalization conditioning gen eral case multivariate random variables. confusing first read ing reader advised consider two univariate random variables in stead. let x two multivariate random variables may draft 20230215 mathematics machine learning. feedback
6.5 gaussian distribution 199 different dimensions. consider effect applying sum rule probability effect conditioning explicitly write gaus sian distribution terms concatenated states x y px y n µx µy σxx σxy σyx σyy . 6.64 σxx covx x σyy covy y marginal covari ance matrices x y respectively σxy covx y cross covariance matrix x y. conditional distribution px y also gaussian illustrated fig ure 6.3c given derived section 2.3 bishop 2006 px y n µx y σx 6.65 µx µx σxyς1 yy y µy 6.66 σx σxx σxyς1 yy σyx . 6.67 note computation mean 6.66 yvalue observation longer random. remark. conditional gaussian distribution shows many places interested posterior distributions kalman filter kalman 1960 one central algorithms state estimation signal processing nothing computing gaussian conditionals joint distributions deisenroth ohlsson 2011 s arkk a 2013. gaussian processes rasmussen williams 2006 prac tical implementation distribution functions. gaussian pro cess make assumptions joint gaussianity random variables. gaussian conditioning observed data determine poste rior distribution functions. latent linear gaussian models roweis ghahramani 1999 mur phy 2012 include probabilistic principal component analysis ppca tipping bishop 1999. look ppca de tail section 10.7. marginal distribution px joint gaussian distribution px y see 6.64 gaussian computed applying sum rule 6.20 given px z px ydy n x µx σxx . 6.68 corresponding result holds py obtained marginaliz ing respect x. intuitively looking joint distribution 6.64 ignore i.e. integrate out everything interested in. illustrated figure 6.3b. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
200 probability distributions example 6.6 figure 6.3 a bivariate gaussian b marginal joint gaussian distribution gaussian c conditional distribution gaussian also gaussian. 1 0 1 x1 4 2 0 2 4 6 8 x2 x2 1 a bivariate gaussian. 1.5 1.0 0.5 0.0 0.5 1.0 1.5 x1 0.0 0.2 0.4 0.6 px1 mean 2σ b marginal distribution. 1.5 1.0 0.5 0.0 0.5 1.0 1.5 x1 0.0 0.2 0.4 0.6 0.8 1.0 1.2 px1x2 1 mean 2σ c conditional distribution. consider bivariate gaussian distribution illustrated figure 6.3 px1 x2 n 0 2 0.3 1 1 5 . 6.69 compute parameters univariate gaussian conditioned x2 1 applying 6.66 6.67 obtain mean vari ance respectively. numerically µx1 x21 0 1 0.2 1 2 0.6 6.70 σ2 x1 x21 0.3 1 0.2 1 0.1 . 6.71 therefore conditional gaussian given px1 x2 1 n 0.6 0.1 . 6.72 marginal distribution px1 contrast obtained apply ing 6.68 essentially using mean variance random variable x1 giving us px1 n 0 0.3 . 6.73 draft 20230215 mathematics machine learning. feedback
6.5 gaussian distribution 201 6.5.2 product gaussian densities linear regression chapter 9 need compute gaussian likeli hood. furthermore may wish assume gaussian prior section 9.3. apply bayes theorem compute posterior results mul tiplication likelihood prior is multiplication two gaussian densities. product two gaussians n x a n x b b derivation exercise end chapter. gaussian distribution scaled c r given c n x c c c a1 b11 6.74 c ca1a b1b 6.75 c 2πd 2 a b1 2 exp 1 2a ba b1a b . 6.76 scaling constant c written form gaussian density either b inflated covariance matrix b i.e. c n a b b n b a b . remark. notation convenience sometimes use n x m describe functional form gaussian density even x random variable. done preceding demonstration wrote c n a b b n b a b . 6.77 here neither b random variables. however writing c way compact 6.76. 6.5.3 sums linear transformations x independent gaussian random variables i.e. joint distri bution given px y pxpy px n x µx σx py n y µy σy x also gaussian distributed given px y n µx µy σx σy . 6.78 knowing px y gaussian mean covariance matrix determined immediately using results 6.46 6.49. property important consider i.i.d. gaussian noise acting random variables case linear regression chap ter 9. example 6.7 since expectations linear operations obtain weighted sum independent gaussian random variables pax by n aµx bµy a2σx b2σy . 6.79 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
202 probability distributions remark. case useful chapter 11 weighted sum gaussian densities. different weighted sum gaussian random variables. theorem 6.12 random variable x density mixture two densities p1x p2x weighted α. theorem generalized multivariate random variable case since linearity expectations holds also multivariate random variables. however idea squared random variable needs replaced xx. theorem 6.12. consider mixture two univariate gaussian densities px αp1x 1 αp2x 6.80 scalar 0 α 1 mixture weight p1x p2x univariate gaussian densities equation 6.62 different parameters i.e. µ1 σ2 1 µ2 σ2 2. mean mixture density px given weighted sum means random variable ex αµ1 1 αµ2 . 6.81 variance mixture density px given vx ασ2 1 1 ασ2 2 αµ2 1 1 αµ2 2 αµ1 1 αµ2 2 . 6.82 proof mean mixture density px given weighted sum means random variable. apply definition mean definition 6.4 plug mixture 6.80 yields ex z xpxdx 6.83a z αxp1x 1 αxp2x dx 6.83b α z xp1xdx 1 α z xp2xdx 6.83c αµ1 1 αµ2 . 6.83d compute variance use rawscore version vari ance 6.44 requires expression expectation squared random variable. use definition expectation function the square random variable definition 6.3 ex2 z x2pxdx 6.84a z αx2p1x 1 αx2p2x dx 6.84b draft 20230215 mathematics machine learning. feedback
6.5 gaussian distribution 203 α z x2p1xdx 1 α z x2p2xdx 6.84c αµ2 1 σ2 1 1 αµ2 2 σ2 2 6.84d last equality used rawscore version variance 6.44 giving σ2 ex2 µ2. rearranged expectation squared random variable sum squared mean variance. therefore variance given subtracting 6.83d 6.84d vx ex2 ex2 6.85a αµ2 1 σ2 1 1 αµ2 2 σ2 2 αµ1 1 αµ22 6.85b ασ2 1 1 ασ2 2 αµ2 1 1 αµ2 2 αµ1 1 αµ2 2 . 6.85c remark. preceding derivation holds density since gaussian fully determined mean variance mixture den sity determined closed form. mixture density individual components considered conditional distributions conditioned component identity. equation 6.85c example conditional variance formula also known law total variance generally states two ran law total variance dom variables x holds vxx ey vxxyvy exxy i.e. total variance x expected conditional variance plus variance conditional mean. consider example 6.17 bivariate standard gaussian random variable x performed linear transformation ax it. outcome gaussian random variable mean zero covariance aa. ob serve adding constant vector change mean distribu tion without affecting variance is random variable x µ gaussian mean µ identity covariance. hence linearaffine transformation gaussian random variable gaussian distributed. linearaffine transformation gaussian random variable also gaussian distributed. consider gaussian distributed random variable x n µ σ . given matrix appropriate shape let random variable ax transformed version x. compute mean exploiting expectation linear operator 6.50 follows ey eax aex aµ . 6.86 similarly variance found using 6.51 vy vax avxa aσa. 6.87 means random variable distributed according py n y aµ aσa. 6.88 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
204 probability distributions let us consider reverse transformation know random variable mean linear transformation another random variable. given full rank matrix rmn n let rm gaussian random variable mean ax i.e. py n y ax σ . 6.89 corresponding probability distribution px invert ible write x a1y apply transformation previous paragraph. however general invertible use approach similar pseudoinverse 3.57. is pre multiply sides aand invert aa symmetric positive definite giving us relation ax aa1ay x . 6.90 hence x linear transformation y obtain px n x aa1ay aa1aσaaa1 . 6.91 6.5.4 sampling multivariate gaussian distributions explain subtleties random sampling computer interested reader referred gentle 2004. case mul tivariate gaussian process consists three stages first need source pseudorandom numbers provide uniform sample interval 01 second use nonlinear transformation boxm uller transform devroye 1986 obtain sample univari ate gaussian third collate vector samples obtain sample multivariate standard normal n 0 . general multivariate gaussian is mean non zero covariance identity matrix use proper ties linear transformations gaussian random variable. assume interested generating samples xi 1 . . . n multivariate gaussian distribution mean µ covariance matrix σ. would compute cholesky factorization matrix required matrix symmetric positive definite section 3.2.3. covariance matrices possess property. like construct sample sampler provides samples multivariate standard normal n 0 . obtain samples multivariate normal n µ σ use properties linear transformation gaussian random variable x n 0 ax µ aa σ gaussian dis tributed mean µ covariance matrix σ. one convenient choice use cholesky decomposition section 4.3 covariance matrix σ aa. cholesky decomposition benefit triangular leading efficient computation. draft 20230215 mathematics machine learning. feedback
6.6 conjugacy exponential family 205 6.6 conjugacy exponential family many probability distributions with names find statis tics textbooks discovered model particular types phenomena. example seen gaussian distribution section 6.5. distributions also related complex ways leemis mcqueston 2008. beginner field overwhelming figure distribution use. addition many distribu tions discovered time statistics computation done computers used job description. pencil paper. natural ask meaningful concepts computing age efron hastie 2016. previous section saw many operations required inference conve niently calculated distribution gaussian. worth recalling point desiderata manipulating probability distributions machine learning context 1. closure property applying rules probability e.g. bayes theorem. closure mean applying particular operation returns object type. 2. collect data need parameters describe distribution. 3. since interested learning data want parameter es timation behave nicely. turns class distributions called exponential family exponential family provides right balance generality retaining favorable compu tation inference properties. introduce exponential fam ily let us see three members named probability distributions bernoulli example 6.8 binomial example 6.9 beta exam ple 6.10 distributions. example 6.8 bernoulli distribution distribution single binary random bernoulli distribution variable x state x 0 1. governed single continuous pa rameter µ 0 1 represents probability x 1. bernoulli distribution berµ defined px µ µx1 µ1x x 0 1 6.92 ex µ 6.93 vx µ1 µ 6.94 ex vx mean variance binary random variable x. example bernoulli distribution used interested modeling probability heads flipping coin. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
206 probability distributions figure 6.1 examples binomial distribution µ 0.1 0.4 0.75 n 15. 0.0 2.5 5.0 7.5 10.0 12.5 15.0 number observations x 1 n 15 experiments 0.0 0.1 0.2 0.3 pm µ 0.1 µ 0.4 µ 0.75 remark. rewriting bernoulli distribution use boolean variables numerical 0 1 express exponents trick often used machine learning textbooks. another oc curence expressing multinomial distribution. example 6.9 binomial distribution binomial distribution generalization bernoulli distribution binomial distribution distribution integers illustrated figure 6.1. particular binomial used describe probability observing occur rences x 1 set n samples bernoulli distribution px 1 µ 0 1. binomial distribution binn µ defined pm n µ n µm1 µnm 6.95 em nµ 6.96 vm nµ1 µ 6.97 em vm mean variance m respectively. example binomial could used want describe probability observing heads n coinflip experiments probability observing head single experiment µ. example 6.10 beta distribution may wish model continuous random variable finite interval. beta distribution distribution continuous random variable beta distribution µ 0 1 often used represent probability binary event e.g. parameter governing bernoulli distribution. beta distribution betaα β illustrated figure 6.2 governed two draft 20230215 mathematics machine learning. feedback
6.6 conjugacy exponential family 207 parameters α 0 β 0 defined pµ α β γα β γαγβµα11 µβ1 6.98 eµ α α β vµ αβ α β2α β 1 6.99 γ gamma function defined γt z 0 xt1 expxdx 0 . 6.100 γt 1 tγt . 6.101 note fraction gamma functions 6.98 normalizes beta distribution. figure 6.2 examples beta distribution different values α β. 0.0 0.2 0.4 0.6 0.8 1.0 µ 0 2 4 6 8 10 pµα β α 0.5 β α 1 β α 2 β 0.3 α 4 β 10 α 5 β 1 intuitively α moves probability mass toward 1 whereas β moves prob ability mass toward 0. special cases murphy 2012 α 1 β obtain uniform distribution u0 1. α β 1 get bimodal distribution spikes 0 1. α β 1 distribution unimodal. α β 1 α β distribution unimodal symmetric centered interval 0 1 i.e. modemean 1 2. remark. whole zoo distributions names related different ways leemis mcqueston 2008. worth keeping mind named distribution created particular reason may applications. knowing reason behind creation particular distribution often allows insight best use it. introduced preceding three distributions able illustrate concepts conjugacy section 6.6.1 exponen tial families section 6.6.3. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
208 probability distributions 6.6.1 conjugacy according bayes theorem 6.23 posterior proportional product prior likelihood. specification prior tricky two reasons first prior encapsulate knowl edge problem see data. often difficult describe. second often possible compute posterior distribu tion analytically. however priors computationally convenient conjugate priors. conjugate prior definition 6.13 conjugate prior. prior conjugate likelihood conjugate function posterior formtype prior. conjugacy particularly convenient algebraically cal culate posterior distribution updating parameters prior distribution. remark. considering geometry probability distributions con jugate priors retain distance structure likelihood agarwal daum e iii 2010. introduce concrete example conjugate priors describe ex ample 6.11 binomial distribution defined discrete random vari ables beta distribution defined continuous random vari ables. example 6.11 betabinomial conjugacy consider binomial random variable x binn µ px n µ n x µx1 µnx x 0 1 . . . n 6.102 probability finding x times outcome heads n coin flips µ probability head. place beta prior pa rameter µ is µ betaα β pµ α β γα β γαγβµα11 µβ1 . 6.103 observe outcome x h is see h heads n coin flips compute posterior distribution µ pµ x h n α β px n µpµ α β 6.104a µh1 µnhµα11 µβ1 6.104b µhα11 µnhβ1 6.104c betah α n h β 6.104d i.e. posterior distribution beta distribution prior i.e. draft 20230215 mathematics machine learning. feedback
6.6 conjugacy exponential family 209 table 6.2 examples conjugate priors common likelihood functions. likelihood conjugate prior posterior bernoulli beta beta binomial beta beta gaussian gaussianinverse gamma gaussianinverse gamma gaussian gaussianinverse wishart gaussianinverse wishart multinomial dirichlet dirichlet beta prior conjugate parameter µ binomial likelihood function. following example derive result similar betabinomial conjugacy result. show beta distribu tion conjugate prior bernoulli distribution. example 6.12 betabernoulli conjugacy let x 0 1 distributed according bernoulli distribution parameter θ 0 1 is px 1 θ θ. also expressed px θ θx1 θ1x. let θ distributed according beta distri bution parameters α β is pθ α β θα11 θβ1. multiplying beta bernoulli distributions get pθ x α β px θpθ α β 6.105a θx1 θ1xθα11 θβ1 6.105b θαx11 θβ1x1 6.105c pθ α x β 1 x . 6.105d last line beta distribution parameters α x β 1 x. table 6.2 lists examples conjugate priors parameters standard likelihoods used probabilistic modeling. distributions gamma prior conjugate precision inverse variance univariate gaussian likelihood wishart prior conjugate precision matrix inverse covariance matrix multivariate gaussian likelihood. multinomial inverse gamma inverse wishart dirichlet found statistical text described bishop 2006 example. beta distribution conjugate prior parameter µ binomial bernoulli likelihood. gaussian likelihood func tion place conjugate gaussian prior mean. reason gaussian likelihood appears twice table need distinguish univariate multivariate case. univariate scalar case inverse gamma conjugate prior variance. multivariate case use conjugate inverse wishart distribution prior covariance matrix. dirichlet distribution conju gate prior multinomial likelihood function. details refer bishop 2006. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
210 probability distributions 6.6.2 sufficient statistics recall statistic random variable deterministic function random variable. example x x1 . . . xnis vector univariate gaussian random variables is xn n µ σ2 sample mean ˆ µ 1 n x1 xn statistic. sir ronald fisher dis covered notion sufficient statistics idea statistics sufficient statistics contain available information inferred data corresponding distribution consideration. words suf ficient statistics carry information needed make inference population is statistics sufficient repre sent distribution. set distributions parametrized θ let x random variable distribution px θ0 given unknown θ0. vector ϕx statistics called sufficient statistics θ0 contain possible informa tion θ0. formal contain possible information means probability x given θ factored part depend θ part depends θ via ϕx. fisherneyman factorization theorem formalizes notion state theorem 6.14 without proof. theorem 6.14 fisherneyman. theorem 6.5 lehmann casella 1998 let x probability density function px θ. statistics fisherneyman theorem ϕx sufficient θ px θ written form px θ hxgθϕx 6.106 hx distribution independent θ gθ captures depen dence θ via sufficient statistics ϕx. px θ depend θ ϕx trivially sufficient statistic function ϕ. interesting case px θ dependent ϕx x itself. case ϕx sufficient statistic θ. machine learning consider finite number samples distribution. one could imagine simple distributions such bernoulli example 6.8 need small number samples estimate parameters distributions. could also consider opposite problem set data a sample unknown distribution distribution gives best fit natural question ask is observe data need parameters θ de scribe distribution turns answer yes general studied nonparametric statistics wasserman 2007. converse question consider class distributions finitedimensional sufficient statistics number parameters needed describe increase arbitrarily. answer exponential family dis tributions described following section. draft 20230215 mathematics machine learning. feedback
6.6 conjugacy exponential family 211 6.6.3 exponential family three possible levels abstraction con sidering distributions of discrete continuous random variables. level one the concrete end spectrum particu lar named distribution fixed parameters example univariate gaussian n 0 1 zero mean unit variance. machine learning often use second level abstraction is fix paramet ric form the univariate gaussian infer parameters data. example assume univariate gaussian n µ σ2 unknown mean µ unknown variance σ2 use maximum likelihood fit deter mine best parameters µ σ2. see example considering linear regression chapter 9. third level abstraction consider families distributions book consider ex ponential family. univariate gaussian example member exponential family. many widely used statistical models includ ing named models table 6.2 members exponential family. unified one concept brown 1986. remark. brief historical anecdote like many concepts mathemat ics science exponential families independently discovered time different researchers. years 19351936 edwin pitman tasmania georges darmois paris bernard koopman new york independently showed exponential families families enjoy finitedimensional sufficient statistics repeated independent sampling lehmann casella 1998. exponential family family probability distributions parame exponential family terized θ rd form px θ hx exp θ ϕxaθ 6.107 ϕx vector sufficient statistics. general inner prod uct section 3.2 used 6.107 concreteness use standard dot product θ ϕx θϕx. note form exponential family essentially particular expression gθϕx fisherneyman theorem theorem 6.14. factor hx absorbed dot product term adding another entry log hx vector sufficient statistics ϕx constraining corresponding parameter θ0 1. term aθ normalization constant ensures distribution sums inte grates one called logpartition function. good intuitive no logpartition function tion exponential families obtained ignoring two terms considering exponential families distributions form px θ exp θϕx . 6.108 form parametrization parameters θ called natural natural parameters 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
212 probability distributions parameters. first glance seems exponential families mun dane transformation adding exponential function result dot product. however many implications allow conve nient modeling efficient computation based fact capture information data ϕx. example 6.13 gaussian exponential family consider univariate gaussian distribution n µ σ2 . let ϕx x x2 . using definition exponential family px θ expθ1x θ2x2 . 6.109 setting θ µ σ2 1 2σ2 6.110 substituting 6.109 obtain px θ exp µx σ2 x2 2σ2 exp 1 2σ2 x µ2 . 6.111 therefore univariate gaussian distribution member expo nential family sufficient statistic ϕx x x2 natural parame ters given θ 6.110. example 6.14 bernoulli exponential family recall bernoulli distribution example 6.8 px µ µx1 µ1x x 0 1. 6.112 written exponential family form px µ exp log µx1 µ1x 6.113a exp x log µ 1 x log1 µ 6.113b exp x log µ x log1 µ log1 µ 6.113c exp h x log µ 1µ log1 µ . 6.113d last line 6.113d identified exponential family form 6.107 observing hx 1 6.114 θ log µ 1µ 6.115 ϕx x 6.116 draft 20230215 mathematics machine learning. feedback
6.6 conjugacy exponential family 213 aθ log1 µ log1 expθ. 6.117 relationship θ µ invertible µ 1 1 expθ . 6.118 relation 6.118 used obtain right equality 6.117. remark. relationship original bernoulli parameter µ natural parameter θ known sigmoid logistic function. ob sigmoid serve µ 0 1 θ r therefore sigmoid function squeezes real value range 0 1. property useful ma chine learning example used logistic regression bishop 2006 section 4.3.2 well nonlinear activation functions neural net works goodfellow et al. 2016 chapter 6. often obvious find parametric form conjugate distribution particular distribution for example table 6.2. exponential families provide convenient way find conjugate pairs distributions. consider random variable x member expo nential family 6.107 px θ hx exp θ ϕxaθ . 6.119 every member exponential family conjugate prior brown 1986 pθ γ hcθ exp γ1 γ2 θ aθ acγ 6.120 γ γ1 γ2 dimension dimθ 1. sufficient statistics conjugate prior θ aθ . using knowledge general form conjugate priors exponential families derive functional forms conjugate priors corresponding particular distributions. example 6.15 recall exponential family form bernoulli distribution 6.113d px µ exp x log µ 1 µ log1 µ . 6.121 canonical conjugate prior form pµ α β µ 1 µ exp α log µ 1 µ β α log1 µ acγ 6.122 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
214 probability distributions defined γ α β αand hcµ µ1 µ. equa tion 6.122 simplifies pµ α β exp α 1 log µ β 1 log1 µ acα β . 6.123 putting nonexponential family form yields pµ α β µα11 µβ1 6.124 identify beta distribution 6.98. example 6.12 assumed beta distribution conjugate prior bernoulli distribution showed indeed conjugate prior. example derived form beta distribution looking canonical conjugate prior bernoulli distribution exponential fam ily form. mentioned previous section main motivation expo nential families finitedimensional sufficient statistics. additionally conjugate distributions easy write down con jugate distributions also come exponential family. infer ence perspective maximum likelihood estimation behaves nicely empirical estimates sufficient statistics optimal estimates pop ulation values sufficient statistics recall mean covariance gaussian. optimization perspective loglikelihood function concave allowing efficient optimization approaches applied chapter 7. 6.7 change variablesinverse transform may seem many known distributions reality set distributions names quite limited. there fore often useful understand transformed random variables distributed. example assuming x random variable dis tributed according univariate normal distribution n 0 1 distribution x2 another example quite common ma chine learning is given x1 x2 univariate standard normal distribution 1 2x1 x2 one option work distribution 1 2x1 x2 calculate mean variance x1 x2 combine them. saw section 6.4.4 calculate mean variance resulting ran dom variables consider affine transformations random vari ables. however may able obtain functional form distribution transformations. furthermore may interested nonlinear transformations random variables closedform expressions readily available. draft 20230215 mathematics machine learning. feedback
6.7 change variablesinverse transform 215 remark notation. section explicit random vari ables values take. hence recall use capital letters x denote random variables small letters x denote val ues target space random variables take. explicitly write pmfs discrete random variables x px x. continuous random variables x section 6.2.2 pdf written fx cdf written fxx. look two approaches obtaining distributions transfor mations random variables direct approach using definition cumulative distribution function changeofvariable approach uses chain rule calculus section 5.2.2. changeofvariable ap moment generating functions also used study transformations random variables casella berger 2002 chapter 2. proach widely used provides recipe attempting compute resulting distribution due transformation. ex plain techniques univariate random variables briefly provide results general case multivariate random variables. transformations discrete random variables understood di rectly. suppose discrete random variable x pmf px x section 6.2.1 invertible function ux. consider trans formed random variable ux pmf py y. py y pux y transformation interest 6.125a px u 1y inverse 6.125b observe x u 1y. therefore discrete random variables transformations directly change individual events with probabilities appropriately transformed. 6.7.1 distribution function technique distribution function technique goes back first principles uses definition cdf fxx px x fact differential pdf fx wasserman 2004 chapter 2. random variable x function u find pdf random variable ux 1. finding cdf fy y py y 6.126 2. differentiating cdf fy y get pdf fy. fy dyfy y . 6.127 also need keep mind domain random variable may changed due transformation u. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
216 probability distributions example 6.16 let x continuous random variable probability density function 0 x 1 fx 3x2 . 6.128 interested finding pdf x2. function f increasing function x therefore resulting value lies interval 0 1. obtain fy y py y definition cdf 6.129a px2 y transformation interest 6.129b px y 1 2 inverse 6.129c fxy 1 2 definition cdf 6.129d z 1 2 0 3t2dt cdf definite integral 6.129e t3ty 1 2 t0 result integration 6.129f 3 2 0 y 1 . 6.129g therefore cdf fy y 3 2 6.130 0 y 1. obtain pdf differentiate cdf fy dyfy y 3 2y 1 2 6.131 0 y 1. example 6.16 considered strictly monotonically increasing func tion fx 3x2. means could compute inverse function. functions inverses called bijective functions section 2.7. general require function interest ux in verse x u 1y. useful result obtained considering cu mulative distribution function fxx random variable x using transformation ux. leads following theorem. theorem 6.15. theorem 2.1.10 casella berger 2002 let x continuous random variable strictly monotonic cumulative distribu tion function fxx. random variable defined fxx 6.132 uniform distribution. theorem 6.15 known probability integral transform probability integral transform used derive algorithms sampling distributions transforming draft 20230215 mathematics machine learning. feedback
6.7 change variablesinverse transform 217 result sampling uniform random variable bishop 2006. algorithm works first generating sample uniform distribu tion transforming inverse cdf assuming available obtain sample desired distribution. probability integral transform also used hypothesis testing whether sample comes particular distribution lehmann romano 2005. idea output cdf gives uniform distribution also forms basis copu las nelsen 2006. 6.7.2 change variables distribution function technique section 6.7.1 derived first principles based definitions cdfs using properties in verses differentiation integration. argument first principles relies two facts 1. transform cdf expression cdf x. 2. differentiate cdf obtain pdf. let us break reasoning step step goal understand ing general changeofvariables approach theorem 6.16. change variables probability relies changeofvariables method calculus tandra 2014. remark. name change variables comes idea chang ing variable integration faced difficult integral. univariate functions use substitution rule integration z fgxgxdx z fudu u gx . 6.133 derivation rule based chain rule calculus 5.32 applying twice fundamental theorem calculus. fundamental theorem calculus formalizes fact integration differentiation somehow inverses other. intuitive understanding rule obtained thinking loosely small changes differen tials equation u gx considering u gxx differential u gx. substituting u gx argument inside integral righthand side 6.133 becomes fgx. pretending term du approximated du u gxx dx x obtain 6.133. consider univariate random variable x invertible function u gives us another random variable ux. assume random variable x states x a b. definition cdf fy y py y . 6.134 interested function u random variable py y pux y 6.135 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
218 probability distributions assume function u invertible. invertible function interval either strictly increasing strictly decreasing. case u strictly increasing inverse u 1 also strictly increasing. applying inverse u 1 arguments pux y obtain pux y pu 1ux u 1y px u 1y . 6.136 rightmost term 6.136 expression cdf x. recall definition cdf terms pdf px u 1y z u 1y fxdx . 6.137 expression cdf terms x fy y z u 1y fxdx . 6.138 obtain pdf differentiate 6.138 respect y fy dyfyy dy z u 1y fxdx . 6.139 note integral righthand side respect x need integral respect differentiating respect y. particular use 6.133 get substitution z fu 1yu 1ydy z fxdx x u 1y . 6.140 using 6.140 righthand side 6.139 gives us fy dy z u 1y fxu 1yu 1ydy . 6.141 recall differentiation linear operator use subscript x remind fxu 1y function x y. invoking fundamental theorem calculus gives us fy fxu 1y dyu 1y . 6.142 recall assumed u strictly increasing function. decreas ing functions turns negative sign follow derivation. introduce absolute value differential expression increasing decreasing u fy fxu 1y dyu 1y . 6.143 called changeofvariable technique. term dyu 1y changeofvariable technique 6.143 measures much unit volume changes applying u see also definition jacobian section 5.3. draft 20230215 mathematics machine learning. feedback
6.7 change variablesinverse transform 219 remark. comparison discrete case 6.125b addi tional factor dyu 1y . continuous case requires care py y 0 y. probability density function fy description probability event involving y. far section studying univariate change vari ables. case multivariate random variables analogous com plicated fact absolute value cannot used multivariate functions. instead use determinant jacobian matrix. recall 5.58 jacobian matrix partial derivatives existence nonzero determinant shows invert ja cobian. recall discussion section 4.1 determinant arises differentials cubes volume transformed paral lelepipeds jacobian. let us summarize preceding discussion following theorem gives us recipe multivariate change variables. theorem 6.16. theorem 17.2 billingsley 1995 let fx value probability density multivariate continuous random variable x. vectorvalued function ux differentiable invertible values within domain x corresponding values y probability density ux given fy fxu 1y det yu 1y . 6.144 theorem looks intimidating first glance key point change variable multivariate random variable follows pro cedure univariate change variable. first need work inverse transform substitute density x. calculate determinant jacobian multiply result. following example illustrates case bivariate random variable. example 6.17 consider bivariate random variable x states x x1 x2 proba bility density function f x1 x2 1 2π exp 1 2 x1 x2 x1 x2 . 6.145 use changeofvariable technique theorem 6.16 derive effect linear transformation section 2.7 random variable. consider matrix r22 defined a b c . 6.146 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
220 probability distributions interested finding probability density function trans formed bivariate random variable states ax. recall change variables require inverse transformation x function y. since consider linear transformations inverse transformation given matrix inverse see section 2.2.2. 2 2 matrices explicitly write formula given x1 x2 a1 y1 y2 1 ad bc b c y1 y2 . 6.147 observe ad bc determinant section 4.1 a. corre sponding probability density function given fx fa1y 1 2π exp 1 2yaa1y . 6.148 partial derivative matrix times vector respect vector matrix section 5.5 therefore ya1y a1 . 6.149 recall section 4.1 determinant inverse inverse determinant determinant jacobian matrix det ya1y 1 ad bc . 6.150 able apply changeofvariable formula theo rem 6.16 multiplying 6.148 6.150 yields fy fx det ya1y 6.151a 1 2π exp 1 2yaa1y ad bc1. 6.151b example 6.17 based bivariate random variable al lows us easily compute matrix inverse preceding relation holds higher dimensions. remark. saw section 6.5 density fx 6.148 actually standard gaussian distribution transformed density fy bivariate gaussian covariance σ aa. use ideas chapter describe probabilistic modeling section 8.4 well introduce graphical language section 8.5. see direct machine learning applications ideas chapters 9 11. draft 20230215 mathematics machine learning. feedback
6.8 reading 221 6.8 reading chapter rather terse times. grinstead snell 1997 walpole et al. 2011 provide relaxed presentations suit able selfstudy. readers interested philosophical aspects probability consider hacking 2001 whereas approach related software engineering presented downey 2014. overview exponential families found barndorffnielsen 2014. see use probability distributions model machine learning tasks chapter 8. ironically recent surge interest neural networks resulted broader appreciation probabilistic models. example idea normalizing flows jimenez rezende mohamed 2015 relies change variables transform ing random variables. overview methods variational inference applied neural networks described chapters 16 20 book goodfellow et al. 2016. side stepped large part difficulty continuous random vari ables avoiding measure theoretic questions billingsley 1995 pollard 2002 assuming without construction real numbers ways defining sets real numbers well appropriate fre quency occurrence. details matter example specifi cation conditional probability py x continuous random variables x proschan presnell 1998. lazy notation hides fact want specify x x which set measure zero. fur thermore interested probability density function y. precise notation would say eyfy σx take expectation test function f conditioned σalgebra x. technical audience interested details probability the ory many options jaynes 2003 mackay 2003 jacod protter 2004 grimmett welsh 2014 including technical discus sions shiryayev 1984 lehmann casella 1998 dudley 2002 bickel doksum 2006 c inlar 2011. alternative way approach proba bility start concept expectation work backward derive necessary properties probability space whittle 2000. machine learning allows us model intricate distributions ever complex types data developer probabilistic machine learn ing models would understand technical aspects. ma chine learning texts probabilistic modeling focus include books mackay 2003 bishop 2006 rasmussen williams 2006 bar ber 2012 murphy 2012. exercises 6.1 consider following bivariate distribution px y two discrete random variables x . 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
222 probability distributions x x1 x2 x3 x4 x5 y3 y2 y1 0.01 0.02 0.03 0.1 0.1 0.05 0.1 0.05 0.07 0.2 0.1 0.05 0.03 0.05 0.04 compute a. marginal distributions px py. b. conditional distributions pxy y1 pyx x3. 6.2 consider mixture two gaussian distributions illustrated figure 6.2 0.4 n 10 2 1 0 0 1 0.6 n 0 0 8.4 2.0 2.0 1.7 . a. compute marginal distributions dimension. b. compute mean mode median marginal distribution. c. compute mean mode twodimensional distribution. 6.3 written computer program sometimes compiles some times code change. decide model apparent stochas ticity success vs. success x compiler using bernoulli distribution parameter µ px µ µx1 µ1x x 0 1 . choose conjugate prior bernoulli likelihood compute pos terior distribution pµ x1 . . . xn. 6.4 two bags. first bag contains four mangos two apples second bag contains four mangos four apples. also biased coin shows heads probability 0.6 tails probability 0.4. coin shows heads. pick fruit random bag 1 otherwise pick fruit random bag 2. friend flips coin you cannot see result picks fruit random corresponding bag presents mango. probability mango picked bag 2 hint use bayes theorem. 6.5 consider timeseries model xt1 axt w w n0 q yt cxt v v n0 r w v i.i.d. gaussian noise variables. further assume px0 nµ0 σ0 . a. form px0 x1 . . . xt justify answer you explicitly compute joint distribution. b. assume pxt y1 . . . yt nµt σt . 1. compute pxt1 y1 . . . yt. draft 20230215 mathematics machine learning. feedback
exercises 223 2. compute pxt1 yt1 y1 . . . yt. 3. time t1 observe value yt1 ˆ y. compute conditional distribution pxt1 y1 . . . yt1. 6.6 prove relationship 6.44 relates standard definition variance rawscore expression variance. 6.7 prove relationship 6.45 relates pairwise difference be tween examples dataset rawscore expression variance. 6.8 express bernoulli distribution natural parameter form ex ponential family see 6.107. 6.9 express binomial distribution exponential family distribution. also express beta distribution exponential family distribution. show product beta binomial distribution also member exponential family. 6.10 derive relationship section 6.5.2 two ways a. completing square b. expressing gaussian exponential family form product two gaussians nx a a nx b b unnormalized gaussian distribution c nx c c c a1 b11 c ca1a b1b c 2πd 2 b 1 2 exp 1 2a ba b1a b . note normalizing constant c considered normalized gaussian distribution either b inflated covariance matrix b i.e. c na b b nb a b. 6.11 iterated expectations. consider two random variables x joint distribution px y. show exx ey exx y . here exx y denotes expected value x conditional distri bution px y. 6.12 manipulation gaussian random variables. consider gaussian random variable x nx µx σx x rd. furthermore ax b w re red b re w nw 0 q indepen dent gaussian noise. independent implies x w independent random variables q diagonal. a. write likelihood py x. b. distribution py r py xpxdx gaussian. compute mean µy covariance σy. derive result detail. c. random variable transformed according measure ment mapping z cy v 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
224 probability distributions z rf c rf e v nv 0 r independent gaus sian measurement noise. write pz y. compute pz i.e. mean µz covariance σz. derive result detail. d. now value ˆ measured. compute posterior distribution px ˆ y. hint solution posterior also gaussian i.e. need de termine mean covariance matrix. start explicitly com puting joint gaussian px y. also requires us compute crosscovariances covxyx y covyxy x. apply rules gaussian conditioning. 6.13 probability integral transformation given continuous random variable x cdf fxx show ran dom variable fxx uniformly distributed theorem 6.15. draft 20230215 mathematics machine learning. feedback
7 continuous optimization since machine learning algorithms implemented computer mathematical formulations expressed numerical optimization meth ods. chapter describes basic numerical methods training ma chine learning models. training machine learning model often boils finding good set parameters. notion good de termined objective function probabilistic model see examples second part book. given objective function finding best value done using optimization algorithms. since consider data models rd optimization problems face continuous optimization problems opposed combinatorial optimization problems discrete variables. chapter covers two main branches continuous optimization fig ure 7.2 unconstrained constrained optimization. assume chapter objective function differentiable see chapter 5 hence access gradient location space help us find optimum value. convention objective functions ma chine learning intended minimized is best value minimum value. intuitively finding best value like finding val leys objective function gradients point us uphill. idea move downhill opposite gradient hope find deepest point. unconstrained optimization concept need several design choices discuss section 7.1. constrained optimization need introduce concepts man age constraints section 7.2. also introduce special class problems convex optimization problems section 7.3 make statements reaching global optimum. consider function figure 7.2. function global minimum global minimum around x 4.5 function value approximately 47. since function smooth gradients used help find min imum indicating whether take step right left. assumes correct bowl exists another local local minimum minimum around x 0.7. recall solve stationary points function calculating derivative setting zero. stationary points real roots derivative is points zero gradient. ℓx x4 7x3 5x2 17x 3 7.1 obtain corresponding gradient dℓx dx 4x3 21x2 10x 17 . 7.2 225 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.
226 continuous optimization figure 7.2 mind map concepts related optimization presented chapter. two main ideas gradient descent convex optimization. continuous optimization unconstrained optimization constrained optimization gradient descent stepsize momentum stochastic gradient descent lagrange multipliers convex optimization duality convex convex conjugate linear programming quadratic programming chapter 10 dimension reduc. chapter 11 density estimation chapter 12 classification since cubic equation general three solutions set zero. example two minimums one maximum around x 1.4. check whether stationary point minimum maximum need take derivative second time check whether second derivative positive negative stationary point. case second derivative d2ℓx dx2 12x2 42x 10 . 7.3 substituting visually estimated values x 4.5 1.4 0.7 observe expected middle point maximum d2ℓx dx2 0 two stationary points minimums. note avoided analytically solving values x previous discussion although loworder polynomials pre ceding could so. general unable find analytic solu tions hence need start value say x0 6 follow negative gradient. negative gradient indicates go draft 20230215 mathematics machine learning. feedback
7.1 optimization using gradient descent 227 figure 7.2 example objective function. negative gradients indicated arrows global minimum indicated dashed blue line. 6 5 4 3 2 1 0 1 2 value parameter 60 40 20 0 20 40 60 objective x4 7x3 5x2 17x 3 right far this called stepsize. furthermore according abelruffini theorem general algebraic solution polynomials degree 5 abel 1826. started right side e.g. x0 0 negative gradient would led us wrong minimum. figure 7.2 illustrates fact x 1 negative gradient points toward minimum right figure larger objective value. section 7.3 learn class functions called convex functions exhibit tricky dependency starting point optimization algorithm. convex functions local minimums global minimum. turns many machine learning objective convex functions local minima global minimum. functions designed convex see ex ample chapter 12. discussion chapter far onedimensional func tion able visualize ideas gradients descent direc tions optimal values. rest chapter develop ideas high dimensions. unfortunately visualize con cepts one dimension concepts generalize directly higher dimensions therefore care needs taken reading. 7.1 optimization using gradient descent consider problem solving minimum realvalued function min x fx 7.4 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
228 continuous optimization f rd r objective function captures machine learning problem hand. assume function f differentiable unable analytically find solution closed form. gradient descent firstorder optimization algorithm. find local minimum function using gradient descent one takes steps propor tional negative gradient function current point. recall section 5.1 gradient points direction use convention row vectors gradients. steepest ascent. another useful intuition consider set lines function certain value fx c value c r known contour lines. gradient points direction orthogonal contour lines function wish optimize. let us consider multivariate functions. imagine surface described function fx ball starting particular location x0. ball released move downhill direction steepest de scent. gradient descent exploits fact fx0 decreases fastest one moves x0 direction negative gradient fx0of f x0. assume book functions differentiable refer reader general settings section 7.4. then x1 x0 γfx0 7.5 small stepsize γ 0 fx1 fx0. note use transpose gradient since otherwise dimensions work out. observation allows us define simple gradient descent algo rithm want find local optimum fx function f rn r x 7fx start initial guess x0 parameters wish optimize iterate according xi1 xi γifxi. 7.6 suitable stepsize γi sequence fx0 fx1 . . . converges local minimum. example 7.1 consider quadratic function two dimensions f x1 x2 1 2 x1 x2 2 1 1 20 x1 x2 5 3 x1 x2 7.7 gradient f x1 x2 x1 x2 2 1 1 20 5 3 . 7.8 starting initial location x0 3 1 iteratively apply 7.6 obtain sequence estimates converge minimum value draft 20230215 mathematics machine learning. feedback
7.1 optimization using gradient descent 229 figure 7.2 gradient descent twodimensional quadratic surface shown heatmap. see example 7.1 description. 4 2 0 2 4 x1 2 1 0 1 2 x2 0.0 10.0 20.0 30.0 40.0 40.0 50.0 50.0 60.0 70.0 80.0 15 0 15 30 45 60 75 90 illustrated figure 7.2. see both figure plug ging x0 7.8 γ 0.085 negative gradient x0 points north east leading x1 1.98 1.21. repeating argument gives us x2 1.32 0.42 on. remark. gradient descent relatively slow close minimum asymptotic rate convergence inferior many methods. us ing ball rolling hill analogy surface long thin valley problem poorly conditioned trefethen bau iii 1997. poorly conditioned convex problems gradient descent increasingly zigzags gradients point nearly orthogonally shortest di rection minimum point see figure 7.2. 7.1.1 stepsize mentioned earlier choosing good stepsize important gradient descent. stepsize small gradient descent slow. stepsize also called learning rate. stepsize chosen large gradient descent overshoot fail con verge even diverge. discuss use momentum next section. method smoothes erratic behavior gradient up dates dampens oscillations. adaptive gradient methods rescale stepsize iteration de pending local properties function. two simple heuris tics toussaint 2012 function value increases gradient step stepsize large. undo step decrease stepsize. function value decreases step could larger. try increase stepsize. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
230 continuous optimization although undo step seems waste resources using heuristic guarantees monotonic convergence. example 7.2 solving linear equation system solve linear equations form ax b practice solve axb 0 approximately finding xthat minimizes squared error ax b2 ax bax b 7.9 use euclidean norm. gradient 7.9 respect x x 2ax ba . 7.10 use gradient directly gradient descent algorithm. how ever particular special case turns analytic solution found setting gradient zero. see solving squared error problems chapter 9. remark. applied solution linear systems equations ax b gradient descent may converge slowly. speed convergence gra dient descent dependent condition number κ σamax σamin condition number ratio maximum minimum singular value section 4.5 a. condition number essentially measures ratio curved direction versus least curved direction corresponds imagery poorly conditioned problems long thin valleys curved one direction flat other. instead di rectly solving ax b one could instead solve p 1ax b 0 p called preconditioner. goal design p 1 p 1a preconditioner better condition number time p 1 easy com pute. information gradient descent preconditioning convergence refer boyd vandenberghe 2004 chapter 9. 7.1.2 gradient descent momentum illustrated figure 7.2 convergence gradient descent may slow curvature optimization surface regions poorly scaled. curvature gradient descent steps hops walls valley approaches optimum small steps. proposed tweak improve convergence give gradient descent memory. goh 2017 wrote intuitive blog post gradient descent momentum. gradient descent momentum rumelhart et al. 1986 method introduces additional term remember happened previous iteration. memory dampens oscillations smoothes gradient updates. continuing ball analogy momentum term emulates phenomenon heavy ball reluctant change di rections. idea gradient update memory implement draft 20230215 mathematics machine learning. feedback
7.1 optimization using gradient descent 231 moving average. momentumbased method remembers update xi iteration determines next update linear combi nation current previous gradients xi1 xi γifxi αxi 7.11 xi xi xi1 αxi1 γi1fxi1 7.12 α 0 1. sometimes know gradient approxi mately. cases momentum term useful since averages different noisy estimates gradient. one particularly useful way obtain approximate gradient using stochastic approximation discuss next. 7.1.3 stochastic gradient descent computing gradient time consuming. however often possible find cheap approximation gradient. approximating gradient still useful long points roughly direction true gradient. stochastic gradient descent stochastic gradient descent often shortened sgd stochastic ap proximation gradient descent method minimizing objective function written sum differentiable functions. word stochastic refers fact acknowledge know gradient precisely instead know noisy approxima tion it. constraining probability distribution approximate gradients still theoretically guarantee sgd converge. machine learning given n 1 . . . n data points often consider objective functions sum losses ln incurred example n. mathematical notation form lθ n x n1 lnθ 7.13 θ vector parameters interest i.e. want find θ minimizes l. example regression chapter 9 negative log likelihood expressed sum loglikelihoods individual examples lθ n x n1 log pynxn θ 7.14 xn rd training inputs yn training targets θ parameters regression model. standard gradient descent introduced previously batch opti mization method i.e. optimization performed using full training set 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
232 continuous optimization updating vector parameters according θi1 θi γilθi θi γi n x n1 lnθi 7.15 suitable stepsize parameter γi. evaluating sum gradient may re quire expensive evaluations gradients individual functions ln. training set enormous andor simple formulas exist evaluating sums gradients becomes expensive. consider term pn n1lnθi 7.15. reduce amount computation taking sum smaller set ln. contrast batch gradient descent uses ln n 1 . . . n randomly choose subset ln minibatch gradient descent. extreme case randomly select single ln estimate gradient. key insight taking subset data sensible realize gradient descent converge require gradient unbiased estimate true gradient. fact term pn n1lnθi 7.15 empirical estimate expected value section 6.4.1 gradient. therefore unbiased empirical estimate ex pected value example using subsample data would suffice convergence gradient descent. remark. learning rate decreases appropriate rate sub ject relatively mild assumptions stochastic gradient descent converges almost surely local minimum bottou 1998. one consider using approximate gradient major rea son practical implementation constraints size central processing unit cpugraphics processing unit gpu memory limits computational time. think size subset used esti mate gradient way thought size sample estimating empirical means section 6.4.1. large minibatch sizes provide accurate estimates gradient reducing variance parameter update. furthermore large minibatches take advantage highly optimized matrix operations vectorized implementations cost gradient. reduction variance leads stable conver gence gradient calculation expensive. contrast small minibatches quick estimate. keep minibatch size small noise gradient estimate allow us get bad local optima may otherwise get stuck in. machine learning optimization methods used training min imizing objective function training data overall goal improve generalization performance chapter 8. since goal machine learning necessarily need precise estimate min imum objective function approximate gradients using minibatch approaches widely used. stochastic gradient descent effective largescale machine learning problems bottou et al. 2018 draft 20230215 mathematics machine learning. feedback
7.2 constrained optimization lagrange multipliers 233 figure 7.1 illustration constrained optimization. unconstrained problem indicated contour lines minimum right side indicated circle. box constraints 1 x 1 1 y 1 require optimal solution within box resulting optimal value indicated star. 3 2 1 0 1 2 3 x1 3 2 1 0 1 2 3 x2 training deep neural networks millions images dean et al. 2012 topic models hoffman et al. 2013 reinforcement learning mnih et al. 2015 training largescale gaussian process models hensman et al. 2013 gal et al. 2014. 7.2 constrained optimization lagrange multipliers previous section considered problem solving min imum function min x fx 7.16 f rd r. section additional constraints. is realvalued functions gi rd r 1 . . . m consider constrained optimization problem see figure 7.1 illustration min x fx 7.17 subject gix 0 1 . . . . worth pointing functions f gi could nonconvex general consider convex case next section. one obvious practical way converting constrained problem 7.17 unconstrained one use indicator function jx fx x i1 1gix 7.18 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
234 continuous optimization 1z infinite step function 1z 0 z 0 otherwise . 7.19 gives infinite penalty constraint satisfied hence would provide solution. however infinite step function equally difficult optimize. overcome difficulty introduc ing lagrange multipliers. idea lagrange multipliers replace lagrange multiplier step function linear function. associate problem 7.17 lagrangian introducing la lagrangian grange multipliers λi 0 corresponding inequality constraint re spectively boyd vandenberghe 2004 chapter 4 lx λ fx x i1 λigix 7.20a fx λgx 7.20b last line concatenated constraints gix vector gx lagrange multipliers vector λ rm. introduce idea lagrangian duality. general duality optimization idea converting optimization problem one set variables x called primal variables another optimization problem different set variables λ called dual variables. introduce two different approaches duality section discuss lagrangian duality section 7.3.3 discuss legendrefenchel duality. definition 7.1. problem 7.17 min x fx 7.21 subject gix 0 1 . . . known primal problem corresponding primal variables x. primal problem associated lagrangian dual problem given lagrangian dual problem max λrm dλ subject λ 0 7.22 λ dual variables dλ minxrd lx λ. remark. discussion definition 7.1 use two concepts also independent interest boyd vandenberghe 2004. first minimax inequality says function minimax inequality two arguments φx y maximin less minimax i.e. max min x φx y min x max φx y . 7.23 draft 20230215 mathematics machine learning. feedback
7.2 constrained optimization lagrange multipliers 235 inequality proved considering inequality x min x φx y max φx y . 7.24 note taking maximum lefthand side 7.24 main tains inequality since inequality true y. similarly take minimum x righthand side 7.24 obtain 7.23. second concept weak duality uses 7.23 show weak duality primal values always greater equal dual values. de scribed detail 7.27. recall difference jx 7.18 lagrangian 7.20b relaxed indicator function linear func tion. therefore λ 0 lagrangian lx λ lower bound jx. hence maximum lx λ respect λ jx max λ0 lx λ . 7.25 recall original problem minimizing jx min xrd max λ0 lx λ . 7.26 minimax inequality 7.23 follows swapping order minimum maximum results smaller value i.e. min xrd max λ0 lx λ max λ0 min xrd lx λ . 7.27 also known weak duality. note inner part right weak duality hand side dual objective function dλ definition follows. contrast original optimization problem constraints minxrd lx λ unconstrained optimization problem given value λ. solving minxrd lx λ easy overall problem easy solve. see observing 7.20b lx λ affine respect λ. therefore minxrd lx λ pointwise min imum affine functions λ hence dλ concave even though f gi may nonconvex. outer problem maximization λ maximum concave function efficiently computed. assuming f gi differentiable find lagrange dual problem differentiating lagrangian respect x setting differential zero solving optimal value. discuss two concrete examples sections 7.3.1 7.3.2 f gi convex. remark equality constraints. consider 7.17 additional equality constraints min x fx subject gix 0 1 . . . hjx 0 j 1 . . . n . 7.28 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
236 continuous optimization model equality constraints replacing two inequality constraints. equality constraint hjx 0 equivalently replace two constraints hjx 0 hjx 0. turns resulting lagrange multipliers unconstrained. therefore constrain lagrange multipliers corresponding inequality constraints 7.28 nonnegative leave la grange multipliers corresponding equality constraints unconstrained. 7.3 convex optimization focus attention particularly useful class optimization prob lems guarantee global optimality. f convex function constraints involving g h convex sets called convex optimization problem. setting strong convex optimization problem strong duality duality optimal solution dual problem opti mal solution primal problem. distinction convex func tions convex sets often strictly presented machine learning literature one often infer implied meaning context. definition 7.2. set c convex set x c scalar convex set θ 0 θ 1 θx 1 θy c . 7.29 figure 7.2 example convex set. convex sets sets straight line connecting two ele ments set lie inside set. figures 7.2 7.3 illustrate convex nonconvex sets respectively. figure 7.3 example nonconvex set. convex functions functions straight line two points function lie function. figure 7.2 shows non convex function figure 7.2 shows convex function. another convex function shown figure 7.2. definition 7.3. let function f rd r function whose domain convex set. function f convex function x domain convex function f scalar θ 0 θ 1 fθx 1 θy θfx 1 θfy . 7.30 remark. concave function negative convex function. concave function constraints involving g h 7.28 truncate functions scalar value resulting sets. another relation convex functions convex sets consider set obtained filling in convex function. convex function bowllike object imagine pouring water fill up. resulting filledin set called epigraph epigraph convex function convex set. function f rn r differentiable specify convexity draft 20230215 mathematics machine learning. feedback
7.3 convex optimization 237 figure 7.2 example convex function. 3 2 1 0 1 2 3 x 0 10 20 30 40 3x2 5x 2 terms gradient xfx section 5.2. function fx convex two points x holds fy fx xfxy x . 7.31 know function fx twice differentiable is hessian 5.147 exists values domain x function fx convex 2 xfx positive semidefinite boyd vandenberghe 2004. example 7.3 negative entropy fx x log2 x convex x 0. visualization function shown figure 7.2 see function convex. illustrate previous definitions convexity let us check calculations two points x 2 x 4. note prove convexity fx would need check points x r. recall definition 7.3. consider point midway two points that θ 0.5 lefthand side f0.5 2 0.5 4 3 log2 3 4.75. righthand side 0.52 log2 2 0.54 log2 4 1 4 5. therefore definition satisfied. since fx differentiable alternatively use 7.31. calculating derivative fx obtain xx log2 x 1 log2 x x 1 x loge 2 log2 x 1 loge 2 . 7.32 using two test points x 2 x 4 lefthand side 7.31 given f4 8. righthand side fx xy x f2 f2 4 2 7.33a 2 1 1 loge 2 2 6.9 . 7.33b 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
238 continuous optimization figure 7.2 negative entropy function which convex tangent x 2. 0 1 2 3 4 5 x 0 5 10 fx x log2 x tangent x 2 check function set convex first principles recalling definitions. practice often rely operations pre serve convexity check particular function set convex. al though details vastly different idea closure introduced chapter 2 vector spaces. example 7.4 nonnegative weighted sum convex functions convex. observe f convex function α 0 nonnegative scalar function αf convex. see multiplying α sides equation definition 7.3 recalling multiplying nonnegative number change inequality. f1 f2 convex functions definition f1θx 1 θy θf1x 1 θf1y 7.34 f2θx 1 θy θf2x 1 θf2y . 7.35 summing sides gives us f1θx 1 θy f2θx 1 θy θf1x 1 θf1y θf2x 1 θf2y 7.36 righthand side rearranged θf1x f2x 1 θf1y f2y 7.37 completing proof sum convex functions convex. combining preceding two facts see αf1x βf2x convex α β 0. closure property extended using sim ilar argument nonnegative weighted sums two convex functions. draft 20230215 mathematics machine learning. feedback
7.3 convex optimization 239 remark. inequality 7.30 sometimes called jensens inequality. jensens inequality fact whole class inequalities taking nonnegative weighted sums convex functions called jensens inequality. summary constrained optimization problem called convex opti convex optimization problem mization problem min x fx subject gix 0 1 . . . hjx 0 j 1 . . . n 7.38 functions fx gix convex functions hjx 0 convex sets. following describe two classes convex optimization problems widely used well understood. 7.3.1 linear programming consider special case preceding functions linear i.e. min xrd cx 7.39 subject ax b rmd b rm. known linear program. linear program linear programs one widely used approaches industry. variables linear constraints. lagrangian given lx λ cx λax b 7.40 λ rm vector nonnegative lagrange multipliers. rear ranging terms corresponding x yields lx λ c aλx λb . 7.41 taking derivative lx λ respect x setting zero gives us c aλ 0 . 7.42 therefore dual lagrangian dλ λb. recall would like maximize dλ. addition constraint due derivative lx λ zero also fact λ 0 resulting following dual optimization problem convention minimize primal maximize dual. max λrm bλ 7.43 subject c aλ 0 λ 0 . also linear program variables. choice solving primal 7.39 dual 7.43 program depending 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
240 continuous optimization whether larger. recall number variables number constraints primal linear program. example 7.5 linear program consider linear program min xr2 5 3 x1 x2 subject 2 2 2 4 2 1 0 1 0 1 x1 x2 33 8 5 1 8 7.44 two variables. program also shown figure 7.1. objective function linear resulting linear contour lines. constraint set standard form translated legend. optimal value must lie shaded feasible region indicated star. figure 7.1 illustration linear program. unconstrained problem indicated contour lines minimum right side. optimal value given constraints shown star. 0 2 4 6 8 10 12 14 16 x1 0 2 4 6 8 10 x2 2x2 33 2x1 4x2 2x1 8 x2 2x1 5 x2 1 x2 8 draft 20230215 mathematics machine learning. feedback
7.3 convex optimization 241 7.3.2 quadratic programming consider case convex quadratic objective function con straints affine i.e. min xrd 1 2xqx cx 7.45 subject ax b rmd b rm c rd. square symmetric matrix q rdd positive definite therefore objective function convex. known quadratic program. observe variables linear constraints. example 7.6 quadratic program consider quadratic program min xr2 1 2 x1 x2 2 1 1 4 x1 x2 5 3 x1 x2 7.46 subject 1 0 1 0 0 1 0 1 x1 x2 1 1 1 1 7.47 two variables. program also illustrated figure 7.1. objec tive function quadratic positive semidefinite matrix q resulting elliptical contour lines. optimal value must lie shaded feasi ble region indicated star. lagrangian given lx λ 1 2xqx cx λax b 7.48a 1 2xqx c aλx λb 7.48b rearranged terms. taking derivative lx λ respect x setting zero gives qx c aλ 0 . 7.49 assuming q invertible get x q1c aλ . 7.50 substituting 7.50 primal lagrangian lx λ get dual lagrangian dλ 1 2c aλq1c aλ λb . 7.51 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
242 continuous optimization therefore dual optimization problem given max λrm 1 2c aλq1c aλ λb subject λ 0 . 7.52 see application quadratic programming machine learning chapter 12. 7.3.3 legendrefenchel transform convex conjugate let us revisit idea duality section 7.2 without considering constraints. one useful fact convex set equiva lently described supporting hyperplanes. hyperplane called supporting hyperplane convex set intersects convex set supporting hyperplane convex set contained one side it. recall fill convex function obtain epigraph convex set. therefore also describe convex functions terms supporting hyper planes. furthermore observe supporting hyperplane touches convex function fact tangent function point. recall tangent function fx given point x0 evaluation gradient function point dfx dx xx0. summary convex sets equivalently described sup porting hyperplanes convex functions equivalently described function gradient. legendre transform formalizes concept. legendre transform physics students often introduced legendre transform relating lagrangian hamiltonian classical mechanics. begin general definition unfortunately counterintuitive form look special cases relate definition intuition described preceding paragraph. legendrefenchel legendrefenchel transform transform transformation in sense fourier transform convex differentiable function fx function depends tangents sx xfx. worth stressing transformation function f variable x function evaluated x. legendrefenchel transform also known convex conjugate for convex conjugate reasons see soon closely related duality hiriarturruty lemar echal 2001 chapter 5. definition 7.4. convex conjugate function f rd r convex conjugate function f defined f s sup xrd s xfx . 7.53 note preceding convex conjugate definition need function f convex differentiable. definition 7.4 used general inner product section 3.2 rest section draft 20230215 mathematics machine learning. feedback
7.3 convex optimization 243 consider standard dot product finitedimensional vectors s x sx avoid many technical details. understand definition 7.4 geometric fashion consider nice derivation easiest understand drawing reasoning progresses. simple onedimensional convex differentiable function example fx x2. note since looking onedimensional problem hyperplanes reduce line. consider line sxc. recall able describe convex functions supporting hyperplanes let us try describe function fx supporting lines. fix gradi ent line r point x0 fx0 graph f find minimum value c line still intersects x0 fx0. note minimum value c place line slope just touches function fx x2. line passing x0 fx0 gradient given fx0 sx x0 . 7.54 yintercept line sx0 fx0. minimum c sx c intersects graph f therefore inf x0 sx0 fx0 . 7.55 preceding convex conjugate convention defined nega tive this. reasoning paragraph rely fact chose onedimensional convex differentiable function holds f rd r nonconvex nondifferentiable. classical legendre transform defined convex differentiable functions rd. remark. convex differentiable functions example fx x2 nice special case need supremum onetoone correspondence function legendre trans form. let us derive first principles. convex differentiable function know x0 tangent touches fx0 fx0 sx0 c . 7.56 recall want describe convex function fx terms gradient xfx xfx0. rearrange get expres sion c obtain c sx0 fx0 . 7.57 note c changes x0 therefore s think function s call f s sx0 fx0 . 7.58 comparing 7.58 definition 7.4 see 7.58 special case without supremum. conjugate function nice properties example convex functions applying legendre transform gets us back orig inal function. way slope fx s slope f s 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
244 continuous optimization x. following two examples show common uses convex conjugates machine learning. example 7.7 convex conjugates illustrate application convex conjugates consider quadratic function fy λ 2 yk1y 7.59 based positive definite matrix k rnn. denote primal variable rn dual variable α rn. applying definition 7.4 obtain function f α sup yrn y αλ 2 yk1y . 7.60 since function differentiable find maximum taking derivative respect setting zero. y αλ 2yk1y y α λk1y 7.61 hence gradient zero 1 λkα. substituting 7.60 yields f α 1 λαkα λ 2 1 λkα k1 1 λkα 1 2λαkα . 7.62 example 7.8 machine learning often use sums functions example ob jective function training set includes sum losses ex ample training set. following derive convex conjugate sum losses ℓt ℓ r r. also illustrates appli cation convex conjugate vector case. let lt pn i1 ℓiti. then lz sup trn z t n x i1 ℓiti 7.63a sup trn n x i1 ziti ℓiti definition dot product 7.63b n x i1 sup trn ziti ℓiti 7.63c draft 20230215 mathematics machine learning. feedback
7.3 convex optimization 245 n x i1 ℓ zi . definition conjugate 7.63d recall section 7.2 derived dual optimization problem using lagrange multipliers. furthermore convex optimization problems strong duality solutions primal dual problem match. legendrefenchel transform described also used derive dual optimization problem. furthermore function convex differentiable supremum unique. investi gate relation two approaches let us consider linear equality constrained convex optimization problem. example 7.9 let fy gx convex functions real matrix appropriate dimensions ax y. min x fax gx min axy fy gx. 7.64 introducing lagrange multiplier u constraints ax y min axy fy gx min xy max u fy gx ax yu 7.65a max u min xy fy gx ax yu 7.65b last step swapping max min due fact fy gx convex functions. splitting dot product term collecting x y max u min xy fy gx ax yu 7.66a max u min yu fy h min x axu gx 7.66b max u min yu fy h min x xau gx 7.66c recall convex conjugate definition 7.4 fact dot prod general inner products ais replaced adjoint a. ucts symmetric max u min yu fy h min x xau gx 7.67a max u f u gau . 7.67b therefore shown min x fax gx max u f u gau . 7.68 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
246 continuous optimization legendrefenchel conjugate turns quite useful ma chine learning problems expressed convex optimization problems. particular convex loss functions apply independently example conjugate loss convenient way derive dual problem. 7.4 reading continuous optimization active area research try provide comprehensive account recent advances. gradient descent perspective two major weaknesses set literature. first challenge fact gradient descent firstorder algorithm use infor mation curvature surface. long valleys gradient points perpendicularly direction interest. idea momentum generalized general class acceleration meth ods nesterov 2018. conjugate gradient methods avoid issues faced gradient descent taking previous directions account shewchuk 1994. secondorder methods newton methods use hessian provide information curvature. many choices choos ing stepsizes ideas like momentum arise considering curvature objective function goh 2017 bottou et al. 2018. quasinewton methods lbfgs try use cheaper computational methods ap proximate hessian nocedal wright 2006. recently interest metrics computing descent directions result ing approaches mirror descent beck teboulle 2003 natural gradient toussaint 2012. second challenge handle nondifferentiable functions. gradi ent methods well defined kinks function. cases subgradient methods used shor 1985. fur ther information algorithms optimizing nondifferentiable func tions refer book bertsekas 1999. vast amount literature different approaches numerically solving continuous optimization problems including algorithms constrained optimization problems. good starting points appreciate literature books luenberger 1969 bonnans et al. 2006. recent survey con tinuous optimization provided bubeck 2015. hugo gonc alves blog also good resource easier introduction legendrefenchel transforms comydaal7hj modern applications machine learning often mean size datasets prohibit use batch gradient descent hence stochastic gradient descent current workhorse largescale machine learning methods. recent surveys literature include hazan 2015 bot tou et al. 2018. duality convex optimization book boyd vanden berghe 2004 includes lectures slides online. mathematical treatment provided bertsekas 2009 recent book one draft 20230215 mathematics machine learning. feedback
exercises 247 key researchers area optimization nesterov 2018. con vex optimization based upon convex analysis reader interested foundational results convex functions referred rock afellar 1970 hiriarturruty lemar echal 2001 borwein lewis 2006. legendrefenchel transforms also covered afore mentioned books convex analysis beginnerfriendly pre sentation available zia et al. 2009. role legendrefenchel transforms analysis convex optimization algorithms surveyed polyak 2016. exercises 7.1 consider univariate function fx x3 6x2 3x 5. find stationary points indicate whether maximum mini mum saddle points. 7.2 consider update equation stochastic gradient descent equation 7.15. write update use minibatch size one. 7.3 consider whether following statements true false a. intersection two convex sets convex. b. union two convex sets convex. c. difference convex set another convex set b convex. 7.4 consider whether following statements true false a. sum two convex functions convex. b. difference two convex functions convex. c. product two convex functions convex. d. maximum two convex functions convex. 7.5 express following optimization problem standard linear program matrix notation max xr2 ξr px ξ subject constraints ξ 0 x0 0 x1 3. 7.6 consider linear program illustrated figure 7.1 min xr2 5 3 x1 x2 subject 2 2 2 4 2 1 0 1 0 1 x1 x2 33 8 5 1 8 derive dual linear program using lagrange duality. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
248 continuous optimization 7.7 consider quadratic program illustrated figure 7.1 min xr2 1 2 x1 x2 2 1 1 4 x1 x2 5 3 x1 x2 subject 1 0 1 0 0 1 0 1 x1 x2 1 1 1 1 derive dual quadratic program using lagrange duality. 7.8 consider following convex optimization problem min wrd 1 2ww subject wx 1 . derive lagrangian dual introducing lagrange multiplier λ. 7.9 consider negative entropy x rd fx x d1 xd log xd . derive convex conjugate function fs assuming standard dot product. hint take gradient appropriate function set gradient zero. 7.10 consider function fx 1 2xax bx c strictly positive definite means invertible. derive convex conjugate fx. hint take gradient appropriate function set gradient zero. 7.11 hinge loss which loss used support vector machine given lα max0 1 α interested applying gradient methods lbfgs want resort subgradient methods need smooth kink hinge loss. compute convex conjugate hinge loss lβ β dual variable. add ℓ2 proximal term compute conjugate resulting function lβ γ 2 β2 γ given hyperparameter. draft 20230215 mathematics machine learning. feedback
part ii central machine learning problems 249 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.

8 models meet data first part book introduced mathematics form foundations many machine learning methods. hope reader would able learn rudimentary forms language mathematics first part use describe discuss machine learning. second part book introduces four pillars machine learning regression chapter 9 dimensionality reduction chapter 10 density estimation chapter 11 classification chapter 12 main aim part book illustrate mathematical concepts introduced first part book used design machine learning algorithms used solve tasks within remit four pillars. intend introduce advanced machine learning concepts instead provide set practical methods allow reader apply knowledge gained first part book. also provides gateway wider machine learning literature readers already familiar mathematics. 8.1 data models learning worth point pause consider problem ma chine learning algorithm designed solve. discussed chapter 1 three major components machine learning system data models learning. main question machine learning what mean good models. word model many subtleties model revisit multiple times chapter. also entirely obvious objectively define word good. one guiding principles machine learning good models perform well unseen data. requires us define performance metrics accu racy distance ground truth well figuring ways well performance metrics. chapter covers necessary bits pieces mathematical statistical language commonly 251 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.
252 models meet data table 8.1 example data fictitious human resource database numerical format. name gender degree postcode age annual salary aditya msc w21bg 36 89563 bob phd ec1a1ba 47 123543 chlo e f becon sw1a1bh 26 23989 daisuke bsc se207at 68 138769 elisabeth f mba se10aa 33 113888 used talk machine learning models. so briefly out line current best practices training model resulting predictor well data yet seen. mentioned chapter 1 two different senses use phrase machine learning algorithm training prediction. describe ideas chapter well idea selecting among different models. introduce framework empirical risk minimization section 8.2 principle maximum likelihood section 8.3 idea probabilistic models section 8.4. briefly outline graphical language specifying probabilistic models sec tion 8.5 finally discuss model selection section 8.6. rest section expands upon three main components machine learning data models learning. 8.1.1 data vectors assume data read computer represented ade quately numerical format. data assumed tabular figure 8.1 think row table representing particular in stance example column particular feature. recent data assumed tidy format wickham 2014 codd 1990. years machine learning applied many types data obviously come tabular numerical format example genomic se quences text image contents webpage social media graphs. discuss important challenging aspects identifying good features. many aspects depend domain expertise re quire careful engineering and recent years put umbrella data science stray 2016 adhikari denero 2018. even data tabular format still choices made obtain numerical representation. example table 8.1 gender column a categorical variable may converted numbers 0 representing male 1 representing female. alternatively gen der could represented numbers 1 1 respectively as shown table 8.2. furthermore often important use domain knowledge constructing representation knowing university degrees progress bachelors masters phd realizing postcode provided string characters actually encodes area london. table 8.2 converted data table 8.1 numerical format postcode represented two numbers draft 20230215 mathematics machine learning. feedback
8.1 data models learning 253 table 8.2 example data fictitious human resource database see table 8.1 converted numerical format. gender id degree latitude longitude age annual salary in degrees in degrees in thousands 1 2 51.5073 0.1290 36 89.563 1 3 51.5074 0.1275 47 123.543 1 1 51.5071 0.1278 26 23.989 1 1 51.5075 0.1281 68 138.769 1 2 51.5074 0.1278 33 113.888 latitude longitude. even numerical data could potentially directly read machine learning algorithm carefully con sidered units scaling constraints. without additional information one shift scale columns dataset empirical mean 0 empirical variance 1. purposes book assume domain expert already converted data ap propriately i.e. input xn ddimensional vector real numbers called features attributes covariates. consider dataset feature attribute covariate form illustrated table 8.2. observe dropped name column table 8.1 new numerical representation. two main reasons desirable 1 expect iden tifier the name informative machine learning task 2 may wish anonymize data help protect privacy employees. part book use n denote number exam ples dataset index examples lowercase n 1 . . . n. assume given set numerical data represented array vectors table 8.2. row particular individual xn often referred example data point machine learning. subscript example data point n refers fact nth example total n exam ples dataset. column represents particular feature interest example index features 1 . . . d. recall data represented vectors means example each data point ddimensional vector. orientation table originates database community machine learning algorithms e.g. chapter 10 convenient represent examples col umn vectors. let us consider problem predicting annual salary age based data table 8.2. called supervised learning problem label yn the salary associated example xn label the age. label yn various names including target re sponse variable annotation. dataset written set example label pairs x1 y1 . . . xn yn . . . xn yn. table examples x1 . . . xn often concatenated written x rnd. fig ure 8.2 illustrates dataset consisting two rightmost columns table 8.2 x age salary. use concepts introduced first part book formalize 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
254 models meet data figure 8.2 toy data linear regression. training data xn yn pairs rightmost two columns table 8.2. interested salary person aged sixty x 60 illustrated vertical dashed red line part training data. 0 10 20 30 40 50 60 70 80 x 0 25 50 75 100 125 150 machine learning problems previous paragraph. representing data vectors xn allows us use concepts linear al gebra introduced chapter 2. many machine learning algorithms need additionally able compare two vectors. see chapters 9 12 computing similarity distance two ex amples allows us formalize intuition examples similar fea tures similar labels. comparison two vectors requires construct geometry explained chapter 3 allows us optimize resulting learning problem using techniques chapter 7. since vector representations data manipulate data find potentially better representations it. discuss finding good representations two ways finding lowerdimensional approximations original feature vector using nonlinear higherdimensional combinations original feature vector. chapter 10 see example finding lowdimensional approximation original data space finding principal components. finding principal components closely related concepts eigenvalue singular value decomposi tion introduced chapter 4. highdimensional representation see explicit feature map ϕ allows us represent in feature map puts xn using higherdimensional representation ϕxn. main mo tivation higherdimensional representations construct new features nonlinear combinations original features turn may make learning problem easier. discuss feature map section 9.2 show feature map leads kernel kernel section 12.4. recent years deep learning methods goodfellow et al. 2016 shown promise using data learn new good fea tures successful areas computer vision speech recognition natural language processing. cover neural networks part book reader referred draft 20230215 mathematics machine learning. feedback
8.1 data models learning 255 figure 8.1 example function black solid diagonal line prediction x 60 i.e. f60 100. 0 10 20 30 40 50 60 70 80 x 0 25 50 75 100 125 150 section 5.6 mathematical description backpropagation key concept training neural networks. 8.1.2 models functions data appropriate vector representation get business constructing predictive function known predictor. predictor chapter 1 yet language precise models. using concepts first part book introduce model means. present two major approaches book predictor function predictor probabilistic model. de scribe former latter next subsection. predictor function that given particular input example in case vector features produces output. now consider output single number i.e. realvalued scalar output. written f rd r 8.1 input vector x ddimensional has features func tion f applied written fx returns real number. fig ure 8.1 illustrates possible function used compute value prediction input values x. book consider general case functions would involve need functional analysis. instead consider special case linear functions fx θx θ0 8.2 unknown θ θ0. restriction means contents chap ters 2 3 suffice precisely stating notion predictor nonprobabilistic in contrast probabilistic view described next 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
256 models meet data figure 8.1 example function black solid diagonal line predictive uncertainty x 60 drawn gaussian. 0 10 20 30 40 50 60 70 80 x 0 25 50 75 100 125 150 view machine learning. linear functions strike good balance generality problems solved amount back ground mathematics needed. 8.1.3 models probability distributions often consider data noisy observations true underlying effect hope applying machine learning identify signal noise. requires us language quantify ing effect noise. often would also like predictors express sort uncertainty e.g. quantify confidence value prediction particular test data point. seen chapter 6 probability theory provides language quan tifying uncertainty. figure 8.1 illustrates predictive uncertainty function gaussian distribution. instead considering predictor single function could con sider predictors probabilistic models i.e. models describing dis tribution possible functions. limit book spe cial case distributions finitedimensional parameters allows us describe probabilistic models without needing stochastic processes random measures. special case think prob abilistic models multivariate probability distributions already allow rich class models. introduce use concepts probability chapter 6 define machine learning models section 8.4 introduce graphical language describing probabilistic models compact way sec tion 8.5. draft 20230215 mathematics machine learning. feedback
8.1 data models learning 257 8.1.4 learning finding parameters goal learning find model corresponding parame ters resulting predictor perform well unseen data. conceptually three distinct algorithmic phases discussing machine learning algorithms 1. prediction inference 2. training parameter estimation 3. hyperparameter tuning model selection prediction phase use trained predictor previously un seen test data. words parameters model choice already fixed predictor applied new vectors representing new input data points. outlined chapter 1 previous subsection consider two schools machine learning book corresponding whether predictor function probabilistic model. probabilistic model discussed section 8.4 predic tion phase called inference. remark. unfortunately agreed upon naming different algorithmic phases. word inference sometimes also used mean parameter estimation probabilistic model less often may also used mean prediction nonprobabilistic models. training parameter estimation phase adjust pre dictive model based training data. would like find good predic tors given training data two main strategies so finding best predictor based measure quality sometimes called finding point estimate using bayesian inference. finding point estimate applied types predictors bayesian inference requires probabilistic models. nonprobabilistic model follow principle empirical risk empirical risk minimization minimization describe section 8.2. empirical risk minimiza tion directly provides optimization problem finding good parame ters. statistical model principle maximum likelihood used maximum likelihood find good set parameters section 8.3. additionally model uncertainty parameters using probabilistic model look detail section 8.4. use numerical methods find good parameters fit data training methods thought hillclimbing approaches find maximum objective example maximum likeli hood. apply hillclimbing approaches use gradients described convention optimization minimize objectives. hence often extra minus sign machine learning objectives. chapter 5 implement numerical optimization approaches chap ter 7. mentioned chapter 1 interested learning model based data performs well future data. enough 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
258 models meet data model fit training data well predictor needs per form well unseen data. simulate behavior predictor future unseen data using crossvalidation section 8.2.4. see crossvalidation chapter achieve goal performing well unseen data need balance fitting well training data finding simple explanations phenomenon. tradeoff achieved us ing regularization section 8.2.3 adding prior section 8.3.2. philosophy considered neither induction deduction called abduction. according stanford encyclopedia philosophy abduction abduction process inference best explanation douven 2017. good movie title ai abduction. often need make highlevel modeling decisions struc ture predictor number components use class probability distributions consider. choice number components example hyperparameter choice af hyperparameter fect performance model significantly. problem choosing among different models called model selection describe model selection section 8.6. nonprobabilistic models model selection often done using nested crossvalidation described section 8.6.1. also nested crossvalidation use model selection choose hyperparameters model. remark. distinction parameters hyperparameters some arbitrary mostly driven distinction numerically optimized versus needs use search techniques. another way consider distinction consider parameters explicit parameters probabilistic model consider hyperparam eters higherlevel parameters parameters control distribution explicit parameters. following sections look three flavors machine learn ing empirical risk minimization section 8.2 principle maximum likelihood section 8.3 probabilistic modeling section 8.4. 8.2 empirical risk minimization mathematics belt posi tion introduce means learn. learning part machine learning boils estimating parameters based training data. section consider case predictor function consider case probabilistic models section 8.3. describe idea empirical risk minimization originally popularized proposal support vector machine described chapter 12. however general principles widely applicable allow us ask question learning without explicitly constructing probabilis tic models. four main design choices cover detail following subsections draft 20230215 mathematics machine learning. feedback
8.2 empirical risk minimization 259 section 8.2.1 set functions allow predictor take section 8.2.2 measure well predictor performs training data section 8.2.3 construct predictors training data performs well unseen test data section 8.2.4 procedure searching space mod els 8.2.1 hypothesis class functions assume given n examples xn rd corresponding scalar la bels yn r. consider supervised learning setting obtain pairs x1 y1 . . . xn yn. given data would like estimate predictor f θ rd r parametrized θ. hope able find good parameter θsuch fit data well is fxn θ yn n 1 . . . n . 8.3 section use notation ˆ yn fxn θ represent output predictor. remark. ease presentation describe empirical risk mini mization terms supervised learning where labels. simplifies definition hypothesis class loss function. also common machine learning choose parametrized class functions example affine functions. example 8.1 introduce problem ordinary leastsquares regression illustrate empirical risk minimization. comprehensive account regression given chapter 9. label yn realvalued popular choice function class predictors set affine functions. choose affine functions often referred linear functions machine learning. compact notation affine function concatenating addi tional unit feature x0 1 xn i.e. xn 1 x1 n x2 n . . . xd n . parameter vector correspondingly θ θ0 θ1 θ2 . . . θd allowing us write predictor linear function fxn θ θxn . 8.4 linear predictor equivalent affine model fxn θ θ0 x d1 θdxd n . 8.5 predictor takes vector features representing single example xn input produces realvalued output i.e. f rd1 r. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
260 models meet data previous figures chapter straight line predictor means assumed affine function. instead linear function may wish consider nonlinear func tions predictors. recent advances neural networks allow efficient computation complex nonlinear function classes. given class functions want search good predictor. move second ingredient empirical risk minimization measure well predictor fits training data. 8.2.2 loss function training consider label yn particular example corresponding pre diction ˆ yn make based xn. define means fit data well need specify loss function ℓyn ˆ yn takes ground loss function truth label prediction input produces nonnegative num ber referred loss representing much error made particular prediction. goal finding good parameter vector expression error often used mean loss. θis minimize average loss set n training examples. one assumption commonly made machine learning set examples x1 y1 . . . xn yn independent identically independent identically distributed distributed. word independent section 6.4.5 means two data points xi yi xj yj statistically depend other mean ing empirical mean good estimate population mean section 6.4.1. implies use empirical mean loss training data. given training set x1 y1 . . . xn yn training set introduce notation example matrix x x1 . . . xn rnd label vector y1 . . . ynrn. using matrix notation average loss given rempf x y 1 n n x n1 ℓyn ˆ yn 8.6 ˆ yn fxn θ. equation 8.6 called empirical risk de empirical risk pends three arguments predictor f data x y. general strategy learning called empirical risk minimization. empirical risk minimization example 8.2 leastsquares loss continuing example leastsquares regression specify measure cost making error training using squared loss ℓyn ˆ yn yn ˆ yn2. wish minimize empirical risk 8.6 draft 20230215 mathematics machine learning. feedback
8.2 empirical risk minimization 261 average losses data min θrd 1 n n x n1 yn fxn θ2 8.7 substituted predictor ˆ yn fxn θ. using choice linear predictor fxn θ θxn obtain optimization problem min θrd 1 n n x n1 yn θxn2 . 8.8 equation equivalently expressed matrix form min θrd 1 n y xθ 2 . 8.9 known leastsquares problem. exists closedform an leastsquares problem alytic solution solving normal equations discuss section 9.2. interested predictor performs well training data. instead seek predictor performs well has low risk unseen test data. formally interested finding predictor f with parameters fixed minimizes expected risk expected risk rtruef exyℓy fx 8.10 label fx prediction based example x. notation rtruef indicates true risk access infinite amount data. expectation infinite set another phrase commonly used expected risk population risk. possible data labels. two practical questions arise desire minimize expected risk address following two subsections change training procedure generalize well estimate expected risk finite data remark. many machine learning tasks specified associated performance measure e.g. accuracy prediction root mean squared error. performance measure could complex cost sensitive capture details particular application. principle de sign loss function empirical risk minimization correspond directly performance measure specified machine learning task. practice often mismatch design loss function performance measure. could due issues ease implementation efficiency optimization. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
262 models meet data 8.2.3 regularization reduce overfitting section describes addition empirical risk minimization al lows generalize well approximately minimizing expected risk. re call aim training machine learning predictor perform well unseen data i.e. predictor generalizes well. sim ulate unseen data holding proportion whole dataset. hold set referred test set. given sufficiently rich class test set even knowing performance predictor test set leaks information blum hardt 2015. functions predictor f essentially memorize training data obtain zero empirical risk. great minimize loss and therefore risk training data would expect predictor generalize well unseen data. practice finite set data hence split data training test set. training set used fit model test set not seen machine learning algorithm training used evaluate generalization performance. important user cycle back new round training observed test set. use subscripts train test denote training test sets respectively. revisit idea using finite dataset evaluate expected risk section 8.2.4. turns empirical risk minimization lead overfitting i.e. overfitting predictor fits closely training data general ize well new data mitchell 1997. general phenomenon hav ing small average loss training set large average loss test set tends occur little data complex hy pothesis class. particular predictor f with parameters fixed phenomenon overfitting occurs risk estimate train ing data rempf xtrain ytrain underestimates expected risk rtruef. since estimate expected risk rtruef using empirical risk test set rempf xtest ytest test risk much larger training risk indication overfitting. revisit idea overfitting section 8.3.3. therefore need somehow bias search minimizer empirical risk introducing penalty term makes harder optimizer return overly flexible predictor. machine learning penalty term referred regularization. regularization way regularization compromise accurate solution empirical risk minimization size complexity solution. example 8.3 regularized least squares regularization approach discourages complex extreme solu tions optimization problem. simplest regularization strategy draft 20230215 mathematics machine learning. feedback
8.2 empirical risk minimization 263 replace leastsquares problem min θ 1 n y xθ 2 . 8.11 previous example regularized problem adding penalty term involving θ min θ 1 n y xθ 2 λ θ 2 . 8.12 additional term θ 2 called regularizer parameter regularizer λ regularization parameter. regularization parameter trades regularization parameter minimizing loss training set magnitude pa rameters θ. often happens magnitude parameter values becomes relatively large run overfitting bishop 2006. regularization term sometimes called penalty term bi penalty term ases vector θ closer origin. idea regularization also appears probabilistic models prior probability parameters. recall section 6.6 posterior distribution form prior distribution prior likelihood need con jugate. revisit idea section 8.3.2. see chapter 12 idea regularizer equivalent idea large margin. 8.2.4 crossvalidation assess generalization performance mentioned previous section measure generalization error estimating applying predictor test data. data also sometimes referred validation set. validation set sub validation set set available training data keep aside. practical issue approach amount data limited ideally would use much data available train model. would require us keep validation set v small would lead noisy estimate with high variance predictive performance. one solu tion contradictory objectives large training set large validation set use crossvalidation. kfold crossvalidation effectively partitions crossvalidation data k chunks k 1 form training set r last chunk serves validation set v similar idea outlined previously. crossvalidation iterates ideally combinations assignments chunks r v see figure 8.2. procedure repeated k choices validation set performance model k runs averaged. partition dataset two sets rv overlap r v v validation set train model r. training assess performance predictor f 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
264 models meet data figure 8.2 kfold crossvalidation. dataset divided k 5 chunks k 1 serve training set blue one validation set orange hatch. training validation validation set v e.g. computing root mean square error rmse trained model validation set. precisely partition k training data rk produces predictor f k applied validation set vk compute empirical risk rf k vk. cycle possible partitionings validation training sets com pute average generalization error predictor. crossvalidation approximates expected generalization error evrf v 1 k k x k1 rf k vk 8.13 rf k vk risk e.g. rmse validation set vk predictor f k. approximation two sources first due finite training set results best possible f k second due finite validation set results inaccurate estimation risk rf k vk. potential disadvantage kfold crossvalidation computational cost training model k times bur densome training cost computationally expensive. practice often sufficient look direct parameters alone. example need explore multiple complexity parameters e.g. multiple regu larization parameters may direct parameters model. evaluating quality model depending hyperparameters may result number training runs exponential number model parameters. one use nested crossvalidation section 8.6.1 search good hyperparameters. however crossvalidation embarrassingly parallel problem i.e. lit embarrassingly parallel tle effort needed separate problem number parallel tasks. given sufficient computing resources e.g. cloud computing server farms crossvalidation require longer single performance assessment. section saw empirical risk minimization based following concepts hypothesis class functions loss function regularization. section 8.3 see effect using probability distribution replace idea loss functions regularization. draft 20230215 mathematics machine learning. feedback
8.3 parameter estimation 265 8.2.5 reading due fact original development empirical risk minimiza tion vapnik 1998 couched heavily theoretical language many subsequent developments theoretical. area study called statistical learning theory vapnik 1999 evgeniou et al. 2000 statistical learning theory hastie et al. 2001 von luxburg sch olkopf 2011. recent machine learning textbook builds theoretical foundations develops efficient learning algorithms shalevshwartz bendavid 2014. concept regularization roots solution illposed in verse problems neumaier 1998. approach presented called tikhonov regularization closely related constrained version tikhonov regularization called ivanov regularization. tikhonov regularization deep relation ships biasvariance tradeoff feature selection b uhlmann van de geer 2011. alternative crossvalidation bootstrap jackknife efron tibshirani 1993 davidson hinkley 1997 hall 1992. thinking empirical risk minimization section 8.2 probabil ity free incorrect. underlying unknown probability distri bution px y governs data generation. however approach empirical risk minimization agnostic choice distribution. contrast standard statistical approaches explicitly re quire knowledge px y. furthermore since distribution joint distribution examples x labels y labels non deterministic. contrast standard statistics need specify noise distribution labels y. 8.3 parameter estimation section 8.2 explicitly model problem using probability distributions. section see use probability distribu tions model uncertainty due observation process uncertainty parameters predictors. section 8.3.1 in troduce likelihood analogous concept loss functions section 8.2.2 empirical risk minimization. concept priors sec tion 8.3.2 analogous concept regularization section 8.2.3. 8.3.1 maximum likelihood estimation idea behind maximum likelihood estimation mle define func maximum likelihood estimation tion parameters enables us find model fits data well. estimation problem focused likelihood function likelihood precisely negative logarithm. data represented random variable x family probability densities px θ parametrized θ negative loglikelihood given negative loglikelihood 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
266 models meet data lxθ log px θ . 8.14 notation lxθ emphasizes fact parameter θ varying data x fixed. often drop reference x writing negative loglikelihood really function θ write lθ random variable representing uncertainty data clear context. let us interpret probability density px θ modeling fixed value θ. distribution models uncertainty data given parameter setting. given dataset x likelihood allows us express preferences different settings parameters θ choose setting likely generated data. complementary view consider data fixed because observed vary parameters θ lθ tell us tells us likely particular setting θ observations x. based second view maximum likelihood estimator gives us likely parameter θ set data. consider supervised learning setting obtain pairs x1 y1 . . . xn yn xn rd labels yn r. inter ested constructing predictor takes feature vector xn input produces prediction yn or something close it i.e. given vec tor xn want probability distribution label yn. words specify conditional probability distribution labels given examples particular parameter setting θ. example 8.4 first example often used specify conditional probability labels given examples gaussian distribution. words assume explain observation uncertainty independent gaussian noise refer section 6.5 zero mean εn n 0 σ2 . assume linear model x n θ used prediction. means specify gaussian likelihood example label pair xn yn pyn xn θ n yn x n θ σ2 . 8.15 illustration gaussian likelihood given parameter θ shown figure 8.1. see section 9.2 explicitly expand preceding expression terms gaussian distribution. assume set examples x1 y1 . . . xn yn independent independent identically distributed identically distributed i.i.d. word independent section 6.4.5 implies likelihood involving whole dataset y y1 . . . yn x x1 . . . xn factorizes product likelihoods draft 20230215 mathematics machine learning. feedback
8.3 parameter estimation 267 individual example py x θ n n1 pyn xn θ 8.16 pyn xn θ particular distribution which gaussian ex ample 8.4. expression identically distributed means term product 8.16 distribution share parameters. often easier optimization viewpoint compute functions decomposed sums simpler functions. hence machine learning often consider negative loglikelihood recall logab loga logb lθ log py x θ n x n1 log pyn xn θ . 8.17 temping interpret fact θ right condi tioning pynxn θ 8.15 hence interpreted observed fixed interpretation incorrect. negative loglikelihood lθ function θ. therefore find good parameter vector θ explains data x1 y1 . . . xn yn well minimize negative log likelihood lθ respect θ. remark. negative sign 8.17 historical artifact due convention want maximize likelihood numerical optimization literature tends study minimization functions. example 8.5 continuing example gaussian likelihoods 8.15 negative loglikelihood rewritten lθ n x n1 log pyn xn θ n x n1 log n yn x n θ σ2 8.18a n x n1 log 1 2πσ2 exp yn x n θ2 2σ2 8.18b n x n1 log exp yn x n θ2 2σ2 n x n1 log 1 2πσ2 8.18c 1 2σ2 n x n1 yn x n θ2 n x n1 log 1 2πσ2 . 8.18d σ given second term 8.18d constant minimizing lθ corresponds solving leastsquares problem compare 8.8 expressed first term. turns gaussian likelihoods resulting optimization 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
268 models meet data figure 8.2 given data maximum likelihood estimate parameters results black diagonal line. orange square shows value maximum likelihood prediction x 60. 0 10 20 30 40 50 60 70 80 x 0 25 50 75 100 125 150 figure 8.1 comparing predictions maximum likelihood estimate map estimate x 60. prior biases slope less steep intercept closer zero. example bias moves intercept closer zero actually increases slope. 0 10 20 30 40 50 60 70 80 x 0 25 50 75 100 125 150 mle map problem corresponding maximum likelihood estimation closed form solution. see details chapter 9. figure 8.2 shows regression dataset function induced maxi mumlikelihood parameters. maximum likelihood estimation may suffer overfitting section 8.3.3 analogous unregularized empirical risk minimization section 8.2.3. likelihood functions i.e. model noise nongaussian distributions maximum likelihood es timation may closedform analytic solution. case resort numerical optimization methods discussed chapter 7. 8.3.2 maximum posteriori estimation prior knowledge distribution parameters θ multiply additional term likelihood. additional term prior probability distribution parameters pθ. given prior draft 20230215 mathematics machine learning. feedback
8.3 parameter estimation 269 observing data x update distribution θ words represent fact specific knowledge θ observing data x bayes theorem discussed section 6.3 gives us principled tool update probability distribu tions random variables. allows us compute posterior distribution posterior pθ x the specific knowledge parameters θ general prior statements prior distribution pθ function px θ prior links parameters θ observed data x called likelihood likelihood pθ x px θpθ px . 8.19 recall interested finding parameter θ maximizes posterior. since distribution px depend θ ignore value denominator optimization obtain pθ x px θpθ . 8.20 preceding proportion relation hides density data px may difficult estimate. instead estimating minimum negative loglikelihood estimate minimum neg ative logposterior referred maximum posteriori estima maximum posteriori estimation tion map estimation. illustration effect adding zeromean map estimation gaussian prior shown figure 8.1. example 8.6 addition assumption gaussian likelihood previous exam ple assume parameter vector distributed multivariate gaussian zero mean i.e. pθ n 0 σ σ covari ance matrix section 6.5. note conjugate prior gaussian also gaussian section 6.6.1 therefore expect posterior distribution also gaussian. see details maximum posteriori estimation chapter 9. idea including prior knowledge good parameters lie widespread machine learning. alternative view saw section 8.2.3 idea regularization introduces addi tional term biases resulting parameters close origin. maximum posteriori estimation considered bridge non probabilistic probabilistic worlds explicitly acknowledges need prior distribution still produces point estimate parameters. remark. maximum likelihood estimate θml possesses following properties lehmann casella 1998 efron hastie 2016 asymptotic consistency mle converges true value 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
270 models meet data figure 8.1 model fitting. parametrized class mθ models optimize model parameters θ minimize distance true unknown model m. mθ mθ mθ0 limit infinitely many observations plus random error ap proximately normal. size samples necessary achieve properties quite large. errors variance decays 1n n number data points. especially small data regime maximum likelihood estimation lead overfitting. principle maximum likelihood estimation and maximum pos teriori estimation uses probabilistic modeling reason uncer tainty data model parameters. however yet taken probabilistic modeling full extent. section resulting train ing procedure still produces point estimate predictor i.e. training returns one single set parameter values represent best predic tor. section 8.4 take view parameter values also treated random variables instead estimating best val ues distribution use full parameter distribution making predictions. 8.3.3 model fitting consider setting given dataset interested fitting parametrized model data. talk fit ting typically mean optimizinglearning model parameters minimize loss function e.g. negative loglikelihood. maximum likelihood section 8.3.1 maximum posteriori estima tion section 8.3.2 already discussed two commonly used algorithms model fitting. parametrization model defines model class mθ operate. example linear regression setting may define relationship inputs x noisefree observations ax b θ a b model parameters. case model parameters θ describe family affine functions i.e. straight lines slope a offset 0 b. assume data comes draft 20230215 mathematics machine learning. feedback
8.3 parameter estimation 271 figure 8.2 fitting by maximum likelihood different model classes regression dataset. 4 2 0 2 4 x 4 2 0 2 4 training data mle a overfitting 4 2 0 2 4 x 4 2 0 2 4 training data mle b underfitting. 4 2 0 2 4 x 4 2 0 2 4 training data mle c fitting well. model unknown us. given training dataset optimize θ mθ close possible close ness defined objective function optimize e.g. squared loss training data. figure 8.1 illustrates setting small model class indicated circle mθ data generation model lies outside set considered models. begin parameter search mθ0. optimization i.e. obtain best pos sible parameters θ distinguish three different cases i overfitting ii underfitting iii fitting well. give highlevel intuition three concepts mean. roughly speaking overfitting refers situation para overfitting metrized model class rich model dataset generated i.e. mθ could model much complicated datasets. instance dataset generated linear function define mθ class seventhorder polynomials could model linear func tions also polynomials degree two three etc. models over fit typically large number parameters. observation often one way detect overfitting practice observe model low training risk high test risk cross validation section 8.2.4. make overly flexible model class mθ uses modeling power reduce training error. training data noisy therefore find useful signal noise itself. cause enormous prob lems predict away training data. figure 8.2a gives example overfitting context regression model pa rameters learned means maximum likelihood see section 8.3.1. discuss overfitting regression section 9.2.2. run underfitting encounter opposite problem underfitting model class mθ rich enough. example dataset generated sinusoidal function θ parametrizes straight lines best optimization procedure get us close true model. however still optimize parameters find best straight line models dataset. figure 8.2b shows example model underfits insufficiently flexible. models underfit typ ically parameters. third case parametrized model class right. then model fits well i.e. neither overfits underfits. means model class rich enough describe dataset given. figure 8.2c shows model fits given dataset fairly well. ideally 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
272 models meet data model class would want work since good generalization properties. practice often define rich model classes mθ many pa rameters deep neural networks. mitigate problem over fitting use regularization section 8.2.3 priors section 8.3.2. discuss choose model class section 8.6. 8.3.4 reading considering probabilistic models principle maximum likeli hood estimation generalizes idea leastsquares regression linear models discuss detail chapter 9. restricting predictor linear form additional nonlinear function φ applied output i.e. pynxn θ φθxn 8.21 consider models prediction tasks binary classification modeling count data mccullagh nelder 1989. alternative view consider likelihoods ex ponential family section 6.6. class models linear dependence parameters data potentially nonlin ear transformation φ called link function referred generalized link function generalized linear model linear models agresti 2002 chapter 4. maximum likelihood estimation rich history originally proposed sir ronald fisher 1930s. expand upon idea probabilistic model section 8.4. one debate among researchers use probabilistic models discussion bayesian fre quentist statistics. mentioned section 6.1.1 boils definition probability. recall section 6.1 one consider probability generalization by allowing uncertainty logical rea soning cheeseman 1985 jaynes 2003. method maximum like lihood estimation frequentist nature interested reader pointed efron hastie 2016 balanced view bayesian frequentist statistics. probabilistic models maximum likelihood esti mation may possible. reader referred advanced sta tistical textbooks e.g. casella berger 2002 approaches method moments mestimation estimating equations. 8.4 probabilistic modeling inference machine learning frequently concerned interpretation analysis data e.g. prediction future events decision making. make task tractable often build models describe generative process generates observed data. generative process draft 20230215 mathematics machine learning. feedback
8.4 probabilistic modeling inference 273 example describe outcome coinflip experiment heads tails two steps. first define parameter µ describes probability heads parameter bernoulli distri bution chapter 6 second sample outcome x head tail bernoulli distribution px µ berµ. parameter µ gives rise specific dataset x depends coin used. since µ un known advance never observed directly need mecha nisms learn something µ given observed outcomes coinflip experiments. following discuss probabilistic modeling used purpose. 8.4.1 probabilistic models probabilistic model specified joint distribution random variables. probabilistic models represent uncertain aspects experiment probability distributions. benefit using probabilistic models offer unified consistent set tools probability theory chapter 6 modeling inference prediction model selection. probabilistic modeling joint distribution px θ observed variables x hidden parameters θ central importance en capsulates information following prior likelihood product rule section 6.3. marginal likelihood px play important role model selection section 8.6 computed taking joint dis tribution integrating parameters sum rule section 6.3. posterior obtained dividing joint marginal likelihood. joint distribution property. therefore probabilistic model specified joint distribution random variables. 8.4.2 bayesian inference parameter estimation phrased optimization problem. key task machine learning take model data uncover values models hidden variables θ given observed variables x. section 8.3.1 already discussed two ways estimating model parameters θ using maximum likelihood maximum posteriori esti mation. cases obtain singlebest value θ key algorithmic problem parameter estimation solving optimization problem. point estimates θare known use make predictions. specifically predictive distribution px θ use θin likelihood function. discussed section 6.3 focusing solely statistic pos terior distribution such parameter θthat maximizes poste rior leads loss information critical system 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
274 models meet data uses prediction px θ make decisions. decisionmaking systems typically different objective functions likelihood bayesian inference learning distribution random variables. squarederror loss misclassification error. therefore full posterior distribution around extremely useful leads robust decisions. bayesian inference finding posterior distri bayesian inference bution gelman et al. 2004. dataset x parameter prior pθ likelihood function posterior pθ x px θpθ px px z px θpθdθ 8.22 obtained applying bayes theorem. key idea exploit bayes bayesian inference inverts relationship parameters data. theorem invert relationship parameters θ data x given likelihood obtain posterior distribution pθ x. implication posterior distribution parameters used propagate uncertainty parameters data. specifically distribution pθ parameters predictions px z px θpθdθ eθpx θ 8.23 longer depend model parameters θ marginalizedintegrated out. equation 8.23 reveals prediction average plausible parameter values θ plausibility encapsulated parameter distribution pθ. discussed parameter estimation section 8.3 bayesian in ference here let us compare two approaches learning. parameter estimation via maximum likelihood map estimation yields consistent point estimate θof parameters key computational problem solved optimization. contrast bayesian inference yields pos terior distribution key computational problem solved integration. predictions point estimates straightforward whereas predictions bayesian framework require solving another integration problem see 8.23. however bayesian inference gives us principled way incorporate prior knowledge account side information incorporate structural knowledge easily done context parameter estimation. moreover propagation parameter uncertainty prediction valuable decisionmaking systems risk assessment exploration context dataefficient learn ing deisenroth et al. 2015 kamthe deisenroth 2018. bayesian inference mathematically principled framework learning parameters making predictions prac tical challenges come integration problems need solve see 8.22 8.23. specifically choose conjugate prior parameters section 6.6.1 integrals 8.22 8.23 analytically tractable cannot compute pos draft 20230215 mathematics machine learning. feedback
8.4 probabilistic modeling inference 275 terior predictions marginal likelihood closed form. cases need resort approximations. here use stochas tic approximations markov chain monte carlo mcmc gilks et al. 1996 deterministic approximations laplace ap proximation bishop 2006 barber 2012 murphy 2012 variational in ference jordan et al. 1999 blei et al. 2017 expectation propaga tion minka 2001a. despite challenges bayesian inference successfully ap plied variety problems including largescale topic modeling hoff man et al. 2013 clickthroughrate prediction graepel et al. 2010 dataefficient reinforcement learning control systems deisenroth et al. 2015 online ranking systems herbrich et al. 2007 largescale rec ommender systems. generic tools bayesian optimiza tion brochu et al. 2009 snoek et al. 2012 shahriari et al. 2016 useful ingredients efficient search meta parameters models algorithms. remark. machine learning literature somewhat ar bitrary separation random variables parameters. parameters estimated e.g. via maximum likelihood variables usually marginalized out. book strict sep aration because principle place prior parameter integrate out would turn parameter random vari able according aforementioned separation. 8.4.3 latentvariable models practice sometimes useful additional latent variables z latent variable besides model parameters θ part model moustaki et al. 2015. latent variables different model parameters θ parametrize model explicitly. latent variables may describe datagenerating process thereby contributing inter pretability model. also often simplify structure model allow us define simpler richer model structures. sim plification model structure often goes hand hand smaller number model parameters paquet 2008 murphy 2012. learning latentvariable models at least via maximum likelihood done principled way using expectation maximization em algorithm demp ster et al. 1977 bishop 2006. examples latent variables helpful principal component analysis dimensionality reduc tion chapter 10 gaussian mixture models density estimation chap ter 11 hidden markov models maybeck 1979 dynamical systems ghahramani roweis 1999 ljung 1999 timeseries modeling meta learning task generalization hausman et al. 2018 sæ mundsson et al. 2018. although introduction latent variables 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
276 models meet data may make model structure generative process easier learning latentvariable models generally hard see chapter 11. since latentvariable models also allow us define process generates data parameters let us look generative pro cess. denoting data x model parameters θ latent vari ables z obtain conditional distribution px z θ 8.24 allows us generate data model parameters latent vari ables. given z latent variables place prior pz them. models discussed previously models latent variables used parameter learning inference within frameworks discussed sections 8.3 8.4.2. facilitate learning e.g. means maximum likelihood estimation bayesian inference fol low twostep procedure. first compute likelihood px θ model depend latent variables. second use likelihood parameter estimation bayesian inference use exactly expressions sections 8.3 8.4.2 respectively. since likelihood function px θ predictive distribution data given model parameters need marginalize latent variables px θ z px z θpzdz 8.25 px z θ given 8.24 pz prior latent variables. note likelihood must depend latent variables likelihood function data model parameters independent latent variables. z function data x model parameters θ. likelihood 8.25 directly allows parameter estimation via maximum likelihood. map estimation also straightforward ad ditional prior model parameters θ discussed section 8.3.2. moreover likelihood 8.25 bayesian inference section 8.4.2 latentvariable model works usual way place prior pθ model parameters use bayes theorem obtain posterior distribution pθ x px θpθ px 8.26 model parameters given dataset x . posterior 8.26 used predictions within bayesian inference framework see 8.23. one challenge latentvariable model like lihood px θ requires marginalization latent variables ac cording 8.25. except choose conjugate prior pz px z θ marginalization 8.25 analytically tractable need resort approximations bishop 2006 paquet 2008 mur phy 2012 moustaki et al. 2015. draft 20230215 mathematics machine learning. feedback
8.4 probabilistic modeling inference 277 similar parameter posterior 8.26 compute posterior latent variables according pz x px zpz px px z z px z θpθdθ 8.27 pz prior latent variables px z requires us integrate model parameters θ. given difficulty solving integrals analytically clear mar ginalizing latent variables model parameters time possible general bishop 2006 murphy 2012. quantity easier compute posterior distribution latent variables conditioned model parameters i.e. pz x θ px z θpz px θ 8.28 pz prior latent variables px z θ given 8.24. chapters 10 11 derive likelihood functions pca gaussian mixture models respectively. moreover compute poste rior distributions 8.28 latent variables pca gaussian mixture models. remark. following chapters may drawing clear distinction latent variables z uncertain model parameters θ call model parameters latent hidden well unobserved. chapters 10 11 use latent variables z pay attention difference two different types hidden variables model parameters θ latent variables z. exploit fact elements probabilistic model random variables define unified language representing them. section 8.5 see concise graphical language representing structure probabilistic models. use graphical language describe probabilistic models subsequent chapters. 8.4.4 reading probabilistic models machine learning bishop 2006 barber 2012 murphy 2012 provide way users capture uncertainty data predictive models principled fashion. ghahramani 2015 presents short review probabilistic models machine learning. given proba bilistic model may lucky enough able compute parameters interest analytically. however general analytic solutions rare computational methods sampling gilks et al. 1996 brooks et al. 2011 variational inference jordan et al. 1999 blei et al. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
278 models meet data 2017 used. moustaki et al. 2015 paquet 2008 provide good overview bayesian inference latentvariable models. recent years several programming languages proposed aim treat variables defined software random variables corresponding probability distributions. objective able write complex functions probability distributions hood compiler automatically takes care rules bayesian inference. rapidly changing field called probabilistic programming. probabilistic programming 8.5 directed graphical models section introduce graphical language specifying prob abilistic model called directed graphical model. provides compact directed graphical model succinct way specify probabilistic models allows reader visually parse dependencies random variables. graphical model visually captures way joint distribution random variables decomposed product factors depending subset variables. section 8.4 identified joint distri bution probabilistic model key quantity interest comprises information prior likelihood posterior. however joint distribution quite complicated directed graphical models also known bayesian networks. tell us anything structural properties probabilis tic model. example joint distribution pa b c tell us anything independence relations. point graphical models come play. section relies concepts independence conditional independence described section 6.4.5. graphical model nodes random variables. figure 8.3a graphical model nodes represent random variables a b c. edges represent probabilistic relations variables e.g. conditional probabilities. remark. every distribution represented particular choice graphical model. discussion found bishop 2006. probabilistic graphical models convenient properties simple way visualize structure probabilistic model. used design motivate new kinds statistical models. inspection graph alone gives us insight properties e.g. con ditional independence. complex computations inference learning statistical models expressed terms graphical manipulations. 8.5.1 graph semantics directed graphical modelsbayesian networks method representing directed graphical modelbayesian network conditional dependencies probabilistic model. provide visual draft 20230215 mathematics machine learning. feedback
8.5 directed graphical models 279 description conditional probabilities hence providing simple lan guage describing complex interdependence. modular description additional assumptions arrows used indicate causal relationships pearl 2009. also entails computational simplification. directed links arrows two nodes random variables indicate conditional probabilities. ex ample arrow b figure 8.3a gives conditional probability pb a b given a. figure 8.3 examples directed graphical models. b c a fully connected. x1 x2 x3 x4 x5 b fully connected. directed graphical models derived joint distributions know something factorization. example 8.7 consider joint distribution pa b c pc a bpb apa 8.29 three random variables a b c. factorization joint distribution 8.29 tells us something relationship random variables c depends directly b. b depends directly a. depends neither b c. factorization 8.29 obtain directed graphical model figure 8.3a. general construct corresponding directed graphical model factorized joint distribution follows 1. create node random variables. 2. conditional distribution add directed link arrow graph nodes corresponding variables distribution conditioned. graph layout depends factorization joint distribution. graph layout depends choice factorization joint dis tribution. discussed get known factorization joint dis tribution corresponding directed graphical model. now 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
280 models meet data exactly opposite describe extract joint distribution set random variables given graphical model. example 8.8 looking graphical model figure 8.3b exploit two proper ties joint distribution px1 . . . x5 seek product set conditionals one node graph. particular example need five conditionals. conditional depends parents corresponding node graph. example x4 conditioned x2. two properties yield desired factorization joint distribu tion px1 x2 x3 x4 x5 px1px5px2 x5px3 x1 x2px4 x2 . 8.30 general joint distribution px px1 . . . xk given px k k1 pxk pak 8.31 pak means the parent nodes xk. parent nodes xk nodes arrows pointing xk. conclude subsection concrete example coinflip experiment. consider bernoulli experiment example 6.8 probability outcome x experiment heads px µ berµ . 8.32 repeat experiment n times observe outcomes x1 . . . xn obtain joint distribution px1 . . . xn µ n n1 pxn µ . 8.33 expression righthand side product bernoulli distribu tions individual outcome experiments indepen dent. recall section 6.4.5 statistical independence means distribution factorizes. write graphical model set ting make distinction unobservedlatent variables observed variables. graphically observed variables denoted shaded nodes obtain graphical model figure 8.4a. see single parameter µ xn n 1 . . . n outcomes xn identically distributed. compact equivalent graphical model setting given figure 8.4b use draft 20230215 mathematics machine learning. feedback
8.5 directed graphical models 281 figure 8.4 graphical models repeated bernoulli experiment. µ x1 xn a version xn explicit. µ xn n 1 . . . n b version plate notation. µ xn β α n 1 . . . n c hyperparameters α β latent µ. plate notation. plate box repeats everything inside in case plate observations xn n times. therefore graphical models equiv alent plate notation compact. graphical models immedi ately allow us place hyperprior µ. hyperprior second layer hyperprior prior distributions parameters first layer priors. fig ure 8.4c places betaα β prior latent variable µ. treat α β deterministic parameters i.e. random variables omit circle around it. 8.5.2 conditional independence dseparation directed graphical models allow us find conditional independence sec tion 6.4.5 relationship properties joint distribution looking graph. concept called dseparation pearl 1988 key this. dseparation consider general directed graph a b c arbitrary nonin tersecting sets nodes whose union may smaller complete set nodes graph. wish ascertain whether particular con ditional independence statement a conditionally independent b given c denoted b c 8.34 implied given directed acyclic graph. so consider possible trails paths ignore direction arrows node nodes b. path said blocked includes node either following true arrows path meet either head tail tail tail node node set c. arrows meet head head node neither node descendants set c. paths blocked said dseparated b c joint distribution variables graph satisfy b c. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
282 models meet data figure 8.1 three types graphical models a directed graphical models bayesian networks b undirected graphical models markov random fields c factor graphs. b c a directed graphical model b c b undirected graphical model b c c factor graph example 8.9 conditional independence figure 8.1 dseparation example. b c e consider graphical model figure 8.1. visual inspection gives us b d a c 8.35 c b 8.36 b d c 8.37 c b e 8.38 directed graphical models allow compact representation proba bilistic models see examples directed graphical models chapters 9 10 11. representation along concept con ditional independence allows us factorize respective probabilistic models expressions easier optimize. graphical representation probabilistic model allows us visually see impact design choices made structure model. often need make highlevel assumptions structure model. modeling assumptions hyperparameters affect prediction performance cannot selected directly using approaches seen far. discuss different ways choose structure section 8.6. draft 20230215 mathematics machine learning. feedback
8.6 model selection 283 8.5.3 reading introduction probabilistic graphical models found bishop 2006 chapter 8 extensive description different applica tions corresponding algorithmic implications found book koller friedman 2009. three main types probabilistic graphical models directed graphical model directed graphical models bayesian networks see figure 8.1a bayesian network undirected graphical model undirected graphical models markov random fields see figure 8.1b markov random field factor graph factor graphs see figure 8.1c graphical models allow graphbased algorithms inference learning e.g. via local message passing. applications range rank ing online games herbrich et al. 2007 computer vision e.g. image segmentation semantic labeling image denoising image restora tion kittler f oglein 1984 sucar gillies 1994 shotton et al. 2006 szeliski et al. 2008 coding theory mceliece et al. 1998 solv ing linear equation systems shental et al. 2008 iterative bayesian state estimation signal processing bickson et al. 2007 deisenroth mohamed 2012. one topic particularly important real applications discuss book idea structured prediction bakir et al. 2007 nowozin et al. 2014 allows machine learning models tackle predictions structured example sequences trees graphs. popularity neural network models allowed flex ible probabilistic models used resulting many useful applica tions structured models goodfellow et al. 2016 chapter 16. recent years renewed interest graphical models due applications causal inference pearl 2009 imbens rubin 2015 peters et al. 2017 rosenbaum 2017. 8.6 model selection machine learning often need make highlevel modeling decisions critically influence performance model. choices make e.g. functional form likelihood influence number type free parameters model thereby also flexibility expressivity model. complex models flexible polynomial a0 a1xa2x2 also describe linear functions setting a2 0 i.e. strictly expressive firstorder polynomial. sense used describe datasets. instance polynomial degree 1 a line a0 a1x used describe linear relations inputs x observations y. polynomial degree 2 additionally describe quadratic relationships inputs observations. one would think flexible models generally preferable simple models expressive. general problem 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
284 models meet data figure 8.1 nested crossvalidation. perform two levels kfold crossvalidation. labeled data training data test data train model validation training time use training set evaluate performance model learn parameters. however per formance training set really interested in. section 8.3 seen maximum likelihood estimation lead overfitting especially training dataset small. ideally model also works well test set which available training time. therefore need mechanisms assessing model generalizes unseen test data. model selection concerned exactly problem. 8.6.1 nested crossvalidation already seen approach crossvalidation section 8.2.4 used model selection. recall crossvalidation provides estimate generalization error repeatedly splitting dataset training validation sets. apply idea one time i.e. split perform another round crossvalidation. sometimes referred nested crossvalidation see figure 8.1. inner nested crossvalidation level used estimate performance particular choice model hyperparameter internal validation set. outer level used estimate generalization performance best choice model chosen inner loop. test different model hyperparameter choices inner loop. distinguish two levels set used estimate generalization performance often called test set set used test set choosing best model called validation set. inner loop validation set estimates expected value generalization error given model 8.39 approximating using empirical error validation set i.e. standard error defined σ k k number experiments σ standard deviation risk experiment. evrv m 1 k k x k1 rvk m 8.39 rv m empirical risk e.g. root mean square error validation set v model m. repeat procedure models choose model performs best. note crossvalidation gives us expected generalization error also obtain high order statistics e.g. standard error estimate uncertain draft 20230215 mathematics machine learning. feedback
8.6 model selection 285 figure 8.1 bayesian inference embodies occams razor. horizontal axis describes space possible datasets d. evidence vertical axis evaluates well model predicts available data. since pd mi needs integrate 1 choose model greatest evidence. adapted mackay 2003. evidence c pd m1 pd m2 mean estimate is. model chosen evaluate final performance test set. 8.6.2 bayesian model selection many approaches model selection covered section. generally attempt trade model complexity data fit. assume simpler models less prone overfitting complex models hence objective model selection find simplest model explains data reasonably well. concept also known occams razor. occams razor remark. treat model selection hypothesis testing problem looking simplest hypothesis consistent data mur phy 2012. one may consider placing prior models favors simpler models. however necessary this automatic occams razor quantitatively embodied application bayesian probability smith spiegelhalter 1980 jefferys berger 1992 mackay 1992. fig ure 8.1 adapted mackay 2003 gives us basic intuition complex expressive models may turn less probable choice modeling given dataset d. let us think horizontal axis predictions quantified normalized probability distribution d i.e. needs integratesum 1. representing space possible datasets d. interested posterior probability pmi d model mi given data d employ bayes theorem. assuming uniform prior pm mod els bayes theorem rewards models proportion much pre dicted data occurred. prediction data given model mi pd mi called evidence mi. simple model m1 evidence predict small number datasets shown pd m1 powerful model m2 has e.g. free parameters m1 able 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
286 models meet data predict greater variety datasets. means however m2 predict datasets region c well m1. suppose equal prior probabilities assigned two models. then dataset falls region c less powerful model m1 probable model. earlier chapter argued models need able explain data i.e. way generate data given model. furthermore model appropriately learned data expect generated data similar empirical data. this helpful phrase model selection hierarchical inference problem allows us compute posterior distribution models. let us consider finite number models m1 . . . mk model mk possesses parameters θk. bayesian model selection bayesian model selection place prior pm set models. corresponding generative generative process process allows us generate data model figure 8.2 illustration hierarchical generative process bayesian model selection. place prior pm set models. model distribution pθ m corresponding model parameters used generate data d. θ mk pm 8.40 θk pθ mk 8.41 pd θk 8.42 illustrated figure 8.2. given training set d apply bayes the orem compute posterior distribution models pmk d pmkpd mk . 8.43 note posterior longer depends model parameters θk integrated bayesian setting since pd mk z pd θkpθk mkdθk 8.44 pθk mk prior distribution model parameters θk model mk. term 8.44 referred model evidence marginal model evidence marginal likelihood likelihood. posterior 8.43 determine map estimate arg max mk pmk d . 8.45 uniform prior pmk 1 k gives every model equal prior probability determining map estimate models amounts pick ing model maximizes model evidence 8.44. remark likelihood marginal likelihood. important differences likelihood marginal likelihood evidence likelihood prone overfitting marginal likelihood typ ically model parameters marginalized i.e. longer fit parameters. furthermore marginal likeli hood automatically embodies tradeoff model complexity data fit occams razor. draft 20230215 mathematics machine learning. feedback
8.6 model selection 287 8.6.3 bayes factors model comparison consider problem comparing two probabilistic models m1 m2 given dataset d. compute posteriors pm1 d pm2 d compute ratio posteriors pm1 d pm2 d z posterior odds pd m1pm1 pd pd m2pm2 pd pm1 pm2 z prior odds pd m1 pd m2 z bayes factor . 8.46 ratio posteriors also called posterior odds. first frac posterior odds tion righthand side 8.46 prior odds measures much prior odds prior initial beliefs favor m1 m2. ratio marginal like lihoods second fraction righthandside called bayes factor bayes factor measures well data predicted m1 compared m2. remark. jeffreyslindley paradox states bayes factor always jeffreyslindley paradox favors simpler model since probability data complex model diffuse prior small murphy 2012. here diffuse prior refers prior favor specific models i.e. many models priori plausible prior. choose uniform prior models prior odds term 8.46 1 i.e. posterior odds ratio marginal likelihoods bayes factor pd m1 pd m2 . 8.47 bayes factor greater 1 choose model m1 otherwise model m2. similar way frequentist statistics guidelines size ratio one consider significance result jeffreys 1961. remark computing marginal likelihood. marginal likelihood plays important role model selection need compute bayes factors 8.46 posterior distributions models 8.43. unfortunately computing marginal likelihood requires us solve integral 8.44. integration generally analytically intractable resort approximation techniques e.g. numerical integration stoer burlirsch 2002 stochastic approximations using monte carlo murphy 2012 bayesian monte carlo techniques ohagan 1991 rasmussen ghahramani 2003. however special cases solve it. section 6.6.1 discussed conjugate models. choose conjugate parameter prior pθ compute marginal likelihood closed form. chap ter 9 exactly context linear regression. seen brief introduction basic concepts machine learning chapter. rest part book see 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
288 models meet data three different flavors learning sections 8.2 8.3 8.4 applied four pillars machine learning regression dimensionality reduction density estimation classification. 8.6.4 reading mentioned start section highlevel modeling choices influence performance model. examples include following degree polynomial regression setting number components mixture model network architecture deep neural network type kernel support vector machine dimensionality latent space pca learning rate schedule optimization algorithm parametric models number parameters often related complexity model class. rasmussen ghahramani 2001 showed automatic occams razor necessarily penalize number parameters model active terms complexity functions. also showed automatic occams razor also holds bayesian nonparametric models many parameters e.g. gaussian processes. focus maximum likelihood estimate exist number heuristics model selection discourage overfitting. called information criteria choose model largest value. akaike information criterion aic akaike 1974 akaike information criterion log px θ m 8.48 corrects bias maximum likelihood estimator addition penalty term compensate overfitting complex models lots parameters. here number model parameters. aic estimates relative information lost given model. bayesian information criterion bic schwarz 1978 bayesian information criterion log px log z px θpθdθ log px θ 1 2m log n 8.49 used exponential family distributions. here n number data points number parameters. bic penalizes model complexity heavily aic. draft 20230215 mathematics machine learning. feedback
9 linear regression following apply mathematical concepts chap ters 2 5 6 7 solve linear regression curve fitting problems. regression aim find function f maps inputs x rd corre regression sponding function values fx r. assume given set train ing inputs xn corresponding noisy observations yn fxnϵ ϵ i.i.d. random variable describes measurementobservation noise potentially unmodeled processes which consider chapter. throughout chapter assume zeromean gaussian noise. task find function models training data generalizes well predicting function values input locations part training data see chapter 8. il lustration regression problem given figure 9.1. typical regression setting given figure 9.1a input values xn observe noisy function values yn fxn ϵ. task infer function f generated data generalizes well function values new input locations. possible solution given figure 9.1b also show three distributions centered function values fx represent noise data. regression fundamental problem machine learning regres sion problems appear diverse range research areas applica figure 9.1 a dataset b possible solution regression problem. 4 2 0 2 4 x 0.4 0.2 0.0 0.2 0.4 a regression problem observed noisy func tion values wish infer underlying function generated data. 4 2 0 2 4 x 0.4 0.2 0.0 0.2 0.4 b regression solution possible function could generated data blue indication measurement noise function value corresponding in puts orange distributions. 289 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.
290 linear regression tions including timeseries analysis e.g. system identification control robotics e.g. reinforcement learning forwardinverse model learn ing optimization e.g. line searches global optimization deep learning applications e.g. computer games speechtotext translation image recognition automatic video annotation. regression also key ingredient classification algorithms. finding regression function re quires solving variety problems including following choice model type parametrization regres sion function. given dataset function classes e.g. polynomi normally type noise could also model choice fix noise gaussian chapter. als good candidates modeling data particular parametrization e.g. degree polynomial choose model selection discussed section 8.6 allows us compare var ious models find simplest model explains training data reasonably well. finding good parameters. chosen model regression function find good model parameters here need look different lossobjective functions they determine good fit is optimization algorithms allow us minimize loss. overfitting model selection. overfitting problem regression function fits training data too well gen eralize unseen test data. overfitting typically occurs underly ing model or parametrization overly flexible expressive see section 8.6. look underlying reasons discuss ways mitigate effect overfitting context linear regression. relationship loss functions parameter priors. loss func tions optimization objectives often motivated induced prob abilistic models. look connection loss functions underlying prior assumptions induce losses. uncertainty modeling. practical setting access finite potentially large amount training data selecting model class corresponding parameters. given finite amount training data cover possible scenarios may want describe remaining parameter uncertainty obtain mea sure confidence models prediction test time smaller training set important uncertainty modeling. consistent mod eling uncertainty equips model predictions confidence bounds. following using mathematical tools chap ters 3 5 6 7 solve linear regression problems. discuss maximum likelihood maximum posteriori map estimation find optimal model parameters. using parameter estimates brief look generalization errors overfitting. toward end chapter discuss bayesian linear regression allows us reason model parameters higher level thereby removing problems encountered maximum likelihood map estimation. draft 20230215 mathematics machine learning. feedback
9.1 problem formulation 291 9.1 problem formulation presence observation noise adopt probabilis tic approach explicitly model noise using likelihood function. specifically throughout chapter consider regression prob lem likelihood function py x n y fx σ2 . 9.1 here x rd inputs r noisy function values targets. 9.1 functional relationship x given fx ϵ 9.2 ϵ n 0 σ2 independent identically distributed i.i.d. gaus sian measurement noise mean 0 variance σ2. objective find function close similar unknown function f generated data generalizes well. chapter focus parametric models i.e. choose para metrized function find parameters θ work well modeling data. time being assume noise variance σ2 known focus learning model parameters θ. linear regression consider special case parameters θ appear linearly model. example linear regression given py x θ n y xθ σ2 9.3 y xθ ϵ ϵ n 0 σ2 9.4 θ rd parameters seek. class functions de scribed 9.4 straight lines pass origin. 9.4 chose parametrization fx xθ. dirac delta delta function zero everywhere except single point integral 1. considered gaussian limit σ2 0. likelihood 9.3 probability density function evalu likelihood ated xθ. note source uncertainty originates observation noise as x θ assumed known 9.3. without ob servation noise relationship x would deterministic 9.3 would dirac delta. example 9.1 x θ r linear regression model 9.4 describes straight lines linear functions parameter θ slope line. fig ure 9.2a shows example functions different values θ. linear regression refers models linear parameters. linear regression model 9.39.4 linear pa rameters also linear inputs x. figure 9.2a shows examples functions. see later ϕxθ nonlinear trans formations ϕ also linear regression model linear regression 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
292 linear regression figure 9.2 linear regression example. a example functions fall category b training set c maximum likelihood estimate. 10 0 10 x 20 0 20 a example functions straight lines described us ing linear model 9.4. 10 5 0 5 10 x 10 0 10 b training set. 10 5 0 5 10 x 10 0 10 c maximum likelihood esti mate. refers models linear parameters i.e. models de scribe function linear combination input features. here fea ture representation ϕx inputs x. following discuss detail find good pa rameters θ evaluate whether parameter set works well. time being assume noise variance σ2 known. 9.2 parameter estimation consider linear regression setting 9.4 assume given training set x1 y1 . . . xn yn consisting n inputs xn training set rd corresponding observationstargets yn r n 1 . . . n. figure 9.3 probabilistic graphical model linear regression. observed random variables shaded deterministic known values without circles. θ yn σ xn n 1 . . . n corresponding graphical model given figure 9.3. note yi yj conditionally independent given respective inputs xi xj likelihood factorizes according py x θ py1 . . . yn x1 . . . xn θ 9.5a n n1 pyn xn θ n n1 n yn x n θ σ2 9.5b defined x x1 . . . xn y1 . . . yn sets training inputs corresponding targets respectively. likelihood factors pyn xn θ gaussian due noise distribution see 9.3. following discuss find optimal parameters θ rd linear regression model 9.4. parameters θare found predict function values using parameter estimate 9.4 arbitrary test input xthe distribution corre sponding target yis py x θ n y x θ σ2 . 9.6 following look parameter estimation maxi mizing likelihood topic already covered degree section 8.3. draft 20230215 mathematics machine learning. feedback
9.2 parameter estimation 293 9.2.1 maximum likelihood estimation widely used approach finding desired parameters θml maximum maximum likelihood estimation likelihood estimation find parameters θml maximize likelihood 9.5b. intuitively maximizing likelihood means maximiz maximizing likelihood means maximizing predictive distribution training data given parameters. ing predictive distribution training data given model param eters. obtain maximum likelihood parameters θml arg max θ py x θ . 9.7 likelihood probability distribution parameters. remark. likelihood py x θ probability distribution θ simply function parameters θ integrate 1 i.e. unnormalized may even integrable respect θ. however likelihood 9.7 normalized probability distribution y. find desired parameters θml maximize likelihood typically perform gradient ascent or gradient descent negative likelihood. case linear regression consider here however since logarithm strictly monotonically increasing function optimum function f identical optimum log f. closedform solution exists makes iterative gradient descent un necessary. practice instead maximizing likelihood directly apply logtransformation likelihood function minimize negative loglikelihood. remark logtransformation. since likelihood 9.5b product n gaussian distributions logtransformation useful since a suffer numerical underflow b differentiation rules turn simpler. specifically numerical underflow prob lem multiply n probabilities n number data points since cannot represent small numbers 10256. furthermore logtransform turn product sum log probabilities corresponding gradient sum individual gradients instead repeated application product rule 5.46 compute gradient product n terms. find optimal parameters θml linear regression problem minimize negative loglikelihood log py x θ log n n1 pyn xn θ n x n1 log pyn xn θ 9.8 exploited likelihood 9.5b factorizes number data points due independence assumption training set. linear regression model 9.4 likelihood gaussian due gaussian additive noise term arrive log pyn xn θ 1 2σ2 yn x n θ2 const 9.9 constant includes terms independent θ. using 9.9 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
294 linear regression negative loglikelihood 9.8 obtain ignoring constant terms lθ 1 2σ2 n x n1 yn x n θ2 9.10a 1 2σ2 y xθy xθ 1 2σ2 y xθ 2 9.10b define design matrix x x1 . . . xnrnd negative loglikelihood function also called error function. design matrix collection training inputs y1 . . . ynrn vector collects training targets. note nth row design matrix x corresponds training input xn. 9.10b used fact squared error often used measure distance. sum squared errors observations yn corresponding model prediction x n θ equals squared distance xθ. recall section 3.1 x2 xx choose dot product inner product. 9.10b concrete form negative loglikelihood function need optimize. immediately see 9.10b quadratic θ. means find unique global solution θml mini mizing negative loglikelihood l. find global optimum computing gradient l setting 0 solving θ. using results chapter 5 compute gradient l respect parameters dl dθ dθ 1 2σ2 y xθy xθ 9.11a 1 2σ2 dθ yy 2yxθ θxxθ 9.11b 1 σ2 yx θxx r1d . 9.11c maximum likelihood estimator θml solves dl dθ 0necessary opti mality condition obtain ignoring possibility duplicate data points rkx n d i.e. parameters data points. dl dθ 09.11c θ mlxx yx 9.12a θ ml yxxx1 9.12b θml xx1xy . 9.12c could rightmultiply first equation xx1 xx positive definite rkx d rkx denotes rank x. remark. setting gradient 0is necessary sufficient condition obtain global minimum since hessian 2 θlθ xx rdd positive definite. remark. maximum likelihood solution 9.12c requires us solve system linear equations form aθ b xx b xy. draft 20230215 mathematics machine learning. feedback
9.2 parameter estimation 295 example 9.2 fitting lines let us look figure 9.2 aim fit straight line fx θx θ unknown slope dataset using maximum likelihood estimation. examples functions model class straight lines shown figure 9.2a. dataset shown figure 9.2b find maximum likelihood estimate slope parameter θ using 9.12c obtain maximum likelihood linear function figure 9.2c. maximum likelihood estimation features far considered linear regression setting described 9.4 allowed us fit straight lines data using maximum likelihood estimation. however straight lines sufficiently expressive linear regression refers linearin theparameters regression models inputs undergo nonlinear transformation. comes fitting interesting data. fortunately linear regression offers us way fit nonlinear functions within linear regression framework since linear regression refers linear parameters perform arbitrary nonlinear transformation ϕx inputs x linearly combine components transformation. corre sponding linear regression model py x θ n y ϕxθ σ2 y ϕxθ ϵ k1 x k0 θkϕkx ϵ 9.13 ϕ rd rk nonlinear transformation inputs x ϕk rd r kth component feature vector ϕ. note feature vector model parameters θ still appear linearly. example 9.3 polynomial regression concerned regression problem ϕxθϵ x r θ rk. transformation often used context ϕx ϕ0x ϕ1x . . . ϕk1x 1 x x2 x3 . . . xk1 rk . 9.14 means lift original onedimensional input space kdimensional feature space consisting monomials xk k 0 . . . k 1. features model polynomials degree k1 within framework linear regression polynomial degree 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
296 linear regression k 1 fx k1 x k0 θkxk ϕxθ 9.15 ϕ defined 9.14 θ θ0 . . . θk1rk contains linear parameters θk. let us look maximum likelihood estimation param eters θ linear regression model 9.13. consider training inputs xn rd targets yn r n 1 . . . n define feature matrix feature matrix design matrix design matrix φ ϕx1 . . . ϕxn ϕ0x1 ϕk1x1 ϕ0x2 ϕk1x2 . . . . . . ϕ0xn ϕk1xn rnk 9.16 φij ϕjxi ϕj rd r. example 9.4 feature matrix secondorder polynomials secondorder polynomial n training points xn r n 1 . . . n feature matrix φ 1 x1 x2 1 1 x2 x2 2 . . . . . . . . . 1 xn x2 n . 9.17 feature matrix φ defined 9.16 negative loglikelihood linear regression model 9.13 written log py x θ 1 2σ2 y φθy φθ const . 9.18 comparing 9.18 negative loglikelihood 9.10b fea turefree model immediately see need replace x φ. since x φ independent parameters θ wish optimize arrive immediately maximum likelihood estimate maximum likelihood estimate θml φφ1φy 9.19 linear regression problem nonlinear features defined 9.13. remark. working without features required xx invertible case rkx d i.e. columns x draft 20230215 mathematics machine learning. feedback
9.2 parameter estimation 297 linearly independent. 9.19 therefore require φφ rkk invertible. case rkφ k. example 9.5 maximum likelihood polynomial fit figure 9.4 polynomial regression a dataset consisting xn yn pairs n 1 . . . 10 b maximum likelihood polynomial degree 4. 4 2 0 2 4 x 4 2 0 2 4 a regression dataset. 4 2 0 2 4 x 4 2 0 2 4 training data mle b polynomial degree 4 determined max imum likelihood estimation. consider dataset figure 9.4a. dataset consists n 10 pairs xn yn xn u5 5 yn sinxn5 cosxn ϵ ϵ n 0 0.22 . fit polynomial degree 4 using maximum likelihood estimation i.e. parameters θml given 9.19. maximum likelihood estimate yields function values ϕxθml test location x. result shown figure 9.4b. estimating noise variance thus far assumed noise variance σ2 known. however also use principle maximum likelihood estimation obtain maximum likelihood estimator σ2 ml noise variance. this follow standard procedure write loglikelihood com pute derivative respect σ2 0 set 0 solve. loglikelihood given log py x θ σ2 n x n1 log n yn ϕxnθ σ2 9.20a n x n1 1 2 log2π 1 2 log σ2 1 2σ2 yn ϕxnθ2 9.20b n 2 log σ2 1 2σ2 n x n1 yn ϕxnθ2 z s const . 9.20c 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
298 linear regression partial derivative loglikelihood respect σ2 log py x θ σ2 σ2 n 2σ2 1 2σ4 0 9.21a n 2σ2 2σ4 9.21b identify σ2 ml n 1 n n x n1 yn ϕxnθ2 . 9.22 therefore maximum likelihood estimate noise variance empirical mean squared distances noisefree function values ϕxnθ corresponding noisy observations yn input lo cations xn. 9.2.2 overfitting linear regression discussed use maximum likelihood estimation fit lin ear models e.g. polynomials data. evaluate quality model computing errorloss incurred. one way compute negative loglikelihood 9.10b minimized determine maximum likelihood estimator. alternatively given noise parameter σ2 free model parameter ignore scaling 1σ2 end squarederrorloss function y φθ 2. instead using squared loss often use root mean root mean square error square error rmse rmse r 1 n y φθ 2 v u u 1 n n x n1 yn ϕxnθ2 9.23 a allows us compare errors datasets different sizes b scale units observed func rmse normalized. tion values yn. example fit model maps postcodes x given latitude longitude house prices yvalues eur rmse also measured eur whereas squared error given eur2. choose include factor σ2 original negative negative loglikelihood unitless. loglikelihood 9.10b end unitless objective i.e. preceding example objective would longer eur eur2. model selection see section 8.6 use rmse or negative loglikelihood determine best degree polynomial finding polynomial degree minimizes objective. given polynomial degree natural number perform bruteforce search enumerate reasonable values m. training set size n sufficient test 0 m n 1. n maximum likelihood estimator unique. n parameters draft 20230215 mathematics machine learning. feedback
9.2 parameter estimation 299 figure 9.5 maximum likelihood fits different polynomial degrees m. 4 2 0 2 4 x 4 2 0 2 4 training data mle a 0 4 2 0 2 4 x 4 2 0 2 4 training data mle b 1 4 2 0 2 4 x 4 2 0 2 4 training data mle c 3 4 2 0 2 4 x 4 2 0 2 4 training data mle d 4 4 2 0 2 4 x 4 2 0 2 4 training data mle e 6 4 2 0 2 4 x 4 2 0 2 4 training data mle f 9 data points would need solve underdetermined system linear equations φφ 9.19 would also longer invertible infinitely many possible maximum likelihood estimators. figure 9.5 shows number polynomial fits determined maximum likelihood dataset figure 9.4a n 10 observations. notice polynomials low degree e.g. constants m 0 linear m 1 fit data poorly and hence poor representations true underlying function. degrees 3 . . . 6 fits look plausible smoothly interpolate data. go higherdegree case n 1 extreme sense otherwise null space corresponding system linear equations would nontrivial would infinitely many optimal solutions linear regression problem. polynomials notice fit data better better. ex treme case n 1 9 function pass every single data point. however highdegree polynomials oscillate wildly poor representation underlying function generated data suffer overfitting. overfitting note noise variance σ2 0. remember goal achieve good generalization making accurate predictions new unseen data. obtain quantita tive insight dependence generalization performance polynomial degree considering separate test set comprising 200 data points generated using exactly procedure used generate training set. test inputs chose linear grid 200 points interval 5 5. choice m evaluate rmse 9.23 training data test data. looking test error qualitive measure gen eralization properties corresponding polynomial notice ini tially test error decreases see figure 9.2 orange. fourthorder polynomials test error relatively low stays relatively constant degree 5. however degree 6 onward test error increases signif icantly highorder polynomials bad generalization proper ties. particular example also evident corresponding 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
300 linear regression figure 9.2 training test error. 0 2 4 6 8 10 degree polynomial 0 2 4 6 8 10 rmse training error test error maximum likelihood fits figure 9.5. note training error blue training error curve figure 9.2 never increases degree polynomial in creases. example best generalization the point smallest test error obtained polynomial degree 4. test error 9.2.3 maximum posteriori estimation saw maximum likelihood estimation prone overfitting. often observe magnitude parameter values becomes relatively large run overfitting bishop 2006. mitigate effect huge parameter values place prior distribution pθ parameters. prior distribution explicitly en codes parameter values plausible before seen data. example gaussian prior pθ n 0 1 single parameter θ encodes parameter values expected lie interval 2 2 two standard deviations around mean value. dataset x available instead maximizing likelihood seek parameters maximize posterior distribution pθ x y. procedure called maximum posteriori map estimation. maximum posteriori map posterior parameters θ given training data x y obtained applying bayes theorem section 6.3 pθ x y py x θpθ py x . 9.24 since posterior explicitly depends parameter prior pθ prior effect parameter vector find maximizer posterior. see explicitly following. parameter vector θmap maximizes posterior 9.24 map estimate. find map estimate follow steps similar flavor maximum likelihood estimation. start logtransform compute logposterior log pθ x y log py x θ log pθ const 9.25 draft 20230215 mathematics machine learning. feedback
9.2 parameter estimation 301 constant comprises terms independent θ. see logposterior 9.25 sum loglikelihood py x θ logprior log pθ map estimate compromise prior our suggestion plausible parameter values observing data datadependent likelihood. find map estimate θmap minimize negative logposterior distribution respect θ i.e. solve θmap arg min θ log py x θ log pθ . 9.26 gradient negative logposterior respect θ d log pθ x y dθ d log py x θ dθ d log pθ dθ 9.27 identify first term righthand side gradient negative loglikelihood 9.11c. conjugate gaussian prior pθ n 0 b2i parameters θ negative logposterior linear regression setting 9.13 obtain negative log posterior log pθ x y 1 2σ2 y φθy φθ 1 2b2 θθ const . 9.28 here first term corresponds contribution loglikelihood second term originates logprior. gradient log posterior respect parameters θ d log pθ x y dθ 1 σ2 θφφ yφ 1 b2 θ. 9.29 find map estimate θmap setting gradient 0and solving θmap. obtain 1 σ2 θφφ yφ 1 b2 θ 0 9.30a θ 1 σ2 φφ 1 b2 1 σ2 yφ 0 9.30b θ φφ σ2 b2 yφ 9.30c θ yφ φφ σ2 b2 1 9.30d map estimate by transposing sides last equality φφ symmetric positive semi definite. additional term 9.31 strictly positive definite inverse exists. θmap φφ σ2 b2 1 φy . 9.31 comparing map estimate 9.31 maximum likelihood es timate 9.19 see difference solutions additional term σ2 b2 inverse matrix. term ensures 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
302 linear regression φφ σ2 b2 symmetric strictly positive definite i.e. inverse exists map estimate unique solution system linear equations. moreover reflects impact regularizer. example 9.6 map estimation polynomial regression polynomial regression example section 9.2.1 place gaus sian prior pθ n 0 parameters θ determine map estimates according 9.31. figure 9.1 show maximum likelihood map estimates polynomials degree 6 left degree 8 right. prior regularizer play significant role lowdegree polynomial keeps function relatively smooth higherdegree polynomials. although map estimate push boundaries overfitting general solution problem need principled approach tackle overfitting. figure 9.1 polynomial regression maximum likelihood map estimates. a polynomials degree 6 b polynomials degree 8. 4 2 0 2 4 x 4 2 0 2 4 training data mle map a polynomials degree 6. 4 2 0 2 4 x 4 2 0 2 4 training data mle map b polynomials degree 8. 9.2.4 map estimation regularization instead placing prior distribution parameters θ also pos sible mitigate effect overfitting penalizing amplitude parameter means regularization. regularized least squares regularization regularized least squares consider loss function y φθ 2 λ θ 2 2 9.32 minimize respect θ see section 8.2.3. here first term datafit term also called misfit term proportional datafit term misfit term negative loglikelihood see 9.10b. second term called regularizer regularization parameter λ 0 controls strict regularizer regularization parameter ness regularization. remark. instead euclidean norm 2 choose pnorm p 9.32. practice smaller values p lead sparser solutions. here sparse means many parameter values θd 0 also draft 20230215 mathematics machine learning. feedback
9.3 bayesian linear regression 303 useful variable selection. p 1 regularizer called lasso lasso least absolute shrinkage selection operator proposed tib shirani 1996. regularizer λ θ 2 2 9.32 interpreted negative log gaussian prior use map estimation see 9.26. specif ically gaussian prior pθ n 0 b2i obtain negative loggaussian prior log pθ 1 2b2 θ 2 2 const 9.33 λ 1 2b2 regularization term negative loggaussian prior identical. given regularized leastsquares loss function 9.32 consists terms closely related negative loglikelihood plus neg ative logprior surprising that minimize loss obtain solution closely resembles map estimate 9.31. specifically minimizing regularized leastsquares loss function yields θrls φφ λi1φy 9.34 identical map estimate 9.31 λ σ2 b2 σ2 noise variance b2 variance isotropic gaussian prior pθ n 0 b2i . point estimate single specific parameter value unlike distribution plausible parameter settings. far covered parameter estimation using maximum likeli hood map estimation found point estimates θthat op timize objective function likelihood posterior. saw maximum likelihood map estimation lead overfitting. next section discuss bayesian linear regression use bayesian inference section 8.4 find posterior distribution unknown parameters subsequently use make predictions. specifically predictions average plausible sets parameters instead focusing point estimate. 9.3 bayesian linear regression previously looked linear regression models estimated model parameters θ e.g. means maximum likelihood map esti mation. discovered mle lead severe overfitting particu lar smalldata regime. map addresses issue placing prior parameters plays role regularizer. bayesian linear regression bayesian linear regression pushes idea parameter prior step even attempt compute point estimate parameters instead full posterior distribution parameters taken account making predictions. means fit parameters compute mean plausible parameters settings according posterior. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
304 linear regression 9.3.1 model bayesian linear regression consider model prior pθ n m0 s0 likelihood py x θ n y ϕxθ σ2 9.35 explicitly place gaussian prior pθ n m0 s0 θ figure 9.2 graphical model bayesian linear regression. θ σ x m0 s0 turns parameter vector random variable. allows us write corresponding graphical model figure 9.2 made parameters gaussian prior θ explicit. full proba bilistic model i.e. joint distribution observed unobserved ran dom variables θ respectively py θ x py x θpθ . 9.36 9.3.2 prior predictions practice usually much interested parameter values θ themselves. instead focus often lies predictions make parameter values. bayesian setting take parameter distribution average plausible parameter settings make predictions. specifically make predictions input x integrate θ obtain py x z py x θpθdθ eθpy x θ 9.37 interpret average prediction y x θ plau sible parameters θ according prior distribution pθ. note pre dictions using prior distribution require us specify input x training data. model 9.35 chose conjugate gaussian prior θ predictive distribution gaussian well and computed closed form prior distribution pθ n m0 s0 obtain predictive distribution py x n ϕxm0 ϕxs0ϕx σ2 9.38 exploited i prediction gaussian due conjugacy see section 6.6 marginalization property gaussians see sec tion 6.5 ii gaussian noise independent vy vθϕxθ vϵϵ 9.39 iii yis linear transformation θ apply rules computing mean covariance prediction analytically using 6.50 6.51 respectively. 9.38 term ϕxs0ϕx predictive variance explicitly accounts uncertainty associated draft 20230215 mathematics machine learning. feedback
9.3 bayesian linear regression 305 parameters θ whereas σ2 uncertainty contribution due measurement noise. interested predicting noisefree function values fx ϕxθ instead noisecorrupted targets ywe obtain pfx n ϕxm0 ϕxs0ϕx 9.40 differs 9.38 omission noise variance σ2 predictive variance. remark distribution functions. since represent distri parameter distribution pθ induces distribution functions. bution pθ using set samples θi every sample θi gives rise function fi θ ϕ follows parameter distribution pθ induces distribution pf functions. use notation explicitly denote functional relationship. example 9.7 prior functions figure 9.3 prior functions. a distribution functions represented mean function black line marginal uncertainties shaded representing 67 95 confidence bounds respectively b samples prior functions induced samples parameter prior. 4 2 0 2 4 x 4 2 0 2 4 a prior distribution functions. 4 2 0 2 4 x 4 2 0 2 4 b samples prior distribution functions. let us consider bayesian linear regression problem polynomials degree 5. choose parameter prior pθ n 0 1 4i . figure 9.3 visualizes induced prior distribution functions shaded area dark gray 67 confidence bound light gray 95 confidence bound induced parameter prior including function samples prior. function sample obtained first sampling parameter vector θi pθ computing fi θ ϕ. used 200 input lo cations x5 5 apply feature function ϕ. uncertainty represented shaded area figure 9.3 solely due parameter uncertainty considered noisefree predictive distribution 9.40. far looked computing predictions using parameter prior pθ. however parameter posterior given train ing data x y principles prediction inference hold 9.37 need replace prior pθ posterior 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
306 linear regression pθ x y. following derive posterior distribution detail using make predictions. 9.3.3 posterior distribution given training set inputs xn rd corresponding observations yn r n 1 . . . n compute posterior parameters using bayes theorem pθ x y py x θpθ py x 9.41 x set training inputs collection correspond ing training targets. furthermore py x θ likelihood pθ parameter prior py x z py x θpθdθ eθpy x θ 9.42 marginal likelihoodevidence independent parameters marginal likelihood evidence θ ensures posterior normalized i.e. integrates 1. marginal likelihood expected likelihood parameter prior. think marginal likelihood likelihood averaged possible parameter settings with respect prior distribution pθ. theorem 9.1 parameter posterior. model 9.35 parameter posterior 9.41 computed closed form pθ x y n θ mn sn 9.43a sn s1 0 σ2φφ1 9.43b mn sns1 0 m0 σ2φy 9.43c subscript n indicates size training set. proof bayes theorem tells us posterior pθ x y propor tional product likelihood py x θ prior pθ posterior pθ x y py x θpθ py x 9.44a likelihood py x θ n y φθ σ2i 9.44b prior pθ n θ m0 s0 . 9.44c instead looking product prior likelihood transform problem logspace solve mean covariance posterior completing squares. sum logprior loglikelihood log n y φθ σ2i log n θ m0 s0 9.45a 1 2 σ2y φθy φθ θ m0s1 0 θ m0 const 9.45b draft 20230215 mathematics machine learning. feedback
9.3 bayesian linear regression 307 constant contains terms independent θ. ignore constant following. factorize 9.45b yields 1 2 σ2yy 2σ2yφθ θσ2φφθ θs1 0 θ 2m 0 s1 0 θ m 0 s1 0 m0 9.46a 1 2 θσ2φφ s1 0 θ 2σ2φy s1 0 m0θ const 9.46b constant contains black terms 9.46a inde pendent θ. orange terms terms linear θ blue terms ones quadratic θ. inspecting 9.46b find equation quadratic θ. fact unnormalized logposterior distribution negative quadratic form implies posterior gaussian i.e. pθ x y explog pθ x y explog py x θ log pθ 9.47a exp 1 2 θσ2φφ s1 0 θ 2σ2φy s1 0 m0θ 9.47b used 9.46b last expression. remaining task bring unnormalized gaussian form proportional n θ mn sn i.e. need identify mean mn covariance matrix sn. this use concept completing squares. desired logposterior completing squares log n θ mn sn 1 2θ mns1 n θ mn const 9.48a 1 2 θs1 n θ 2m ns1 n θ m ns1 n mn . 9.48b here factorized quadratic form θ mns1 n θ mn since pθ x y n mn sn holds θmap mn. term quadratic θ alone blue term linear θ orange constant term black. allows us find sn mn matching colored expressions 9.46b 9.48b yields s1 n φσ2iφ s1 0 9.49a sn σ2φφ s1 0 1 9.49b m ns1 n σ2φy s1 0 m0 9.50a mn snσ2φy s1 0 m0 . 9.50b 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
308 linear regression remark general approach completing squares. given equation xax 2ax const1 9.51 symmetric positive definite wish bring form x µσx µ const2 9.52 setting σ 9.53 µ σ1a 9.54 const2 const1 µσµ. see terms inside exponential 9.47b form 9.51 σ2φφ s1 0 9.55 σ2φy s1 0 m0 . 9.56 since a difficult identify equations like 9.46a of ten helpful bring equations form 9.51 decouples quadratic term linear terms constants simplifies finding desired solution. 9.3.4 posterior predictions 9.37 computed predictive distribution yat test input xusing parameter prior pθ. principle predicting pa rameter posterior pθ x y fundamentally different given conjugate model prior posterior gaussian with different parameters. therefore following reasoning section 9.3.2 obtain posterior predictive distribution py x y x z py x θpθ x ydθ 9.57a z n y ϕxθ σ2n θ mn sn dθ 9.57b n y ϕxmn ϕxsnϕx σ2 . 9.57c term ϕxsnϕx reflects posterior uncertainty associated ey x y x ϕxmn ϕxθmap. parameters θ. note sn depends training inputs φ see 9.43b. predictive mean ϕxmn coincides predictions made map estimate θmap. draft 20230215 mathematics machine learning. feedback
9.3 bayesian linear regression 309 remark marginal likelihood posterior predictive distribution. replacing integral 9.57a predictive distribution equiv alently written expectation eθ xypy x θ expec tation taken respect parameter posterior pθ x y. writing posterior predictive distribution way highlights close resemblance marginal likelihood 9.42. key difference marginal likelihood posterior predictive distribution i marginal likelihood thought predicting training targets test targets y ii marginal likelihood av erages respect parameter prior parameter poste rior. remark mean variance noisefree function values. many cases interested predictive distribution py x y x noisy observation y. instead would like obtain distribu tion noisefree function values fx ϕxθ. determine corresponding moments exploiting properties means variances yields efx x y eθϕxθ x y ϕxeθθ x y ϕxmn m nϕx 9.58 vθfx x y vθϕxθ x y ϕxvθθ x yϕx ϕxsnϕx . 9.59 see predictive mean predictive mean noisy observations noise mean 0 predictive variance differs σ2 variance measurement noise predict noisy function values need include σ2 source uncertainty term needed noisefree predictions. here remaining uncertainty stems parameter posterior. integrating parameters induces distribution functions. remark distribution functions. fact integrate parameters θ induces distribution functions sample θi pθ x y parameter posterior obtain single function re alization θ ϕ. mean function i.e. set expected function mean function values eθf θ x y distribution functions m nϕ. marginal variance i.e. variance function f given ϕsnϕ. example 9.8 posterior functions let us revisit bayesian linear regression problem polynomials degree 5. choose parameter prior pθ n 0 1 4i . figure 9.3 visualizes prior functions induced parameter prior sample functions prior. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
310 linear regression figure 9.4 shows posterior functions obtain via bayesian linear regression. training dataset shown panel a panel b shows posterior distribution functions including functions would obtain via maximum likelihood map estimation. function obtain using map estimate also corresponds posterior mean function bayesian linear regression setting. panel c shows plausible realizations samples functions pos terior functions. figure 9.4 bayesian linear regression posterior functions. a training data b posterior distribution functions c samples posterior functions. 4 2 0 2 4 x 4 2 0 2 4 a training data. 4 2 0 2 4 x 4 2 0 2 4 training data mle map blr b posterior functions rep resented marginal uncer tainties shaded showing 67 95 predictive con fidence bounds maximum likelihood estimate mle map estimate map latter identical posterior mean function. 4 2 0 2 4 x 4 2 0 2 4 c samples posterior functions in duced samples parameter posterior. figure 9.5 shows posterior distributions functions induced parameter posterior. different polynomial degrees m left panels show maximum likelihood function θ mlϕ map func tion θ mapϕ which identical posterior mean function 67 95 predictive confidence bounds obtained bayesian linear regression represented shaded areas. right panels show samples posterior functions here sampled parameters θi parameter posterior computed function ϕxθi single realization function posterior distribution functions. loworder polynomials parameter posterior allow parameters vary much sampled functions nearly identical. make model flexible adding parameters i.e. end higherorder polynomial parameters sufficiently constrained pos terior sampled functions easily visually separated. also see corresponding panels left uncertainty increases especially boundaries. although seventhorder polynomial map estimate yields rea sonable fit bayesian linear regression model additionally tells us draft 20230215 mathematics machine learning. feedback
9.3 bayesian linear regression 311 figure 9.5 bayesian linear regression. left panels shaded areas indicate 67 dark gray 95 light gray predictive confidence bounds. mean bayesian linear regression model coincides map estimate. predictive uncertainty sum noise term posterior parameter uncertainty depends location test input. right panels sampled functions posterior distribution. 4 2 0 2 4 x 4 2 0 2 4 training data mle map blr 4 2 0 2 4 x 4 2 0 2 4 a posterior distribution polynomials degree 3 left samples pos terior functions right. 4 2 0 2 4 x 4 2 0 2 4 training data mle map blr 4 2 0 2 4 x 4 2 0 2 4 b posterior distribution polynomials degree 5 left samples posterior functions right. 4 2 0 2 4 x 4 2 0 2 4 training data mle map blr 4 2 0 2 4 x 4 2 0 2 4 c posterior distribution polynomials degree 7 left samples pos terior functions right. posterior uncertainty huge. information critical use predictions decisionmaking system bad deci sions significant consequences e.g. reinforcement learning robotics. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
312 linear regression 9.3.5 computing marginal likelihood section 8.6.2 highlighted importance marginal likelihood bayesian model selection. following compute marginal likelihood bayesian linear regression conjugate gaussian prior parameters i.e. exactly setting discussing chapter. recap consider following generative process θ n m0 s0 9.60a yn xn θ n x n θ σ2 9.60b n 1 . . . n. marginal likelihood given marginal likelihood interpreted expected likelihood prior i.e. eθpy x θ. py x z py x θpθdθ 9.61a z n y xθ σ2i n θ m0 s0 dθ 9.61b integrate model parameters θ. compute marginal likelihood two steps first show marginal likelihood gaussian as distribution y second compute mean co variance gaussian. 1. marginal likelihood gaussian section 6.5.2 know i product two gaussian random variables unnormalized gaussian distribution ii linear transformation gaussian random variable gaussian distributed. 9.61b require linear transformation bring n y xθ σ2i form n θ µ σ µ σ. done integral solved closed form. result normalizing constant product two gaus sians. normalizing constant gaussian shape see 6.76. 2. mean covariance. compute mean covariance matrix marginal likelihood exploiting standard results means covariances affine transformations random variables see sec tion 6.4.4. mean marginal likelihood computed ey x eθϵxθ ϵ xeθθ xm0 . 9.62 note ϵ n 0 σ2i vector i.i.d. random variables. covariance matrix given covyx covθϵxθ ϵ covθxθ σ2i 9.63a x covθθx σ2i xs0x σ2i . 9.63b hence marginal likelihood py x 2πn 2 detxs0x σ2i1 2 9.64a exp 1 2y xm0xs0x σ2i1y xm0 draft 20230215 mathematics machine learning. feedback
9.4 maximum likelihood orthogonal projection 313 figure 9.6 geometric interpretation least squares. a dataset b maximum likelihood solution interpreted projection. 4 2 0 2 4 x 4 2 0 2 4 a regression dataset consisting noisy ob servations yn blue function values fxn input locations xn. 4 2 0 2 4 x 4 2 0 2 4 projection observations maximum likelihood estimate b orange dots projections noisy observations blue dots onto line θmlx. maximum likelihood solution linear regression problem finds subspace line onto overall projection er ror orange lines observations mini mized. n y xm0 xs0x σ2i . 9.64b given close connection posterior predictive distribution see remark marginal likelihood posterior predictive distribution ear lier section functional form marginal likelihood surprising. 9.4 maximum likelihood orthogonal projection crunched much algebra derive maximum likelihood map estimates provide geometric interpretation maximum likelihood estimation. let us consider simple linear regression setting xθ ϵ ϵ n 0 σ2 9.65 consider linear functions f r r go origin we omit features clarity. parameter θ determines slope line. figure 9.6a shows onedimensional dataset. training data set x1 y1 . . . xn yn recall results section 9.2.1 obtain maximum likelihood estimator slope parameter θml xx1xy xy xx r 9.66 x x1 . . . xnrn y1 . . . ynrn. means training inputs x obtain optimal maximum likelihood reconstruction training targets xθml x xy xx xx xx 9.67 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
314 linear regression i.e. obtain approximation minimum leastsquares error xθ. looking solution xθ think linear regression problem solving systems linear equations. there linear regression thought method solving systems linear equations. fore relate concepts linear algebra analytic geometry discussed chapters 2 3. particular looking carefully 9.67 see maximum likelihood estimator θml ex ample 9.65 effectively orthogonal projection onto onedimensional subspace spanned x. recalling results or maximum likelihood linear regression performs orthogonal projection. thogonal projections section 3.8 identify xx xx projection matrix θml coordinates projection onto onedimensional subspace rn spanned x xθml orthogonal projection onto subspace. therefore maximum likelihood solution provides also geometri cally optimal solution finding vectors subspace spanned x closest corresponding observations y clos est means smallest squared distance function values yn xnθ. achieved orthogonal projections. figure 9.6b shows projection noisy observations onto subspace minimizes squared distance original dataset projection note xcoordinate fixed corresponds maximum likelihood solution. general linear regression case ϕxθ ϵ ϵ n 0 σ2 9.68 vectorvalued features ϕx rk interpret maxi mum likelihood result φθml 9.69 θml φφ1φy 9.70 projection onto kdimensional subspace rn spanned columns feature matrix φ see section 3.8.2. feature functions ϕk use construct feature ma trix φ orthonormal see section 3.7 obtain special case columns φ form orthonormal basis see section 3.5 φφ i. lead projection φφφ1φy φφy k x k1 ϕkϕ k 9.71 maximum likelihood projection simply sum projections onto individual basis vectors ϕk i.e. columns φ. further more coupling different features disappeared due orthogonality basis. many popular basis functions signal process ing wavelets fourier bases orthogonal basis functions. draft 20230215 mathematics machine learning. feedback
9.5 reading 315 basis orthogonal one convert set linearly inde pendent basis functions orthogonal basis using gramschmidt process see section 3.8.3 strang 2003. 9.5 reading chapter discussed linear regression gaussian likelihoods conjugate gaussian priors parameters model. al lowed closedform bayesian inference. however applications may want choose different likelihood function. example binary classification setting observe two possible categorical classification outcomes gaussian likelihood inappropriate setting. in stead choose bernoulli likelihood return probability predicted label 1 or 0. refer books barber 2012 bishop 2006 murphy 2012 indepth introduction classifi cation problems. different example nongaussian likelihoods important count data. counts nonnegative integers case binomial poisson likelihood would better choice gaussian. examples fall category generalized linear models flex generalized linear model ible generalization linear regression allows response variables error distributions gaussian distribution. glm generalized linear models building blocks deep neural networks. generalizes linear regression allowing linear model related observed values via smooth invertible function σ may nonlinear σfx fx θϕx linear regression model 9.13. therefore think generalized linear model terms function composition σ f f linear regression model σ activation function. note although talking generalized linear models outputs longer linear parameters θ. logistic regression choose logistic regression logistic sigmoid σf 1 1expf 0 1 interpreted logistic sigmoid probability observing 1 bernoulli random variable 0 1. function σ called transfer function activation function transfer function activation function inverse called canonical link function. perspective canonical link function ordinary linear regression activation function would simply identity. also clear generalized linear models building blocks deep feedforward neural networks consider generalized linear model σax b weight matrix b bias vector iden tify generalized linear model singlelayer neural network activation function σ. recursively compose functions via great post relation glms deep networks available comglmdnn. xk1 f kxk f kxk σkakxk bk 9.72 k 0 . . . k 1 x0 input features xk observed outputs f k1 f 0 klayer deep neural network. therefore building blocks deep neural network 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
316 linear regression generalized linear models defined 9.72. neural networks bishop 1995 goodfellow et al. 2016 significantly expressive flexi ble linear regression models. however maximum likelihood parame ter estimation nonconvex optimization problem marginalization parameters fully bayesian setting analytically intractable. briefly hinted fact distribution parameters in duces distribution regression functions. gaussian processes ras gaussian process mussen williams 2006 regression models concept distribution function central. instead placing distribution parameters gaussian process places distribution directly space functions without detour via parameters. so gaussian process exploits kernel trick sch olkopf smola 2002 kernel trick allows us compute inner products two function values fxi fxj looking corresponding input xi xj. gaus sian process closely related bayesian linear regression sup port vector regression also interpreted bayesian neural network single hidden layer number units tends infinity neal 1996 williams 1997. excellent introductions gaussian processes found mackay 1998 rasmussen williams 2006. focused gaussian parameter priors discussions chap ter allow closedform inference linear regression mod els. however even regression setting gaussian likelihoods may choose nongaussian prior. consider setting inputs x rd training set small size n d. means regression problem underdetermined. case choose parameter prior enforces sparsity i.e. prior tries set many parameters 0 possible variable selection. prior provides variable selection stronger regularizer gaussian prior often leads in creased prediction accuracy interpretability model. laplace prior one example frequently used purpose. linear re gression model laplace prior parameters equivalent linear regression l1 regularization lasso tibshirani 1996. lasso laplace distribution sharply peaked zero its first derivative discon tinuous concentrates probability mass closer zero gaussian distribution encourages parameters 0. therefore nonzero parameters relevant regression problem reason also speak variable selection. draft 20230215 mathematics machine learning. feedback
10 dimensionality reduction principal component analysis working directly highdimensional data images comes 640 480 pixel color image data point milliondimensional space every pixel responds three dimensions one color channel red green blue. difficulties hard analyze interpretation difficult visualiza tion nearly impossible from practical point view storage data vectors expensive. however highdimensional data often properties exploit. example highdimensional data often overcomplete i.e. many dimensions redundant ex plained combination dimensions. furthermore dimensions highdimensional data often correlated data possesses intrinsic lowerdimensional structure. dimensionality reduction exploits structure correlation allows us work compact rep resentation data ideally without losing information. think dimensionality reduction compression technique similar jpeg mp3 compression algorithms images music. chapter discuss principal component analysis pca principal component analysis pca algorithm linear dimensionality reduction. pca proposed pearson dimensionality reduction 1901 hotelling 1933 around 100 years still one commonly used techniques data compres sion data visualization. also used identification simple patterns latent factors structures highdimensional data. figure 10.1 illustration dimensionality reduction. a original dataset vary much along x2 direction. b data a represented using x1coordinate alone nearly loss. 5.0 2.5 0.0 2.5 5.0 x1 4 2 0 2 4 x2 a dataset x1 x2 coordinates. 5.0 2.5 0.0 2.5 5.0 x1 4 2 0 2 4 x2 b compressed dataset x1 coor dinate relevant. 317 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.
318 dimensionality reduction principal component analysis signal processing community pca also known karhunenlo eve karhunenlo eve transform transform. chapter derive pca first principles drawing understanding basis basis change sections 2.6.1 2.7.2 projections section 3.8 eigenvalues section 4.2 gaussian distribu tions section 6.5 constrained optimization section 7.2. dimensionality reduction generally exploits property highdimen sional data e.g. images often lies lowdimensional subspace. figure 10.1 gives illustrative example two dimensions. although data figure 10.1a quite lie line data vary much x2direction express line nearly loss see figure 10.1b. describe data figure 10.1b x1coordinate required data lies onedimensional subspace r2. 10.1 problem setting pca interested finding projections xn data points xn similar original data points possible sig nificantly lower intrinsic dimensionality. figure 10.1 gives illustration could look like. concretely consider i.i.d. dataset x x1 . . . xn xn rd mean 0 possesses data covariance matrix 6.42 data covariance matrix 1 n n x n1 xnx n . 10.1 furthermore assume exists lowdimensional compressed rep resentation code zn bxn rm 10.2 xn define projection matrix b b1 . . . bm rdm . 10.3 assume columns b orthonormal definition 3.7 b bj 0 j b bi 1. seek mdimensional columns b1 . . . bm b form basis mdimensional subspace projected data x bbx rd live. subspace u rd dimu onto project data. denote projected data xn u coordinates with respect basis vectors b1 . . . bm u zn. aim find projections xn rd or equivalently codes zn basis vectors b1 . . . bm similar original data xn minimize loss due compression. example 10.1 coordinate representationcode consider r2 canonical basis e1 1 0 e2 0 1. draft 20230215 mathematics machine learning. feedback
10.1 problem setting 319 figure 10.1 graphical illustration pca. pca find compressed version z original data x. compressed data reconstructed x lives original data space intrinsic lowerdimensional representation x. x x z original compressed reconstructed rd rd rm chapter 2 know x r2 represented linear combina tion basis vectors e.g. 5 3 5e1 3e2 . 10.4 however consider vectors form x 0 z r2 z r 10.5 always written 0e1 ze2. represent vectors sufficient rememberstore coordinatecode z x respect e2 vector. dimension vector space corresponds number basis vectors see section 2.6.1. precisely set x vectors with standard vector addition scalar multiplication forms vector subspace u see section 2.4 dimu 1 u spane2. section 10.2 find lowdimensional representations re tain much information possible minimize compression loss. alternative derivation pca given section 10.3 looking minimizing squared reconstruction error xn xn 2 be tween original data xn projection xn. figure 10.1 illustrates setting consider pca z repre sents lowerdimensional representation compressed data x plays role bottleneck controls much information flow x x. pca consider linear relationship original data x lowdimensional code z z bx x bz suitable matrix b. based motivation thinking pca data compression technique interpret arrows figure 10.1 pair operations representing encoders decoders. linear mapping represented b thought decoder maps lowdimensional code z rm back original data space rd. similarly bcan thought encoder encodes original data x lowdimensional compressed code z. throughout chapter use mnist digits dataset re 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
320 dimensionality reduction principal component analysis figure 10.2 examples handwritten digits mnist dataset. http yann.lecun. comexdbmnist. occurring example contains 60000 examples handwritten digits 0 9. digit grayscale image size 2828 i.e. contains 784 pixels interpret every image dataset vector x r784. examples digits shown figure 10.2. 10.2 maximum variance perspective figure 10.1 gave example twodimensional dataset represented using single coordinate. figure 10.1b chose ig nore x2coordinate data add much in formation compressed data similar original data figure 10.1a. could chosen ignore x1coordinate compressed data dissimilar original data much information data would lost. interpret information content data space filling dataset is describe information contained data looking spread data. section 6.4.1 know variance indicator spread data derive pca dimensionality reduction algorithm maximizes variance lowdimensional representation data retain much information possible. figure 10.2 illustrates this. considering setting discussed section 10.1 aim find matrix b see 10.3 retains much information possible compressing data projecting onto subspace spanned columns b1 . . . bm b. retaining information data com pression equivalent capturing largest amount variance lowdimensional code hotelling 1933. remark. centered data data covariance matrix 10.1 assumed centered data. make assumption without loss gen erality let us assume µ mean data. using properties variance discussed section 6.4.4 obtain vzz vxbx µ vxbx bµ vxbx 10.6 i.e. variance lowdimensional code depend mean data. therefore assume without loss generality data mean 0 remainder section. assumption mean lowdimensional code also 0 since ezz exbx bexx 0. draft 20230215 mathematics machine learning. feedback
10.2 maximum variance perspective 321 figure 10.2 pca finds lowerdimensional subspace line maintains much variance spread data possible data blue projected onto subspace orange. 10.2.1 direction maximal variance maximize variance lowdimensional code using sequential approach. start seeking single vector b1 rd maximizes vector b1 first column matrix b therefore first orthonormal basis vectors span lowerdimensional subspace. variance projected data i.e. aim maximize variance first coordinate z1 z rm v1 vz1 1 n n x n1 z2 1n 10.7 maximized exploited i.i.d. assumption data defined z1n first coordinate lowdimensional representation zn rm xn rd. note first component zn given z1n b 1 xn 10.8 i.e. coordinate orthogonal projection xn onto one dimensional subspace spanned b1 section 3.8. substitute 10.8 10.7 yields v1 1 n n x n1 b 1 xn2 1 n n x n1 b 1 xnx n b1 10.9a b 1 1 n n x n1 xnx n b1 b 1 sb1 10.9b data covariance matrix defined 10.1. 10.9a used fact dot product two vectors symmetric respect arguments is b 1 xn x n b1. notice arbitrarily increasing magnitude vector b1 in creases v1 is vector b1 two times longer result v1 potentially four times larger. therefore restrict solutions b12 1 b1 1. b1 2 1 results constrained optimization problem seek direction along data varies most. restriction solution space unit vectors vector b1 points direction maximum variance found 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
322 dimensionality reduction principal component analysis constrained optimization problem max b1 b 1 sb1 subject b1 2 1 . 10.10 following section 7.2 obtain lagrangian lb1 λ b 1 sb1 λ11 b 1 b1 10.11 solve constrained optimization problem. partial derivatives l respect b1 λ1 l b1 2b 1 2λ1b 1 l λ1 1 b 1 b1 10.12 respectively. setting partial derivatives 0 gives us relations sb1 λ1b1 10.13 b 1 b1 1 . 10.14 comparing definition eigenvalue decomposition section 4.4 see b1 eigenvector data covariance matrix s lagrange multiplier λ1 plays role correspond ing eigenvalue. eigenvector property 10.13 allows us rewrite quantity λ1 also called loading unit vector b1 represents standard deviation data accounted principal subspace spanb1. variance objective 10.10 v1 b 1 sb1 λ1b 1 b1 λ1 10.15 i.e. variance data projected onto onedimensional subspace equals eigenvalue associated basis vector b1 spans subspace. therefore maximize variance lowdimensional code choose basis vector associated largest eigenvalue data covariance matrix. eigenvector called first principal principal component component. determine effectcontribution principal com ponent b1 original data space mapping coordinate z1n back data space gives us projected data point xn b1z1n b1b 1 xn rd 10.16 original data space. remark. although xn ddimensional vector requires single coordinate z1n represent respect basis vector b1 rd. 10.2.2 mdimensional subspace maximal variance assume found first 1 principal components 1 eigenvectors associated largest 1 eigenvalues. since symmetric spectral theorem theorem 4.15 states use eigenvectors construct orthonormal eigenbasis draft 20230215 mathematics machine learning. feedback
10.2 maximum variance perspective 323 m 1dimensional subspace rd. generally mth principal com ponent found subtracting effect first 1 principal components b1 . . . bm1 data thereby trying find principal components compress remaining information. arrive new data matrix ˆ x x m1 x i1 bib x x bm1x 10.17 x x1 . . . xn rdn contains data points column matrix ˆ x ˆ x1 . . . ˆ xn rdn 10.17 contains information data yet compressed. vectors bm1 pm1 i1 bib projection matrix projects onto subspace spanned b1 . . . bm1. remark notation. throughout chapter follow con vention collecting data x1 . . . xn rows data matrix define columns x. means data ma trix x n matrix instead conventional n matrix. reason choice algebra operations work smoothly without need either transpose matrix redefine vectors row vectors leftmultiplied onto matrices. find mth principal component maximize variance vm vzm 1 n n x n1 z2 mn 1 n n x n1 b ˆ xn2 b ˆ sbm 10.18 subject bm 2 1 followed steps 10.9b defined ˆ data covariance matrix transformed dataset ˆ x ˆ x1 . . . ˆ xn. previously looked first principal component alone solve constrained optimization problem dis cover optimal solution bm eigenvector ˆ associated largest eigenvalue ˆ s. turns bm also eigenvector s. generally sets eigenvectors ˆ identical. since ˆ sym metric find onb eigenvectors spectral theorem 4.15 i.e. exist distinct eigenvectors ˆ s. next show every eigenvector eigenvector ˆ s. assume already found eigenvectors b1 . . . bm1 ˆ s. consider eigenvector bi s i.e. sbi λibi. general ˆ sbi 1 n ˆ x ˆ x bi 1 n x bm1xx bm1xbi 10.19a s sbm1 bm1s bm1sbm1bi . 10.19b distinguish two cases. m i.e. bi eigenvector among first m1 principal components bi orthogo nal first m1 principal components bm1bi 0. m i.e. bi among first 1 principal components bi basis vector 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
324 dimensionality reduction principal component analysis principal subspace onto bm1 projects. since b1 . . . bm1 onb principal subspace obtain bm1bi bi. two cases summarized follows bm1bi bi bm1bi 0 m . 10.20 case m using 10.20 10.19b obtain ˆ sbi s bm1sbi sbi λibi i.e. bi also eigenvector ˆ eigen value λi. specifically ˆ sbm sbm λmbm . 10.21 equation 10.21 reveals bm eigenvector also ˆ s. specifically λm largest eigenvalue ˆ λm mth largest eigenvalue s associated eigenvector bm. case m using 10.20 10.19b obtain ˆ sbi s sbm1 bm1s bm1sbm1bi 0 0bi 10.22 means b1 . . . bm1 also eigenvectors ˆ s as sociated eigenvalue 0 b1 . . . bm1 span null space ˆ s. overall every eigenvector also eigenvector ˆ s. however eigenvectors part m 1 dimensional principal subspace associated eigenvalue ˆ 0. derivation shows intimate connection mdimensional subspace maximal variance eigenvalue decomposition. revisit connection section 10.4. relation 10.21 b mbm 1 variance data pro jected onto mth principal component vm b msbm 10.21 λmb mbm λm . 10.23 means variance data projected onto m dimensional subspace equals sum eigenvalues associ ated corresponding eigenvectors data covariance matrix. example 10.2 eigenvalues mnist 8 figure 10.3 properties training data mnist 8. a eigenvalues sorted descending order b variance captured principal components associated largest eigenvalues. 0 50 100 150 200 index 0 10 20 30 40 50 eigenvalue a eigenvalues sorted descending order data covariance matrix digits 8 mnist training set. 0 50 100 150 200 number principal components 100 200 300 400 500 captured variance b variance captured principal compo nents. draft 20230215 mathematics machine learning. feedback
10.3 projection perspective 325 figure 10.1 illustration projection approach find subspace line minimizes length difference vector projected orange original blue data. taking digits 8 mnist training data compute eigen values data covariance matrix. figure 10.3a shows 200 largest eigenvalues data covariance matrix. see value differs significantly 0. therefore variance projecting data onto subspace spanned cor responding eigenvectors captured principal components shown figure 10.3b. overall find mdimensional subspace rd retains much information possible pca tells us choose columns matrix b 10.3 eigenvectors data covariance matrix associated largest eigenvalues. maximum amount variance pca capture first principal components vm x m1 λm 10.24 λm largest eigenvalues data covariance matrix s. consequently variance lost data compression via pca jm x jm1 λj vd vm . 10.25 instead absolute quantities define relative variance captured vm vd relative variance lost compression 1 vm vd . 10.3 projection perspective following derive pca algorithm directly mini mizes average reconstruction error. perspective allows us in terpret pca implementing optimal linear autoencoder. draw heavily chapters 2 3. previous section derived pca maximizing variance projected space retain much information possible. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
326 dimensionality reduction principal component analysis figure 10.2 simplified projection setting. a vector x r2 red cross shall projected onto onedimensional subspace u r2 spanned b. b shows difference vectors x candidates x. 1.0 0.5 0.0 0.5 1.0 1.5 2.0 x1 0.5 0.0 0.5 1.0 1.5 2.0 2.5 x2 b u a setting. 1.0 0.5 0.0 0.5 1.0 1.5 2.0 x1 0.5 0.0 0.5 1.0 1.5 2.0 2.5 x2 b u b differences x xi 50 different xi shown red lines. following look difference vectors original data xn reconstruction xn minimize distance xn xn close possible. figure 10.1 illustrates setting. 10.3.1 setting objective assume ordered orthonormal basis onb b b1 . . . bd rd i.e. b bj 1 j 0 otherwise. section 2.5 know basis b1 . . . bd rd x rd written linear combination basis vectors rd i.e. vectors x u could vectors plane r3. dimensionality plane 2 vectors still three coordinates respect standard basis r3. x x d1 ζdbd x m1 ζmbm x jm1 ζjbj 10.26 suitable coordinates ζd r. interested finding vectors x rd live lower dimensional subspace u rd dimu m x x m1 zmbm u rd 10.27 similar x possible. note point need assume coordinates zm x ζm x identical. following use exactly kind representation x find optimal coordinates z basis vectors b1 . . . bm x sim ilar original data point x possible i.e. aim minimize euclidean distance x x. figure 10.2 illustrates setting. without loss generality assume dataset x x1 . . . xn xn rd centered 0 i.e. ex 0. without zeromean assump draft 20230215 mathematics machine learning. feedback
10.3 projection perspective 327 tion would arrive exactly solution notation would substantially cluttered. interested finding best linear projection x onto lower dimensional subspace u rd dimu orthonormal basis vectors b1 . . . bm. call subspace u principal subspace. principal subspace projections data points denoted xn x m1 zmnbm bzn rd 10.28 zn z1n . . . zmnrm coordinate vector xn respect basis b1 . . . bm. specifically interested xn similar xn possible. similarity measure use following squared distance euclidean norm x x 2 x x. therefore define ob jective minimizing average squared euclidean distance reconstruction reconstruction error error pearson 1901 jm 1 n n x n1 xn xn2 10.29 make explicit dimension subspace onto project data m. order find optimal linear projection need find orthonormal basis principal subspace coordinates zn rm projections respect basis. find coordinates zn onb principal subspace follow twostep approach. first optimize coordinates zn given onb b1 . . . bm second find optimal onb. 10.3.2 finding optimal coordinates let us start finding optimal coordinates z1n . . . zmn projec tions xn n 1 . . . n. consider figure 10.2b principal subspace spanned single vector b. geometrically speaking finding optimal coordinates z corresponds finding representation linear projection x respect b minimizes distance x x. figure 10.2b clear orthogonal projection following show exactly this. assume onb b1 . . . bm u rd. find optimal co ordinates zm respect basis require partial derivatives jm zin jm xn xn zin 10.30a jm xn 2 n xn xnr1d 10.30b 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
328 dimensionality reduction principal component analysis figure 10.3 optimal projection vector x r2 onto onedimensional subspace continuation figure 10.2. a distances x xfor x u. b orthogonal projection optimal coordinates. 1.0 0.5 0.0 0.5 1.0 1.5 2.0 x1 1.25 1.50 1.75 2.00 2.25 2.50 2.75 3.00 3.25 x x a distances x xfor x z1b u spanb see panel b setting. 1.0 0.5 0.0 0.5 1.0 1.5 2.0 x1 0.5 0.0 0.5 1.0 1.5 2.0 2.5 x2 b u x b vector x minimizes distance panel a orthogonal projection onto u. coordinate projection x respect basis vector b spans u factor need scale b order reach x. xn zin 10.28 zin x m1 zmnbm bi 10.30c 1 . . . m obtain jm zin 10.30b 10.30c 2 n xn xnbi 10.28 2 n xn x m1 zmnbm bi 10.31a onb 2 n x n bi zinb bi 2 n x n bi zin . 10.31b since b bi 1. setting partial derivative 0 yields immediately coordinates optimal projection xn respect basis vectors b1 . . . bm coordinates orthogonal projection xn onto principal subspace. optimal coordinates zin x n bi b xn 10.32 1 . . . n 1 . . . n. means optimal co ordinates zin projection xn coordinates orthogonal projection see section 3.8 original data point xn onto one dimensional subspace spanned bi. consequently optimal linear projection xn xn orthogonal projection. coordinates xn respect basis b1 . . . bm coordinates orthogonal projection xn onto principal sub space. orthogonal projection best linear mapping given objec tive 10.29. coordinates ζm x 10.26 coordinates zm x 10.27 draft 20230215 mathematics machine learning. feedback
10.3 projection perspective 329 must identical 1 . . . since u spanbm1 . . . bd orthogonal complement see section 3.6 u spanb1 . . . bm. remark orthogonal projections orthonormal basis vectors. let us briefly recap orthogonal projections section 3.8. b1 . . . bd orthonormal basis rd b j x coordinate orthogonal projection x onto subspace spanned bj. x bjb j bj1b j x bjb j x rd 10.33 orthogonal projection x onto subspace spanned jth ba sis vector zj b j x coordinate projection respect basis vector bj spans subspace since zjbj x. figure 10.3b illustrates setting. generally aim project onto mdimensional subspace rd obtain orthogonal projection x onto mdimensional subspace orthonormal basis vectors b1 . . . bm x bbb z i 1bx bbx 10.34 defined b b1 . . . bm rdm. coordinates projection respect ordered basis b1 . . . bm z bx discussed section 3.8. think coordinates representation projected vector new coordinate system defined b1 . . . bm. note al though x rd need coordinates z1 . . . zm represent vector m coordinates respect basis vectors bm1 . . . bd always 0. far shown given onb find optimal coordinates x orthogonal projection onto principal subspace. following determine best basis is. 10.3.3 finding basis principal subspace determine basis vectors b1 . . . bm principal subspace rephrase loss function 10.29 using results far. make easier find basis vectors. reformulate loss func tion exploit results obtain xn x m1 zmnbm 10.32 x m1 x n bmbm . 10.35 exploit symmetry dot product yields xn x m1 bmb xn . 10.36 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
330 dimensionality reduction principal component analysis figure 10.1 orthogonal projection displacement vectors. projecting data points xn blue onto subspace u1 obtain xn orange. displacement vector xn xn lies completely orthogonal complement u2 u1. 5 0 5 x1 6 4 2 0 2 4 6 x2 u u since generally write original data point xn linear combi nation basis vectors holds xn x d1 zdnbd 10.32 x d1 x n bdbd x d1 bdb xn 10.37a x m1 bmb xn x jm1 bjb j xn 10.37b split sum terms sum sum m terms. result find displacement vector xn xn i.e. difference vector original data point projection xn xn x jm1 bjb j xn 10.38a x jm1 x n bjbj . 10.38b means difference exactly projection data point onto orthogonal complement principal subspace identify ma trix pd jm1 bjb j 10.38a projection matrix performs projection. hence displacement vector xn xn lies subspace orthogonal principal subspace illustrated figure 10.1. remark lowrank approximation. 10.38a saw projec tion matrix projects x onto x given x m1 bmb bb. 10.39 construction sum rankone matrices bmb see bbis draft 20230215 mathematics machine learning. feedback
10.3 projection perspective 331 symmetric rank m. therefore average squared reconstruction error also written 1 n n x n1 xn xn 2 1 n n x n1 xn bbxn 2 10.40a 1 n n x n1 i bbxn 2 . 10.40b finding orthonormal basis vectors b1 . . . bm minimize differ pca finds best rankm approximation identity matrix. ence original data xn projections xn equivalent finding best rankm approximation bbof identity matrix see section 4.6. tools reformulate loss function 10.29. jm 1 n n x n1 xn xn2 10.38b 1 n n x n1 x jm1 b j xnbj 2 . 10.41 explicitly compute squared norm exploit fact bj form onb yields jm 1 n n x n1 x jm1 b j xn2 1 n n x n1 x jm1 b j xnb j xn 10.42a 1 n n x n1 x jm1 b j xnx n bj 10.42b exploited symmetry dot product last step write b j xn x n bj. swap sums obtain jm x jm1 b j 1 n n x n1 xnx n z s bj x jm1 b j sbj 10.43a x jm1 trb j sbj x jm1 trsbjb j tr x jm1 bjb j z projection matrix 10.43b exploited property trace operator tr see 4.18 linear invariant cyclic permutations arguments. since assumed dataset centered i.e. ex 0 identify data covariance matrix. since projection matrix 10.43b con structed sum rankone matrices bjb j rank m. equation 10.43a implies formulate average squared reconstruction error equivalently covariance matrix data 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
332 dimensionality reduction principal component analysis projected onto orthogonal complement principal subspace. min imizing average squared reconstruction error therefore equivalent minimizing average squared reconstruction error equivalent minimizing projection data covariance matrix onto orthogonal complement principal subspace. minimizing variance data projected onto subspace ignore i.e. orthogonal complement principal subspace. equiva lently maximize variance projection retain principal subspace links projection loss immediately maximumvariance formulation pca discussed section 10.2. also means obtain solution obtained minimizing average squared reconstruction error equivalent maximizing variance projected data. maximumvariance perspective. therefore omit derivation identical one presented section 10.2 summarize results earlier light projection perspective. average squared reconstruction error projecting onto m dimensional principal subspace jm x jm1 λj 10.44 λj eigenvalues data covariance matrix. therefore minimize 10.44 need select smallest m eigenvalues implies corresponding eigenvectors basis orthogonal complement principal subspace. consequently means basis principal subspace comprises eigenvectors b1 . . . bm associated largest eigenvalues data covariance matrix. example 10.3 mnist digits embedding figure 10.1 embedding mnist digits 0 blue 1 orange twodimensional principal subspace using pca. four embeddings digits 0 1 principal subspace highlighted red corresponding original digit. figure 10.1 visualizes training data mmist digits 0 1 embedded vector subspace spanned first two principal com ponents. observe relatively clear separation 0s blue dots 1s orange dots see variation within individual draft 20230215 mathematics machine learning. feedback
10.4 eigenvector computation lowrank approximations 333 cluster. four embeddings digits 0 1 principal subspace highlighted red corresponding original digit. figure reveals variation within set 0 significantly greater variation within set 1. 10.4 eigenvector computation lowrank approximations previous sections obtained basis principal subspace eigenvectors associated largest eigenvalues data covariance matrix 1 n n x n1 xnx n 1 n xx 10.45 x x1 . . . xn rdn . 10.46 note x n matrix i.e. transpose typical data matrix bishop 2006 murphy 2012. get eigenvalues and corresponding eigenvectors s follow two approaches use eigendecomposition svd compute eigenvectors. perform eigendecomposition see section 4.2 compute eigenvalues eigenvectors directly. use singular value decomposition see section 4.5. since symmetric factorizes xxignoring factor 1 n eigen values squared singular values x. specifically svd x given x z dn u z dd σ z dn v z nn 10.47 u rdd v rnn orthogonal matrices σ rdn matrix whose nonzero entries singular values σii 0. follows 1 n xx 1 n uς v v z in σu 1 n uσςu . 10.48 results section 4.5 get columns u columns u eigenvectors s. eigenvectors xxand therefore s. furthermore eigenvalues λd related singular values x via λd σ2 n . 10.49 relationship eigenvalues singular values x provides connection maximum variance view sec tion 10.2 singular value decomposition. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
334 dimensionality reduction principal component analysis 10.4.1 pca using lowrank matrix approximations maximize variance projected data or minimize average squared reconstruction error pca chooses columns u 10.48 eigenvectors associated largest eigenvalues data covariance matrix identify u projection ma trix b 10.3 projects original data onto lowerdimensional subspace dimension m. eckartyoung theorem theorem 4.25 eckartyoung theorem section 4.6 offers direct way estimate lowdimensional represen tation. consider best rankm approximation xm argminrkam x a2 rdn 10.50 x 2 spectral norm defined 4.93. eckartyoung theorem states xm given truncating svd topm singular value. words obtain xm u z dm σm z mm v z mn rdn 10.51 orthogonal matrices u u1 . . . um rdm v v1 . . . vm rnm diagonal matrix σm rmm whose diago nal entries largest singular values x. 10.4.2 practical aspects finding eigenvalues eigenvectors also important funda mental machine learning methods require matrix decompositions. theory discussed section 4.2 solve eigenvalues roots characteristic polynomial. however matrices larger 44 possible would need find roots poly nomial degree 5 higher. however abelruffini theorem ruffini abelruffini theorem 1799 abel 1826 states exists algebraic solution problem polynomials degree 5 more. therefore practice np.linalg.eigh np.linalg.svd solve eigenvalues singular values using iterative methods implemented modern packages linear algebra. many applications such pca presented chapter require eigenvectors. would wasteful compute full de composition discard eigenvectors eigenvalues beyond first few. turns interested first eigenvectors with largest eigenvalues iterative processes directly optimize eigenvectors computationally effi cient full eigendecomposition or svd. extreme case needing first eigenvector simple method called power iteration power iteration efficient. power iteration chooses random vector x0 draft 20230215 mathematics machine learning. feedback
10.5 pca high dimensions 335 null space follows iteration xk1 sxk sxk k 0 1 . . . . 10.52 means vector xk multiplied every iteration invertible sufficient ensure x0 0. normalized i.e. always xk 1. sequence vectors con verges eigenvector associated largest eigenvalue s. original google pagerank algorithm page et al. 1999 uses al gorithm ranking web pages based hyperlinks. 10.5 pca high dimensions order pca need compute data covariance matrix. dimensions data covariance matrix d matrix. computing eigenvalues eigenvectors matrix computationally expensive scales cubically d. therefore pca discussed earlier infeasible high dimensions. example xn images 10000 pixels e.g. 100 100 pixel images would need compute eigendecomposition 10000 10000 covariance matrix. following provide solution problem case substantially fewer data points dimensions i.e. n d. assume centered dataset x1 . . . xn xn rd. data covariance matrix given 1 n xxrdd 10.53 x x1 . . . xn n matrix whose columns data points. assume n d i.e. number data points smaller dimensionality data. duplicate data points rank covariance matrix n n 1 many eigen values 0. intuitively means redundancies. following exploit turn dd covariance matrix n n covariance matrix whose eigenvalues positive. pca ended eigenvector equation sbm λmbm 1 . . . 10.54 bm basis vector principal subspace. let us rewrite equation bit defined 10.53 obtain sbm 1 n xxbm λmbm . 10.55 multiply xrnd lefthand side yields 1 n xx z nn xbm z cm λmxbm 1 n xxcm λmcm 10.56 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
336 dimensionality reduction principal component analysis get new eigenvectoreigenvalue equation λm remains eigen value confirms results section 4.5.3 nonzero eigenvalues xxequal nonzero eigenvalues xx. obtain eigenvector matrix 1 n xx rnn associated λm cm xbm. assuming duplicate data points matrix rank n invertible. also implies 1 n xx nonzero eigenvalues data covariance matrix s. n n matrix compute eigenvalues eigenvectors much efficiently original d data covariance matrix. eigenvectors 1 n xx going re cover original eigenvectors still need pca. currently know eigenvectors 1 n xx. leftmultiply eigenvalue eigenvector equation x get 1 n xx z xcm λmxcm 10.57 recover data covariance matrix again. also means recover xcm eigenvector s. remark. want apply pca algorithm discussed sec tion 10.6 need normalize eigenvectors xcm norm 1. 10.6 key steps pca practice following go individual steps pca using running example summarized figure 10.2. given twodimensional dataset figure 10.2a want use pca project onto onedimensional subspace. 1. mean subtraction start centering data computing mean µ dataset subtracting every single data point. ensures dataset mean 0 figure 10.2b. mean sub traction strictly necessary reduces risk numerical prob lems. 2. standardization divide data points standard deviation σd dataset every dimension 1 . . . d. data unit free variance 1 along axis indicated two arrows figure 10.2c. step completes standardization standardization data. 3. eigendecomposition covariance matrix compute data covariance matrix eigenvalues corresponding eigenvectors. since covariance matrix symmetric spectral theorem the orem 4.15 states find onb eigenvectors. fig ure 10.2d eigenvectors scaled magnitude corre draft 20230215 mathematics machine learning. feedback
10.6 key steps pca practice 337 figure 10.2 steps pca. a original dataset b centering c divide standard deviation d eigendecomposi tion e projection f mapping back original data space. 0 5 x1 2.5 0.0 2.5 5.0 x2 a original dataset. 0 5 x1 2.5 0.0 2.5 5.0 x2 b step 1 centering sub tracting mean data point. 0 5 x1 2.5 0.0 2.5 5.0 x2 c step 2 dividing standard deviation make data unit free. data variance 1 along axis. 0 5 x1 2.5 0.0 2.5 5.0 x2 d step 3 compute eigenval ues eigenvectors arrows data covariance matrix ellipse. 0 5 x1 2.5 0.0 2.5 5.0 x2 e step 4 project data onto principal subspace. 0 5 x1 2.5 0.0 2.5 5.0 x2 f undo standardization move projected data back original data space a. sponding eigenvalue. longer vector spans principal subspace denote u. data covariance matrix represented ellipse. 4. projection project data point xrd onto principal subspace get right need standardize xusing mean µd standard deviation σd training data dth dimension respectively xd xd µd σd 1 . . . 10.58 xd dth component x. obtain projection x bbx 10.59 coordinates z bx 10.60 respect basis principal subspace. here b ma trix contains eigenvectors associated largest eigenvalues data covariance matrix columns. pca returns coordinates 10.60 projections x. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
338 dimensionality reduction principal component analysis standardized dataset 10.59 yields projections context standardized dataset. obtain projection original data space i.e. standardization need undo standardization 10.58 multiply standard deviation adding mean obtain xd xd σd µd 1 . . . . 10.61 figure 10.2f illustrates projection original data space. example 10.4 mnist digits reconstruction following apply pca mnist digits dataset contains 60000 examples handwritten digits 0 9. digit image size 2828 i.e. contains 784 pixels interpret every image dataset vector x r784. examples digits shown figure 10.2. figure 10.1 effect increasing number principal components reconstruction. original pcs 1 pcs 10 pcs 100 pcs 500 illustration purposes apply pca subset mnist dig its focus digit 8. used 5389 training images digit 8 determined principal subspace detailed chap ter. used learned projection matrix reconstruct set test images illustrated figure 10.1. first row figure 10.1 shows set four original digits test set. following rows show reconstructions exactly digits using principal sub space dimensions 1 10 100 500 respectively. see even singledimensional principal subspace get halfway decent re construction original digits which however blurry generic. increasing number principal components pcs reconstruc tions become sharper details accounted for. 500 prin draft 20230215 mathematics machine learning. feedback
10.7 latent variable perspective 339 cipal components effectively obtain nearperfect reconstruction. choose 784 pcs would recover exact digit without compression loss. figure 10.2 shows average squared reconstruction error 1 n n x n1 xn xn 2 x im1 λi 10.62 function number principal components. see importance principal components drops rapidly marginal gains achieved adding pcs. matches exactly observation figure 10.3 discovered variance projected data captured principal compo nents. 550 pcs essentially fully reconstruct training data contains digit 8 some pixels around boundaries show variation across dataset always black. figure 10.2 average squared reconstruction error function number principal components. average squared reconstruction error sum eigenvalues orthogonal complement principal subspace. 0 200 400 600 800 number pcs 0 100 200 300 400 500 average squared reconstruction error 10.7 latent variable perspective previous sections derived pca without notion prob abilistic model using maximumvariance projection perspec tives. one hand approach may appealing allows us sidestep mathematical difficulties come probability the ory hand probabilistic model would offer us flex ibility useful insights. specifically probabilistic model would come likelihood function explicitly deal noisy observations which even discuss earlier allow us bayesian model comparison via marginal likelihood discussed section 8.6 view pca generative model allows us simulate new data 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
340 dimensionality reduction principal component analysis allow us make straightforward connections related algorithms deal data dimensions missing random applying bayes theorem give us notion novelty new data point give us principled way extend model e.g. mixture pca models pca derived earlier sections special case allow fully bayesian treatment marginalizing model parameters introducing continuousvalued latent variable z rm possible phrase pca probabilistic latentvariable model. tipping bishop 1999 proposed latentvariable model probabilistic pca ppca. probabilistic pca ppca ppca addresses aforementioned issues pca solution obtained maximizing variance projected space minimizing reconstruction error obtained special case maximum likelihood estimation noisefree setting. 10.7.1 generative process probabilistic model ppca explicitly write probabilistic model linear di mensionality reduction. assume continuous latent variable z rm standardnormal prior pz n 0 linear rela tionship latent variables observed x data x bz µ ϵ rd 10.63 ϵ n 0 σ2i gaussian observation noise b rdm µ rd describe linearaffine mapping latent observed variables. therefore ppca links latent observed variables via pxz b µ σ2 n x bz µ σ2i . 10.64 overall ppca induces following generative process zn n z 0 10.65 xn zn n x bzn µ σ2i 10.66 generate data point typical given model parameters follow ancestral sampling scheme first sample latent variable zn ancestral sampling pz. use zn 10.64 sample data point conditioned sampled zn i.e. xn px zn b µ σ2. generative process allows us write probabilistic model i.e. joint distribution random variables see section 8.4 px zb µ σ2 pxz b µ σ2pz 10.67 immediately gives rise graphical model figure 10.2 using results section 8.5. draft 20230215 mathematics machine learning. feedback
10.7 latent variable perspective 341 figure 10.2 graphical model probabilistic pca. observations xn explicitly depend corresponding latent variables zn n 0 . model parameters b µ likelihood parameter σ shared across dataset. xn b zn σ µ n 1 . . . n remark. note direction arrow connects latent variables z observed data x arrow points z x means ppca model assumes lowerdimensional latent cause z high dimensional observations x. end obviously interested finding something z given observations. get apply bayesian inference invert arrow implicitly go observations latent variables. example 10.5 generating new data using latent variables figure 10.1 generating new mnist digits. latent variables z used generate new data x bz. closer stay training data realistic generated data. figure 10.1 shows latent coordinates mnist digits 8 found pca using twodimensional principal subspace blue dots. query vector zin latent space generate image x bzthat resembles digit 8. show eight generated images corresponding latent space representation. depending query latent space generated images look different shape rotation size etc. query away training data see artifacts e.g. topleft topright digits. note intrinsic dimensionality generated images two. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
342 dimensionality reduction principal component analysis 10.7.2 likelihood joint distribution likelihood depend latent variables z. using results chapter 6 obtain likelihood proba bilistic model integrating latent variable z see section 8.4.3 px b µ σ2 z px z b µ σ2pzdz 10.68a z n x bz µ σ2i n z 0 dz . 10.68b section 6.5 know solution integral gaussian distribution mean exx ezbz µ eϵϵ µ 10.69 covariance matrix vx vzbz µ vϵϵ vzbz σ2i 10.70a bvzzb σ2i bb σ2i . 10.70b likelihood 10.68b used maximum likelihood map estimation model parameters. remark. cannot use conditional distribution 10.64 maxi mum likelihood estimation still depends latent variables. likelihood function require maximum likelihood or map estima tion function data x model parameters must depend latent variables. section 6.5 know gaussian random variable z linearaffine transformation x bz jointly gaussian dis tributed. already know marginals pz n z 0 px n x µ bb σ2i . missing crosscovariance given covx z covzbz µ b covzz z b . 10.71 therefore probabilistic model ppca i.e. joint distribution latent observed random variables explicitly given px z b µ σ2 n x z µ 0 bb σ2i b b 10.72 mean vector length covariance matrix size d m d m. 10.7.3 posterior distribution joint gaussian distribution px z b µ σ2 10.72 allows us determine posterior distribution pz x immediately applying draft 20230215 mathematics machine learning. feedback
10.8 reading 343 rules gaussian conditioning section 6.5.1. posterior distribu tion latent variable given observation x pz x n z m c 10.73 bbb σ2i1x µ 10.74 c bbb σ2i1b . 10.75 note posterior covariance depend observed data x. new observation xin data space use 10.73 determine posterior distribution corresponding latent variable z. co variance matrix c allows us assess confident embedding is. covariance matrix c small determinant which measures volumes tells us latent embedding zis fairly certain. obtain pos terior distribution pz x much variance may faced outlier. however explore posterior distribution under stand data points x plausible posterior. this exploit generative process underlying ppca allows us explore posterior distribution latent variables generating new data plausible posterior 1. sample latent variable zpz x posterior distribution latent variables 10.73. 2. sample reconstructed vector xpx z b µ σ2 10.64. repeat process many times explore posterior dis tribution 10.73 latent variables zand implications observed data. sampling process effectively hypothesizes data plausible posterior distribution. 10.8 reading derived pca two perspectives a maximizing variance projected space b minimizing average reconstruction error. how ever pca also interpreted different perspectives. let us recap done took highdimensional data x rd used matrix bto find lowerdimensional representation z rm. columns b eigenvectors data covariance matrix associated largest eigenvalues. lowdimensional representation z get highdimensional version in orig inal data space x x bz bbx rd bbis projection matrix. also think pca linear autoencoder illustrated fig autoencoder ure 10.2. autoencoder encodes data xn rd code zn rm code decodes xn similar xn. mapping data code called encoder mapping code back orig encoder inal data space called decoder. consider linear mappings decoder 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
344 dimensionality reduction principal component analysis figure 10.2 pca viewed linear autoencoder. encodes highdimensional data x lowerdimensional representation code z rm decodes z using decoder. decoded vector x orthogonal projection original data x onto mdimensional principal subspace. b x x z b encoder decoder original code rd rd rm code given zn bxn rm interested minimiz ing average squared error data xn reconstruction xn bzn n 1 . . . n obtain 1 n n x n1 xn xn2 1 n n x n1 xn bbxn 2 . 10.76 means end objective function 10.29 discussed section 10.3 obtain pca solution minimize squared autoencoding loss. replace linear map ping pca nonlinear mapping get nonlinear autoencoder. prominent example deep autoencoder linear func tions replaced deep neural networks. context encoder also known recognition network inference network whereas recognition network inference network decoder also called generator. generator another interpretation pca related information theory. think code smaller compressed version original data point. reconstruct original data using code get exact data point back slightly distorted noisy version it. means compression lossy. intuitively want code compressed version original data. maximize correlation original data lower dimensional code. formally related mutual information. would get solution pca discussed section 10.3 maximizing mutual information core concept information the ory mackay 2003. discussion ppca assumed parameters model i.e. b µ likelihood parameter σ2 known. tipping bishop 1999 describe derive maximum likelihood estimates parameters ppca setting note use different notation chapter. maximum likelihood parameters pro draft 20230215 mathematics machine learning. feedback
10.8 reading 345 jecting ddimensional data onto mdimensional subspace µml 1 n n x n1 xn 10.77 bml λ σ2i 1 2 r 10.78 σ2 ml 1 m x jm1 λj 10.79 rdm contains eigenvectors data covariance matrix matrix λ σ2i 10.78 guaranteed positive semidefinite smallest eigenvalue data covariance matrix bounded noise variance σ2. λ diagλ1 . . . λm rmm diagonal matrix eigenvalues associated principal axes diagonal r rmm arbitrary orthogonal matrix. maximum likelihood solution bml unique arbitrary orthogonal transformation e.g. right multiply bml rotation matrix r 10.78 essentially singular value decomposition see section 4.5. outline proof given tipping bishop 1999. maximum likelihood estimate µ given 10.77 sample mean data. maximum likelihood estimator observation noise variance σ2 given 10.79 average variance orthog onal complement principal subspace i.e. average leftover vari ance cannot capture first principal components treated observation noise. noisefree limit σ 0 ppca pca provide identical solutions since data covariance matrix symmetric di agonalized see section 4.4 i.e. exists matrix eigenvectors λt 1 . 10.80 ppca model data covariance matrix covariance matrix gaussian likelihood px b µ σ2 bbσ2i see 10.70b. σ 0 obtain bbso data covariance must equal pca data covariance and factorization given 10.80 covx λt 1 bb b λ 1 2 r 10.81 i.e. obtain maximum likelihood estimate 10.78 σ 0. 10.78 10.80 becomes clear ppca performs de composition data covariance matrix. streaming setting data arrives sequentially recom mended use iterative expectation maximization em algorithm maximum likelihood estimation roweis 1998. determine dimensionality latent variables the length code dimensionality lowerdimensional subspace onto project data gavish donoho 2014 suggest heuristic that estimate noise variance σ2 data 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
346 dimensionality reduction principal component analysis discard singular values smaller 4σ 3 . alternatively use nested crossvalidation section 8.6.1 bayesian model selection cri teria discussed section 8.6.2 determine good estimate intrinsic dimensionality data minka 2001b. similar discussion linear regression chapter 9 place prior distribution parameters model integrate out. so a avoid point estimates parameters issues come point estimates see section 8.6 b al low automatic selection appropriate dimensionality latent space. bayesian pca proposed bishop 1999 bayesian pca prior pµ b σ2 placed model parameters. generative process allows us integrate model parameters instead condi tioning them addresses overfitting issues. since integration analytically intractable bishop 1999 proposes use approximate in ference methods mcmc variational inference. refer work gilks et al. 1996 blei et al. 2017 details approximate inference techniques. ppca considered linear model pxn zn n xn bzn µ σ2i prior pzn n 0 observation dimensions affected amount noise. allow observation dimension different variance σ2 d obtain factor analysis factor analysis fa spearman 1904 bartholomew et al. 2011. means fa gives likelihood flexibility ppca still forces data explained model parameters b µ.however fa overly flexible likelihood would able explain noise. longer allows closedform maximum likelihood solution need use iterative scheme expectation maximization algorithm estimate model parameters. ppca station ary points global optima longer holds fa. compared ppca fa change scale data return different solutions rotate data. algorithm also closely related pca independent com independent component analysis ponent analysis ica hyvarinen et al. 2001. starting ica latentvariable perspective pxn zn n xn bzn µ σ2i change prior zn nongaussian distributions. ica used blindsource separation. imagine busy train station blindsource separation many people talking. ears play role microphones linearly mix different speech signals train station. goal blind source separation identify constituent parts mixed signals. discussed previously context maximum likelihood estimation ppca original pca solution invariant rotation. therefore pca identify best lowerdimensional subspace sig nals live signals murphy 2012. ica addresses issue modifying prior distribution pz latent sources draft 20230215 mathematics machine learning. feedback
10.8 reading 347 require nongaussian priors pz. refer books hyvarinen et al. 2001 murphy 2012 details ica. pca factor analysis ica three examples dimensionality re duction linear models. cunningham ghahramani 2015 provide broader survey linear dimensionality reduction. ppca model discussed allows several important ex tensions. section 10.5 explained pca in put dimensionality significantly greater number n data points. exploiting insight pca performed computing many inner products idea pushed extreme consid ering infinitedimensional features. kernel trick basis kernel kernel trick kernel pca pca allows us implicitly compute inner products infinite dimensional features sch olkopf et al. 1998 sch olkopf smola 2002. nonlinear dimensionality reduction techniques de rived pca burges 2010 provides good overview. auto encoder perspective pca discussed previously section used render pca special case deep autoencoder. deep autoencoder deep autoencoder encoder decoder represented multilayer feedforward neural networks nonlinear mappings. set activation functions neural networks identity model becomes equivalent pca. different approach nonlinear dimensionality reduction gaussian process latentvariable gaussian process latentvariable model model gplvm proposed lawrence 2005. gplvm starts gplvm latentvariable perspective used derive ppca replaces linear relationship latent variables z observations x gaussian process gp. instead estimating parameters mapping as ppca gplvm marginalizes model parameters makes point estimates latent variables z. similar bayesian pca bayesian gplvm proposed titsias lawrence bayesian gplvm 2010 maintains distribution latent variables z uses approx imate inference integrate well. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
11 density estimation gaussian mixture models earlier chapters covered already two fundamental problems machine learning regression chapter 9 dimensionality reduction chapter 10. chapter look third pillar ma chine learning density estimation. journey introduce impor tant concepts expectation maximization em algorithm latent variable perspective density estimation mixture models. apply machine learning data often aim represent data way. straightforward way take data points them selves representation data see figure 11.1 example. however approach may unhelpful dataset huge interested representing characteristics data. density esti mation represent data compactly using density paramet ric family e.g. gaussian beta distribution. example may looking mean variance dataset order represent data compactly using gaussian distribution. mean variance found using tools discussed section 8.3 maximum likelihood maximum posteriori estimation. use mean variance gaussian represent distribution underlying data i.e. think dataset typical realization distribution sample it. figure 11.1 twodimensional dataset cannot meaningfully represented gaussian. 5 0 5 x1 4 2 0 2 4 x2 348 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.
11.1 gaussian mixture model 349 practice gaussian or similarly distributions encoun tered far limited modeling capabilities. example gaussian approximation density generated data figure 11.1 would poor approximation. following look ex pressive family distributions use density estimation mixture models. mixture model mixture models used describe distribution px convex combination k simple base distributions px k x k1 πkpkx 11.1 0 πk 1 k x k1 πk 1 11.2 components pk members family basic distributions e.g. gaussians bernoullis gammas πk mixture weights. mixture weight mixture models expressive corresponding base distri butions allow multimodal data representations i.e. describe datasets multiple clusters example fig ure 11.1. focus gaussian mixture models gmms basic distributions gaussians. given dataset aim maximize likelihood model parameters train gmm. purpose use results chapter 5 chapter 6 section 7.2. however unlike applications discussed earlier linear regression pca find closedform maximum likelihood solution. instead arrive set dependent simultaneous equations solve iteratively. 11.1 gaussian mixture model gaussian mixture model density model combine finite gaussian mixture model number k gaussian distributions n x µk σk px θ k x k1 πkn x µk σk 11.3 0 πk 1 k x k1 πk 1 11.4 defined θ µk σk πk k 1 . . . k collection parameters model. convex combination gaussian distri bution gives us significantly flexibility modeling complex densi ties simple gaussian distribution which recover 11.3 k 1. illustration given figure 11.1 displaying weighted 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
350 density estimation gaussian mixture models figure 11.1 gaussian mixture model. gaussian mixture distribution black composed convex combination gaussian distributions expressive individual component. dashed lines represent weighted gaussian components. 4 2 0 2 4 6 8 x 0.00 0.05 0.10 0.15 0.20 0.25 0.30 px component 1 component 2 component 3 gmm density components mixture density given px θ 0.5n x 2 1 2 0.2n x 1 2 0.3n x 4 1 . 11.5 11.2 parameter learning via maximum likelihood assume given dataset x x1 . . . xn xn n 1 . . . n drawn i.i.d. unknown distribution px. ob jective find good approximationrepresentation unknown distribution px means gmm k mixture components. parameters gmm k means µk covariances σk mixture weights πk. summarize free parameters θ πk µk σk k 1 . . . k. example 11.1 initial setting figure 11.1 initial setting gmm black mixture three mixture components dashed seven data points discs. 5 0 5 10 15 x 0.00 0.05 0.10 0.15 0.20 0.25 0.30 px π1nxµ1 σ2 1 π2nxµ2 σ2 2 π3nxµ3 σ2 3 gmm density throughout chapter simple running example helps us illustrate visualize important concepts. draft 20230215 mathematics machine learning. feedback
11.2 parameter learning via maximum likelihood 351 consider onedimensional dataset x 3 2.5 1 0 2 4 5 consisting seven data points wish find gmm k 3 components models density data. initialize mixture components p1x n x 4 1 11.6 p2x n x 0 0.2 11.7 p3x n x 8 3 11.8 assign equal weights π1 π2 π3 1 3. corresponding model and data points shown figure 11.1. following detail obtain maximum likelihood esti mate θml model parameters θ. start writing like lihood i.e. predictive distribution training data given pa rameters. exploit i.i.d. assumption leads factorized likelihood px θ n n1 pxn θ pxn θ k x k1 πkn xn µk σk 11.9 every individual likelihood term pxn θ gaussian mixture density. obtain loglikelihood log px θ n x n1 log pxn θ n x n1 log k x k1 πkn xn µk σk z l . 11.10 aim find parameters θ ml maximize loglikelihood l defined 11.10. normal procedure would compute gradient dldθ loglikelihood respect model parameters θ set 0 solve θ. however unlike previous examples max imum likelihood estimation e.g. discussed linear regression section 9.2 cannot obtain closedform solution. however exploit iterative scheme find good model parameters θml turn em algorithm gmms. key idea update one model parameter time keeping others fixed. remark. consider single gaussian desired density sum k 11.10 vanishes log applied directly gaussian component get log n x µ σ d 2 log2π 1 2 log detσ 1 2x µσ1x µ. 11.11 simple form allows us find closedform maximum likelihood esti mates µ σ discussed chapter 8. 11.10 cannot move 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
352 density estimation gaussian mixture models log sum k cannot obtain simple closedform maximum likelihood solution. local optimum function exhibits property gradi ent respect parameters must vanish necessary condition see chapter 7. case obtain following necessary conditions optimize loglikelihood 11.10 respect gmm param eters µk σk πk l µk 0 n x n1 log pxn θ µk 0 11.12 l σk 0 n x n1 log pxn θ σk 0 11.13 l πk 0 n x n1 log pxn θ πk 0 . 11.14 three necessary conditions applying chain rule see sec tion 5.2.2 require partial derivatives form log pxn θ θ 1 pxn θ pxn θ θ 11.15 θ µk σk πk k 1 . . . k model parameters 1 pxn θ 1 pk j1 πjn xn µj σj . 11.16 following compute partial derivatives 11.12 11.14. this introduce quantity play central role remainder chapter responsibilities. 11.2.1 responsibilities define quantity rnk πkn xn µk σk pk j1 πjn xn µj σj 11.17 responsibility kth mixture component nth data point. responsibility responsibility rnk kth mixture component data point xn proportional likelihood pxn πk µk σk πkn xn µk σk 11.18 mixture component given data point. therefore mixture com rn follows boltzmanngibbs distribution. ponents high responsibility data point data point could plausible sample mixture component. note rn rn1 . . . rnkrk normalized probability vector i.e. draft 20230215 mathematics machine learning. feedback
11.2 parameter learning via maximum likelihood 353 p k rnk 1 rnk 0. probability vector distributes probabil ity mass among k mixture components think rn soft assignment xn k mixture components. therefore re responsibility rnk probability kth mixture component generated nth data point. sponsibility rnk 11.17 represents probability xn generated kth mixture component. example 11.2 responsibilities example figure 11.1 compute responsibilities rnk 1.0 0.0 0.0 1.0 0.0 0.0 0.057 0.943 0.0 0.001 0.999 0.0 0.0 0.066 0.934 0.0 0.0 1.0 0.0 0.0 1.0 rnk . 11.19 nth row tells us responsibilities mixture components xn. sum k responsibilities data point sum every row 1. kth column gives us overview responsibility kth mixture component. see third mixture component third column responsible first four data points takes much responsibility remaining data points. sum entries column gives us values nk i.e. total responsibility kth mixture component. example get n1 2.058 n2 2.008 n3 2.934. following determine updates model parameters µk σk πk given responsibilities. see update equa tions depend responsibilities makes closedform solu tion maximum likelihood estimation problem impossible. however given responsibilities updating one model parameter time keeping others fixed. this recompute responsibilities. iterating two steps eventually converge lo cal optimum specific instantiation em algorithm. discuss detail section 11.3. 11.2.2 updating means theorem 11.1 update gmm means. update mean pa rameters µk k 1 . . . k gmm given µnew k pn n1 rnkxn pn n1 rnk 11.20 responsibilities rnk defined 11.17. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
354 density estimation gaussian mixture models remark. update means µk individual mixture compo nents 11.20 depends means covariance matrices σk mix ture weights πk via rnk given 11.17. therefore cannot obtain closedform solution µk once. proof 11.15 see gradient loglikelihood respect mean parameters µk k 1 . . . k requires us compute partial derivative pxn θ µk k x j1 πj n xn µj σj µk πk n xn µk σk µk 11.21a πkxn µkσ1 k n xn µk σk 11.21b exploited kth mixture component depends µk. use result 11.21b 11.15 put everything together desired partial derivative l respect µk given l µk n x n1 log pxn θ µk n x n1 1 pxn θ pxn θ µk 11.22a n x n1 xn µkσ1 k πkn xn µk σk pk j1 πjn xn µj σj z rnk 11.22b n x n1 rnkxn µkσ1 k . 11.22c used identity 11.16 result partial deriva tive 11.21b get 11.22b. values rnk responsibilities defined 11.17. solve 11.22c µnew k lµnew k µk 0and obtain n x n1 rnkxn n x n1 rnkµnew k µnew k pn n1 rnkxn pn n1 rnk 1 nk n x n1 rnkxn 11.23 defined nk n x n1 rnk 11.24 total responsibility kth mixture component entire dataset. concludes proof theorem 11.1. intuitively 11.20 interpreted importanceweighted monte carlo estimate mean importance weights data point xn responsibilities rnk kth cluster xn k 1 . . . k. draft 20230215 mathematics machine learning. feedback
11.2 parameter learning via maximum likelihood 355 therefore mean µk pulled toward data point xn strength figure 11.2 update mean parameter mixture component gmm. mean µ pulled toward individual data points weights given corresponding responsibilities. r1 r2 r3 x1 x2 x3 µ given rnk. means pulled stronger toward data points corresponding mixture component high responsibility i.e. high likelihood. figure 11.2 illustrates this. also interpret mean up date 11.20 expected value data points distri bution given rk r1k . . . rnknk 11.25 normalized probability vector i.e. µk erkx . 11.26 example 11.3 mean updates figure 11.3 effect updating mean values gmm. a gmm updating mean values b gmm updating mean values µk retaining variances mixture weights. 5 0 5 10 15 x 0.00 0.05 0.10 0.15 0.20 0.25 0.30 px π1nxµ1 σ2 1 π2nxµ2 σ2 2 π3nxµ3 σ2 3 gmm density a gmm density individual components prior updating mean values. 5 0 5 10 15 x 0.00 0.05 0.10 0.15 0.20 0.25 0.30 px π1nxµ1 σ2 1 π2nxµ2 σ2 2 π3nxµ3 σ2 3 gmm density b gmm density individual components updating mean values. example figure 11.1 mean values updated fol lows µ1 4 2.7 11.27 µ2 0 0.4 11.28 µ3 8 3.7 11.29 see means first third mixture component move toward regime data whereas mean second component change dramatically. figure 11.3 illustrates change figure 11.3a shows gmm density prior updating means figure 11.3b shows gmm density updating mean values µk. update mean parameters 11.20 look fairly straight forward. however note responsibilities rnk function πj µj σj j 1 . . . k updates 11.20 depend parameters gmm closedform solution ob tained linear regression section 9.2 pca chapter 10 cannot obtained. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
356 density estimation gaussian mixture models 11.2.3 updating covariances theorem 11.2 updates gmm covariances. update co variance parameters σk k 1 . . . k gmm given σnew k 1 nk n x n1 rnkxn µkxn µk 11.30 rnk nk defined 11.17 11.24 respectively. proof prove theorem 11.2 approach compute partial derivatives loglikelihood l respect covariances σk set 0 solve σk. start general approach l σk n x n1 log pxn θ σk n x n1 1 pxn θ pxn θ σk . 11.31 already know 1pxn θ 11.16. obtain remaining par tial derivative pxn θσk write definition gaus sian distribution pxn θ see 11.9 drop terms kth. obtain pxn θ σk 11.32a σk πk2πd 2 detσk1 2 exp 1 2xn µkσ1 k xn µk 11.32b πk2πd 2 σk detσk1 2 exp 1 2xn µkσ1 k xn µk detσk1 2 σk exp 1 2xn µkσ1 k xn µk . 11.32c use identities σk detσk1 2 5.101 1 2 detσk1 2 σ1 k 11.33 σk xn µkσ1 k xn µk 5.103 σ1 k xn µkxn µkσ1 k 11.34 obtain after rearranging desired partial derivative required 11.31 pxn θ σk πk n xn µk σk 1 2σ1 k σ1 k xn µkxn µkσ1 k . 11.35 putting everything together partial derivative loglikelihood draft 20230215 mathematics machine learning. feedback
11.2 parameter learning via maximum likelihood 357 respect σk given l σk n x n1 log pxn θ σk n x n1 1 pxn θ pxn θ σk 11.36a n x n1 πkn xn µk σk pk j1 πjn xn µj σj z rnk 1 2σ1 k σ1 k xn µkxn µkσ1 k 11.36b 1 2 n x n1 rnkσ1 k σ1 k xn µkxn µkσ1 k 11.36c 1 2σ1 k n x n1 rnk z nk 1 2σ1 k n x n1 rnkxn µkxn µk σ1 k . 11.36d see responsibilities rnk also appear partial derivative. setting partial derivative 0 obtain necessary optimality condition nkς1 k σ1 k n x n1 rnkxn µkxn µk σ1 k 11.37a nki n x n1 rnkxn µkxn µk σ1 k . 11.37b solving σk obtain σnew k 1 nk n x n1 rnkxn µkxn µk 11.38 rk probability vector defined 11.25. gives us sim ple update rule σk k 1 . . . k proves theorem 11.2. similar update µk 11.20 interpret update covariance 11.30 importanceweighted expected value square centered data xk x1 µk . . . xn µk. example 11.4 variance updates example figure 11.1 variances updated follows σ2 1 1 0.14 11.39 σ2 2 0.2 0.44 11.40 σ2 3 3 1.53 11.41 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
358 density estimation gaussian mixture models see variances first third component shrink significantly whereas variance second component increases slightly. figure 11.4 illustrates setting. figure 11.4a identical but zoomed in figure 11.3b shows gmm density indi vidual components prior updating variances. figure 11.4b shows gmm density updating variances. figure 11.4 effect updating variances gmm. a gmm updating variances b gmm updating variances retaining means mixture weights. 4 2 0 2 4 6 8 x 0.00 0.05 0.10 0.15 0.20 0.25 0.30 px π1nxµ1 σ2 1 π2nxµ2 σ2 2 π3nxµ3 σ2 3 gmm density a gmm density individual components prior updating variances. 4 2 0 2 4 6 8 x 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 px π1nxµ1 σ2 1 π2nxµ2 σ2 2 π3nxµ3 σ2 3 gmm density b gmm density individual components updating variances. similar update mean parameters interpret 11.30 monte carlo estimate weighted covariance data points xn associated kth mixture component weights responsibilities rnk. updates mean parameters up date depends πj µj σj j 1 . . . k responsibilities rnk prohibits closedform solution. 11.2.4 updating mixture weights theorem 11.3 update gmm mixture weights. mixture weights gmm updated πnew k nk n k 1 . . . k 11.42 n number data points nk defined 11.24. proof find partial derivative loglikelihood respect weight parameters πk k 1 . . . k account con straint p k πk 1 using lagrange multipliers see section 7.2. lagrangian l l λ k x k1 πk 1 11.43a draft 20230215 mathematics machine learning. feedback
11.2 parameter learning via maximum likelihood 359 n x n1 log k x k1 πkn xn µk σk λ k x k1 πk 1 11.43b l loglikelihood 11.10 second term encodes equality constraint mixture weights need sum 1. obtain partial derivative respect πk l πk n x n1 n xn µk σk pk j1 πjn xn µj σj λ 11.44a 1 πk n x n1 πkn xn µk σk pk j1 πjn xn µj σj z nk λ nk πk λ 11.44b partial derivative respect lagrange multiplier λ l λ k x k1 πk 1 . 11.45 setting partial derivatives 0 necessary condition optimum yields system equations πk nk λ 11.46 1 k x k1 πk . 11.47 using 11.46 11.47 solving πk obtain k x k1 πk 1 k x k1 nk λ 1 n λ 1 λ n . 11.48 allows us substitute n λ 11.46 obtain πnew k nk n 11.49 gives us update weight parameters πk proves theo rem 11.3. identify mixture weight 11.42 ratio to tal responsibility kth cluster number data points. since n p k nk number data points also interpreted total responsibility mixture components together πk relative importance kth mixture component dataset. remark. since nk pn i1 rnk update equation 11.42 mix ture weights πk also depends πj µj σj j 1 . . . k via re sponsibilities rnk. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
360 density estimation gaussian mixture models example 11.5 weight parameter updates figure 11.5 effect updating mixture weights gmm. a gmm updating mixture weights b gmm updating mixture weights retaining means variances. note different scales vertical axes. 4 2 0 2 4 6 8 x 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 px π1nxµ1 σ2 1 π2nxµ2 σ2 2 π3nxµ3 σ2 3 gmm density a gmm density individual components prior updating mixture weights. 4 2 0 2 4 6 8 x 0.00 0.05 0.10 0.15 0.20 0.25 0.30 px π1nxµ1 σ2 1 π2nxµ2 σ2 2 π3nxµ3 σ2 3 gmm density b gmm density individual components updating mixture weights. running example figure 11.1 mixture weights up dated follows π1 1 3 0.29 11.50 π2 1 3 0.29 11.51 π3 1 3 0.42 11.52 see third component gets weightimportance components become slightly less important. figure 11.5 illustrates effect updating mixture weights. figure 11.5a identical figure 11.4b shows gmm density individual components prior updating mixture weights. figure 11.5b shows gmm density updating mixture weights. overall updated means variances weights once obtain gmm shown figure 11.5b. compared initialization shown figure 11.1 see parameter updates caused gmm density shift mass toward data points. updating means variances weights once gmm fit figure 11.5b already remarkably better initialization figure 11.1. also evidenced loglikelihood values in creased 28.3 initialization 14.4 one complete update cycle. 11.3 em algorithm unfortunately updates 11.20 11.30 11.42 consti tute closedform solution updates parameters µk σk πk mixture model responsibilities rnk depend pa rameters complex way. however results suggest simple iterative scheme finding solution parameters estimation problem via maximum likelihood. expectation maximization algorithm em algo em algorithm draft 20230215 mathematics machine learning. feedback
11.3 em algorithm 361 rithm proposed dempster et al. 1977 general iterative scheme learning parameters maximum likelihood map mixture models and generally latentvariable models. example gaussian mixture model choose initial values µk σk πk alternate convergence estep evaluate responsibilities rnk posterior probability data point n belonging mixture component k. mstep use updated responsibilities reestimate parameters µk σk πk. every step em algorithm increases loglikelihood function neal hinton 1999. convergence check loglikelihood parameters directly. concrete instantiation em algorithm estimating parameters gmm follows 1. initialize µk σk πk. 2. estep evaluate responsibilities rnk every data point xn using cur rent parameters πk µk σk rnk πkn xn µk σk p j πjn xn µj σj . 11.53 3. mstep reestimate parameters πk µk σk using current responsi bilities rnk from estep updated means µk 11.54 subsequently used 11.55 update corresponding covariances. µk 1 nk n x n1 rnkxn 11.54 σk 1 nk n x n1 rnkxn µkxn µk 11.55 πk nk n . 11.56 example 11.6 gmm fit figure 11.6 em algorithm applied gmm figure 11.1. a final gmm fit b negative loglikelihood function em iteration. 5 0 5 10 15 x 0.00 0.05 0.10 0.15 0.20 0.25 0.30 px π1nxµ1 σ2 1 π2nxµ2 σ2 2 π3nxµ3 σ2 3 gmm density a final gmm fit. five iterations em algorithm converges returns gmm. 0 1 2 3 4 5 iteration 14 16 18 20 22 24 26 28 negative loglikelihood b negative loglikelihood function em iterations. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
362 density estimation gaussian mixture models figure 11.7 illustration em algorithm fitting gaussian mixture model three components twodimensional dataset. a dataset b negative loglikelihood lower better function em iterations. red dots indicate iterations mixture components corresponding gmm fits shown c f. yellow discs indicate means gaussian mixture components. figure 11.8a shows final gmm fit. 10 5 0 5 10 x1 10 5 0 5 10 x2 a dataset. 0 20 40 60 em iteration 104 4 103 6 103 negative loglikelihood b negative loglikelihood. 10 5 0 5 10 x1 10 5 0 5 10 x2 c em initialization. 10 5 0 5 10 x1 10 5 0 5 10 x2 d em one iteration. 10 5 0 5 10 x1 10 5 0 5 10 x2 e em 10 iterations. 10 5 0 5 10 x1 10 5 0 5 10 x2 f em 62 iterations. run em example figure 11.1 obtain final result shown figure 11.6a five iterations figure 11.6b shows negative loglikelihood evolves function em iterations. final gmm given px 0.29n x 2.75 0.06 0.28n x 0.50 0.25 0.43n x 3.64 1.63 . 11.57 applied em algorithm twodimensional dataset shown figure 11.1 k 3 mixture components. figure 11.7 illustrates steps em algorithm shows negative loglikelihood function em iteration figure 11.7b. figure 11.8a shows draft 20230215 mathematics machine learning. feedback
11.4 latentvariable perspective 363 figure 11.8 gmm fit responsibilities em converges. a gmm fit em converges b data point colored according responsibilities mixture components. 5 0 5 x1 6 4 2 0 2 4 6 x2 a gmm fit 62 iterations. 5 0 5 x1 6 4 2 0 2 4 6 x2 b dataset colored according respon sibilities mixture components. corresponding final gmm fit. figure 11.8b visualizes final respon sibilities mixture components data points. dataset colored according responsibilities mixture components em converges. single mixture component clearly responsible data left overlap two data clusters right could generated two mixture components. becomes clear data points cannot uniquely assigned single component either blue yellow responsibilities two clusters points around 0.5. 11.4 latentvariable perspective look gmm perspective discrete latentvariable model i.e. latent variable z attain finite set val ues. contrast pca latent variables continuous valued numbers rm. advantages probabilistic perspective i jus tify ad hoc decisions made previous sections ii allows concrete interpretation responsibilities posterior probabil ities iii iterative algorithm updating model parameters derived principled manner em algorithm maximum likelihood parameter estimation latentvariable models. 11.4.1 generative process probabilistic model derive probabilistic model gmms useful think generative process i.e. process allows us generate data using probabilistic model. assume mixture model k components data point x generated exactly one mixture component. introduce binary indicator variable zk 0 1 two states see section 6.2 indicates whether kth mixture component generated data point 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
364 density estimation gaussian mixture models px zk 1 n x µk σk . 11.58 define z z1 . . . zkrk probability vector consisting k 1 many 0s exactly one 1. example k 3 valid z would z z1 z2 z3 0 1 0 would select second mixture component since z2 1. remark. sometimes kind probability distribution called multi noulli generalization bernoulli distribution two values murphy 2012. properties z imply pk k1 zk 1. therefore z onehot onehot encoding encoding also 1ofk representation. 1ofk representation thus far assumed indicator variables zk known. how ever practice case place prior distribution pz π π1 . . . πk k x k1 πk 1 11.59 latent variable z. kth entry πk pzk 1 11.60 probability vector describes probability kth mixture component generated data point x. figure 11.9 graphical model gmm single data point. π z x σk µk k 1 . . . k remark sampling gmm. construction latentvariable model see corresponding graphical model figure 11.9 lends it self simple sampling procedure generative process generate data 1. sample zi pz. 2. sample xi px zi 1. first step select mixture component via onehot encod ing z random according pz π second step draw sample corresponding mixture component. discard samples latent variable left xi valid samples gmm. kind sampling samples random variables depend samples variables parents graphical model called ancestral sampling. ancestral sampling generally probabilistic model defined joint distribution data latent variables see section 8.4. prior pz defined 11.59 11.60 conditional px z 11.58 obtain k components joint distribution via px zk 1 px zk 1pzk 1 πkn x µk σk 11.61 draft 20230215 mathematics machine learning. feedback
11.4 latentvariable perspective 365 k 1 . . . k px z px z1 1 . . . px zk 1 π1n x µ1 σ1 . . . πkn x µk σk 11.62 fully specifies probabilistic model. 11.4.2 likelihood obtain likelihood px θ latentvariable model need marginalize latent variables see section 8.4.3. case done summing latent variables joint px z 11.62 px θ x z px θ zpz θ θ µk σk πk k 1 . . . k . 11.63 explicitly condition parameters θ probabilistic model previously omitted. 11.63 sum k possible one hot encodings z denoted p z. since single nonzero single entry z k possible configurations settings z. example k 3 z configurations 1 0 0 0 1 0 0 0 1 . 11.64 summing possible configurations z 11.63 equivalent looking nonzero entry zvector writing px θ x z px θ zpz θ 11.65a k x k1 px θ zk 1pzk 1 θ 11.65b desired marginal distribution given px θ 11.65b k x k1 px θ zk 1pzk 1θ 11.66a k x k1 πkn x µk σk 11.66b identify gmm model 11.3. given dataset x immediately obtain likelihood px θ n n1 pxn θ 11.66b n n1 k x k1 πkn xn µk σk 11.67 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
366 density estimation gaussian mixture models figure 11.1 graphical model gmm n data points. π zn xn σk µk n 1 . . . n k 1 . . . k exactly gmm likelihood 11.9. therefore latent variable model latent indicators zk equivalent way thinking gaussian mixture model. 11.4.3 posterior distribution let us brief look posterior distribution latent variable z. according bayes theorem posterior kth component generated data point x pzk 1 x pzk 1px zk 1 px 11.68 marginal px given 11.66b. yields posterior distribution kth indicator variable zk pzk 1 x pzk 1px zk 1 pk j1 pzj 1px zj 1 πkn x µk σk pk j1 πjn x µj σj 11.69 identify responsibility kth mixture component data point x. note omitted explicit conditioning gmm parameters πk µk σk k 1 . . . k. 11.4.4 extension full dataset thus far discussed case dataset consists single data point x. however concepts prior posterior directly extended case n data points x x1 . . . xn. probabilistic interpretation gmm every data point xn pos sesses latent variable zn zn1 . . . znkrk . 11.70 previously when considered single data point x omitted index n becomes important. draft 20230215 mathematics machine learning. feedback
11.4 latentvariable perspective 367 share prior distribution π across latent variables zn. corresponding graphical model shown figure 11.1 use plate notation. conditional distribution px1 . . . xn z1 . . . zn factorizes data points given px1 . . . xn z1 . . . zn n n1 pxn zn . 11.71 obtain posterior distribution pznk 1 xn follow reasoning section 11.4.3 apply bayes theorem obtain pznk 1 xn pxn znk 1pznk 1 pk j1 pxn znj 1pznj 1 11.72a πkn xn µk σk pk j1 πjn xn µj σj rnk . 11.72b means pzk 1 xn posterior probability kth mixture component generated data point xn corresponds re sponsibility rnk introduced 11.17. responsibilities also intuitive also mathematically justified interpreta tion posterior probabilities. 11.4.5 em algorithm revisited em algorithm introduced iterative scheme maximum likelihood estimation derived principled way latent variable perspective. given current setting θt model parameters estep calculates expected loglikelihood qθ θt ez xθtlog px z θ 11.73a z log px z θpz x θtdz 11.73b expectation log px z θ taken respect poste rior pz x θt latent variables. mstep selects updated set model parameters θt1 maximizing 11.73b. although em iteration increase loglikelihood guarantees em converges maximum likelihood solution. possible em algorithm converges local maximum loglikelihood. different initializations parameters θ could used multiple em runs reduce risk ending bad local optimum. go details here refer excellent expositions rogers girolami 2016 bishop 2006. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
368 density estimation gaussian mixture models 11.5 reading gmm considered generative model sense straightforward generate new data using ancestral sampling bishop 2006. given gmm parameters πk µk σk k 1 . . . k sample index k probability vector π1 . . . πkand sample data point x n µk σk . repeat n times obtain dataset generated gmm. figure 11.1 generated using procedure. throughout chapter assumed number components k known. practice often case. however could use nested crossvalidation discussed section 8.6.1 find good models. gaussian mixture models closely related kmeans clustering algorithm. kmeans also uses em algorithm assign data points clusters. treat means gmm cluster centers ignore covariances or set i arrive kmeans. also nicely described mackay 2003 kmeans makes hard assignment data points cluster centers µk whereas gmm makes soft assignment via responsibilities. touched upon latentvariable perspective gmms em algorithm. note em used parameter learning general latentvariable models e.g. nonlinear statespace models ghahramani roweis 1999 roweis ghahramani 1999 reinforcement learning discussed barber 2012. therefore latentvariable per spective gmm useful derive corresponding em algorithm principled way bishop 2006 barber 2012 murphy 2012. discussed maximum likelihood estimation via em algo rithm finding gmm parameters. standard criticisms maximum likelihood also apply here linear regression maximum likelihood suffer severe overfitting. gmm case happens mean mix ture component identical data point covariance tends 0. then likelihood approaches infinity. bishop 2006 barber 2012 discuss issue detail. obtain point estimate parameters πk µk σk k 1 . . . k give indication uncertainty pa rameter values. bayesian approach would place prior param eters used obtain posterior distribution param eters. posterior allows us compute model evidence marginal likelihood used model comparison gives us principled way determine number mixture components. un fortunately closedform inference possible setting conjugate prior model. however approximations variational inference used obtain approximate posterior bishop 2006. draft 20230215 mathematics machine learning. feedback
11.5 reading 369 figure 11.1 histogram orange bars kernel density estimation blue line. kernel density estimator produces smooth estimate underlying density whereas histogram unsmoothed count measure many data points black fall single bin. 4 2 0 2 4 6 8 x 0.00 0.05 0.10 0.15 0.20 0.25 0.30 px data kde histogram chapter discussed mixture models density estimation. plethora density estimation techniques available. practice often use histograms kernel density estimation. histogram histograms provide nonparametric way represent continuous den sities proposed pearson 1895. histogram con structed binning data space count many data points fall bin. bar drawn center bin height bar proportional number data points within bin. bin size critical hyperparameter bad choice lead overfit ting underfitting. crossvalidation discussed section 8.2.4 used determine good bin size. kernel density estimation kernel density estimation independently proposed rosenblatt 1956 parzen 1962 nonparametric way density estimation. given n i.i.d. samples kernel density estimator represents underlying distribution px 1 nh n x n1 k x xn h 11.74 k kernel function i.e. nonnegative function integrates 1 h 0 smoothingbandwidth parameter plays similar role bin size histograms. note place kernel every single data point xn dataset. commonly used kernel functions uniform distribution gaussian distribution. kernel density esti mates closely related histograms choosing suitable kernel guarantee smoothness density estimate. figure 11.1 illus trates difference histogram kernel density estimator with gaussianshaped kernel given dataset 250 data points. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
12 classification support vector machines many situations want machine learning algorithm predict one number discrete outcomes. example email client sorts mail personal mail junk mail two outcomes. another example telescope identifies whether object night sky galaxy star planet. usually small number outcomes importantly usually additional structure outcomes. chapter consider predictors output binary val example structure outcomes ordered like case small medium large tshirts. ues i.e. two possible outcomes. machine learning task called binary classification. contrast chapter 9 binary classification considered prediction problem continuousvalued outputs. binary classification set possible values labeloutput attain binary chapter denote 1 1. words consider predictors form f rd 1 1 . 12.1 recall chapter 8 represent example data point xn feature vector real numbers. labels often referred input example xn may also referred inputs data points features instances. positive negative classes respectively. one careful class infer intuitive attributes positiveness 1 class. example cancer detection task patient cancer often labeled 1. principle two distinct values used e.g. true false 0 1 red blue. problem binary classification well studied probabilistic models mathematically convenient use 0 1 binary representation see remark example 6.12. defer survey approaches section 12.6. present approach known support vector machine svm solves binary classification task. regression su pervised learning task set examples xn rd along corresponding binary labels yn 1 1. given train ing data set consisting examplelabel pairs x1 y1 . . . xn yn would like estimate parameters model give smallest classification error. similar chapter 9 consider linear model hide away nonlinearity transformation ϕ examples 9.13. revisit ϕ section 12.4. svm provides stateoftheart results many applications sound theoretical guarantees steinwart christmann 2008. two main reasons chose illustrate binary classification using 370 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.
classification support vector machines 371 figure 12.1 example 2d data illustrating intuition data find linear classifier separates orange crosses blue discs. x1 x2 svms. first svm allows geometric way think supervised machine learning. chapter 9 considered machine learning problem terms probabilistic models attacked using maximum likelihood estimation bayesian inference consider alternative approach reason geometrically machine learning task. relies heavily concepts inner products projections discussed chapter 3. second reason find svms instructive contrast chapter 9 optimization problem svm admit analytic solution need resort variety optimization tools introduced chapter 7. svm view machine learning subtly different max imum likelihood view chapter 9. maximum likelihood view pro poses model based probabilistic view data distribution optimization problem derived. contrast svm view starts designing particular function optimized training based geometric intuitions. seen something similar already chapter 10 derived pca geometric principles. svm case start designing loss function minimized training data following principles empirical risk minimization section 8.2. let us derive optimization problem corresponding training svm examplelabel pairs. intuitively imagine binary classification data separated hyperplane illustrated figure 12.1. here every example xn a vector dimension 2 twodimensional location x1 n x2 n corresponding binary label yn one two different symbols orange cross blue disc. hyperplane word commonly used machine learning encountered hyper planes already section 2.8. hyperplane affine subspace di mension 1 if corresponding vector space dimension d. examples consist two classes there two possible labels features the components vector representing example arranged way allow us separateclassify draw ing straight line. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
372 classification support vector machines following formalize idea finding linear separator two classes. introduce idea margin extend linear separators allow examples fall wrong side incur ring classification error. present two equivalent ways formalizing svm geometric view section 12.2.4 loss function view section 12.2.5. derive dual version svm using lagrange multipliers section 7.2. dual svm allows us observe third way formalizing svm terms convex hulls examples class section 12.3.2. conclude briefly describing kernels numerically solve nonlinear kernelsvm optimization problem. 12.1 separating hyperplanes given two examples represented vectors xi xj one way compute similarity using inner product xi xj. recall section 3.2 inner products closely related angle two vectors. value inner product two vectors depends length norm vector. furthermore inner products allow us rigorously define geometric concepts orthogonality pro jections. main idea behind many classification algorithms represent data rd partition space ideally way examples label and examples partition. case binary classification space would divided two parts corresponding positive negative classes respectively. consider particularly convenient partition linearly split space two halves using hyperplane. let example x rd element data space. consider function f rd r 12.2a x 7fx w x b 12.2b parametrized w rd b r. recall section 2.8 hy perplanes affine subspaces. therefore define hyperplane separates two classes binary classification problem x rd fx 0 . 12.3 illustration hyperplane shown figure 12.2 vector w vector normal hyperplane b intercept. derive w normal vector hyperplane 12.3 choosing two examples xa xb hyperplane showing vector orthogonal w. form equation fxa fxb w xa b w xb b 12.4a w xa xb 12.4b draft 20230215 mathematics machine learning. feedback
12.1 separating hyperplanes 373 figure 12.2 equation separating hyperplane 12.3. a standard way representing equation 3d. b ease drawing look hyperplane edge on. w a separating hyperplane 3d w . 0 .positive . negative b b projection setting a onto plane second line obtained linearity inner product section 3.2. since chosen xa xb hyperplane implies fxa 0 fxb 0 hence w xa xb 0. recall two vectors orthogonal inner product zero. w orthogonal vector hyperplane. therefore obtain w orthogonal vector hyperplane. remark. recall chapter 2 think vectors different ways. chapter think parameter vector w arrow indicating direction i.e. consider w geometric vector. contrast think example vector x data point as indicated coordinates i.e. consider x coordinates vector respect standard basis. presented test example classify example pos itive negative depending side hyperplane occurs. note 12.3 defines hyperplane additionally de fines direction. words defines positive negative side hyperplane. therefore classify test example xtest calcu late value function fxtest classify example 1 fxtest 0 1 otherwise. thinking geometrically positive ex amples lie above hyperplane negative examples below hyperplane. training classifier want ensure examples positive labels positive side hyperplane i.e. w xn b 0 yn 1 12.5 examples negative labels negative side i.e. w xn b 0 yn 1 . 12.6 refer figure 12.2 geometric intuition positive negative examples. two conditions often presented single equation ynw xn b 0 . 12.7 equation 12.7 equivalent 12.5 12.6 multiply sides 12.5 12.6 yn 1 yn 1 respectively. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
374 classification support vector machines figure 12.1 possible separating hyperplanes. many linear classifiers green lines separate orange crosses blue discs. x1 x2 12.2 primal support vector machine based concept distances points hyperplane position discuss support vector machine. dataset x1 y1 . . . xn yn linearly separable infinitely many candidate hyperplanes refer figure 12.1 therefore classifiers solve classification problem without training errors. find unique solution one idea choose separating hyperplane maximizes margin positive negative examples. words want positive negative examples separated large margin section 12.2.1. following compute dis classifier large margin turns generalize well steinwart christmann 2008. tance example hyperplane derive margin. recall closest point hyperplane given point example xn obtained orthogonal projection section 3.8. 12.2.1 concept margin concept margin intuitively simple distance margin separating hyperplane closest examples dataset assuming could two closest examples hyperplane. dataset linearly separable. however trying formalize distance technical wrinkle may confusing. tech nical wrinkle need define scale measure distance. potential scale consider scale data i.e. raw values xn. problems this could change units measurement xn change values xn and hence change distance hyperplane. see shortly define scale based equation hyperplane 12.3 itself. consider hyperplane w x b example xa illustrated figure 12.2. without loss generality consider example xa positive side hyperplane i.e. w xa b 0. would like compute distance r 0 xa hyperplane. considering orthogonal projection section 3.8 xa onto hyperplane denote x a. since w orthogonal draft 20230215 mathematics machine learning. feedback
12.2 primal support vector machine 375 figure 12.2 vector addition express distance hyperplane xa x r w w. . 0 .xa w . x r hyperplane know distance r scaling vector w. length w known use scaling factor r factor work absolute distance xa x a. convenience choose use vector unit length its norm 1 obtain dividing w norm w w. using vector addition section 2.4 obtain xa x r w w. 12.8 another way thinking r coordinate xa subspace spanned w w. expressed distance xa hyperplane r choose xa point closest hyperplane distance r margin. recall would like positive examples r hyperplane negative examples dis tance r in negative direction hyperplane. analogously combination 12.5 12.6 12.7 formulate ob jective ynw xn b r . 12.9 words combine requirements examples least r away hyperplane in positive negative direction one single inequality. since interested direction add assumption model parameter vector w unit length i.e. w 1 use euclidean norm w ww section 3.1. see choices inner products section 3.2 section 12.4. assumption also allows intuitive interpretation distance r 12.8 since scaling factor vector length 1. remark. reader familiar presentations margin would notice definition w 1 different standard presentation svm one provided sch olkopf smola 2002 example. section 12.2.3 show equivalence approaches. collecting three requirements single constrained optimization 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
376 classification support vector machines figure 12.1 derivation margin r 1 w. .xa w w x b 0 w x b 1 . x r problem obtain objective max wbr r z margin subject ynw xn b r z data fitting w 1 z normalization r 0 12.10 says want maximize margin r ensuring data lies correct side hyperplane. remark. concept margin turns highly pervasive ma chine learning. used vladimir vapnik alexey chervonenkis show margin large complexity function class low hence learning possible vapnik 2000. turns concept useful various different approaches theoret ically analyzing generalization error steinwart christmann 2008 shalevshwartz bendavid 2014. 12.2.2 traditional derivation margin previous section derived 12.10 making observation interested direction w length leading assumption w 1. section derive margin max imization problem making different assumption. instead choosing parameter vector normalized choose scale data. choose scale value predictor w x b 1 closest example. let us also denote example dataset recall currently consider linearly separable data. closest hyperplane xa. figure 12.1 identical figure 12.2 except rescaled axes example xa lies exactly margin i.e. w xa b 1. since x orthogonal projection xa onto hyperplane must definition lie hyperplane i.e. w x a b 0 . 12.11 draft 20230215 mathematics machine learning. feedback
12.2 primal support vector machine 377 substituting 12.8 12.11 obtain w xa r w w b 0 . 12.12 exploiting bilinearity inner product see section 3.2 get w xa b rw w w 0 . 12.13 observe first term 1 assumption scale i.e. w xa b 1. 3.16 section 3.1 know w w w2. hence second term reduces rw. using simplifications obtain r 1 w. 12.14 means derived distance r terms normal vector w hyperplane. first glance equation counterintuitive also think distance projection error incurs projecting xa onto hyperplane. seem derived distance hyperplane terms length vector w yet know vector. one way think consider distance r temporary variable use derivation. therefore rest section denote distance hyperplane 1 w. section 12.2.3 see choice margin equals 1 equivalent previous assumption w 1 section 12.2.1. similar argument obtain 12.9 want positive negative examples least 1 away hyperplane yields condition ynw xn b 1 . 12.15 combining margin maximization fact examples need correct side hyperplane based labels gives us max wb 1 w 12.16 subject ynw xn b 1 n 1 . . . n. 12.17 instead maximizing reciprocal norm 12.16 often minimize squared norm. also often include constant 1 2 squared norm results convex quadratic programming problem svm section 12.5. affect optimal w b yields tidier form compute gradient. then objective becomes min wb 1 2w2 12.18 subject ynw xn b 1 n 1 . . . n . 12.19 equation 12.18 known hard margin svm. reason hard margin svm expression hard formulation allow vi olations margin condition. see section 12.2.4 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
378 classification support vector machines hard condition relaxed accommodate violations data linearly separable. 12.2.3 set margin 1 section 12.2.1 argued would like maximize value r represents distance closest example hyperplane. section 12.2.2 scaled data closest example distance 1 hyperplane. section relate two derivations show equivalent. theorem 12.1. maximizing margin r consider normalized weights 12.10 max wbr r z margin subject ynw xn b r z data fitting w 1 z normalization r 0 12.20 equivalent scaling data margin unity min wb 1 2 w 2 z margin subject ynw xn b 1 z data fitting . 12.21 proof consider 12.20. since square strictly monotonic trans formation nonnegative arguments maximum stays consider r2 objective. since w 1 reparametrize equation new weight vector w normalized explicitly using w w. obtain max wbr r2 subject yn w w xn b r r 0 . 12.22 equation 12.22 explicitly states distance r positive. therefore divide first constraint r yields note r 0 assumed linear separability hence issue divide r. max wbr r2 subject yn w wr z w xn b r z b 1 r 0 12.23 draft 20230215 mathematics machine learning. feedback
12.2 primal support vector machine 379 figure 12.2 a linearly separable b nonlinearly separable data. x1 x2 a linearly separable data large margin x1 x2 b nonlinearly separable data renaming parameters w b. since w w wr rearranging r gives w w wr 1 r w w 1 r . 12.24 substituting result 12.23 obtain max wb 1 w 2 subject yn w xn b 1 . 12.25 final step observe maximizing 1 w2 yields solution minimizing 1 2 w 2 concludes proof theorem 12.1. 12.2.4 soft margin svm geometric view case data linearly separable may wish allow examples fall within margin region even wrong side hyperplane illustrated figure 12.2. model allows classification errors called soft soft margin svm margin svm. section derive resulting optimization problem using geometric arguments. section 12.2.5 derive equiv alent optimization problem using idea loss function. using la grange multipliers section 7.2 derive dual optimization problem svm section 12.3. dual optimization problem al lows us observe third interpretation svm hyperplane bisects line convex hulls corresponding positive negative data examples section 12.3.2. key geometric idea introduce slack variable ξn corresponding slack variable examplelabel pair xn yn allows particular example within margin even wrong side hyperplane refer 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
380 classification support vector machines figure 12.2 soft margin svm allows examples within margin wrong side hyperplane. slack variable ξ measures distance positive example x positive margin hyperplane w x b 1 x wrong side. . x w w x b 0 w x b 1 . ξ figure 12.2. subtract value ξn margin constraining ξn nonnegative. encourage correct classification samples add ξn objective min wbξ 1 2w2 c n x n1 ξn 12.26a subject ynw xn b 1 ξn 12.26b ξn 0 12.26c n 1 . . . n. contrast optimization problem 12.18 hard margin svm one called soft margin svm. parameter soft margin svm c 0 trades size margin total amount slack have. parameter called regularization parameter since regularization parameter see following section margin term objective func tion 12.26a regularization term. margin term w2 called regularizer many books numerical optimization reg regularizer ularization parameter multiplied term section 8.2.3. contrast formulation section. large value c implies low regularization give slack variables larger weight hence giving priority examples lie correct side margin. alternative parametrizations regularization 12.26a also often referred csvm. remark. formulation soft margin svm 12.26a w reg ularized b regularized. see observing regularization term contain b. unregularized term b com plicates theoretical analysis steinwart christmann 2008 chapter 1 decreases computational efficiency fan et al. 2008. 12.2.5 soft margin svm loss function view let us consider different approach deriving svm following principle empirical risk minimization section 8.2. svm draft 20230215 mathematics machine learning. feedback
12.2 primal support vector machine 381 choose hyperplanes hypothesis class fx w x b. 12.27 see section margin corresponds regulariza tion term. remaining question is loss function con loss function trast chapter 9 consider regression problems the output predictor real number chapter consider binary classification problems the output predictor one two labels 1 1. therefore errorloss function single example label pair needs appropriate binary classification. example squared loss used regression 9.10b suitable bi nary classification. remark. ideal loss function binary labels count num ber mismatches prediction label. means predictor f applied example xn compare output fxn label yn. define loss zero match one match. denoted 1fxn yn called zeroone loss. unfortunately zeroone loss results combinatorial zeroone loss optimization problem finding best parameters w b. combinatorial optimization problems in contrast continuous optimization problems discussed chapter 7 general challenging solve. loss function corresponding svm consider error output predictor fxn label yn. loss de scribes error made training data. equivalent way derive 12.26a use hinge loss hinge loss ℓt max0 1 t yfx yw x b . 12.28 fx correct side based corresponding label y hyperplane distance 1 means 1 hinge loss returns value zero. fx correct side close hyperplane 0 1 example x within margin hinge loss returns positive value. example wrong side hyperplane t 0 hinge loss returns even larger value increases linearly. words pay penalty closer margin hyperplane even prediction correct penalty increases linearly. alternative way express hinge loss considering two linear pieces ℓt 0 1 1 t 1 12.29 illustrated figure 12.2. loss corresponding hard margin svm 12.18 defined ℓt 0 1 1 . 12.30 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
382 classification support vector machines figure 12.2 hinge loss convex upper bound zeroone loss. 2 0 2 0 2 4 max0 1 t zeroone loss hinge loss loss interpreted never allowing examples inside margin. given training set x1 y1 . . . xn yn seek minimize total loss regularizing objective ℓ2regularization see section 8.2.3. using hinge loss 12.28 gives us unconstrained optimization problem min wb 1 2w2 z regularizer c n x n1 max0 1 ynw xn b z error term . 12.31 first term 12.31 called regularization term regularizer regularizer see section 8.2.3 second term called loss term error loss term error term term. recall section 12.2.4 term 1 2 w 2 arises directly margin. words margin maximization interpreted regularization. regularization principle unconstrained optimization problem 12.31 directly solved subgradient descent methods described section 7.1. see 12.31 12.26a equivalent observe hinge loss 12.28 essentially consists two linear parts expressed 12.29. consider hinge loss single examplelabel pair 12.28. equivalently replace minimization hinge loss minimization slack variable ξ two constraints. equation form min max0 1 t 12.32 equivalent min ξt ξ subject ξ 0 ξ 1 t . 12.33 substituting expression 12.31 rearranging one constraints obtain exactly soft margin svm 12.26a. remark. let us contrast choice loss function section loss function linear regression chapter 9. recall section 9.2.1 finding maximum likelihood estimators usually minimize draft 20230215 mathematics machine learning. feedback
12.3 dual support vector machine 383 negative loglikelihood. furthermore since likelihood term linear regression gaussian noise gaussian negative loglikelihood example squared error function. squared error function loss function minimized looking maximum likelihood solution. 12.3 dual support vector machine description svm previous sections terms vari ables w b known primal svm. recall consider inputs x rd features. since w dimension x means number parameters the dimension w opti mization problem grows linearly number features. following consider equivalent optimization problem the socalled dual view independent number features. in stead number parameters increases number examples training set. saw similar idea appear chapter 10 expressed learning problem way scale num ber features. useful problems features number examples training dataset. dual svm also additional advantage easily allows kernels applied shall see end chapter. word dual appears often mathematical literature particular case refers convex duality. following subsections essentially application convex duality discussed section 7.2. 12.3.1 convex duality via lagrange multipliers recall primal soft margin svm 12.26a. call variables w b ξ corresponding primal svm primal variables. use αn chapter 7 used λ lagrange multipliers. section follow notation commonly chosen svm literature use α γ. 0 lagrange multiplier corresponding constraint 12.26b examples classified correctly γn 0 lagrange multi plier corresponding nonnegativity constraint slack variable see 12.26c. lagrangian given lw b ξ α γ 1 2w2 c n x n1 ξn 12.34 n x n1 αnynw xn b 1 ξn z constraint 12.26b n x n1 γnξn z constraint 12.26c . 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
384 classification support vector machines differentiating lagrangian 12.34 respect three primal variables w b ξ respectively obtain l w w n x n1 αnynxn 12.35 l b n x n1 αnyn 12.36 l ξn c αn γn . 12.37 find maximum lagrangian setting partial derivatives zero. setting 12.35 zero find w n x n1 αnynxn 12.38 particular instance representer theorem kimeldorf representer theorem wahba 1970. equation 12.38 states optimal weight vector representer theorem actually collection theorems saying solution minimizing empirical risk lies subspace section 2.4.3 defined examples. primal linear combination examples xn. recall sec tion 2.6.1 means solution optimization problem lies span training data. additionally constraint obtained setting 12.36 zero implies optimal weight vector affine combination examples. representer theorem turns hold general settings regularized empirical risk minimization hof mann et al. 2008 argyriou dinuzzo 2014. theorem general versions sch olkopf et al. 2001 necessary sufficient conditions existence found yu et al. 2013. remark. representer theorem 12.38 also provides explanation name support vector machine. examples xn corresponding parameters αn 0 contribute solution w all. examples αn 0 called support vectors since support vector support hyperplane. substituting expression w lagrangian 12.34 obtain dual dξ α γ 1 2 n x i1 n x j1 yiyjαiαj xi xj n x i1 yiαi n x j1 yjαjxj xi c n x i1 ξi b n x i1 yiαi n x i1 αi n x i1 αiξi n x i1 γiξi . 12.39 note longer terms involving primal variable w. setting 12.36 zero obtain pn n1 ynαn 0. therefore term involving b also vanishes. recall inner products symmetric draft 20230215 mathematics machine learning. feedback
12.3 dual support vector machine 385 bilinear see section 3.2. therefore first two terms 12.39 objects. terms colored blue simplified obtain lagrangian dξ α γ 1 2 n x i1 n x j1 yiyjαiαj xi xj n x i1 αi n x i1 c αi γiξi . 12.40 last term equation collection terms contain slack variables ξi. setting 12.37 zero see last term 12.40 also zero. furthermore using equation recalling lagrange multiplers γi nonnegative conclude αi c. obtain dual optimization problem svm ex pressed exclusively terms lagrange multipliers αi. recall lagrangian duality definition 7.1 maximize dual problem. equivalent minimizing negative dual problem end dual svm dual svm min α 1 2 n x i1 n x j1 yiyjαiαj xi xj n x i1 αi subject n x i1 yiαi 0 0 αi c 1 . . . n . 12.41 equality constraint 12.41 obtained setting 12.36 zero. inequality constraint αi 0 condition imposed la grange multipliers inequality constraints section 7.2. inequality constraint αi c discussed previous paragraph. set inequality constraints svm called box constraints limit vector α α1 αnrn lagrange mul tipliers inside box defined 0 c axis. axisaligned boxes particularly efficient implement numerical solvers dost al 2009 chapter 5. turns examples lie exactly margin examples whose dual parameters lie strictly inside box constraints 0 αi c. derived using karush kuhn tucker conditions example sch olkopf smola 2002. obtain dual parameters α recover primal pa rameters w using representer theorem 12.38. let us call op timal primal parameter w. however remains question obtain parameter b. consider example xn lies exactly margins boundary i.e. w xn b yn. recall yn either 1 1. therefore unknown b computed b yn w xn. 12.42 remark. principle may examples lie exactly margin. case compute yn w xn support vectors take median value absolute value difference 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
386 classification support vector machines figure 12.3 convex hulls. a convex hull points lie within boundary b convex hulls around positive negative examples. a convex hull. c b convex hulls around positive blue negative orange examples. distance be tween two convex sets length difference vector c d. value b. derivation found eu20120607thesvmbiastermconspiracy. 12.3.2 dual svm convex hull view another approach obtain dual svm consider alternative geometric argument. consider set examples xn label. would like build convex set contains examples smallest possible set. called convex hull illustrated figure 12.3. let us first build intuition convex combination points. consider two points x1 x2 corresponding nonnegative weights α1 α2 0 α1α2 1. equation α1x1α2x2 describes point line x1 x2. consider happens add third point x3 along weight α3 0 p3 n1 αn 1. convex combination three points x1 x2 x3 spans two dimensional area. convex hull area triangle formed convex hull edges corresponding pair points. add points number points becomes greater number dimen sions points inside convex hull see figure 12.3a. general building convex convex hull done introducing nonnegative weights αn 0 corresponding example xn. convex hull described set conv x n x n1 αnxn n x n1 αn 1 αn 0 12.43 draft 20230215 mathematics machine learning. feedback
12.3 dual support vector machine 387 n 1 . . . n. two clouds points corresponding positive negative classes separated convex hulls overlap. given training data x1 y1 . . . xn yn form two con vex hulls corresponding positive negative classes respectively. pick point c convex hull set positive exam ples closest negative class distribution. similarly pick point convex hull set negative examples closest positive class distribution see figure 12.3b. define difference vector c w c d . 12.44 picking points c preceding cases requiring closest equivalent minimizing lengthnorm w end corresponding optimization problem arg min w w arg min w 1 2 w 2 . 12.45 since c must positive convex hull expressed convex combination positive examples i.e. nonnegative coefficients α n c x nyn1 α n xn . 12.46 12.46 use notation n yn 1 indicate set indices n yn 1. similarly examples negative labels obtain x nyn1 α n xn . 12.47 substituting 12.44 12.46 12.47 12.45 obtain objective min α 1 2 x nyn1 α n xn x nyn1 α n xn 2 . 12.48 let α set coefficients i.e. concatenation α α. recall require convex hull coefficients sum one x nyn1 α n 1 x nyn1 α n 1 . 12.49 implies constraint n x n1 ynαn 0 . 12.50 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
388 classification support vector machines result seen multiplying individual classes n x n1 ynαn x nyn1 1α n x nyn1 1α n 12.51a x nyn1 α n x nyn1 α n 1 1 0 . 12.51b objective function 12.48 constraint 12.50 along assumption α 0 give us constrained convex optimization prob lem. optimization problem shown dual hard margin svm bennett bredensteiner 2000a. remark. obtain soft margin dual consider reduced hull. reduced hull similar convex hull upper bound reduced hull size coefficients α. maximum possible value elements α restricts size convex hull take. words bound α shrinks convex hull smaller volume bennett bredensteiner 2000b. 12.4 kernels consider formulation dual svm 12.41. notice in ner product objective occurs examples xi xj. inner products examples parameters. therefore consider set features ϕxi represent xi change dual svm replace inner product. mod ularity choice classification method the svm choice feature representation ϕx considered separately provides flexibility us explore two problems independently. section discuss representation ϕx briefly introduce idea kernels go technical details. since ϕx could nonlinear function use svm which assumes linear classifier construct classifiers nonlinear examples xn. provides second avenue addition soft margin users deal dataset linearly separable. turns many algorithms statistical methods property observed dual svm inner products occur examples. instead explicitly defining nonlinear feature map ϕ computing resulting inner product examples xi xj define similarity function kxi xj be tween xi xj. certain class similarity functions called kernels kernel similarity function implicitly defines nonlinear feature map ϕ. kernels definition functions k x x r exists inputs x kernel function general necessarily restricted rd. hilbert space h ϕ x h feature map kxi xj ϕxi ϕxjh . 12.52 draft 20230215 mathematics machine learning. feedback
12.4 kernels 389 figure 12.4 svm different kernels. note decision boundary nonlinear underlying problem solved linear separating hyperplane albeit nonlinear kernel. first feature second feature a svm linear kernel first feature second feature b svm rbf kernel first feature second feature c svm polynomial degree 2 kernel first feature second feature d svm polynomial degree 3 kernel unique reproducing kernel hilbert space associated every kernel k aronszajn 1950 berlinet thomasagnan 2004. unique association ϕx k x called canonical feature map. canonical feature map generalization inner product kernel function 12.52 known kernel trick sch olkopf smola 2002 shawetaylor kernel trick cristianini 2004 hides away explicit nonlinear feature map. matrix k rnn resulting inner products appli cation k dataset called gram matrix often gram matrix referred kernel matrix. kernels must symmetric positive kernel matrix semidefinite functions every kernel matrix k symmetric positive semidefinite section 3.2.3 z rn zkz 0 . 12.53 popular examples kernels multivariate realvalued data xi rd polynomial kernel gaussian radial basis function kernel rational quadratic kernel sch olkopf smola 2002 rasmussen 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
390 classification support vector machines williams 2006. figure 12.4 illustrates effect different kernels separating hyperplanes example dataset. note still solving hyperplanes is hypothesis class functions still linear. nonlinear surfaces due kernel function. remark. unfortunately fledgling machine learner mul tiple meanings word kernel. chapter word kernel comes idea reproducing kernel hilbert space rkhs aron szajn 1950 saitoh 1988. discussed idea kernel lin ear algebra section 2.7.3 kernel another word null space. third common use word kernel machine learning smoothing kernel kernel density estimation section 11.5. since explicit representation ϕx mathematically equivalent kernel representation kxi xj practitioner often design kernel function computed efficiently inner product explicit feature maps. example consider polynomial kernel sch olkopf smola 2002 number terms explicit expansion grows quickly even polynomials low degree input dimension large. kernel function requires one multiplication per input dimension provide significant computational savings. another example gaussian ra dial basis function kernel sch olkopf smola 2002 rasmussen williams 2006 corresponding feature space infinite dimen sional. case cannot explicitly represent feature space still compute similarities pair examples using kernel. choice kernel well parameters kernel often chosen using nested crossvalidation section 8.6.1. another useful aspect kernel trick need original data already represented multivariate realvalued data. note inner product defined output function ϕ restrict input real numbers. hence function ϕ kernel function k defined object e.g. sets sequences strings graphs distributions benhur et al. 2008 g artner 2008 shi et al. 2009 sriperumbudur et al. 2010 vishwanathan et al. 2010. 12.5 numerical solution conclude discussion svms looking express problems derived chapter terms concepts presented chapter 7. consider two different approaches finding optimal solution svm. first consider loss view svm 8.2.2 ex press unconstrained optimization problem. express constrained versions primal dual svms quadratic programs standard form 7.3.2. consider loss function view svm 12.31. convex unconstrained optimization problem hinge loss 12.28 dif draft 20230215 mathematics machine learning. feedback
12.5 numerical solution 391 ferentiable. therefore apply subgradient approach solving it. however hinge loss differentiable almost everywhere except one single point hinge 1. point gradient set possible values lie 0 1. therefore subgradient g hinge loss given gt 1 1 1 0 1 0 1 . 12.54 using subgradient apply optimization methods presented section 7.1. primal dual svm result convex quadratic pro gramming problem constrained optimization. note primal svm 12.26a optimization variables size dimen sion input examples. dual svm 12.41 optimization variables size number n examples. express primal svm standard form 7.45 quadratic programming let us assume use dot product 3.5 inner product. rearrange equation primal svm 12.26a recall section 3.2 use phrase dot product mean inner product euclidean vector space. optimization variables right inequality constraint matches standard form. yields optimization min wbξ 1 2w2 c n x n1 ξn subject ynx n w ynb ξn 1 ξn 0 12.55 n 1 . . . n. concatenating variables w b xn single vector carefully collecting terms obtain following matrix form soft margin svm min wbξ 1 2 w b ξ id 0dn1 0n1d 0n1n1 w b ξ 0d11 c1n1 w b ξ subject y x y in 0nd1 in w b ξ 1n1 0n1 . 12.56 preceding optimization problem minimization pa rameters w b ξrd1n use notation im rep resent identity matrix size m 0mn represent matrix zeros size n 1mn represent matrix ones size n. addition vector labels y1 yn diagy 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
392 classification support vector machines n n matrix elements diagonal y x rnd matrix obtained concatenating examples. similarly perform collection terms dual version svm 12.41. express dual svm standard form first express kernel matrix k entry kij kxi xj. explicit feature representation xi define kij xi xj. convenience notation introduce matrix zeros everywhere except diagonal store labels is diagy. dual svm written min α 1 2αy ky α 1 n1α subject y y in α 0n21 c1n1 . 12.57 remark. sections 7.3.1 7.3.2 introduced standard forms constraints inequality constraints. express dual svms equality constraint two inequality constraints i.e. ax b replaced ax b ax b . 12.58 particular software implementations convex optimization methods may provide ability express equality constraints. since many different possible views svm many approaches solving resulting optimization problem. ap proach presented here expressing svm problem standard convex optimization form often used practice. two main implemen tations svm solvers chang lin 2011 which open source joachims 1999. since svms clear welldefined optimiza tion problem many approaches based numerical optimization tech niques nocedal wright 2006 applied shawetaylor sun 2011. 12.6 reading svm one many approaches studying binary classification. approaches include perceptron logistic regression fisher dis criminant nearest neighbor naive bayes random forest bishop 2006 murphy 2012. short tutorial svms kernels discrete se quences found benhur et al. 2008. development svms closely linked empirical risk minimization discussed section 8.2. hence svm strong theoretical properties vapnik 2000 stein wart christmann 2008. book kernel methods sch olkopf smola 2002 includes many details support vector machines draft 20230215 mathematics machine learning. feedback
12.6 reading 393 optimize them. broader book kernel methods shawe taylor cristianini 2004 also includes many linear algebra approaches different machine learning problems. alternative derivation dual svm obtained using idea legendrefenchel transform section 7.3.3. derivation considers term unconstrained formulation svm 12.31 separately calculates convex conjugates rifkin lippert 2007. readers interested functional analysis view also reg ularization methods view svms referred work wahba 1990. theoretical exposition kernels aronszajn 1950 schwartz 1964 saitoh 1988 manton amblard 2015 requires basic ground ing linear operators akhiezer glazman 1993. idea kernels generalized banach spaces zhang et al. 2009 kre ın spaces ong et al. 2004 loosli et al. 2016. observe hinge loss three equivalent representations shown 12.28 12.29 well constrained optimization problem 12.33. formulation 12.28 often used compar ing svm loss function loss functions steinwart 2007. twopiece formulation 12.29 convenient computing subgra dients piece linear. third formulation 12.33 seen section 12.5 enables use convex quadratic programming sec tion 7.3.2 tools. since binary classification wellstudied task machine learning words also sometimes used discrimination separation decision. furthermore three quantities out put binary classifier. first output linear function often called score take real value. output used ranking examples binary classification thought picking threshold ranked examples shawetaylor cris tianini 2004. second quantity often considered output binary classifier output determined passed nonlinear function constrain value bounded range ex ample interval 0 1. common nonlinear function sigmoid function bishop 2006. nonlinearity results wellcalibrated probabilities gneiting raftery 2007 reid williamson 2011 called class probability estimation. third output binary classifier final binary decision 1 1 one com monly assumed output classifier. svm binary classifier naturally lend probabilistic interpretation. several approaches converting raw output linear function the score calibrated class probability estimate py 1x x involve additional cal ibration step platt 2000 zadrozny elkan 2001 lin et al. 2007. training perspective many related probabilistic ap proaches. mentioned end section 12.2.5 re 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
394 classification support vector machines lationship loss function likelihood also compare sec tions 8.2 8.3. maximum likelihood approach corresponding wellcalibrated transformation training called logistic regres sion comes class methods called generalized linear mod els. details logistic regression point view found agresti 2002 chapter 5 mccullagh nelder 1989 chapter 4. naturally one could take bayesian view classifier output estimating posterior distribution using bayesian logistic regression. bayesian view also includes specification prior includes design choices conjugacy section 6.6.1 likelihood. ad ditionally one could consider latent functions priors results gaussian process classification rasmussen williams 2006 chapter 3. draft 20230215 mathematics machine learning. feedback
references abel niels h. 1826. d emonstration de limpossibilit e de la r esolution alg ebrique des equations g en erales qui passent le quatri eme degr e. grøndahl søn. adhikari ani denero john. 2018. computational inferential thinking foundations data science. gitbooks. agarwal arvind daum e iii hal. 2010. geometric view conjugate priors. machine learning 811 99113. agresti a. 2002. categorical data analysis. wiley. akaike hirotugu. 1974. new look statistical model identification. ieee transactions automatic control 196 716723. akhiezer naum i. glazman izrail m. 1993. theory linear operators hilbert space. dover publications. alpaydin ethem. 2010. introduction machine learning. mit press. amari shunichi. 2016. information geometry applications. springer. argyriou andreas dinuzzo francesco. 2014. unifying view representer theorems. in proceedings international conference machine learning. aronszajn nachman. 1950. theory reproducing kernels. transactions amer ican mathematical society 68 337404. axler sheldon. 2015. linear algebra done right. springer. bakir g okhan hofmann thomas sch olkopf bernhard smola alexander j. taskar ben vishwanathan s. v. n. eds. 2007. predicting structured data. mit press. barber david. 2012. bayesian reasoning machine learning. cambridge university press. barndorffnielsen ole. 2014. information exponential families statistical the ory. wiley. bartholomew david knott martin moustaki irini. 2011. latent variable models factor analysis unified approach. wiley. baydin atılım g. pearlmutter barak a. radul alexey a. siskind jeffrey m. 2018. automatic differentiation machine learning survey. journal machine learning research 18 143. beck amir teboulle marc. 2003. mirror descent nonlinear projected subgra dient methods convex optimization. operations research letters 313 167 175. belabbas mohamedali wolfe patrick j. 2009. spectral methods machine learning new strategies large datasets. proceedings national academy sciences 0810600105. belkin mikhail niyogi partha. 2003. laplacian eigenmaps dimensionality reduction data representation. neural computation 156 13731396. benhur asa ong cheng soon sonnenburg s oren sch olkopf bernhard r atsch gunnar. 2008. support vector machines kernels computational biology. plos computational biology 410 e1000173. 395 material published cambridge university press mathematics machine learning marc peter deisenroth a. aldo faisal cheng soon ong 2020. version free view download personal use only. redistribution resale use derivative works. by m. p. deisenroth a. a. faisal c. s. ong 2021.
396 references bennett kristin p. bredensteiner erin j. 2000a. duality geometry svm classifiers. in proceedings international conference machine learning. bennett kristin p. bredensteiner erin j. 2000b. geometry learning. pages 132145 of geometry work. mathematical association america. berlinet alain thomasagnan christine. 2004. reproducing kernel hilbert spaces probability statistics. springer. bertsekas dimitri p. 1999. nonlinear programming. athena scientific. bertsekas dimitri p. 2009. convex optimization theory. athena scientific. bickel peter j. doksum kjell. 2006. mathematical statistics basic ideas selected topics. vol. 1. prentice hall. bickson danny dolev danny shental ori siegel paul h. wolf jack k. 2007. linear detection via belief propagation. in proceedings annual allerton con ference communication control computing. billingsley patrick. 1995. probability measure. wiley. bishop christopher m. 1995. neural networks pattern recognition. clarendon press. bishop christopher m. 1999. bayesian pca. in advances neural information pro cessing systems. bishop christopher m. 2006. pattern recognition machine learning. springer. blei david m. kucukelbir alp mcauliffe jon d. 2017. variational inference review statisticians. journal american statistical association 112518 859877. blum arvim hardt moritz. 2015. ladder reliable leaderboard ma chine learning competitions. in international conference machine learning. bonnans j. fr ed eric gilbert j. charles lemar echal claude sagastiz abal clau dia a. 2006. numerical optimization theoretical practical aspects. springer. borwein jonathan m. lewis adrian s. 2006. convex analysis nonlinear optimization. 2nd edn. canadian mathematical society. bottou l eon. 1998. online algorithms stochastic approximations. pages 942 of online learning neural networks. cambridge university press. bottou l eon curtis frank e. nocedal jorge. 2018. optimization methods largescale machine learning. siam review 602 223311. boucheron stephane lugosi gabor massart pascal. 2013. concentration in equalities nonasymptotic theory independence. oxford university press. boyd stephen vandenberghe lieven. 2004. convex optimization. cambridge university press. boyd stephen vandenberghe lieven. 2018. introduction applied linear alge bra. cambridge university press. brochu eric cora vlad m. de freitas nando. 2009. tutorial bayesian optimization expensive cost functions application active user modeling hierarchical reinforcement learning. tech. rept. tr2009023. department computer science university british columbia. brooks steve gelman andrew jones galin l. meng xiaoli eds. 2011. hand book markov chain monte carlo. chapman hallcrc. brown lawrence d. 1986. fundamentals statistical exponential families ap plications statistical decision theory. institute mathematical statistics. bryson arthur e. 1961. gradient method optimizing multistage allocation processes. in proceedings harvard university symposium digital computers applications. bubeck s ebastien. 2015. convex optimization algorithms complexity. founda tions trends machine learning 834 231357. b uhlmann peter van de geer sara. 2011. statistics highdimensional data. springer. draft 20230215 mathematics machine learning. feedback
references 397 burges christopher. 2010. dimension reduction guided tour. foundations trends machine learning 24 275365. carroll j douglas chang jihjie. 1970. analysis individual differences multidimensional scaling via nway generalization eckartyoung decom position. psychometrika 353 283319. casella george berger roger l. 2002. statistical inference. duxbury. c inlar erhan. 2011. probability stochastics. springer. chang chihchung lin chihjen. 2011. libsvm library support vector machines. acm transactions intelligent systems technology 2 2712727. cheeseman peter. 1985. defense probability. in proceedings international joint conference artificial intelligence. chollet francois allaire j. j. 2018. deep learning r. manning publications. codd edgar f. 1990. relational model database management. addisonwesley longman publishing. cunningham john p. ghahramani zoubin. 2015. linear dimensionality reduc tion survey insights generalizations. journal machine learning research 16 28592900. datta biswa n. 2010. numerical linear algebra applications. siam. davidson anthony c. hinkley david v. 1997. bootstrap methods appli cation. cambridge university press. dean jeffrey corrado greg s. monga rajat chen et al. 2012. large scale distributed deep networks. in advances neural information processing systems. deisenroth marc p. mohamed shakir. 2012. expectation propagation gaus sian process dynamical systems. pages 26182626 of advances neural informa tion processing systems. deisenroth marc p. ohlsson henrik. 2011. general perspective gaussian filtering smoothing explaining current deriving new algorithms. in proceedings american control conference. deisenroth marc p. fox dieter rasmussen carl e. 2015. gaussian processes dataefficient learning robotics control. ieee transactions pattern analysis machine intelligence 372 408423. dempster arthur p. laird nan m. rubin donald b. 1977. maximum likelihood incomplete data via em algorithm. journal royal statistical society 391 138. deng li seltzer michael l. yu dong acero alex mohamed abdelrahman hinton geoffrey e. 2010. binary coding speech spectrograms using deep autoencoder. in proceedings interspeech. devroye luc. 1986. nonuniform random variate generation. springer. donoho david l. grimes carrie. 2003. hessian eigenmaps locally linear embedding techniques highdimensional data. proceedings national academy sciences 10010 55915596. dost al zden ek. 2009. optimal quadratic programming algorithms applications variational inequalities. springer. douven igor. 2017. abduction. in stanford encyclopedia philosophy. meta physics research lab stanford university. downey allen b. 2014. think stats exploratory data analysis. 2nd edn. oreilly media. dreyfus stuart. 1962. numerical solution variational problems. journal mathematical analysis applications 51 3045. drumm volker weil wolfgang. 2001. lineare algebra und analytische geometrie. lecture notes universit karlsruhe th. dudley richard m. 2002. real analysis probability. cambridge university press. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
398 references eaton morris l. 2007. multivariate statistics vector space approach. institute mathematical statistics lecture notes. eckart carl young gale. 1936. approximation one matrix another lower rank. psychometrika 13 211218. efron bradley hastie trevor. 2016. computer age statistical inference algorithms evidence data science. cambridge university press. efron bradley tibshirani robert j. 1993. introduction bootstrap. chap man hallcrc. elliott conal. 2009. beautiful differentiation. in international conference func tional programming. evgeniou theodoros pontil massimiliano poggio tomaso. 2000. statistical learning theory primer. international journal computer vision 381 913. fan rongen chang kaiwei hsieh chojui wang xiangrui lin chihjen. 2008. liblinear library large linear classification. journal machine learning research 9 18711874. gal yarin van der wilk mark rasmussen carl e. 2014. distributed variational inference sparse gaussian process regression latent variable models. in advances neural information processing systems. g artner thomas. 2008. kernels structured data. world scientific. gavish matan donoho david l. 2014. optimal hard threshold singular values 4 3. ieee transactions information theory 608 50405053. gelman andrew carlin john b. stern hal s. rubin donald b. 2004. bayesian data analysis. chapman hallcrc. gentle james e. 2004. random number generation monte carlo methods. springer. ghahramani zoubin. 2015. probabilistic machine learning artificial intelligence. nature 521 452459. ghahramani zoubin roweis sam t. 1999. learning nonlinear dynamical sys tems using em algorithm. in advances neural information processing systems. mit press. gilks walter r. richardson sylvia spiegelhalter david j. 1996. markov chain monte carlo practice. chapman hallcrc. gneiting tilmann raftery adrian e. 2007. strictly proper scoring rules pre diction estimation. journal american statistical association 102477 359378. goh gabriel. 2017. momentum really works. distill. gohberg israel goldberg seymour krupnik nahum. 2012. traces determi nants linear operators. birkh auser. golan jonathan s. 2007. linear algebra beginning graduate student ought know. springer. golub gene h. van loan charles f. 2012. matrix computations. jhu press. goodfellow ian bengio yoshua courville aaron. 2016. deep learning. mit press. graepel thore candela joaquin qui nonerocandela borchert thomas her brich ralf. 2010. webscale bayesian clickthrough rate prediction sponsored search advertising microsofts bing search engine. in proceedings interna tional conference machine learning. griewank andreas walther andrea. 2003. introduction automatic differenti ation. in proceedings applied mathematics mechanics. griewank andreas walther andrea. 2008. evaluating derivatives principles techniques algorithmic differentiation. siam. grimmett geoffrey r. welsh dominic. 2014. probability introduction. oxford university press. draft 20230215 mathematics machine learning. feedback
references 399 grinstead charles m. snell j. laurie. 1997. introduction probability. american mathematical society. hacking ian. 2001. probability inductive logic. cambridge university press. hall peter. 1992. bootstrap edgeworth expansion. springer. hallin marc paindaveine davy ˇ siman miroslav. 2010. multivariate quan tiles multipleoutput regression quantiles ℓ1 optimization halfspace depth. annals statistics 38 635669. hasselblatt boris katok anatole. 2003. first course dynamics panorama recent developments. cambridge university press. hastie trevor tibshirani robert friedman jerome. 2001. elements sta tistical learning data mining inference prediction. springer. hausman karol springenberg jost t. wang ziyu heess nicolas riedmiller martin. 2018. learning embedding space transferable robot skills. in proceedings international conference learning representations. hazan elad. 2015. introduction online convex optimization. foundations trends optimization 234 157325. hensman james fusi nicol o lawrence neil d. 2013. gaussian processes big data. in proceedings conference uncertainty artificial intelligence. herbrich ralf minka tom graepel thore. 2007. trueskilltm bayesian skill rating system. in advances neural information processing systems. hiriarturruty jeanbaptiste lemar echal claude. 2001. fundamentals convex analysis. springer. hoffman matthew d. blei david m. bach francis. 2010. online learning latent dirichlet allocation. advances neural information processing systems. hoffman matthew d. blei david m. wang chong paisley john. 2013. stochas tic variational inference. journal machine learning research 141 13031347. hofmann thomas sch olkopf bernhard smola alexander j. 2008. kernel meth ods machine learning. annals statistics 363 11711220. hogben leslie. 2013. handbook linear algebra. chapman hallcrc. horn roger a. johnson charles r. 2013. matrix analysis. cambridge university press. hotelling harold. 1933. analysis complex statistical variables principal components. journal educational psychology 24 417441. hyvarinen aapo oja erkki karhunen juha. 2001. independent component anal ysis. wiley. imbens guido w. rubin donald b. 2015. causal inference statistics social biomedical sciences. cambridge university press. jacod jean protter philip. 2004. probability essentials. springer. jaynes edwin t. 2003. probability theory logic science. cambridge university press. jefferys william h. berger james o. 1992. ockhams razor bayesian anal ysis. american scientist 80 6472. jeffreys harold. 1961. theory probability. oxford university press. jimenez rezende danilo mohamed shakir. 2015. variational inference nor malizing flows. in proceedings international conference machine learning. jimenez rezende danilo mohamed shakir wierstra daan. 2014. stochastic backpropagation approximate inference deep generative models. in pro ceedings international conference machine learning. joachims thorsten. 1999. advances kernel methods support vector learning. mit press. chap. making largescale svm learning practical pages 169184. jordan michael i. ghahramani zoubin jaakkola tommi s. saul lawrence k. 1999. introduction variational methods graphical models. machine learn ing 37 183233. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
400 references julier simon j. uhlmann jeffrey k. 1997. new extension kalman filter nonlinear systems. in proceedings aerosense symposium aerospacedefense sensing simulation controls. kaiser marcus hilgetag claus c. 2006. nonoptimal component placement short processing paths due longdistance projections neural systems. plos computational biology 27 e95. kalman dan. 1996. singularly valuable decomposition svd matrix. col lege mathematics journal 271 223. kalman rudolf e. 1960. new approach linear filtering prediction problems. transactions asme journal basic engineering 82series d 3545. kamthe sanket deisenroth marc p. 2018. dataefficient reinforcement learning probabilistic model predictive control. in proceedings international conference artificial intelligence statistics. katz victor j. 2004. history mathematics. pearsonaddisonwesley. kelley henry j. 1960. gradient theory optimal flight paths. ars journal 3010 947954. kimeldorf george s. wahba grace. 1970. correspondence bayesian estimation stochastic processes smoothing splines. annals mathemat ical statistics 412 495502. kingma diederik p. welling max. 2014. autoencoding variational bayes. in proceedings international conference learning representations. kittler josef f oglein janos. 1984. contextual classification multispectral pixel data. image vision computing 21 1329. kolda tamara g. bader brett w. 2009. tensor decompositions applications. siam review 513 455500. koller daphne friedman nir. 2009. probabilistic graphical models. mit press. kong linglong mizera ivan. 2012. quantile tomography using quantiles multivariate data. statistica sinica 22 15981610. lang serge. 1987. linear algebra. springer. lawrence neil d. 2005. probabilistic nonlinear principal component analysis gaussian process latent variable models. journal machine learning research 6nov. 17831816. leemis lawrence m. mcqueston jacquelyn t. 2008. univariate distribution relationships. american statistician 621 4553. lehmann erich l. romano joseph p. 2005. testing statistical hypotheses. springer. lehmann erich leo casella george. 1998. theory point estimation. springer. liesen j org mehrmann volker. 2015. linear algebra. springer. lin hsuantien lin chihjen weng ruby c. 2007. note platts probabilistic outputs support vector machines. machine learning 68 267276. ljung lennart. 1999. system identification theory user. prentice hall. loosli ga elle canu st ephane ong cheng soon. 2016. learning svm kre ın spaces. ieee transactions pattern analysis machine intelligence 386 1204 1216. luenberger david g. 1969. optimization vector space methods. wiley. mackay david j. c. 1992. bayesian interpolation. neural computation 4 415447. mackay david j. c. 1998. introduction gaussian processes. pages 133165 of bishop c. m. ed neural networks machine learning. springer. mackay david j. c. 2003. information theory inference learning algorithms. cambridge university press. magnus jan r. neudecker heinz. 2007. matrix differential calculus appli cations statistics econometrics. wiley. draft 20230215 mathematics machine learning. feedback
references 401 manton jonathan h. amblard pierreolivier. 2015. primer reproducing kernel hilbert spaces. foundations trends signal processing 812 1126. markovsky ivan. 2011. low rank approximation algorithms implementation appli cations. springer. maybeck peter s. 1979. stochastic models estimation control. academic press. mccullagh peter nelder john a. 1989. generalized linear models. crc press. mceliece robert j. mackay david j. c. cheng jungfu. 1998. turbo decoding instance pearls belief propagation algorithm. ieee journal selected areas communications 162 140152. mika sebastian r atsch gunnar weston jason sch olkopf bernhard m uller klausrobert. 1999. fisher discriminant analysis kernels. pages 4148 of proceedings workshop neural networks signal processing. minka thomas p. 2001a. family algorithms approximate bayesian inference. ph.d. thesis massachusetts institute technology. minka tom. 2001b. automatic choice dimensionality pca. in advances neural information processing systems. mitchell tom. 1997. machine learning. mcgrawhill. mnih volodymyr kavukcuoglu koray silver david et al. 2015. humanlevel control deep reinforcement learning. nature 518 529533. moonen marc de moor bart. 1995. svd signal processing iii algorithms architectures applications. elsevier. moustaki irini knott martin bartholomew david j. 2015. latentvariable mod eling. american cancer society. pages 110. m uller andreas c. guido sarah. 2016. introduction machine learning python guide data scientists. oreilly publishing. murphy kevin p. 2012. machine learning probabilistic perspective. mit press. neal radford m. 1996. bayesian learning neural networks. ph.d. thesis depart ment computer science university toronto. neal radford m. hinton geoffrey e. 1999. view em algorithm justifies incremental sparse variants. pages 355368 of learning graphical models. mit press. nelsen roger. 2006. introduction copulas. springer. nesterov yuri. 2018. lectures convex optimization. springer. neumaier arnold. 1998. solving illconditioned singular linear systems tu torial regularization. siam review 40 636666. nocedal jorge wright stephen j. 2006. numerical optimization. springer. nowozin sebastian gehler peter v. jancsary jeremy lampert christoph h. eds. 2014. advanced structured prediction. mit press. ohagan anthony. 1991. bayeshermite quadrature. journal statistical planning inference 29 245260. ong cheng soon mary xavier canu st ephane smola alexander j. 2004. learn ing nonpositive kernels. in proceedings international conference machine learning. ormoneit dirk sidenbladh hedvig black michael j. hastie trevor. 2001. learning tracking cyclic human motion. in advances neural information processing systems. page lawrence brin sergey motwani rajeev winograd terry. 1999. pagerank citation ranking bringing order web. tech. rept. stanford info lab. paquet ulrich. 2008. bayesian inference latent variable models. ph.d. thesis uni versity cambridge. parzen emanuel. 1962. estimation probability density function mode. annals mathematical statistics 333 10651076. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
402 references pearl judea. 1988. probabilistic reasoning intelligent systems networks plausible inference. morgan kaufmann. pearl judea. 2009. causality models reasoning inference. 2nd edn. cambridge university press. pearson karl. 1895. contributions mathematical theory evolution. ii. skew variation homogeneous material. philosophical transactions royal society a mathematical physical engineering sciences 186 343414. pearson karl. 1901. lines planes closest fit systems points space. philosophical magazine 211 559572. peters jonas janzing dominik sch olkopf bernhard. 2017. elements causal inference foundations learning algorithms. mit press. petersen kaare b. pedersen michael s. 2012. matrix cookbook. tech. rept. technical university denmark. platt john c. 2000. probabilistic outputs support vector machines compar isons regularized likelihood methods. in advances large margin classifiers. pollard david. 2002. users guide measure theoretic probability. cambridge university press. polyak roman a. 2016. legendre transformation modern optimization. pages 437507 of goldengorin b. ed optimization applications control data sciences. springer. press william h. teukolsky saul a. vetterling william t. flannery brian p. 2007. numerical recipes art scientific computing. cambridge university press. proschan michael a. presnell brett. 1998. expect unexpected condi tional expectation. american statistician 523 248252. raschka sebastian mirjalili vahid. 2017. python machine learning machine learning deep learning python scikitlearn tensorflow. packt publish ing. rasmussen carl e. ghahramani zoubin. 2001. occams razor. in advances neural information processing systems. rasmussen carl e. ghahramani zoubin. 2003. bayesian monte carlo. in ad vances neural information processing systems. rasmussen carl e. williams christopher k. i. 2006. gaussian processes ma chine learning. mit press. reid mark williamson robert c. 2011. information divergence risk binary experiments. journal machine learning research 12 731817. rifkin ryan m. lippert ross a. 2007. value regularization fenchel duality. journal machine learning research 8 441479. rockafellar ralph t. 1970. convex analysis. princeton university press. rogers simon girolami mark. 2016. first course machine learning. chap man hallcrc. rosenbaum paul r. 2017. observation experiment introduction causal inference. harvard university press. rosenblatt murray. 1956. remarks nonparametric estimates density function. annals mathematical statistics 273 832837. roweis sam t. 1998. em algorithms pca spca. pages 626632 of advances neural information processing systems. roweis sam t. ghahramani zoubin. 1999. unifying review linear gaussian models. neural computation 112 305345. roy anindya banerjee sudipto. 2014. linear algebra matrix analysis statistics. chapman hallcrc. rubinstein reuven y. kroese dirk p. 2016. simulation monte carlo method. wiley. draft 20230215 mathematics machine learning. feedback
references 403 ruffini paolo. 1799. teoria generale delle equazioni cui si dimostra impossibile la soluzione algebraica delle equazioni generali di grado superiore al quarto. stampe ria di s. tommaso daquino. rumelhart david e. hinton geoffrey e. williams ronald j. 1986. learning representations backpropagating errors. nature 3236088 533536. sæmundsson steind or hofmann katja deisenroth marc p. 2018. meta rein forcement learning latent variable gaussian processes. in proceedings conference uncertainty artificial intelligence. saitoh saburou. 1988. theory reproducing kernels applications. longman scientific technical. s arkk a simo. 2013. bayesian filtering smoothing. cambridge university press. sch olkopf bernhard smola alexander j. 2002. learning kernels support vector machines regularization optimization beyond. mit press. sch olkopf bernhard smola alexander j. m uller klausrobert. 1997. kernel principal component analysis. in proceedings international conference artificial neural networks. sch olkopf bernhard smola alexander j. m uller klausrobert. 1998. nonlinear component analysis kernel eigenvalue problem. neural computation 105 12991319. sch olkopf bernhard herbrich ralf smola alexander j. 2001. generalized representer theorem. in proceedings international conference computa tional learning theory. schwartz laurent. 1964. sous espaces hilbertiens despaces vectoriels topologiques et noyaux associ es. journal danalyse math ematique 13 115256. schwarz gideon e. 1978. estimating dimension model. annals statistics 62 461464. shahriari bobak swersky kevin wang ziyu adams ryan p. de freitas nando. 2016. taking human loop review bayesian optimization. proceedings ieee 1041 148175. shalevshwartz shai bendavid shai. 2014. understanding machine learning theory algorithms. cambridge university press. shawetaylor john cristianini nello. 2004. kernel methods pattern analysis. cambridge university press. shawetaylor john sun shiliang. 2011. review optimization methodologies support vector machines. neurocomputing 7417 36093618. shental ori siegel paul h. wolf jack k. bickson danny dolev danny. 2008. gaussian belief propagation solver systems linear equations. pages 1863 1867 of proceedings international symposium information theory. shewchuk jonathan r. 1994. introduction conjugate gradient method with agonizing pain. shi jianbo malik jitendra. 2000. normalized cuts image segmentation. ieee transactions pattern analysis machine intelligence 228 888905. shi qinfeng petterson james dror gideon langford john smola alexander j. vishwanathan s. v. n. 2009. hash kernels structured data. journal machine learning research 26152637. shiryayev albert n. 1984. probability. springer. shor naum z. 1985. minimization methods nondifferentiable functions. springer. shotton jamie winn john rother carsten criminisi antonio. 2006. texton boost joint appearance shape context modeling multiclass object recog nition segmentation. in proceedings european conference computer vision. smith adrian f. m. spiegelhalter david. 1980. bayes factors choice criteria linear models. journal royal statistical society b 422 213220. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.
404 references snoek jasper larochelle hugo adams ryan p. 2012. practical bayesian op timization machine learning algorithms. in advances neural information processing systems. spearman charles. 1904. general intelligence objectively determined mea sured. american journal psychology 152 201292. sriperumbudur bharath k. gretton arthur fukumizu kenji sch olkopf bernhard lanckriet gert r. g. 2010. hilbert space embeddings metrics proba bility measures. journal machine learning research 11 15171561. steinwart ingo. 2007. compare different loss functions risks. constructive approximation 26 225287. steinwart ingo christmann andreas. 2008. support vector machines. springer. stoer josef burlirsch roland. 2002. introduction numerical analysis. springer. strang gilbert. 1993. fundamental theorem linear algebra. american mathematical monthly 1009 848855. strang gilbert. 2003. introduction linear algebra. wellesleycambridge press. stray jonathan. 2016. curious journalists guide data. tow center digital journalism columbias graduate school journalism. strogatz steven. 2014. writing math perplexed traumatized. notices american mathematical society 613 286291. sucar luis e. gillies duncan f. 1994. probabilistic reasoning highlevel vision. image vision computing 121 4260. szeliski richard zabih ramin scharstein daniel et al. 2008. compar ative study energy minimization methods markov random fields smoothnessbased priors. ieee transactions pattern analysis machine in telligence 306 10681080. tandra haryono. 2014. relationship change variable theorem fundamental theorem calculus lebesgue integral. teaching mathematics 172 7683. tenenbaum joshua b. de silva vin langford john c. 2000. global geometric framework nonlinear dimensionality reduction. science 2905500 2319 2323. tibshirani robert. 1996. regression selection shrinkage via lasso. journal royal statistical society b 581 267288. tipping michael e. bishop christopher m. 1999. probabilistic principal compo nent analysis. journal royal statistical society series b 613 611622. titsias michalis k. lawrence neil d. 2010. bayesian gaussian process latent variable model. in proceedings international conference artificial intelli gence statistics. toussaint marc. 2012. notes gradient descent. stuttgart.demlrmarcnotesgradientdescent.pdf. trefethen lloyd n. bau iii david. 1997. numerical linear algebra. siam. tucker ledyard r. 1966. mathematical notes threemode factor analysis. psychometrika 313 279311. vapnik vladimir n. 1998. statistical learning theory. wiley. vapnik vladimir n. 1999. overview statistical learning theory. ieee transac tions neural networks 105 988999. vapnik vladimir n. 2000. nature statistical learning theory. springer. vishwanathan s. v. n. schraudolph nicol n. kondor risi borgwardt karsten m. 2010. graph kernels. journal machine learning research 11 1201 1242. von luxburg ulrike sch olkopf bernhard. 2011. statistical learning theory models concepts results. pages 651706 of d. m. gabbay s. hartmann j. woods ed handbook history logic vol. 10. elsevier. draft 20230215 mathematics machine learning. feedback
references 405 wahba grace. 1990. spline models observational data. society industrial applied mathematics. walpole ronald e. myers raymond h. myers sharon l. ye keying. 2011. probability statistics engineers scientists. prentice hall. wasserman larry. 2004. statistics. springer. wasserman larry. 2007. nonparametric statistics. springer. whittle peter. 2000. probability via expectation. springer. wickham hadley. 2014. tidy data. journal statistical software 59 123. williams christopher k. i. 1997. computing infinite networks. in advances neural information processing systems. yu yaoliang cheng hao schuurmans dale szepesv ari csaba. 2013. charac terizing representer theorem. in proceedings international conference machine learning. zadrozny bianca elkan charles. 2001. obtaining calibrated probability esti mates decision trees naive bayesian classifiers. in proceedings international conference machine learning. zhang haizhang xu yuesheng zhang jun. 2009. reproducing kernel banach spaces machine learning. journal machine learning research 10 27412775. zia royce k. p. redish edward f. mckay susan r. 2009. making sense legendre transform. american journal physics 77614 614622. 2021 m. p. deisenroth a. a. faisal c. s. ong. published cambridge university press 2020.

lecture notes discrete mathematics k lal pati july 9 2016
2 general instructions throughout book item label 2.1.3 means 3rd item 1st section chapter 2. deﬁning new term new notation shall use bold face letters. symbol used places deﬁning term using equality symbol. symbol end statement reminds reader verify statement writing proof necessary. assume reader familiar basic counting. reader not may avoid counting items initial parts till start discuss counting. also assume reader familiar basic deﬁnitions involving sets. book written primary purpose making reader understand discussion. intend write elaborate proofs reader read end elaboration. request reader take statement book best possible natural meaning. collected quotes mainly intended inspire authors. albert einstein . value college education learning many facts training mind think. . imagination important knowledge. knowledge limited whereas imagination embraces entire world stimulating progress giving birth evolution. is strictly speaking real factor scientiﬁc research. . everything made simple possible simpler. . worry diﬃculties mathematics. assure mine still greater.
contents 1 basic set theory 5 1.1 common notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2 preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.3 principle mathematical induction . . . . . . . . . . . . . . . . . . . . 12 1.4 integers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2 advanced topics set theory 29 2.1 families sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.2 relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2.3 functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 2.4 supplying bijections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3 countability cardinal numbers partial order 43 3.1 countableuncountable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.2 partial orders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 4 logic 57 4.1 propositional logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 4.2 predicate logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 5 lattices boolean algebra 77 5.1 lattices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 5.2 boolean algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 6 counting 89 6.1 permutations combinations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 6.1.1 multinomial theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 6.2 circular permutations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 6.3 solutions nonnegative integers . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 6.4 set partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 6.5 lattice paths catalan numbers . . . . . . . . . . . . . . . . . . . . . . . . . . 110 6.6 generalizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 3
4 contents 7 advanced counting principles 115 7.1 pigeonhole principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 7.2 principle inclusion exclusion . . . . . . . . . . . . . . . . . . . . . . . . . 119 7.3 generating functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122 7.4 recurrence relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132 7.5 generating function recurrence relation . . . . . . . . . . . . . . . . . . . . 135 8 graphs 145 8.1 basic concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 8.2 connectedness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151 8.3 isomorphism graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 8.4 trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157 8.5 connectivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 8.6 eulerian graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166 8.7 hamiltonian graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169 8.8 bipartite graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172 8.9 matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173 8.10 ramsey numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176 8.11 degree sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177 8.12 planar graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178 8.13 vertex coloring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182 8.14 adjacency matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182 8.15 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
chapter 1 basic set theory 1.1 common notations following notations shall follow throughout document. n set natural numbers n0 set n 0 called set whole numbers z set integers q set rational numbers r set real numbers n set 1 2 . . . n ac complement set set clear context pa power set b cartesian product b empty set pa integer p divides integer 1.2 preliminaries expect readers familiarity following deﬁnitions. deﬁnition 1.2.1. let b two sets. 1. subset set c set element c also element a c said subset set a denoted c a. 2. equality sets sets b said equal b b a denoted b. 3. cartesian product sets cartesian product b denoted b set ordered pairs a b a b b. speciﬁcally b a b a b b. 4. set complement let c a. complement c a denoted cc set contains every element element c. speciﬁcally cc x a x c. 5
6 chapter 1. basic set theory 5. set union union b denoted b set exactly contains elements elements b. speciﬁcally b x x a x b. 6. set intersection intersection b denoted b set contains common elements b. speciﬁcally b x x a x b. set b said disjoint b . 7. set diﬀerence set diﬀerence b denoted b set contains elements b. speciﬁcally b x a x b. 8. symmetric diﬀerence symmetric diﬀerence b denoted ab equals a b b a. example 1.2.2. let b c b c b b a b c. 1. b b 2. b a b c b c b c 3. b b c b c 4. b a c 5. ab b c b c a c. following well known facts. readers supposed verify clarity. fact 1.2.3. 1. set a a a. 2. b necessary b a. 3. set b sets b b b pairwise disjoint. thus b disjoint union. is b a b a b b a. 1.1 4. set b sets ab ab disjoint. thus disjoint union. is a b a b 1.2 5. b a a b b follows 1.2. 6. ab a b a b follows 1.1 item 3. deﬁnition 1.2.4. power set let set b a. then set contains subsets b called power set b denoted pa. example 1.2.5. 1. let . then p a . 2. let . then pa a . 3. let a b c. then pa a b c a b a c b c a b c.
1.2. preliminaries 7 4. let b c b c. then pa b c b c b c b c . deﬁnition 1.2.6. relation domain set codomain set let b two sets. relation f b denoted f b subset b. set called domain set set b called codomain set. thus sets b sets b always relations b. example 1.2.7. let 3 b a b c f 1 a 1 b 2 c. then f b relation. draw picture f.1 1 3 2 b c f deﬁnition 1.2.8. 1. domain range inverse relation let f b relation. then domain f 2 mean set dom f a a y f range f mean set rng f b x b f. inverse f f 1 y x x y f. notice f 1 relation b a. example relation f example 1.2.7 dom f 1 2 rng f a b c f 1 a 1 b 1 c 2. 2. preimage image let f b relation x y f. call x preimage image x. also set x deﬁne fx y x y f x x. thus fx if x a . write fx mean fx. write fx mean fx y. example consider relation f example 1.2.7. then a f1 a b f2 c f3 . b f 1c 2 f 1b 1 f 11 f 14 . c x 1 4 c one fx a b f 1x 2. deﬁnition 1.2.9. single valued relation function relation f b single valued fx singleton dom f. function f b single valued relation dom f a. henceforth function f assume dom f . example relation f example 1.2.7 single valued. however delete 1 a f single valued. moreover make f function b need add either 3 a 3 b 3 c. particular relations g1 1 b 2 c 3 b g2 1 b 2 c 3 a indeed functions. following immediate consequence deﬁnition. 1we use pictures help understanding parts proof. 2the domain set set deﬁne relations dom f domain particular relation f. diﬀerent.
8 chapter 1. basic set theory proposition 1.2.10. let f b relation set. then 1. fs domf s . 2. f 1s rngf s . proof. prove one way implication. way left reader. part 1 since fs one ﬁnd s a b b a b f. this turn implies domf. s domf s. part 2 since rngf s one ﬁnd b rngf s a a b f. this turn implies f 1b f 1s b s. deﬁnition 1.2.11. oneoneinjection function f b called oneone also called injection f 1b 1 b b. equivalently f b oneone fx fy true pair x a. equivalently f oneone x true pair x a whenever fx fy. convention let px polynomial x integer coeﬃcients. then writing f z z function deﬁned fx px mean function f a pa z. example function fx x2 stands set a a2 z. example 1.2.12. 1. deﬁnition 1.2.9 function g2 oneone whereas g1 oneone. 2. function f z z deﬁned fx x2 oneone. 3. function f n n0 deﬁned fx x2 oneone. 4. function f 3 a b c d deﬁned f1 c f2 b f3 a oneone. verify 24 oneone functions f 3 a b c d. 5. let b. then fx x oneone map b. 6. oneone function set 3 proper subset 2. 7. oneone functions f set n proper subset 2 3 . . . one given f1 3 f2 2 fn n 1 n 3. deﬁnition 1.2.13. restriction function let f x y function x . then fa mean restriction f denote fa. is fa x y x y f x a. example 1.2.14. deﬁne f r r fx 1 x irrational fx 0 x rational. then fq q r constant 0 function. is fqx 0 x dom f q. proposition 1.2.15. let f b oneone function c nonempty subset a. then fc also oneone. proof. let possible fcx fcy x c. then deﬁnition fc fx fy. f oneone get x y. thus fc oneone.
1.2. preliminaries 9 deﬁnition 1.2.16. ontosurjection function f b called onto also called surjection f 1b b b. equivalently f b onto each z b preimage a. example 1.2.17. 1. deﬁnition 1.2.9 function g2 onto whereas g1 onto. 2. 6 onto functions 3 2. example f1 1 f2 2 f3 2 one function. 3. let b. choose a. then gy a b a. onto map b a. 4. onto function set 2 proper superset 3. 5. onto functions f set 2 3 . . . proper superset n. one fx x 1. deﬁnition 1.2.18. bijection equivalent set let b two sets. function f b said bijection f oneone well onto. sets b said equivalent exists bijection f b. example 1.2.19. 1. deﬁnition 1.2.9 function g2 bijection. 2. function f 3 a b c deﬁned f1 c f2 b f3 a bijection. thus set a b c equivalent 3. 3. let a. then fx x bijection. thus set equivalent itself. 4. f b bijection f 1 b a bijection. thus equivalent b b equivalent a. 5. set n equivalent 2 3 . . . indeed function f n 2 3 . . . deﬁned f1 3 f2 2 fn n 1 n 3 bijection. following known principle mathematical induction weak form. axiom 1.2.20. principle mathematical induction pmi weak form let n set satisﬁes 1. 1 s 2. k 1 s whenever k s. then n.1 fact 1.2.21. 1. let a b c sets let f b g b c bijections. then h c deﬁned hx gfx bijection. 1pmi actually part peano axioms deﬁnes n as a 1 n. b n n successor sn n. c 1 successor natural number. d sm sn happens natural numbers n n. e let n 1 s sk s k s. then n.
10 chapter 1. basic set theory proof. h oneone let possible hx hy x a. then deﬁnition gfx gfy fx fy b. g oneone get fx fy. now using f oneone get x hence h oneone. h onto let c c. then condition g onto implies exists b b gb c. also b b condition f onto implies exists a fa b. thus see ha gfa gb c hence required result follows. 2. let b two disjoint sets let f n g b m two bijections. then function h b m n deﬁned hx fx x a gx n x b bijection. 3. fix n 2 let f n bijection ﬁxed element a one fa k. then g a n 1 deﬁned gx fx fx k 1 fx 1 fx k 1 bijection. 4. positive integers n k bijection n n k. proof. let us ﬁx k prove result induction n. result clearly true n 1 k 1 2. so let result true n. need prove n 1. contrary assume exists bijection f n 1 n 1 k. then fact 1.2.21.3 get bijection g n n k n 1. thus arrive contradiction induction assumption. deﬁnition 1.2.22. number elements set set said ﬁnite either empty equivalent n natural number n. set ﬁnite called inﬁnite. say a n elements the number elements n mean a equivalent n. write a n mean n elements. conventionally number elements empty set zero. f n a bijection listed a1 f1 . . . fn. fact 1.2.23. 1. let b two disjoint sets a b n. then ab n. proof. use fact 1.2.21.2. 2. subset n ﬁnite. proof. use pmi prove this. true n 1. let result true n 1. now let n. n s n 1 hence using pmi result follows. n s let n. then pmi ﬁnite hence fact 1.2.23.1 ﬁnite disjoint union n. 3. subset ﬁnite set ﬁnite. proof. let s n n n. then bijection f n. let s. empty nothing prove. else consider map ft ft.
1.2. preliminaries 11 map bijection. fact 1.2.23.2 ft n ﬁnite hence ﬁnite use fact 1.2.21.1. 4. let b two ﬁnite sets a b a b a b. proof. using 1.1 b a b a b b a. sets b b b ﬁnite pairwise disjoint result follows fact 1.2.23.1. 5. let nonempty ﬁnite set. then set b a a b a b. proof. ﬁnite b b also ﬁnite. now use fact 1.2.23.1. 6. let nonempty ﬁnite set b a b a. particular b a b a. proof. since b b result follows fact 1.2.23.5. 7. let n k two ﬁxed positive integers. then oneone function n k n. proof. suppose exists oneone function f n k n n k. put b fnk n. then notice f nk b bijection hence sets b n k equivalent. thus deﬁnition fact 1.2.23.6 n k b n n 1. equivalently k 1 contradicting assumption k 1. 8. set n inﬁnite. proof. assume set n ﬁnite n n natural number n. let f n n bijection. then fn1 restriction f n 1. thus proposition 1.2.15 fn1 also oneone contradicting fact 1.2.23.7. 9. let ﬁnite nonempty set x ﬁxed symbol. now consider set b x a a. then a b. proof. deﬁne function f b fa x a a. then f bijection. 10. let inﬁnite set b a. then b inﬁnite. proof. b ﬁnite fact 1.2.23.3 ﬁnite. contradiction inﬁnite set. 11. let inﬁnite set b ﬁnite set. then b also inﬁnite. particular a a also inﬁnite. proof. b ﬁnite fact 1.2.23.1 set a b b also ﬁnite. abb hence fact 1.2.23.3 ﬁnite well. contradiction inﬁnite set. 12. set inﬁnite oneone function f n a. proof. let inﬁnite. so . let a1 a. put f1 a1 a1 a1. fact 1.2.23.11 a1 inﬁnite. assume deﬁned f1 . . . fk obtained ak ak1 ak. ak1 inﬁnite fact 1.2.23.11 ak inﬁnite. hence ak .
12 chapter 1. basic set theory let ak1 ak. deﬁne fk 1 ak1 ak1 ak ak1. applying induction f gets deﬁned n. notice construction ak1 a1 . . . ak. hence f oneone. conversely let f n a oneone. then f n fn bijection. fn ﬁnite n ﬁnite well contradicting fact 1.2.23.8. hence fn inﬁnite. fn a using fact 1.2.23.10 inﬁnite well. 13. set inﬁnite equivalent proper subset itself. proof. let inﬁnite set. then fact 1.2.23.12 oneone function f n s. deﬁne map g s f1 gx x x fn fk 1 x fk. then g indeed bijection. thus equivalent proper subset f1. conversely let set proper subset bijection f t. suppose ﬁnite let s n. then fact 1.2.23.6 also ﬁnite t n. hand assumption ﬁnite bijection t n contradiction. exercise 1.2.24. optional 1. exist unique sets x x 1 3 5 7 x 2 4 8 2. class 60 students students play either football cricket. 20 students play football cricket determine number players game number students play football a 14 number students play cricket. b exactly 5 times number students play cricket. c multiple 2 3 leaves remainder 3 divided 5. d factor 90 number students play cricket factor 70. 1.3 principle mathematical induction following known principle mathematical induction strong form. theorem 1.3.1. principle mathematical induction pmi strong form let n set satisﬁes 1. 1 s 2. k 1 s whenever k s holds. then n. proof. deﬁne k s k s. then 1 t 1 s 1 s. now suppose k t. then deﬁnition k s. therefore hypothesis implies k 1 s hence k 1 k k 1 s. thus k 1 t. hence using weak form pmi t conclude n turn implies n.
1.3. principle mathematical induction 13 theorem 1.3.2. another form pmi let z set satisﬁes 1. k0 s 2. k 1 s whenever k0 k0 1 . . . k s. then k0 k0 1 . . . s. proof. consider x k0 1 x s x k0. then 1 t k0 s 1 k0 k0 1. now let k t. then k0 k0 1 . . . k0 k 1 s. hence hypothesis k0 k 1 1 k0 k s. therefore deﬁnition t k 1 t hence using strong form pmi n. thus required result follows. next result commonly known wellordering principle states every nonempty subset natural numbers contains least element. theorem 1.3.3. application pmi strong form nonempty subset n contains minimum let n. then least element member a. proof. ﬁxed positive integer k let pk mean statement each nonempty subset n contains k also contains minimum. notice p1 true. now assume p1 . . . pk true. need show pk 1 true well. hence consider set k 1 a. 1 . . . k a k 1 min a done. r 1 . . . k a r k hence induction hypothesis pr true. so pk 1 true. hence strong form pmi required result follows. using theorem 1.3.2 also prove following generalization theorem 1.3.3. proof similar proof theorem 1.3.3 left reader. theorem 1.3.4. wellordering principle fix k z. let nonempty subset k k 1 . . . then contains minimum element. theorem 1.3.5. archimedean property positive integers let x n. then exists n n nx y. proof. contrary assume n n exist. is nx every n n. is nx n n n. now consider set y nx n n0. then s hence nonempty subset n0. therefore wellordering principle theorem 1.3.4 contains least element say mx. then assumption integer m 1x 0 m 1x s m 1x mx. contradicts minimality mx. thus assumption invalid hence required result follows. next result gives equivalence weak form pmi strong form pmi. theorem 1.3.6. equivalence pmi weak form pmi strong form fix natural number k0 let pn statement natural number n. suppose p means statement pn true n n n k0. then p proved using weak form pmi p proved using strong form pmi.
14 chapter 1. basic set theory proof. let us assume statement p proved using weak form pmi. hence pk0 true. further whenever pn true able establish pn 1 true. therefore establish pn 1 true pk0 . . . pn true. hence p proved using strong form pmi. so let us assume statement p proved using strong form pmi. now deﬁne qn mean pℓ holds ℓ k0 k0 1 . . . n. notice qk0 true. suppose qn true this means pℓ true ℓ k0 k0 1 . . . n. hypothesis know p proved using strong form pmi. is pn1 true whenever pℓ true ℓ k0 k0 1 . . . n. this turn means qn 1 true. hence weak form pmi qn true n k0. thus able prove p using weak form pmi. theorem 1.3.7. optional application pmi weak form amgm inequality fix positive integer n let a1 a2 . . . nonnegative real numbers. arithmetic mean am a1 n n a1 gm geometric mean. proof. inequality clearly holds n 1 2. assume holds every choice n nonnegative real numbers. now let a1 . . . an an1 set n1 nonnegative real numbers a1 maxa1 . . . an1 an1 mina1 . . . an1. deﬁne a1a2an1 n1 . then note a1 a an1. hence a1 aa an1 0 i.e. aa1 an1 a a1an1. now apply induction hypothesis n nonnegative real numbers a2 . . . an a1 an1 a get n p a2 a1 an1 a a2 a1 an1 a n a. so an1 a2 a3 a1 an1 aa a2 a3 an a1an1. therefore pmi inequality holds n n. next example illustrate use pmi establish given identities properties statements involving natural numbers. example 1.3.8. prove 1 2 n nn 1 2 . ans result clearly true n 1. so let us assume 1 2 n nn1 2 . then using induction hypothesis 1 2 n n 1 nn 1 2 n 1 n 1 2 n 2 . thus result holds n 1 hence weak form pmi result follows. exercise 1.3.9. optional prove using pmi. 1. 12 22 n2 nn 12n 1 6 . 2. 1 3 2n 1 n2 n n. 3. nn 1 even n n. 4. 3 divides n3 n n n.
1.3. principle mathematical induction 15 5. 5 divides n5 n n n. practice 1.3.10. wrong use pmi ﬁnd error following incorrect proof if set n balls contains green ball balls set green. find error. proof. statement holds trivially n 1. assume statement true n k. take collection bk1 k 1 balls contains least one green ball. bk1 pick collection bk k balls contains least one green ball. then induction hypothesis ball bk green. now remove one ball bk put ball left beginning. call b k. induction hypothesis ball b k green. thus ball bk1 green. hence pmi proof complete. exercise 1.3.11. optional 1. let x r x 1. then prove 1 x x2 xn n p k0 xk xn1 1 x 1 . 2. let a ad a2d . . . an1d ﬁrst n terms arithmetic progression. then n1 x i0 a id a d a n 1d n 2 2a n 1d . 3. let a ar ar2 . . . arn1 ﬁrst n terms geometric progression r 1. then ar arn1 n1 p i0 ari arn 1 r 1 . 4. prove a 6 divides n3 n n n. b 7 divides n7 n n n. c 3 divides 22n 1 n n. d 9 divides 22n 3n 1 n n. e 10 divides n9 n n n. f 12 divides 22n2 3n4 3n2 4 n n. g 13 23 n3 nn 1 2 2 . 5. determine formula 1 2 2 3 3 4 n 1 n prove it. 6. determine formula 1 2 3 2 3 4 3 4 5 n 1 n n 1 prove it. 7. determine formula 1 3 5 2 4 6 n n 2 n 4 prove it. 8. informative n 32 exist nonnegative integers x n 5x 9y. hint first prove starting 5 numbers 32 33 34 35 36. 9. informative prove that n 40 exist nonnegative integers x n 5x 11y. 10. every positive integer n 3 prove 2n n2 2n 1.
16 chapter 1. basic set theory 11. let r r r 1. then prove 1 rn 1 rn n n. 1.3 12. informative prove µ 0 p l1 1 lµ 1 pp 1 2 µ 1 2 p2p 12 4 pp 12p 1 6 µ2. 13. informative lshaped piece mean piece type shown picture. consider 2n 2n square one unit square cut. see picture. lshaped piece b c α β γ 4 4 8 8 squares unit square cut show 2n 2n square one unit square cut covered lshaped pieces. 14. informative verify k15k5 5k410k310k25k1. now put k 1 2 . . . n add get n 15 1 5 n p k1 k4 10 n p k1 k3 10 n p k1 k2 5 n p k1 k n p k1 1. now use formulas n p k1 k3 n p k1 k2 n p k1 k n p k1 1 get expression n p k1 k4. 15. informative general result amgm a let a1 . . . a9 nonnegative real numbers sum a1 a9 5. assume a1 a2. consider a1a2 2 a1a2 2 a3 . . . a9. argue a1 a9 a1a2 2 2 a3 a9. b let a1 . . . nonnegative real numbers sum a1 r0. argue highest value a1 obtained a1 r0n. c let a1 . . . ﬁxed nonnegative real numbers sum a1 r0. conclude previous item r0nn a1 an amgm inequality. 1.4 integers section study properties integers. start division algorithm. lemma 1.4.1. division algorithm let b two integers b 0. then exist unique integers q r qb r 0 r b. integer q called quotient r remainder. proof. existence take a bx x z n0. then ab s. hence nonempty subset n0. therefore wellordering principle contains minimum say s0. so s0 bx0 x0 z. notice s0 0. claim s0 b.
1.4. integers 17 s0 b s0 b 0 hence s0 b bx0 1 s contradiction s0 minimum element s. now put q x0 r s0. thus obtained q r qb r 0 r b. uniqueness assume exist integers q1 q2 r1 r2 satisfying q1br1 0 r1 b q2br2 0 r2 b. without loss generality assume r1 r2. then 0 r2r1 b. notice r2 r1 q1 q2b. so 0 q1 q2b b. integer multiple b lies 0 b 0. hence q1 q2 0. thus r1 r2 well. completes proof. deﬁnition 1.4.2. divisibility 1. divisor let a b z b 0. bc c z b said divide be divisor of denoted b a. discussion nonzero integer set positive divisors always nonempty as 1 a ﬁnite as positive divisor less equal a. 2. greatest common divisor let b two nonzero integers. then set common positive divisors nonempty ﬁnite. thus contains greatest element. element called greatest common divisor b denoted gcda b. similar lines one deﬁne greatest common divisor nonzero integers a1 a2 . . . largest positive integer divides a1 a2 . . . an denoted gcda1 . . . an. 3. relatively primecoprime integers integer said relatively prime integer b gcda b 1. or two integers b said coprime gcda b 1. next remark follows directly deﬁnition division algorithm. remark 1.4.3. let a b z 0 gcda b. then positive common divisor c b one c d. next result often stated the gcda b integer linear combination b. theorem 1.4.4. b ezouts identity let b two nonzero integers. then exist integers x0 y0 ax0 by0 gcda b. proof. consider set ax x z n. then either s a s. thus nonempty subset n. hence wellordering principle contains least element say d. s ax0 by0 x0 y0 z. claim gcda b. note positive. let c positive common divisor b. then c ax0by0 x0 y0 z. show b. division algorithm exist integers q r dq r 0 r d. thus need show r 0. contrary assume 0 r d. r dq qax0 by0 a1 qx0 bqy0 ax x z.
18 chapter 1. basic set theory hence r positive integer strictly less d. contradicts fact least element s. thus r 0 hence da. similarly db. division algorithm gives us idea algorithmically compute greatest common divisor two integers commonly known euclids algorithm. discussion 1.4.5. 1. note d number obtained application well ordering principle proof theorem 1.4.4 property divides ax by x z. so every choice integers x y gcda b divides ax by. 2. let a b z 0. division algorithm bq r integers q r z 0 r b. then gcda b gcda b gcdb r. show second equality note r bq hence gcda b r. thus gcda b gcdb r. similarly gcdb r gcda b bq r. 3. apply idea repeatedly ﬁnd greatest common divisor two given nonzero integers. called euclids algorithm. example ﬁnd gcd155 275 proceed follows 275 2 155 35 so gcd275 155 gcd155 35 155 4 35 15 so gcd155 35 gcd35 15 35 2 15 5 so gcd35 15 gcd15 5 15 3 5 so gcd15 5 5. write 5 gcd155 275 form 155x0 275y0 notice 5 35215 352155435 9352155 927521552155 927516155. also note 275 555 155 531 thus 5 931x2751655x155 x z. therefore see inﬁnite number choices pair x y z2 ax by. 4. euclids algorithm general given two nonzero integers b algorithm proceeds follows bq0 r0 0 r0 b b r0q1 r1 0 r1 r0 r0 r1q2 r2 0 r2 r1 r1 r2q3 r3 0 r3 r2 . . . . . . rℓ1 rℓqℓ1 rℓ1 0 rℓ1 rℓ rℓ rℓ1qℓ2. process take b 1 steps 0 r0 b. also note gcda b rℓ1 rℓ1 recursively obtained using backtracking. is rℓ1 rℓ1 rℓqℓ1 rℓ1 qℓ1 rℓ2 rℓ1qℓ rℓ1 1 qℓ1qℓ qℓ1rℓ2 .
1.4. integers 19 exercise 1.4.6. 1. let a b c n. then prove following a gcda b d gcda d b d 1. b gcda bc 1 gcda b 1 gcda c 1. 2. prove system 15x 12y b solution x z 3 divides b. 3. diophantine equation let a b c z 0. then linear system ax c unknowns x z solution gcda b divides c. furthermore determine pairs x y z z ax indeed c. 4. let a1 a2 . . . n. then prove gcda1 a2 . . . an gcdgcda1 a2 a3 . . . an. 5. euclids algorithm sometimes applied check whether two numbers func tions unknown integer n relatively prime not example use algorithm prove gcd2n 3 5n 7 1 every n z. 6. informative suppose milkman 3 cans sizes 7 9 16 liters. milkman 16 litres milk using 3 cans speciﬁed above minimum number operations required deliver 1 liter milk customer explain. proceed further need following deﬁnitions. deﬁnition 1.4.7. primecomposite numbers 1. unity positive integer 1 called unity or unit element z. 2. prime positive integer p said prime p unit p exactly two positive divisors namely 1 p. 3. composite positive integer r called composite r neither unit prime. ready prove important result helps us proving fundamental theorem arithmetic. lemma 1.4.8. euclids lemma let p prime let a b z. p ab either p p b. proof. p a done. so assume p a. p prime gcdp a 1. thus ﬁnd integers x 1 ax py. p ab p abx pby bax py b 1 b. thus pab either pa pb. repeated application lemma 1.4.8 following result hence proof omitted. corollary 1.4.9. let n integer n ab gcdn a 1. then n b.
20 chapter 1. basic set theory now ready prove fundamental theorem arithmetic states every positive integer greater 1 either prime product primes. product unique except order prime factors appear. theorem 1.4.10. fundamental theorem arithmetic let n n n 2. then exist prime numbers p1 p2 pk positive integers s1 s2 . . . sk n ps1 1 ps2 2 psk k k 1. moreover n also equals qt1 1 qt2 2 qtℓ ℓ distinct primes q1 q2 qℓand positive integers t1 t2 . . . tℓthen k ℓand i 1 i k pi qi si ti. proof. prove result using strong form principle mathematical induction. result clearly true n 2. so let result true m 2 m n 1. n prime nothing prove. else n prime divisor p. then apply induction n p get required result. theorem 1.4.11. euclid inﬁnitude primes number primes inﬁnite. proof. contrary assume number primes ﬁnite say p1 2 p2 3 . . . pk. now consider positive integer n p1p2 pk 1. then see none primes p1 p2 . . . pk divides n contradicts theorem 1.4.10. thus result follows. proposition 1.4.12. primality testing let n n n 2. suppose every prime p n p divide n n prime. proof. suppose n xy 2 x n. then either x n n. without loss generality assume x n. x prime done. else take prime divisor x get contradiction. exercise 1.4.13. informative prove inﬁnitely many primes form 4n1. deﬁnition 1.4.14. least common multiple let a b z. then least common mul tiple b denoted lcma b smallest positive integer multiple b. theorem 1.4.15. let a b n. then gcda b lcma b ab. thus lcma b ab gcda b 1. proof. let gcda b. then bt s z a1d b b2d a1 b1 n. need show lcma b a1b1d ab1 a1b clearly multiple b. let c n common multiple b. show a1b1d divides c. note c a1b1d cd a1d b1d cas bt ab c bs c z c a c b z s z. thus a1b1d lcma b divides c hence lcma b indeed smallest. thus required result follows. exercise 1.4.16. 1. gcdb c 1 gcda bc gcda b gcda c.
1.4. integers 21 2. gcda b d gcdan bn dn n n. deﬁnition 1.4.17. modular arithmetic fix positive integer n. then an integer said congruent integer b modulo n denoted b mod n n divides b. example 1.4.18. 1. numbers 10 22 equivalent modulo 4 4 12 22 10 4 32 22 10. 2. let n n perfect square. is exists integer n m2. then n 0 1 mod 4 integer 0 1 2 mod 4 hence m2 0 1 mod 4. 3. let 15 115 215 . . . then doesnt contain perfect square s 3 mod 4. 4. easily veriﬁed two even odd integers equivalent modulo 2 2 2l m 2l 2m 2 2l m 2l 1 2m 1. 5. let n ﬁxed positive integer let 0 1 2 . . . n 1. a then division algorithm z exists unique b s b a mod n. number mod n in short b called residue modulo n. b thus set integers z n1 a0 akn k z. is every integer congruent element s. set taken standard representative set residue classes modulo n. theorem 1.4.19. let n positive integer. then following results hold. 1. let b mod n b c mod n a b c z. then c mod n. 2. let b mod n a b z. then ac bc mod n ac bc mod n ac bc mod n c z. 3. let b mod n c d mod n a b c z. then ac bd mod n ac bd mod n. particular bm mod n n. 4. let ac bc mod n nonzero a b c z. then b mod n whenever gcdc n 1. general b mod n gcdc n. proof. prove two parts. readers supply proof parts. part 3 note ac bd ac bc bc bd ca b bc d. thus n ac bd whenever n b n c d. particular taking c b repeatedly applying result one bm mod n n. part 4 let gcdc n d. then exist nonzero c1 n1 z c c1d n n1d. thus n ac bc means n1d c1da b. this turn implies n1 c1a b. hence corollary 1.4.9 get n gcdc n n1 b. application following result popularly known fermats little theorem. theorem 1.4.20. fermats little theorem let p prime n n. then np n mod p.
22 chapter 1. basic set theory proof. note pn obviously np n mod p. so let us assume gcdp n 1. then need show np1 1 mod p. this consider set n mod p 2n mod p . . . p 1n mod p. since gcdp n 1 second fourth parts theorem 1.4.19 bn mod p b mod p. thus 1 2 . . . p 1. hence np1p 1 n 2n p 1n 1 2 p 1. thus condition gcdp 1 p 1 implies np1 1 mod p. coming next result look following examples. example 1.4.21. 1. gcd251 13 1 see 25112 1 mod 13. hence 25112 leaves remainder 1 divided 13. 2. 255 2 mod 23 gcd255 23 1. hence 25527 25522 2555 mod 23 25 mod 23 32 mod 23 9 mod 23. 3. note 3 9 13 2 1 mod 13. so system 9x 4 mod 13 solution x x 1 x 3 9 13 2 3 9x 3 4 12 mod 13. 4. verify 9 5 23 2 1. hence system 9x 1 mod 23 solution x x 1 x 9 5 23 2 5 9x 5 18 mod 23. 5. system 3x 15 mod 30 solutions x 5 15 25 whereas system 7x 15 solution x 15. also verify system 3x 5 mod 30 solution. theorem 1.4.22. linear congruence let n positive integer let b nonzero integers. then system ax b mod n least one solution gcda n b. moreover gcda n ax b mod n exactly solutions 0 1 2 . . . n 1. proof. let x0 solution ax b mod n. then deﬁnition ax0 b nq q z. thus b ax0 nq. but gcda n a n hence gcda n ax0 nq b. suppose gcda n b. then b b1d b1 z. also euclidean algorithm exists x0 y0 z ax0 ny0 d. hence ax0b1 b1ax0 b1ax0 ny0 b1d b mod n. completes proof ﬁrst part. proceed further assume x1 x2 two solutions. then ax1 ax2 mod n hence theorem 1.4.19.4 x1 x2 mod n . thus ﬁnd x2 0 1 . . . n d x x2 kn solution ax b mod n 0 k d1. verify xs distinct lie 0 n 1. hence required result follows. exercise 1.4.23. 1. prove theorem 1.4.19.
1.4. integers 23 2. prove numbers 19 119 219 . . . cannot perfect squares. 3. prove numbers 10 110 210 . . . cannot perfect squares. 4. prove system 3x 4 mod 28 equivalent system x 20 mod 28. 5. determine solutions system 3x 5 mod 65. 6. determine solutions system 15x 295 mod 100. 7. prove pair systems 3x 4 mod 28 4x 2 mod 27 equivalent pair x 20 mod 28 x 14 mod 27. hence prove system equivalent solving either 20 28k 14 mod 27 14 27k 20 mod 28 unknown quantity k. thus verify k 21 solution ﬁrst case k 22 other. hence x 20 28 21 608 14 22 27 solution pair. 8. let p prime. then prove p p k p kp k 1 k p 1. 9. informative let p prime. then set a zp 0 1 2 . . . p 1 following properties i. every a b zp b mod p zp. ii. every a b zp b b mod p. iii. every a b c zp b c a b c mod p. iv. every zp 0 a mod p. v. every zp p a 0 mod p. b z p 1 2 . . . p 1 following properties i. every a b zp b mod p z p. ii. every a b z p b b mod p. iii. every a b c z p b c a b c mod p. iv. every z p 1 a mod p. v. every z p b 1 mod p. see this note gcda p 1. hence euclids algorithm exists x z ax py 1. deﬁne b x mod p. then b a x a x p 1 mod p. algebra set say f addition multiplication deﬁned way properties satisﬁed f called ﬁeld. so zp 0 1 2 . . . p 1 example ﬁeld. general well known examples ﬁelds are i. q set rational numbers. ii. r set real numbers. iii. c set complex numbers. c let p odd prime. i. then equation x2 1 mod p exactly two solutions namely x 1 x p 1 z p. true p prime dividing x2 1 x 1x 1
24 chapter 1. basic set theory implies either px 1 px 1. also 0 number z p divisible p. ii. then 2 3 . . . p 2 ﬁnd number b 2 3 . . . p 2 z p b 1 mod p b a. iii. thus 1 i p1 2 pairs ai bi pairwise disjoint satisfy ai bi 1 mod p. moreover p1 2 i1 ai bi 2 3 . . . p 2. iv. hence 2 3 p 2 1 mod p. v. thus following famous theorem called wilsons theorem let p prime. then p 1 1 mod p. proof. note previous step p 1 1 p 1 2 3 p 2 1 1 1 mod p. vi. primality testing let n positive integer. then n 1 1 mod n n prime. theorem 1.4.24. chinese remainder theorem fix positive integer let n1 n . . . nm pairwise coprime positive integers. then linear system x a1 mod n1 x a2 mod n2 . . . x am mod nm unique solution modulo n n1n2 nm. proof. 1 k m deﬁne mk nk . then gcdmk nk 1 hence exist integers xk yk mkxk nkyk 1 1 k m. then mkxk 1 mod nk mkxk 0 mod nℓ ℓ k. deﬁne x0 p k1 mkxkak. then easily veriﬁed x0 satisﬁes required congruence relations. example 1.4.25. 1. let us come back exercise 1.4.23.7. case note a1 20 a2 14 n1 28 n2 27. so 28 27 756 m1 27 m2 28. as 27 1 28 1 1 see x1 1 x2 1. thus x0 27 1 20 28 1 14 540 392 148 608 mod 756. alternate note 27 1 28 1 1. so 20 27 1 20 mod 28 14 28 1 14 mod 27. thus 27 1 20 28 1 14 27 1 20 mod 28 20 mod 28 28 1 14 mod 27 14 mod 27. but 148 608 mod 756 hence x0 608 required answer.
1.4. integers 25 2. let x number divided 8 15 17 gives remainders 5 6 8 respectively. then remainder x divided 2040 ans note question reduces ﬁnding x n x 5 mod 8 x 6 mod 15 x 8 mod 17. also 2040 m1 255 m2 136 m3 120 8 32 1 255 1 1 136 915 1 1 120 7 17 1. hence required remainder 501 255 1 5 136 1 6 120 1 8 m1x1a1 m2x2a2 m3x3a3 mod 2040 5 255 mod 8 5 mod 8 6 136 mod 15 6 mod 15 8 120 mod 17 8 mod 17. exercise 1.4.26. 1. find smallest positive integer divided 4 leaves remained 1 divided 9 leaves remainder 2. 2. find smallest positive integer divided 8 leaves remained 4 divided 15 leaves remainder 10. 3. exist positive integer n n 4 mod 14 n 6 mod 18 give reasons answer. replace 6 4 odd number 4. informative let n positive integer. then set a zn 0 1 2 . . . n 1 following properties i. every a b zn b mod n zn. ii. every a b zn b b mod n. iii. every a b c zn b c a b c mod n. iv. every zn 0 a mod n. v. every zn p a 0 mod n. vi. every a b zn b mod n zn. vii. every a b zn b b mod n. viii. every a b c zn b c a b c mod n. ix. every zn 1 a mod n. algebra set say r addition multiplication deﬁned way properties satisﬁed r called commutative ring unity. so zn 0 1 2 . . . n 1 example commutative ring unity. general well known examples commutative ring unity are i. z set integers.
26 chapter 1. basic set theory ii. q set rational numbers. iii. r set real numbers. iv. c set complex numbers. b now let n two coprime positive integers. then above sets zm zn zmn commutative rings unity. following show onetoone correspondence ring isomorphism zm zn zmn. so deﬁne f zmn zm zn fx x mod m x mod n x zmn. then deﬁning addition multiplication zm zn componentwise using theorem 1.4.19 following i. fx y fx fy x zmn. ii. fx y fx fy x zmn. iii. every a b zm zn crt exists unique x zmn x a mod m x b mod n. iv. also zm zn zmn mn. hence obtained required oneone correspondence commonly known ring isomorphism. is two rings zm zn zmn isomorphic. 5. arithmetic function function f n c called arithmetic function. multiplicative function arithmetic function f called multiplicative function fmn fmfn m n n gcdm n 1. give list arithmetic functions quite popular number theory. reader determine functions multiplicative. a consider function idn n n n. b consider function un 1 n n. c fix positive integer n n consider function δmn 1 n 0 otherwise. d recall number n n called squarefree prime p p divides n p2 doesnt divide n. consider function µ n c deﬁned µn 0 n squarefree 1 n squarefree even number prime factors 1 n squarefree odd number prime factors function commonly known m obius function. example µ1 1 µ2 1 σ3 1 µ4 0 . . . µ10 1 . . . e consider function ϕ n c deﬁned ϕn k 1 k n gcdk n 1 n n. function popularly known totient phi euler phi function counts number positive integers less equal n coprime n. example ϕ1 1 ϕ2 1 ϕ3 2 ϕ4 2 . . . ϕ10 4 . . .
1.4. integers 27 i. let m n n gcdm n 1. then use exercise 1.4.6.1b prove ϕmn ϕm ϕn. so function ϕ multiplicative function. hence need determine ϕpn every prime p n n. ii. show p prime n n ϕpn pn1p 1 pn 1 1 p . iii. eulers theorem let n n let z gcda n 1. then aϕn 1 mod n.a generalization fermats little theorem. f consider function π n c deﬁned πn k 1 k n k prime n n. function counts number primes less equal n. example π1 0 π2 1 π3 2 π4 2 . . . π10 4 . . . g consider function n c deﬁned dn p rn 1 n n. function counts number positive divisors n. example d1 1 d2 2 d3 2 d4 3 . . . d10 4 . . . h consider function σ n c deﬁned σn p dn d n n. function gives sum positive divisors n. example σ1 1 σ2 3 σ3 4 σ4 7 . . . σ10 18 . . . i fix positive integer k consider function σk n c deﬁned σkn p dn dk n n. function gives sum kth powers positive divisors n. example σk1 1 σk2 1 2k σ3 1 3k . . . σ10 1 2k 5k 10k . . . also note σ0n dn σ1n σn n n. j let f arithmetic function. then deﬁne function set arithmetic functions itself called divisor sum function dfn p dn fd.
28 chapter 1. basic set theory
chapter 2 advanced topics set theory 2.1 families sets deﬁnition 2.1.1. let set. x a take new set ax. then collection axxa ax x a family sets indexed elements index set. unless otherwise mentioned assume index set class sets nonempty. deﬁnition 2.1.2. let bααs nonempty class sets. deﬁne 1. union αs bα x x bα α 2. intersection αs bα x x bα α. 3. convention union empty class . intersection empty class subsets x x1. example 2.1.3. 1. take 3 a1 1 2 a2 2 3 a3 4 5. then aα α a a1 a2 a3 n 1 2 2 3 4 5 . thus αa aα 5 αa aα . 2. take n n n 1 . . . then family aα α a a1 a2 . . . n 1 2 . . . 2 3 . . . . . . . thus αa aα n αa aα . 3. prove intersection nn 1 n 2 n 0. give set important rules whose proofs left reader. theorem 2.1.4. algebra union intersection let aααl nonempty class subsets x b set. then following statements true. 1. b αl aα αlb aα. 2. b αl aα αlb aα. 3. αl aα c αl ac α. 1the way see convention follows first agree intersection empty class subsets subset x. now let x x x αs bα. implies exists α s x bα. since empty α exist. 29
30 chapter 2. advanced topics set theory 4. αl aα c αl ac α. proof. give proofs part 1 4. part 1 see x b αl aα x b x αl aα x b x aα α l x b aα α l x αlb aα. part 4 x αl aα c x αl aα x aα α l x ac α α l x αl ac α. proceed similar lines complete proofs parts. exercise 2.1.5. 1. consider axxr ax x x 1. xr ax xr ax 2. x 0 1 write zx zx z z ax r zx. xr ax xr ax 3. write closed interval 1 2 nn in open intervals. 4. write r union inﬁnite number pairwise disjoint inﬁnite sets. 5. write set 4 intersection inﬁnite number inﬁnite sets. 6. suppose ab b. 7. prove theorem 2.1.4. 2.2 relations proposition 2.2.1. properties union intersection relation let f x y relation aααl px. then following statements hold. 1. f αl aα αl faα. 2. f αl aα αl faα. give example inclusion strict. proof. part 1 f αl aα x y f x αl aα x y f x aα α l y faα α l y αl faα. part 2 assume αl aα . then f αl aα x y f x αl aα x y f x aα α l y faα α l y αl faα. thus required result follows.
2.2. relations 31 remark 2.2.2. important note following proof theorem y faα α l implies for α l ﬁnd xα aα xα y f. is xαs need same. gives idea construct counterexample. deﬁne f 1 2 3 4 a b f 1 a 2 a 2 b 3 b 4 b. take a1 1 3 a2 1 2 4 verify inclusion part 2 theorem 2.2.1 strict. also ﬁnd xis b. deﬁnition 2.2.3. composition relations deﬁne composition g f relations f g g f n x z x y f y z g rngf domg . relation. composition f g deﬁned similarly. case f g functions f gx f gx. example 2.2.4. take f β a 3 b 3 c g b β c β. then g f 3 β f g b a c a. deﬁnition 2.2.5. identity relation relation x x called relation x. relation x x x x called identity relation x. also function x . deﬁnition 2.2.6. equivalence relation let f relation x. call f reﬂexive f. call f symmetric f f 1. call f transitive f f f. equivalence relation x relation reﬂexive symmetric transitive. example 2.2.7. x 5 1. equivalence relation 2. f 1 2 2 1 also equivalence relation 3. g 1 2 2 1 1 33 1 reﬂexive symmetric transitive 3 2 g. 4. let f a b z z 10 b. then f equivalence relation. 5. fix positive integer n let f a b z z n b. then f equivalence relation. example 2.2.8. some examples relations 1. let x . then is symmetric transitive relation x reﬂexive. 2. let a b c d. then relations r are a r a. b r a a b b c c d d a b a c b c. c r a a b b c c.
32 chapter 2. advanced topics set theory d r a a a b b a b b c d. e r a a a b b a a c c a c c b b. f r a b b c a c d d. 3. relations z follows a r a b z2 7 divides b. b r a b z2 b. c r a b z2 b. d r a b z z ab z z 0. 4. consider set r2. also let us write x x1 x2 y1 y2. then relations r2 follows a r x y r2 r2 x2 x2 1 x2 2 y2 1 y2 2 y2. b r x y r2 r2 x αy α r. c r x y r2 r2 4x2 1 9x2 2 4y2 1 9y2 2. d r x y r2 r2 x y α1 1 α r. e fix c r. now deﬁne r x y r2 r2 y2 x2 cy1 x1. f r x y r2 r2 x αy positive real number α. g r x y r2 r2 x1x2 y1y2. h let x r2 x2 1 x2 2 1. deﬁne relation i. r x y s x1 y1 x2 y2. ii. r x y s x y. 5. let set triangles plane. then r a b a2 b stands similarity triangles. 6. r deﬁne relation r a b r2 b integer. 7. let nonempty set consider set pa. then one deﬁne relation r pa r s t pa pa t.
2.2. relations 33 picture relation x draw pictures relations x. ﬁrst put point element x x label x. x y relation put directed line x y. x x draw loop x. following pictures relations example 2.2.7. 1 2 3 4 5 1 2 3 4 5 f 1 2 3 4 5 g deﬁnition 2.2.9. equivalence class let f equivalence relation x x. set x x x a f called equivalence class denoted ea. example 2.2.10. consider relations example 2.2.7. 1. relation i 5 equivalence classes namely 1 2 3 4 5. 2. relation f 4 equivalence classes namely 1 2 3 4 5. 3. g 3 2 2 3 3 equivalence classes namely 1 2 3 4 5. 4. notice three cases diﬀerent equivalence classes disjoint sets picture appear disconnected even joining 2 3 directed edges g. proposition 2.2.11. equivalence relation divides set disjoint classes let f equivalence relation x. then following statements true. 1. a b f ea eb. 2. a b f ea eb . 3. furthermore x ax ea. thus equivalence relation f x divides x disjoint equivalence classes. example 2.2.12. let f equivalence relation 5 whose equivalence classes 1 2 3 5 4. then f must 1 2 2 1 3 5 5 3. proposition 2.2.13. constructing equivalence relation equivalence classes let f equivalence relation x whose disjoint equivalence classes ea a. then f aax y x ea.
34 chapter 2. advanced topics set theory exercise 2.2.14. 1. let x two nonempty sets f x y relation. let ix iy identity relations x respectively. then a necessary f 1 f ix b necessary f 1 f ix c necessary f f 1 iy d necessary f f 1 iy 2. suppose f function. then a necessary f f 1 iy b necessary f 1 f ix 3. write equivalence classes equivalence relations appear exam ples 2.2.7 2.2.8. 4. take . equivalence relation a yes equivalence classes 5. nonempty set a smallest equivalence relation in sense every equivalence relation contain equivalence relation recall relation set exercise 2.2.15. optional 1. let x 5 f relation x. checking whether f reﬂexive not whether f symmetric whether f transitive not see 8 types relations x. give one example type. 2. let b 3. then number a relations b b relations f 3 a b c dom f 1 3 c relations f 3 3 f f 1 d single valued relations 3 3 many functions e equivalence relations 5. 3. important let f x y single valued relation x b y bββi nonempty family subsets . then show a f 1 βi bβ βi f 1bβ. b f 1 βi bβ βi f 1bβ. c f 1bc dom f f 1b. d f f 1b a b fa. note equality fails f single valued. 4. let f g two nonequivalence relations r. then possible f g equivalence relation give reasons answer.
2.3. functions 35 5. let f g two equivalence relations r. then provedisprove following statements. a f g necessarily equivalence relation. b f g necessarily equivalence relation. c f g necessarily equivalence relation. d f gc necessarily equivalence relation. 6. show set written union ﬁnite sets. 7. give example equivalence relation n 7 equivalence classes exactly 5 inﬁnite. 8. show union ﬁnitely many ﬁnite sets ﬁnite set. 2.3 functions recall function f b single valued relation dom f a. readers need carefully read following important remark proceeding further. remark 2.3.1. 1. convention one assumes function called empty function b. 2. b easily observed function b. 3. books use word map place function. so words used inter changeably throughout notes. 4. throughout notes whenever phrase let f b function used assumed b nonempty sets. example 2.3.2. 1. let a b c b 1 2 3 c 3 4. then verify examples given indeed functions. a f b deﬁned fa 3 fb 3 fc 3. b f b deﬁned fa 3 fb 2 fc 2. c f b deﬁned fa 3 fb 1 fc 2. d f c deﬁned fa 3 fb 3 fc 3. e f c a deﬁned f3 a f4 c. 2. note following relations f z z indeed functions. a f x 1 x even x 5 x odd. b f x 1 x z. c f x x mod 10 x z.
36 chapter 2. advanced topics set theory d f x 1 x 0 0 0 x 1 x 0. exercise 2.3.3. following relations represent functions give reasons answer. 1. let f z z deﬁned a f x 1 2 divides x x 5 3 divides x. b f x 1 x s x 1 x sc set perfect squares z. c f x x3 x z. 2. let f r r deﬁned f x x x r. 3. let f r r deﬁned f x x x r. 4. let f r c deﬁned f x x x r. 5. let f rr deﬁned f x loge x x r. 6. let f r r deﬁned f x tan x x r. exercise 2.3.4. 1. deﬁne f n z f x x 2 x even x x1 2 x odd. f oneone onto 2. deﬁne f n z g z z f x 2x x n g x x 2 x even x 0 x odd. f g g f oneone onto 3. let class subsets 9 size 5 b class 5 digit numbers strictly increasing digits. a deﬁne fa number obtained arranging elements increasing order. f oneone onto proposition 2.3.5. algebra composition functions let f b g b c h c d functions. 1. then h g f d h g f d functions. moreover h g f h g f associativity holds. 2. f g injections g f c injection. 3. f g surjections g f c surjection. 4. let b sets least two elements let f b bijection. then number bijections b least 2. 5. extension let f b g c d bijections. suppose dom f dom g rng f rng g . then f g a c b d bijection f g x fx x a x gx x c. deﬁnition 2.3.6. fix set let ida a deﬁned idaa a a. then function ea called identity function map a.
2.3. functions 37 subscript deﬁnition 2.3.6 removed whenever chance confusion domain function. theorem 2.3.7. properties identity function let b nonempty sets f b g b a two functions. then id map deﬁned above one 1. id oneone onto map. 2. map f id f. 3. map id g g. proof. part 1 let id ida. then deﬁnition ida a a hence clear id oneone onto. part 2 deﬁnition f ida fida fa a. hence f id f. part 3 readers advised supply proof. theorem 2.3.8. bijection principle let f b g b a functions. prove 1. g fi i a f oneone. 2. f gj j j b f onto. proof. let g fi i a. then assumption fi fj implies g fi g fj j. thus f oneone proof ﬁrst part over. second part let f gj j j b. see f onto let b b put gb a. then given assumption fa fgb b. exercise 2.3.9. 1. let f g n n deﬁned f x 2x x n g x x 2 x even x 0 x odd. then verify g f identity map n whereas f g maps even numbers maps odd numbers 0. 2. let f b function. then f 1 function f bijection. cantors experiment student happen take plain paper. 1. left draw oval of vertical length write elements 4 inside it one other. right draw similar large oval write elements p4 inside it one other. 2. draw directed line 1 on left element right. repeat 2 3 4. drawn function. call f. 3. notice f1 f2 f3 f4 sets. find set i fi. locate set right. 4. guaranteed directed line touching a. why
38 chapter 2. advanced topics set theory lemma 2.3.10. cantor let set f ps function. then exists ps preimage. is surjection ps. proof. contrary assume f ps surjection. then x x fx exists s fa a. so fa x x fx. show neither belongs ac. a deﬁnition a fa a. similarly a means fa a. thus a ac s contradiction. exercise 2.3.11. 1. deﬁne f n2 n fm n 2m12n 1. f bijection 2. following relations single valued functions oneone onto andor bijections a f r r deﬁned f x sin x x r. b f x y x2 y2 1 x r domf rngf c f x y x2 y2 1 x 0 0 domf rngf d f r r deﬁned f x tan x π 2 x π 2 . 3. let f x y oneone aααl nonempty family subsets x. f αl aα αl faα 4. let f x y bijection x. fac fac 5. let f x y g x two functions a f gy holds y . b g fx x holds x x. show f bijection g f 1. conclude without assuming second condition 2.4 supplying bijections experiment 1 make horizontal list elements n using once. now horizontally list elements z list n using once. draw vertical lines supply bijection n z. supply another changing second list little bit experiment 1 suppose open interval a b. center c ab 2 distance center one end l 2 ba 2 . view line segment real line. stretch a b uniformly without disturbing center make length equal l. c in r c l 2 c l 2 c α l 2 ﬁxed α 1 1 now use idea ﬁnd bijection a b s t hint fix center ﬁrst.
2.4. supplying bijections 39 exercise 2.4.1. 1. supply two bijections 1 5 one scaling translating. 2. take reciprocal supply bijection 0 1 1 . also use exponential function get this. 3. supply bijection 1 1 . 4. supply bijection 0 1 0 1 r r. trainseat argument ﬁnd bijection let f p 0 1 t 3 5 bijection. imagine elements p persons elements seats train. so f assign seat person train full. 1. suppose new person 0 arriving. wants seat. manage it let us unseat two persons 1 2 1 3. so two seats f1 2 f1 3 vacant. 3 persons take seats. giving person seat possible. 2. suppose unseat 1 2 1 3 1 30 manage it 3. suppose unseat 1 2 1 3 manage now 4. two new persons arriving fifty new persons arriving set a1 a2 new persons arriving next result left exercise students. theorem 2.4.2. let set containing set a1 a2 . . . let f b bijection. then prove that collection 1. c1 . . . ck elements outside a function hx fx x a a1 a2 . . . faik x ai n fai x ci 1 2 . . . k. bijection c1 . . . ck b. 2. c1 c2 . . . elements outside a function hx fx x a a1 a2 . . . fa2n1 x an n n fa2n x cn n n bijection c1 c2 . . . b. proof. exercise. exercise 2.4.3. following give bijections b 1. 0 1 b 0 1. 2. 0 1 1 2 3 4 b 0 1.
40 chapter 2. advanced topics set theory 3. 0 1 n 0 1. 4. 0 1 b 0 1 1 1 1 3 1 5 . 5. r b r n. 6. 0 1 b r n. 7. 0 1 b r n. 8. 0 1 b 1 2 3 4. 9. r z b r n. creating bijections injections let x n. take injections f x y g x deﬁned fx x 2 gx x 1. picture x left right. x y f draw solid line joining x y. y x g draw dotted line joining x. b b b b b b b b b b b b b b b b b b b b b b b b b b 1 2 3 4 5 6 7 1 2 3 4 5 6 7 . . . . . . want create bijection h x erasing lines. 1. thus h1 must 3. so dotted line 3 4 cannot used h. 2. so h4 must 6. so dotted line 6 7 cannot used h. 3. so h7 must 9. continue two steps realize happening. so bijection h x y given hx fx x 3n 2 n n g1x otherwise. exercise 2.4.4. take x n. supply bijections using given injections f x y g x. 1. fx x 1 gx x 2. 2. fx x 1 gx x 3. 3. fx x 1 gx 2x. theorem 2.4.5. schr oderbernstein creating bijection let b two nonempty sets let f b g b a injections. then exists bijection b.
2.4. supplying bijections 41 proof. g onto nothing prove. so assume g onto. put gb φ g f e φo φ2o . use φ0o denote o. notice g fe φe φ n0 φno n1 φno e o g map o. hence g maps fe e bijectively. recall set points mapped g e g already mapped fe onto e o. hence g must map fec ec bijectively. so function hx g1x x ec fx x e bijection b. alternate. g onto nothing prove. so assume g onto. put gb φ g f e φo φ2o . use φ0o denote o. notice φe g fe φe φ n0 φno n1 φno e o g map o. observe φ e e bijection. deﬁne h a hx x x a e φx x e. then note h bijection hence h1 g bijection b a. alternate. let f t a g ftc t c. ft gftc ftc f g note f. put u tf t. then u f g fuc g f tf c g tf ft c g tf f tc tf g f tc tf c u c. thus u maximal element f. claim u c g fuc. see this take x u c g fuc put v u x. then fu fv fv c fuc. thus g fv c g fuc u c xc v c contradiction maximality u f. so g fuc u c. now deﬁne h b hx fx x u g1x else. easy see h bijection.
42 chapter 2. advanced topics set theory exercise 2.4.6. 1. give oneone function n q. deﬁne f q n fx 2r3s x r s gcdr s 1 r 0 0 5r3s x r gcdr s 1 r 0 0 1 x 0 argue f oneone. apply schr oderbernstein theorem prove q equivalent n. 2. give oneone map 0 1 0 1 0 1. x 0 1 let .x1x2 nonterminating decimal representations1 x. x .x1x2x3 .y1y2y3 deﬁne fx y .x1y1x2y2x3y3 . argue f injection 0 1 0 1 0 1. hence show 0 1 equivalent 0 10 1. hence show rr equivalent r. 3. fix k n. supply oneone map n nk. use k distinct primes supply oneone map nk n. conclude nk equivalent n. 4. supply bijection 0 1 1 2 3 4 5 6 7 8 . 5. show using schr oderbernstein 0 1 equivalent 0 1. 1recall every real number unique nonterminating decimal representation.
chapter 3 countability cardinal numbers partial order 3.1 countableuncountable deﬁnition 3.1.1. set either ﬁnite equivalent n called countable set. set countable uncountable set. deﬁnition 3.1.2. let countably inﬁnite set. then deﬁnition bijection f n a. so list elements f1 f2 . . . list called enumeration elements a. example 3.1.3. 1. know z countably inﬁnite set. 2. set n 99 countably inﬁnite set. 3. set pn uncountable cantors lemma. 4. let set 01sequences x x1 x2 . . . deﬁne f pn fx n xn 1. f bijection. hence uncountable cantors lemma. 5. let x 0 1 x decimal expansion containing digits 0 1 only. uncountable. proof. one proof follows previous idea. alternate. cantors diagonalization countable clearly inﬁnite let x1 x2 enumeration. let xn .xn1xn2 xni 0 1. put ynn 1 xnn 0 ynn 0 otherwise. consider number .y11y22 s. notice n xn. is s enumeration list. contradiction. theorem 3.1.4. set inﬁnite countably inﬁnite subset. proof. follows fact 1.2.23.12. lemma 3.1.5. let a1 a2 . . . countably inﬁnite b a. b countable. 43
44 chapter 3. countability cardinal numbers partial order proof. b ﬁnite deﬁnition countable. so assume b inﬁnite. hence theorem 3.1.4 b countable inﬁnite subset say c c1 c2 . . . thus f c b deﬁned fai ci 1 2 . . . oneone map. hand idb b a oneone map. hence schr oderbernstein theorem b equivalent. corollary following result. corollary 3.1.6. let uncountable b. b uncountable. proof. b countable lemma 3.1.5 must countable contradiction. theorem 3.1.7. inﬁnite ps uncountable. proof. inﬁnite oneone map say f n s. now deﬁne map g pn ps ga fa. then g clearly oneone hence g pn uncountable by cantors lemma. hence ps superset uncountable corollary 3.1.6. theorem 3.1.8. countable union countable sets union countable class countable sets countable. proof. let aiin countable class countable sets put x ai. x ﬁnite done. so let x inﬁnite. hence fact 1.2.23.12 oneone map f n x. deﬁne g x n gx 2i3k smallest positive integer x ai x appears kth position enumeration ai. g oneone. now schr oderbernstein theorem equivalent n. theorem 3.1.9. set pn equivalent 0 1. furthermore pn equivalent r. proof. already know oneone map f pn 0 1 see examples 3.1.3.4 3.1.3.5. let r 0 1. consider nonterminating binary representation r. denote fr set positions 1 representation. now deﬁne g 0 1 pn gr fr r 0 g0 . g oneone. now schr oderbernstein theorem pn equivalent 0 1. next statement follows 0 1 equivalent 0 1 see exercise 2.4.6.5 0 1 equivalent r. deﬁnition 3.1.10. cardinal numbers 1. cardinal numbers symbols associated sets equivalent sets get symbol. denote cardinal number associated a. 2. injection f b write b. b mean b a. 3. bijection f b write b. 4. write n n as 0. thus ﬁnite set a a. 5. use ℵ0 denote n. x cardinal number 2x mean pa. fact 3.1.11. 1. x y z cardinal numbers x y z x z. words says oneone map b oneone map b c oneone map c.
3.1. countableuncountable 45 2. let x cardinal number. x 2x. cantors lemma. 3. cardinal numbers know till 0 1 2 3 . . . ℵ0 n 2ℵ0 r 22ℵ0 . . . 4. cardinal numbers ℵ0 n 2ℵ0 r 22ℵ0 . . . called inﬁnite cardinal num bers. 5. generalized continuum hypothesis says cardinal number inﬁnite cardinal number x 2x. example 3.1.12. 1. let set inﬁnite sequences formed using 0 1 b set inﬁnite sequences formed using 0 1 2. one larger cardinality why ans x x1 x2 a let us deﬁne fx .x1x2 binary. f 0 1 surjection hence 0 1. y y1 y2 b let us deﬁne gy .y1y2 decimal. g b 0 1 oneone. so b 0 1 hence b a. also ida b injection. thus b. 2. write r union pairwise disjoint sets size 5. ans note r 2 2 3 3 6 6 7 7 ﬁve sets cardinality. let f g h bijections 2 2 3 3 6 6 7 7 respectively. r r2r fr gr hr tr. 3. let countable set points unit circle r2. consider line segments ls one end origin end point s. fix lines. allowed rotate circle anticlockwise the lines move. let another countable set points unit circle. rotate circle angle θ line ls touches points t ans let θij angle rotation required point pi touches line lj. set θij countable set 0 2π uncountable. 4. complex number algebraic root polynomial equation integer coeﬃcients. numbers transcendental. show set algebraic numbers countable. ans point a0 a1 a2 . . . ak zk z 0 let sk roots polynomial equation a0 a1x akxk 0. take ak azkz0 sk k1 ak. set algebraic numbers. set countable ak countable union countable set. 5. give bijection r r q. ans recall q enumerated. first get bijection r q itself. now use trainseat argument adjust q.
46 chapter 3. countability cardinal numbers partial order 3.2 partial orders deﬁnition 3.2.1. let f relation x. call f antisymmetric x y f x implies y x f. is x y y x cannot f whenever x distinct. relation x called partial order reﬂexive transitive antisymmetric. let f partial order x a b x. say b comparable either a b f b a f. example 3.2.2. 1. let x 5. a identity relation reﬂexive transitive antisymmetric. so partial order. b relation 1 2 also partial order. c relation 1 2 2 1 reﬂexive transitive. antisymmetric 1 2 2 1 f. d relation 1 2 3 4 also partial order. 2. let x n. f a b divides b partial order. 3. let x nonempty class sets. f a b b partial order x. 4. r set f a b b 0 partial order. called usual partial order r. list 5 elements f. usual partial order subset r deﬁned similarly. exercise 3.2.3. give partial order 5 1. maximum number elements it. 2. minimum number elements it. deﬁnition 3.2.4. 1. partially ordered sets tuple x f called partially or dered set in short poset f partial order x. common use instead f. say x y mean x related. say x mean x y x y. 2. linear order partial order f x called linearcompletetotal order either x y f y x f pair x x. 3. linear ordered set tuple x f said linearly ordered set f linear order x. may imagine elements linearly ordered set points line. 4. chain linearly ordered subset poset called chain. maximum size chain called height poset. 5. antichain let x f poset x. suppose two elements comparable. called antichain. maximum size antichain called width poset. example 3.2.5. 1. poset example 3.2.2.1a height 1 resp. chain 1 width 5 respectively antichain 1 2 3 4 5.
3.2. partial orders 47 2. poset example 3.2.2.1b height 2 resp. chain 1 2 width 4 resp. antichain 2 3 4 5 1 3 4 5. 3. poset example 3.2.2.1d height 2 resp. chain 1 2 3 4 width 3 resp. antichain 1 3 5. find antichains 4. set n usual order linearly ordered set. 5. x f nonempty linearly ordered set height x x width x 1. 6. set n b divides b linearly ordered. however set 1 2 4 8 16 chain. completely ordered subset poset. larger chains example 2k k 0 1 2 . . . height n width n. 7. poset p5 linearly ordered. however 2 5 chain it. so 2 2 3 4 5. height 6. width deﬁnition 3.2.6. let σ nonempty ﬁnite linearly ordered set like english al phabets b c z σbe set words elements σ. a1a2 an b b1b2 bm σdeﬁne b a a1 b1 b ai bi 1 . . . k ak1 bk1 c ai bi 1 . . . n. σ linearly ordered set. ordering called lexicographic dictionary ordering. sometimes σ called alphabet set σis called dictionary. exercise 3.2.7. let d1 dictionary words made a b c d2 dictionary words made a b d. two sets equivalent discussion 3.2.8. 1. directed graph representation ﬁnite poset often represent nonempty ﬁnite poset x picture. process described below. a put dot element x label it. b b join dot dot b arrow a directed line. c put loop dot a x. 2. directed graph representation 1 2 3 9 18 divides relation a b b given below.
48 chapter 3. countability cardinal numbers partial order 1 2 3 9 18 deﬁnition 3.2.9. hasse diagram hasse diagram nonempty ﬁnite poset x picture drawn following way. 1. element x represented point labeled element. 2. b point representing must appear lower height point representing b two points joined line. 3. b b c line c removed. later shall show every nonempty ﬁnite poset x hasse diagram drawn. example 3.2.10. hasse diagram 1 2 3 9 18 divides relation. 1 2 3 9 18 exercise 3.2.11. draw hasse diagram 3 4 lexicographic order. proposition 3.2.12. let f nonempty family single valued relations either f g g f is f linearly ordered. let h ff f. following true. 1. h single valued. 2. domh ff domf. 3. rngh ff rngf. 4. every element f oneone from domain range h also oneone. proof. shall prove ﬁrst two items.
3.2. partial orders 49 1. let x domh x y x z h. f g f x y f x z g. f chain either f g g f say f g. then g single valued contradiction. 2. note x domh means x y h y. means x y f f. is x domf function f. means x ff domf. deﬁnition 3.2.13. 1. bounds let x f poset x. say x x upper bound z a z x f. words means each element x. term lower bound deﬁned analogously. 2. maximal element x a maximal whenever exists z a x z f x z. words means no element strictly larger x. term minimal deﬁned analogously. 3. maximum element x a called maximum a x upper bound a. words means an upper bound contained a. element exists unique. term minimum deﬁned analogously. 4. least upper bound element x x called least upper bound lub x upper bound upper bound a x y f. words x minimumleast set upper bounds a. term greatest lower bound glb deﬁned analogously. example 3.2.14. consider two posets described following picture. b c x b c y figure 3.1 posets x 1. consider poset figure 3.1 let x a b c. a maximal elements b c b minimal element a c lower bound x d upper bound x e maximum element f minimum element a g element x lub h glb x.
50 chapter 3. countability cardinal numbers partial order 2. consider posets figures 3.1. then deﬁnitions illustrated following table. note x a b c a b c d. ab c x aa c x ab c y maximal elements b c c b c minimal elements b c b c lower bounds xy upper bounds xy doesnt exist c maximum element doesnt exist c doesnt exist minimum element doesnt exist doesnt exist lub xy doesnt exist c glb xy exercise 3.2.15. determine maximal elements minimal elements lower bounds upper bounds maximum minimum lub glb following posets x f. 1. take x z usual order z. 2. take x n f i i n 4 5 6 7. discussion 3.2.16. bounds empty set let x f nonempty poset. x x upper bound as well lower bound . so lub may may exist. example x 3 f usual order lub 1. whereas x z f usual order lub does exist. similar statements hold glb. deﬁnition 3.2.17. linear order f x said well order nonempty subset x minimal element in a. call x f well ordered set mean f well order x. note a minimal element exists a minimum case. example 3.2.18. 1. set z usual ordering well ordered 1 2 . . . nonempty subset minimal element. 2. ordering 0 1 1 2 2 3 3 describes well order z. 3. set n usual ordering well ordered. 4. set r usual ordering well ordered set 0 1 doesnt minimal element 0 1. exercise 3.2.19. consider dictionary order n2. show well order. deﬁnition 3.2.20. let w well ordered w. initial segment deﬁned ia x x w x a. example 3.2.21. take n usual order. i5 4 i1 . theorem 3.2.22. principle transﬁnite induction let w nonempty well ordered set. let w satisﬁes whenever iw a w a. w.
3.2. partial orders 51 proof. w ac . w well ordered let minimal element ac. so element x a. is is a. hypothesis a contradiction. fact 3.2.23. principle transﬁnite induction principle mathematical induction w n. proof. see this let pn statement needs proved mathematical induction. put n n pn true. assume able show in a n a. means shown 1 a i1 a. also shown n 2 p1 . . . pn 1 true pn true well in n 1. deﬁnition 3.2.24. product sets recall product a1 a2 x1 x2 xi ai may written f1 f2 f 2 a1 a2 function f1 a1 f2 a2 . moreover a1 a2 ﬁnite sets a1 a2 a1 a2. general deﬁne product sets aααl l αl aα f f l αl aα function fα aα α l . example 3.2.25. 1. take l n 0 1. q αl aα class functions f l 0 1. is class 01sequences. 2. deﬁnition product class sets among one is empty. product class sets one empty nonempty could proved using standard set theory. fact proved question cannot answered using standard set theory. so new axiom called axiom choice introduced. axiom 3.2.26. axiom choice product nonempty class nonempty sets nonempty. proposition 3.2.27. injectionsurjection let b nonempty sets. then surjection g b injection f b a. proof. let g b onto. shall ﬁnd injection b a. start with notice b b set g1b . then axiom choice q bb g1b . let f q bb g1b. then deﬁnition 3.2.24 f b a function. g function g1bs disjoint hence f oneone. conversely let f b a oneone. fix element b b. deﬁne g b gx f 1x x fb b x a fb. observe g onto.
52 chapter 3. countability cardinal numbers partial order deﬁnition 3.2.28. class f sets called family ﬁnite character satisﬁes a f ﬁnite subset also f. example 3.2.29. 1. family ﬁnite character. 2. power sets families ﬁnite character. 3. 1 2 family ﬁnite character. 4. b pa pb family ﬁnite character. 5. set a 0 r family ﬁnite character. class linearly independent sets r. 6. let v non trivial vector space f class linearly independent subsets v. f family ﬁnite character. exercise 3.2.30. 1. let l a1 a2 a3 3. set q αl aα equal class functions f 3 3 give reasons answer. 2. give sets an n n q nn 6 elements. give another.1 equivalent axioms axiom choice axiom choice cartesian product nonempty collection nonempty sets nonempty. zorns lemma partially ordered set every chain upper bound maximal element. zermelos well ordering principle every set well ordered. hausdorﬀs maximality principle every nonempty partially ordered set contains maximal chain. tukeys lemma every nonempty family ﬁnite character maximal element. exercise 3.2.31. 1. exist poset exactly 5 maximal chains size number elements it 2 3 4 5 6 respectively 2 maximal elements yes draw hasse diagram. no argue it. 2. let x f nonempty poset x. deﬁne fy a b f a b y . show fy partial order . induced partial order . 3. apply induction show nonempty ﬁnite poset maximal element minimal element. discussion 3.2.32. drawing hasse diagram ﬁnite poset x f let x1 . . . xk minimal elements x. draw k points horizontal line label x1 . . . xk. consider x x1 . . . xk fy . induction picture y fy drawn. put k dots. let y1 . . . ym minimal elements . now draw lines xi yj xi yj f. hasse diagram x f. 1when ask one example encourage reader get examples diﬀerent types possible.
3.2. partial orders 53 discussion 3.2.33. existence hamel basis let v vector space least two elements. recall collection f linearly independent subsets v family ﬁnite character. recall basis hamel basis maximal linearly independent subset v. v least 2 elements nonzero element say a. a f. hence f . thus tukeys lemma set f maximal element. maximal set required basis. hence proved every vector space least 2 elements hamel basis. exercise 3.2.34. 1. let n n. deﬁne pn k n k divides n. deﬁne relation n pn n a b divides b. show pn n poset n n. give necessary suﬃcient condition n pn n completely ordered set. 2. take x n 1 1 1 2 1 3 . . . n 2 13 1 4 1 . . . . ordering deﬁned f m n n n n1 m 1 n o m n n n nm 1 n 1 o . x maximal minimal elements x linearly ordered true every nonempty set minimal element true every nonempty set minimum type nonempty sets always minimum 3. prove disprove a least 5 functions f r r partial orders. b let set sequences xn xn 0 1 . . . 9 n n if xk xk1 xk1 xk2 . countable. c take n usual order. dictionary order n2 well order. d let set nonincreasing sequences made natural numbers. countable. e let set nondecreasing sequences made natural numbers. countable. f take n usual order n2 dictionary order. nonempty subset n2 bounded lub. g every nonempty countable linearly ordered set well ordered respect ordering. h every nonempty countable chain bounded below partially ordered set well ordered respect ordering. i set q well ordered. j ﬁxed n n let bn nonempty sets let rn oneone relation bn. then n rn oneone relation. k let set words length 8 using letters 3 a a b c c. want deﬁne lexicographic order make dictionary. 500 ways that. l inﬁnite poset nonempty ﬁnite set minimum must linearly ordered.
54 chapter 3. countability cardinal numbers partial order m nonempty ﬁnite poset nonempty ﬁnite set minimum must well ordered. n inﬁnite poset nonempty ﬁnite set minimum must well ordered. 4. let x y x2 y2 1 x 0. relation r r. draw picture inverse relation. 5. construct hasse diagram relation pa b c. 6. draw hasse diagram partial order describing divides relations set 2 3 4 5 6 7 8. 7. draw hasse diagram 1 2 3 6 9 18 divides relation. a height width. b let 2 3 6. maximal elements minimal elements maximum minimum lower bounds upper bounds glb lub a. 8. show following three deﬁnitions equivalent. a set ﬁnite either or n n n. b tarski set ﬁnite every nonempty family subsets minimal element. c dedekind set inﬁnite equivalent proper subset itself. set ﬁnite inﬁnite. 9. let x f nonempty poset. show exists linear order g x f g. 10. let g nonabelian group h abelian subgroup g. show maximal abelian subgroup j g h j. 11. let f family ﬁnite character b chain f. show ab f. 12. let and f ﬁeld. let fa f f function f. let γ f fa a a fa 0 ﬁnite. show γ vector space f respect pointwise addition functions pointwise scalar multiplication. also show every vector space v isomorphic γ suitable choice a. 13. let x vector space nonempty linearly independent subset x. let x satisfy spans x. show a hamel basis b b s. 14. let l nonempty linearly ordered set. prove w l well orders w x l w satisfying x y. example l r take w n. 15. show r ﬁnite dimensional vector space q. hint assume r vector space q dimension k. argue r isomorphic qk countable contradiction.
3.2. partial orders 55 16. let nonempty set. element a. 17. let nonempty set. exists b b and b. 18. let b two nonempty sets. show set c c a and c b. 19. let b nonempty sets. put b b. show either b b a. 20. let b b ab . deﬁne b b ab b. a let inﬁnite cardinal number. show aa a. b let a b c cardinal numbers. show b a c b c ac bc. 21. suppose u v two inﬁnite cardinal numbers. show u v v uv v.
56 chapter 3. countability cardinal numbers partial order
chapter 4 logic 4.1 propositional logic study logic diﬀerentiate valid invalid arguments. argument set statements two parts premise conclusion. many statements premise. conclusion one statement. argument structure premise statement1 . . . statementk therefore conclusion statementc. consider following examples. . statement1 today monday mr. x gets rs. 5. statement2 today monday. statementc therefore mr. x gets rs. 5 statementc. . statement1 today monday mr. x gets rs. 5. statement2 mr. x gets rs. 5. statementc therefore today monday. . statement1 today monday mr. x gets rs. 5. statement2 today tuesday. statementc therefore mr. x gets rs. 5. . statement1 today monday mr. x gets rs. 5. statement2 today tuesday. statementc therefore mr. x get rs. 5. understand ﬁrst one valid argument whereas next three not. order diﬀerentiate valid invalid argument need analyze argument. order that ﬁrst understand what statement. simple statement expression either false true both. create complex statements old ones using and or not. example today monday statement. today tuesday also statement. today monday today tuesday also statement. today monday also statement. 57
58 chapter 4. logic one way analyze argument writing using symbols. following deﬁnition captures notion statement. deﬁnition 4.1.1. 1. atomic formulae truth values consider nonempty ﬁnite set symbols f. shall call element f atomic formula also called atomic variable. these simple statements. truth value element f exactly one for true f for false. normally use symbols p q p1 p2 . . . atomic formulae. 2. operations create new formulae use three symbols called disjunctionor called conjunctionand called negation create new formulae. way used way attribute truth value new formula described below. p q formulae p q p q p formulae. truth value p q deﬁned truth values p q t. truth value deﬁned f cases. truth value p q deﬁned truth values least one p q t. truth value deﬁned f truth values p q f. truth value p deﬁned truth value p f. truth value p deﬁned f truth value p t. understanding and following tables describe attribute truth values p q p q p. p q p q f f f f f f f p q p q f f f f f p p f f read tables look row 3 leftmost table exclude header. tells formula p q takes truth value f p takes truth value f q takes t. remark 4.1.2. use brackets creating new formulae make meaning unambiguous. example expression p q r ambiguous p q r unambiguous. deﬁnition 4.1.3. 1. sometimes write fp1 . . . pk formula mean f formula involving atomic formulae p1 . . . pk. 2. let fp1 . . . pk formula. then truth value f determined based truth values atomic formulae p1 . . . pk. since 2 assignments pi 1 i k 2k ways assigning truth values atomic formulae. assignment truth values atomic formulae nothing function p1 . . . pk t f. 3. saying tft assignment atomic variables p q r mean truth value p t q f r t. keeping mind possible
4.1. propositional logic 59 assignments p q r listed below. notice that dictionary order is fff appears fft list words dictionary. reader notice table given above followed reverse dictionary order writing truth table natural us. create confusion. p q r f f f f f f f f f f f f 4. truth table formula fp1 . . . pk table systematically lists truth values f every possible assignment truth values involved atomic formulae. following truth table formulae p q r. p q r q r p q r f f f f f f f f f f f f f f f f f f f f f 5. previous table ﬁll fourth column arbitrarily using ts fs truth table formula involving p q r shall talk later. already noted use and create new formulae old ones. indeed important. deﬁnition 4.1.4. conditional formulae 1. p implies q p q formulae formula p q denoted p q read p implies q. truth table p q p q f f f f f
60 chapter 4. logic observe a p q takes truth value f p takes truth value q takes truth value f. b assignment p q takes truth value t in assignment p t follows assignment q must t. p q called if p q. c phrases used if p q p suﬃcient q p q q necessary condition p. d sometimes use p q mean q p. 2. p q formula p q called p q means p qq p. note p q takes truth value t whenever p q take truth values takes truth value f whenever p q take diﬀerent truth values. truth table p q p q f f f f f f 3. conversecontrapositive formula q p called converse p q formula q p called contrapositive p q. discussion 4.1.5. understanding conditional formula assign diﬀerent english statements involved atomic formulae get english statement corresponding formulae. example formula p q consider following statements p attend class. q understand subject. then p q statement if attend class understand subject. formula p q true following three cases. 1. p true q true means you attend class understand subject. 2. p false q false means you attend class understand subject. 3. p false q true means you attend class understand subject. formula p q false p true q false means you attend class understand subject. deﬁnition 4.1.6. connectives symbols and are called connectives. set well formed formulae wﬀ deﬁned inductively. atomic variable wﬀ. f g two wﬀ f g f g f f g f g wﬀ. brackets used avoid ambiguity.
4.1. propositional logic 61 example 4.1.7. 1. p q q p qare wﬀ make sense. 2. p q r wﬀas clear means. use brackets get p q r p q r wﬀ. 3. p q r p q r p q wﬀ. notice connectives always connect two old formulae create new one. called binary connectives. connective used single old formula give new one. so called unary connective. deﬁnition 4.1.8. let set assignments variables p1 . . . pk. function f t f called truth function. since a 2k 22k truth functions. example 4.1.9. table left describes truth function f right describes truth table particular formula. p q f f f f f f f p q p q p q f f f f f f exercise 4.1.10. 1. draw truth table formula p p p q . 2. formulae p q q p false assignment p q deﬁnition 4.1.11. 1. contradiction tautology contradiction f formula takes truth value f assignment. example pp. tautology t formula takes truth value assignment. example p p. 2. equivalence formulae two formulae f g said equivalent denoted f g truth table involving atomic variables f g. is f g carry truth values assignment involved atomic variables. example 4.1.12. 1. p q q p yes truth tables. p q f p q g q p f f f f f f
62 chapter 4. logic 2. p p q q yes truth tables. p q f p g p q q f f f f f f f f remark 4.1.13. 1. another way establish equivalence two formulae f g. show f truth value or f g truth value. example show p q q p proceed following way. step 1 suppose p q truth value f assignment a. ap aq f. then assignment q p f. is a q p f. step 2 suppose p q truth value assignment a. tt ft ff. tt p f q f q p t. ft p q f q p t. ff p q t q p t. thus equivalent. 2. let fp1 . . . pk formula q1 . . . qr new atomic variables. f f q1 q1 qr qr. argued using induction. thus f viewed formula involving atomic variables p1 . . . pk q1 . . . qr. 3. seen a p q p q b p q p q q p. thus connectives and enough writing formula place 5 connectives and . 4. recall formula variables p q r truth function. exactly 223 28 nonequivalent formulae variables p q r. exercise 4.1.14. p p q q deﬁnition 4.1.15. substitution instance suppose b formula involves vari ables including p. then substituting formula appearance variable p b gives us new formula. new formula called substitution instance b. may substitute one variables simultaneously. note may involve old new variables. example 4.1.16. let b p q p. substitute p q p p q b obtain following substitution instance b. p q p p q
4.1. propositional logic 63 following result one fundamental results subject. theorem 4.1.17. substitution instance tautology tautology. proof. let pp1 . . . pk tautology. suppose replace occurrence p1 formula f obtain formula r. consider atomic variables involved p f. view p r formulae involving atomic variables. let assignment atomic variables. f takes value a value r nothing value pt p2 . . . pk a p tautology. f takes value f a value r nothing value pf p2 . . . pk a p tautology. thus r takes value assignment. exercise 4.1.18. show substitution instance contradiction contradiction. deﬁnition 4.1.19. functionally complete subset connectives called function ally completeadequate formula equivalent formula written using connectives s. example 4.1.20. already know adequate. exercise 4.1.21. 1. determine adequate. i ii . 2. fill blanks prove f g f g tautology. proof. assume f g. let b assignment. then value f g b. thus value f g b. b assignment see f g . therefore f b g b. is f g g f b. thus f g assignment b. conversely suppose f g . assume f g. then take diﬀerent . so suppose f takes g takes f b. f b hence f g takes f b contradiction. similar contradiction obtained f takes f g takes b. proof next result left exercise readers. proposition 4.1.22. rules p q r formulae 1. p q q p p q q p commutative 2. p q r p q r p q r p q r associative 3. p q r p q p r p q r p q p r distributive 4. p q p q p q p q de morgans law 5. p p p p p p idempotence
64 chapter 4. logic 6. f p p f p f 7. p t p p 8. p p 9. p p q p p p q p absorption law proof. first six may proved suing direct arguments rest using ﬁrst six. exercise 4.1.23. absorption law imply p p q p p p q p discussion 4.1.24. rules used simplify formula show equivalence formulae. example p q r p q r p p p q p q r associativity p q r de morgans law p q r p p p q notice 3 ways prove f g. 1. using truth table. 2. arguing f false assignment of variables involved both g false assignment. 3. using rules reducing f g g f.
4.1. propositional logic 65 experiment consider variables p q r. give formula takes value assignment ttt. give formula takes value assignment ttf. p q r give formula takes value assignment ftf. give formula takes value assignments ttf ftf. give formula takes value assignments tft ttf tff. give formula f takes value assignments ftf fff whose truth table following p q r f f f f f f f f f f f f f f f f f f f lemma 4.1.25. let f truth function involving variables p1 . . . pk. then formula g involving p1 . . . pk whose truth table described f. proof. rng f write q p1p1p2 pk. otherwise collect assignments b fb t. call set a1. b a1 deﬁne formulae q r1 r2 rk 1 j k rj pj bpj pj otherwise. then formulae q takes value assignment b. thus taking disjunctions qs related b a1 get required result. exercise 4.1.26. illustrate 4.1.25 truth function f p q f f f f f f f deﬁnition 4.1.27. normal forms atomic formula its negation called literal. say formula f disjunctive normal form in short dnf expressed disjunction conjunctions literals. say formula f conjunctive normal form in short cnf expressed conjunction disjunctions literals.
66 chapter 4. logic example 4.1.28. p p q p q p q r p q q r r s dnf. write 5 formulae cnf involving p q r. theorem 4.1.29. formula equivalent formulae dnf. similarly formula equivalent formulae cnf. proof. proof ﬁrst assertion follows lemma 4.1.25. second assertion write one proof similar way. alternate proof take f consider f get dnf p f consider p. exercise 4.1.30. write truth functions two variables write formulae them. deﬁnition 4.1.31. principal connectives let h formula. principal connective h deﬁned following way. 1. h expressed format f principal connective h. 2. h expressed format f g is principal connective h. 3. h expressed format f g is principal connective h. exercise 4.1.32. use induction number connectives show formula equivalent formulae dnf formula cnf. deﬁnition 4.1.33. dual dual p of formula p involving connectives obtained interchanging with and special variable special variable f. example 4.1.34. note dual p q r p q r. lemma 4.1.35. let ap1 . . . pk formula atomic variables pi involving connectives and . ap1 . . . pk obtained replacing pi pi a ap1 . . . pk ap1 . . . pk. proof. use induction number connectives. b c a bc bp1 . . . pk cp1 . . . pk b cp1 . . . pk ap1 . . . pk. remaining parts similar hence left reader. theorem 4.1.36. let f g formulae using connectives and . f g f g. proof. lemma 4.1.35 note f b fb gb gb assignment b. thus f g.
4.1. propositional logic 67 discussion 4.1.37. tree representation formula represented tree. example r q q p following representation. b b b b b b b b b b p q r q deﬁnition 4.1.38. polish notation formula may expressed using polish notation. deﬁned inductively follows. let pf denote polish notation f. pf g pfpg pf g pfpg pf pf. notation use brackets. connectives written front expressions connect. advantage takes less space storage. disadvantage its complicated look. example 4.1.39. polish notation r q q p becomes rq qp. exercise 4.1.40. write formula involving 8 connectives variables p q r. draw its tree. write its polish notation. deﬁnition 4.1.41. 1. satisﬁable formula satisﬁable contradiction. 2. order operations reduce use brackets ﬁx order operations . discussion 4.1.42. another way making truth table formula. consider p q r. draw table like following give truth values atomic formulae. evaluate connectives subformulae one one. example sequence column operations is 5 2 4. p q r f f f f f f f f f f f f p q r f f f f f f f f f f f f f f f f f f f deﬁnition 4.1.43. inference say g logical conclusion f1 fn f1 f2 fn g tautology. denote f1 . . . fn g. times write f1 . . . fn g mean f1 . . . fn g. here g called conclusion f1 . . . fn called hypothesispremise.
68 chapter 4. logic example 4.1.44. 1. consider following three statements. x 4 discrete math bad b discrete math bad c x 4. c logically follow a b ans no. denote x 4 p discrete mathematics bad q. then question asking whether p q q p true. is whether pp q p q q p tautology. ﬁnd that suppose assignment p takes value f. so assignment p must f p q q must true. p q q true q must t. so assignment must ft. notice p q value assignment. thus pp q takes f ft. hence tautology. so c logically follow a b. 2. consider following three statements. if discrete math bad x 4 b discrete math bad c x 4. c logically follow a b ans yes. denote x 4 p discrete mathematics bad q. then question asking whether q p q p true. is whether pp q q p q p tautology. ﬁnd that suppose assignment p takes value f. so assignment p must f q p q must true. q p q true q must q p must t. so assignment must ft. see q p value f assignment. contradiction. thus assignment pp q takes f. hence tautology. c logically follows a b. deﬁnition 4.1.45. write f g mean f g g f. notice let f g h formulae. f g h means whenever f g t h also t. is if f g assignment h assignment. thus f g f g. example 4.1.46. 1. show α β β γ γ δ α δ.
4.1. propositional logic 69 ans suppose α δ f. α δ f. assume propositions hypothesis true. δ f γ δ t γ must f. continuing get α f contradiction. 2. determine validity argument. meeting take place members informed advance quorum a minimum number members present. quorum least 15 members present. members would informed advance postal strike. therefore meeting canceled either fewer 15 members present postal strike. let us denote diﬀerent statements symbols say m meeting takes place a members informed f least ﬁfteen members present q meeting quorum p postal strike. so reformulate problem whether qa m f q p a m fp ﬁrst two statements get f a m. considering third statement get f p m. conclusion contrapositive statement. alternate. suppose conclusion f. means m f p takes value f q a m f q p a takes value t. ﬁrst one implies f p takes value f m takes value t. hence see variables m f p take values f f respectively. second one implies three expressions q a m f q p a take value t. since second statement takes value f value t see q take value t. similarly using third statement see take value t. so see ﬁrst statement q a m takes value assignment q t. so must value t contradicting value f taken previous paragraph. exercise 4.1.47. 1. list nonequivalent formulae involving variables p q take truth value exactly half assignments. 2. assume f t. let f g two truth functions variables p1 . . . p9. suppose assignment a fa ga. imply f g tautology 3. let f g two formulae involving variables p1 . . . pk. prove f g the truth table f g tautology. 4. without using write equivalent simpliﬁed statement p q p q r . 5. determine following logically equivalent. a p r s q r s .
70 chapter 4. logic b p r s p p s r . c q s. d s q r q s r . e p s q p p q s . 6. let p formula written using connectives and and involving atomic variables p1 pk k. show truth value p assignment fpi t i. 7. adequate 8. verify following assertions. a p q p b p p q c p p q d p q p e p p q q f p p q q g q p q p h p q q r p r i p q p r q r r j p q p q p q k p q p q q r l p q p q m p0 p1 p1 p2 . . . p9 p10 p0 p5. n p q r q t p t p r s q. o p q r s s t q s s p r u w t u w. 9. h set formulae h α β h α β. 10. prove equivalence following three diﬀerent ways truth table simpliﬁcation logical consequence other p q r p q r. 11. determine following conclusions correct. a lecture proceeds either black board used slides shown tablet pc used. black board used students back bench comfortable reading black board. slides shown students comfortable speed. tablet pc used causes lots small irritating disturbances instructor. lecture proceeds students comfortable. so deduced instructor faces disturbances. b three persons mr x mr mr z making statements. mr x wrong mr right. mr wrong mr z right. mr z wrong mr x right. therefore two always right.
4.2. predicate logic 71 12. consider set nonequivalent formulae written using two atomic variables p q. f g s deﬁne f g f g. prove partial order s. draw its hasse diagram. 13. consider set nonequivalent formulae written using three atomic variables p q r. f g s deﬁne f g f g. let f1 g1 two formulae truth tables p q r f1 f f f f f f f f f f f f f f f f p q r g1 f f f f f f f f f f f f f f f f many nonequivalent formulae h f1 g1 h 14. many assignments truth values p q r w p q r w true guess formula terms number variables. 15. check validity argument. discrete math bad computer programming bad. linear algebra good discrete math good. complex analysis good discrete math bad. computer programming good linear algebra bad. complex analysis bad hence least one subject bad. 4.2 predicate logic deﬁnition 4.2.1. kplace predicate propositional function px1 . . . xk state ment involving variables x1 . . . xk. truth value assigned predicate px1 xk assignment x1 . . . xk respective universe discourses in short ud the set values xis take ith ud. example 4.2.2. let px mean x 0. px 1place predicate ud. let px y mean x2 y2 1. px y 2place predicate ud. deﬁnition 4.2.3. quantiﬁers call symbols and quantiﬁers. formulae involving called quantiﬁed formulae. statement x px true x in ud property px t. statement x px px x ud. example 4.2.4. let ud set human beings. consider 2place predicate fx y x runs faster y. 1. x y fx y means each human runs faster every human being. 2. x y fx y means for human human runs slower. 3. x y fx y means there human runs faster human being. 4. x y fx y means there human runs faster every human .
72 chapter 4. logic deﬁnition 4.2.5. 1. scope quantiﬁer quantiﬁed formulae x px x px formula px called scope quantiﬁer extent quantiﬁcation applies. 2. xbound part formula part form x px x qx. occurrence x xbound part formula bound occurrence x. occurrence x free occurrence x. example 4.2.6. x px y occurrence free occurrences x bound. y x px y occurrences x bound. deﬁnition 4.2.7. 1. quantiﬁed formulae well formed created using following rules. a atomic formula of form p px y px b y wﬀ. b b wﬀs b b b b a wﬀs. c wﬀand x variable x x wﬀs. 2. let f formula. interpretation for f means process specifying ud speciﬁcations predicates assigning values free variables ud. interpretation f mean formula f given interpretation. example 4.2.8. consider wﬀx px y. 1. take n ud. let px y specify x y. let us assign 1 free variable y. then get interpretation each natural number greater 1 truth value f. 2. take n ud. let px y mean x integer take 2. then get interpretation when add 2 natural number get integer truth value t. discussion 4.2.9. translation expect see our developments logic help us drawing appropriate conclusions. order that must know translate english statement formal logical statement involves english words. may introduce appropriate variables required predicates. may specify ud normally use general ud. example 4.2.10. 1. translate each person class room either btech student msc student. a statement guarantee person room no. says is person certain properties. let px mean x person class room bx mean x btech student mx mean x msc student. then formal expression x px bx mx . 2. translate there student class room speaks hindi english.
4.2. predicate logic 73 a statement guarantee student room yes. let sx mean x student class room hx mean x speaks hindi ex mean x speaks english. then formal expression x sx hx ex . note x sx hx ex correct expression. why remember x sx tx never asserts sx xsx tx asserts sx tx. practice 4.2.11. translate formal logic. 1. every natural number either square natural number its square root irra tional. 2. every real number x real number x 0. 3. subset rn called compact write formal statement here. 4. function f r r called continuous point a write formal statement here. 5. function f r r called continuous write formal statement here. 6. function f r r called uniformly continuous write formal statement here. 7. subset rn called connected write formal statement here. 8. set called group write formal statement here. 9. subset rn called subspace write formal statement here. 10. function f t called bijection write formal statement here. 11. function f rn rk called linear transformation write formal statement here. 12. function f s t called group isomorphism write formal statement here. 13. function f v w called vector space isomorphism write formal statement here. deﬁnition 4.2.12. quantiﬁed formula called valid every interpretation truth value t. two quantiﬁed formulae b called equivalent a b b valid. example 4.2.13. 1. x px x px valid. 2. x y px y y x px y a yes. denote x y px y l y x px y r. suppose l r f. means interpretation l r f. r f see px y f x ud. case l f contradiction. so l r t. similarly r l t.
74 chapter 4. logic 3. x y px y y x px y. 4. x y px y y x px y. see take px y x y. notice two quantiﬁed formulae b equivalent interpretations the ud speciﬁcation predicates values free variables truth value. 5. x rx y ry px y xy rx ry px y a want know x rx y ry px y xy rx ry px y valid. let us see whether x rx y ry px y xy rx ry px y valid. suppose invalid. interpretation right hand side f left hand side t. right hand side f see x say x0 y rx ry px y f. is y formula rx0 ry px0 y f. is rx0 see ry px0 y f. is rx0 yry px0 y f. is formula rx0 yry px0 y f. is x rx y ry px y f contradiction. part exercise. alternate. take rx y ry px y b y rx ry px y . consider x0 ud. rx0 f b value t. rx0 t. notice rx0 y ry px0 y y rx0 ry px0 y truth value. thus b. hence xa xb. 6. student appears exam gets score 30 gets f grade. mr x0 student written exam. therefore x0 get f grade. agree a let sx mean x student ex mean x writes exam bx mean x gets score 30 fx mean x gets f grade. want see whether n xsx ex bx fx sx0 ex0 fx0 take following interpretation sx x positive real number ex x rational number bx x integer fx x natural number x0 2. interpretation statements premise mean every positive integer natural number 2 positive real number rational. true. whereas conclusion means 2 natural number false. so argument incorrect.
4.2. predicate logic 75 7. translate following formal statements. all scientists human beings. therefore children scientists children human beings. a let sx x scientist hx x human being cxy x child y. let hypothesis xsx hx. then possible translations conclusion following. a xysy cxy zhz cxz. means for x x scientist father x human father. b xysy cxy zhz cxz. wrong statement means for x x common child scientists x common child human beings. c xsx ycyx zhz cyz. means for x x scientist child x human father. d xysx cyx xyhx cxy. what means if x scientist child x including x self x human child x. exercise 4.2.14. 1. write formal deﬁnition lim xa fx l. 2. x px qx x px x qx valid its converse valid 3. common ones r involve x establish following assertions. a x px x px x px x px b x px qx x px x qx x px qx x px x qx. c x px qx x px x qx x px qx x px x qx. d x r qx r x qx x r qx r x qx e x r qx r x qx x r qx r x qx. f x px r x px r x px r x px r . 4. translate check validity following arguments. a recall decimal representation rational number either terminates begins repeat ﬁnite sequence digits whereas irrational number neither terminates repeats. square root natural number either decimal representation terminating decimal representation nonterminating nonrepeating. square root natural numbers squares terminating decimal representation. therefore square root natural number square irrational number. b two algebraic numbers b 0 1 b irrational ab transcendental. number imaginary unit irrational algebraic. number equal 0 1. therefore number ii transcendental. 5. a give interpretation show x rx y ry px y valid.
76 chapter 4. logic b give interpretation show incorrectness x px qx x px qx . 6. write formal statement taking ud students iits india following. for student iitg student iitg cpi. 7. let ud r px x integer qx x rational number. translate following statements english. a x px qx b x px qx c x px x 2 x qx x 2 d ǫ 0 δ 00 x a δ fx l ǫ 8. take general ud. check whether following conclusion valid not. student writes exam using blue ink black ink. student writes exam using black ink write hisher roll number gets f grade. student writes exam using blue ink hisher id card gets f grade. student hisher id card written exam black ink. therefore student passes exam must written roll number. ans let sx x student bx x writes exam using blue ink blx x writes exam using black ink rx x writes hisher roll number ix x hisher id card fx x gets f grade. determine whether following conclusion valid. continue.
chapter 5 lattices boolean algebra 5.1 lattices discussion 5.1.1. poset necessary two elements x common upper bound ans no. take 6 divides partial order. elements 5 3 common upper bound. poset pair x y least one upper bound necessary x y lub ans no. consider third poset described its hasse diagram figure 5.1. then pair a b c upper bounds lub a b. b b b b b 0 c 1 distributive lattice b b b b b b b 0 b c 1 nondistributive lattice b b b b b b b b c b c b lattice figure 5.1 hasse diagrams deﬁnition 5.1.2. lattice 1. poset l called lattice pair x l lub denoted x y glb denoted x y. 2. lattice called distributive lattice satisﬁes following two properties. b c a b a c. b c a b a c. distributive laws example 5.1.3. 1. let l 0 1 z deﬁne b maxa b b mina b. then l chain well distributive lattice. 77
78 chapter 5. lattices boolean algebra 2. set n usual order max min distributive lattice. consider two cases verify b c a b a c. second distributive identity left exercise reader. a case 1 minb c. then either b c say b. hence b c maxa minb c minmaxa b a maxa c a a b a c. b case 2 minb c. then b c. hence b c maxa minb c minb c minmaxa b b maxa c c a b a c. 3. prove ﬁrst ﬁgure figure 5.1 distributive lattice. 4. prove second ﬁgure figure 5.1 lattice distributive lattice. 5. let a b c. ps deﬁne b b b b. then easily veriﬁed ps lattice. 6. fix positive integer n let dn denote poset obtained using divides partial order lcm gcd. then prove dn distributive lattice. example n 12 30 36 corresponding lattices shown below. b b b b b b b b 2 1 3 12 4 6 b b b b b b b b b b b b b b b b 2 1 5 30 6 10 3 15 b b b b b b b b b b b b b b 2 1 3 36 4 6 9 18 12 exercise 5.1.4. 1. fix prime p positive integer n. draw hasse diagram dpn. correspond chain give reasons answer. 2. let n positive integer. then prove dn chain n pm prime p positive integer m. 3. let x f nonempty chain lub glb. distributive lattice proposition 5.1.5. properties lattice let l lattice. then following statements true. a operations and are idempotent i.e. luba a glba a a. b commutative so . c is associative so .
5.1. lattices 79 d a b a b absorption i.e. glba luba b luba glba b. e b a b b a b a. f b c a b a c b a c isotonicity . f1 a b c d a c b d c b d. g b c a b a c b c a b a c distributive inequalities . h c a b c a b c modular inequality . proof. prove parts. rest left reader. c let b c. then lub a b c. thus upper bound a b a c. so a b a c. therefore a b c hence upper bound a b c. so greater equals lub a b c i.e. a b c. thus ﬁrst part result follows. e let b. b upper bound a b b luba b b. also b upper bound a b hence b b. so get b b. conversely let b b. b upper bound a b a b b. thus ﬁrst part result follows. f let b c. note c a c c b. so c upper bound a b. thus c luba b b hence prove ﬁrst part over. f1 using isotonicity c b c b d. similarly using isotonicity again c b c b d. g note a b a c. thus a a b a c. b a b c a c get b c a b a c. using f1 obtain required result i.e. b c a b a c. h let c. then c c hence distributive inequality b c a b a c a b c. conversely let b c a b c. then a b c a b c c required result follows. practice 5.1.6. show lattice one distributive equality implies other. deﬁnition 5.1.7. li i 1 2 lattices lub glb. then l1l2 poset a1 a2 b1 b2 b a1 1 b1 a2 2 b2 is b dominates entrywise. case see b a1 1 b1 a2 2 b2 b a1 1 b1 a2 2 b2. thus l1 l2 lattice called direct product li i 1 2. example 5.1.8. 1. consider l 0 1 usual order. set binary strings ln length n poset order a1 . . . an b1 . . . bn ai bi i. nfold direct product l. called lattice ntuples 0 1. 2. consider lattices 3 4 usual orders. hasse diagram direct product 3 4 given below.
80 chapter 5. lattices boolean algebra b b b b b b b b b b b b b b b 1 1 3 1 3 4 1 4 b b b b b b practice 5.1.9. consider n usual order. lattice order deﬁned n2 direct product diﬀerent lexicographic order n2. draw pictures a b 5 6 orders see argument. proposition 5.1.10. direct product two distributive lattices distributive lattice. proof. direct product two lattices lattice deﬁnition. note a1 b1 a2 b2 a3 b3 a1 a2 b1 b2 a3 b3 a1 a2 a3 b1 b2 b3 a1 a3 a2 a3 b1 b3 b2 b3 a1 a3 b1 b3 a2 a3 b2 b3 a1 b1 a3 b3 a2 b2 a3 b3 deﬁnition 5.1.11. let li i 1 2 two lattices. function f l1 l2 satisfying fa 1 b fa 2 fb fa 1 b fa 2 fb called lattice homomorphism. furthermore f bijection called lattice isomorphism. example 5.1.12. 1. let set words english dictionary dictionary ordering. then prove lattice. now consider set words length six ﬁrstpartwords length six. note lattice again. deﬁne f s fd length six otherwise fd ﬁrstpartword length 6 d. then f homomorphism. isomorphism fstupid fstupidity. 2. consider lattice n usual order. let 0 1 2 usual order. let f n s homomorphism. fm 0 fn 1 n else 0 fm fm n fm fn 0 1 1. thus map f must one following forms. draw pictures understand this. a f 10 n. b f 10 k f 11 k 1 . . . c f 10 k f 11 r k f 12 n r k.
5.1. lattices 81 deﬁnition 5.1.13. lattice l complete a lub a a glb a exist l nonempty subset l. example 5.1.14. 1. verify every ﬁnite lattice complete. 2. every complete lattice least element 0 greatest element 1. lattice two elements called bounded lattice. 3. set 0 5 usual order bounded complete lattice. so set 0 12 3. 4. set 0 5 lattice neither bounded complete. 5. set 0 1 2 3 bounded lattice though complete. 6. set r usual order lattice. complete lattice sense. conditionally complete is every bounded nonempty subset glb lub exist. think reason implies importance condition nonemptiness b b b b b b b b b b b b b b b b 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 0 1 1 b b b b b b b b b b b b b b b b 2 1 5 30 6 10 3 15 b b b b b b b b b b b b b b b b a c a b c a b a c b b c 7. fix n n let p1 p2 . . . pn n distinct primes. prove lattice dn n p1p2 pn isomorphic lattice ln the lattice ntuples 0 1 lattice ps 1 2 . . . n. hasse diagram n 3 shown above. deﬁnition 5.1.15. complement let l bounded lattice. then complement b l element if exists c l b c 1 b c 0. lattice called complemented every element least one complement. shall use b denote b complement b. example 5.1.16. 1. interval 0 1 usual ordering distributive lattice complemented. 2. verify captions two ﬁgures given below. also compute 0 a b c 1. b b b b b b b 0 b c 1 complemented distributive b b b b b b 0 1 f distributive complemented
82 chapter 5. lattices boolean algebra discussion 5.1.17. the comparison table let l lattice let a b c l. then following table lists properties hold make sense speciﬁed type lattices. properties lattice type are idempotent lattice are commutative lattice are associative lattice absorption a b a b lattice b a b a b b lattice isotonicity b c a b a c b a c lattice distributive inequalities b c a b a c b c a b a c lattice modular inequality c a b c a b c lattice 0 unique 1 unique bounded lattice complement b b also complement bounded lattice 0 unique 1 1 unique 0 bounded lattice element unique complement distributive complemented lattice cancellation n c b c c b c a b n c b c c b c a b distributive complemented lattice demorgan a b a b a b a b distributive complemented lattice b 1 a b b 0 a b distributive complemented lattice proof. prove properties appear last three rows. properties left exercise reader. prove cancellation property note b b 0 b c c b c b c a c a c c c 0 b b 1 b c c b c b c a c a c c c 1 a. prove demorgans property note a b a b a b a a b b 1 1 1 a b a b a a b b a b 0 0 0. hence deﬁnition 5.1.15 get a b a b. similarly note a b a b aabbab 11 1 abab abaabb 00 0.
5.2. boolean algebras 83 thus deﬁnition 5.1.15 get a b a b. prove next assertion note b 1 b b a b a b a b 1 b. conversely b b a b b 1. similar lines one completes proof second part left exercise reader. exercise 5.1.18. 1. prove every linearly ordered set distributive. 2. draw hasse diagrams 3 4 dictionary order lattice order m n p q p n q. 3. give partial order n make bounded lattice. may draw hasse diagram representing it. 4. exist partial order n nonempty subset ﬁnitely many at least one upper bounds ﬁnitely many at least one lower bounds 5. consider lattice n2 lexicographic order. isomorphic direct product n itself is usual order 6. show 0 1 2 . . . complete lattice divisibility relation allow 0 0 relation. characterize sets a 0. 7. draw many hasse diagrams nonisomorphic lattices size 6 can. 8. lattice 2 2 2 2 isomorphic 4 4 9. provedisprove l lattice complete l n. 10. draw hasse diagram ﬁnite complemented lattice distributive. 11. many lattice homomorphisms 2 9 5.2 boolean algebras deﬁnition 5.2.1. boolean algebra boolean algebra set closed binary operations called join called meet x y z s satisﬁes following properties. 1. x y x x y x commutative . 2. x y z x y x z x y z x y x z distributive . 3. 0 1 s x 0 x x 1 x identity elements . 4. x s y s x y 1 x y 0 inverse . proposition 5.2.2. let boolean algebra. then following statements true. 1. elements 0 1 unique. 2. s s unique. therefore x s x called inverse x. 3. inverse x x inverse y. is x x.
84 chapter 5. lattices boolean algebra proof. 1. let 01 02 two elements. then 01 x x x x 02 x s. hence 01 01 02 02. thus required result follows. similar argument implies 1 unique. 2. suppose exists t r s t 1 t 0 r 1 r 0. then t1 tsr tstr 0tr srtr str 1r r. 3. directly follows deﬁnition inverse. example 5.2.3. 1. let . then ps boolean algebra a ac 0 and 1 s. so boolean algebras ﬁnite size well uncountable size. 2. take n n n30 b lcma b b gcda b a 30 0 1 1 30. boolean algebra. 3. let b t f 0 f 1 usual . boolean algebra. 4. let b set truth functions involving variables p1 . . . pn usual . take 0 f 1 t. free boolean algebra generators p1 . . . pn. 5. class ﬁnite length formulae involving variables p1 p2 . . . countable inﬁnite boolean algebra usual operations. observation. rules boolean algebra treat 0 1 equally. notice second part rules deﬁnition 5.2.1 obtained replacing with and 0 1. thus statement one derive rules dual version derivable rules. called principle duality. theorem 5.2.4. rules let s boolean algebra. then following rules well dual hold true. 1. 0 1. 2. s s idempotence . 3. s 1 1. 4. s s s t absorption . 5. t r t t r t r cancellation . 6. s t r t r associative . proof. give proof ﬁrst part item dual left reader. 1. 1 0 0 0. 2. 0 s s s s s s s s 1 s s.
5.2. boolean algebras 85 3. 1 s s 1 s s s 1 1 s 1 1. 4. s t s 1 s t 1 t 1 s. 5. 0 t t s t s t r t r t r t t r 0 r. 6. prove using absorption cancellation. using absorption s t s r s s. thus s t r s s t s r s r s s. using absorption also s t r s hence s t r s s t r s. now see s t r s 0 t r s t s r s similar lines s t r s t s r s. thus s t r s s t r s. hence applying cancellation property required result follows. example 5.2.5. let l distributive complemented lattice. then deﬁnition 5.1.2 l two binary operations and and deﬁnition 5.1.15 operation x. easily veriﬁed l indeed boolean algebra. now let b boolean algebra. then two elements a b b deﬁne b b a. next result shows is partial order b. partial order generally called induced partial order. thus see boolean algebra b induced partial order distributive complemented lattice. theorem 5.2.6. let b boolean algebra. deﬁne b b a. then is partial order b. furthermore b luba b b glba b. proof. ﬁrst verify b indeed partial order. reﬂexive idempotence s hence is reﬂexive. antisymmetry let t s. then t t. transitive let t r. then using associativity sr str str st thus r. now show b luba b. since b boolean algebra using absorption get a b a hence a b. similarly b a b. so b upper bound a b. now let x upper bound a b. then distributive property a b x a x b x b. so b x. thus b lub a b. rest proof similar hence left reader. thus observe onetoone correspondence set boolean algebras set distributive complemented lattice. deﬁnition 5.2.7. atom let b boolean algebra. exists b b b 0 b minimal element b b called atom. example 5.2.8. 1. powerset boolean algebra singleton sets atoms.
86 chapter 5. lattices boolean algebra 2. atoms divides 30 boolean algebra 2 3 5. 3. f t boolean algebra one atom namely t. exercise 5.2.9. 1. determine atoms free boolean algebra generators p1 . . . pn 2. necessary every boolean algebra least one atom deﬁnition 5.2.10. boolean homomorphism let b1 b2 two boolean algebras. function f b1 b2 boolean homomorphism preserves 0 1 . is f01 02 f11 12 fa b fa fb fa b fa fb fa fa. boolean isomorphism boolean homomorphism bijection. exercise 5.2.11. let b1 b2 two boolean algebras let f b1 b2 function satisﬁes four conditions f01 02 f11 12 fa b fa fb fa b fa fb. then prove f also satisﬁes ﬁfth condition namely fa fa. example 5.2.12. function f pj4 pj3 deﬁned fs 4 boolean homomorphism. check two properties rest left exercise. fa b fa b a b 4 a 4 b 4 fa fb. f11 fj4 j4 4 j3 12. proposition 5.2.13. let b boolean algebra p q two distinct atoms. then pq 0. proof. suppose p q 0. p q p p atom must p q p i.e. q p. p q q atom follows p cannot atom. proposition 5.2.14. let b boolean algebra three distinct atoms p q r. then p q p q r. proof. let possible p q p q r. then r r 0 r p q p q r p q r p q p q r p q p q r p q p q p q r p r q r 0 0 0 contradiction r atom i.e. r nonzero. example 5.2.15. let b boolean algebra distinct atoms p q r. then b least 23 elements. show this deﬁne f pa b f 0 a fs w xs x claim f oneone function. suppose fs ft. then fs fs ft fs t. view proposition 5.2.14 t i.e. s. similarly ft ft s t hence t. thus f oneone function. therefore fs distinct subset thus b least 23 elements.
5.2. boolean algebras 87 theorem 5.2.16. let b boolean algebra distinct atoms p q r s. let b b b 0. suppose atoms x x b p q r. then b p q r. proof. clear p q r b. suppose p q r b. then b bpq rpq r bpq rbpq r pq rbpq r. therefore equality implies b p q r 0. so atom say x x b p q r. thus x b x p q r. notice x p q r x 0 possible. so x p q r atom s contradiction. theorem 5.2.17. representation let b ﬁnite boolean algebra. then exists set x b isomorphic px. proof. put x atoms b. note x . deﬁne f b px fb atoms b. show f required boolean isomorphism. injection let b1 b2. then either b1 b2 b2 b1. without loss generality let b1 b2. now imagine power set boolean algebra. saying b1 b2 b1 b2. case element b1 b2. is b1 bc 2 . is singleton subset b1 bc 2. exactly aiming for i.e. prove b1 b2 0. note b1 b1 b2 b2 b1 b2b1 b2. also assumption b1 b2 implies b1 b2 b1 hence b1 b2 0. so exists atom x b1 b2 hence x x b1 b2. therefore x b1 x b1 b2 b1 x b1 b2 x. thus x b1. similarly x b2. x 0 cannot x b2 the condition x b2 x b2 implies x b2 b2 0. thus fb1 fb2. surjection let x1 . . . xk x put b x1 xk if k 0 b 0. clearly fb. need show fb. so let fb i.e. atom b b x1 xk y x1 y xk. since 0 proposition 5.2.13 follows xi0 0 i0 1 2 . . . k. xi0 atoms xi xi hence a. thus f surjection. preserving 0 1 clearly f0 and f1 x. preserving deﬁnition x fb1 b2 x b1 b2 x b1 x b2 x fb1 x fb2 x fb1 fb2. now let x fb1 b2. then deﬁnition x xb1b2 xb1xb2. so exists xbi 0 say xb1. as x atom x b1 hence x fb1 fb1fb2. conversely let x fb1 fb2. without loss generality let x fb1. thus x b1 hence x b1 b2 turn implies x fb1 b2. direct corollary following result.
88 chapter 5. lattices boolean algebra corollary 5.2.18. let b ﬁnite boolean algebra exactly k atoms. then b isomorphic p1 2 . . . k hence exactly 2k elements. exercise 5.2.19. 1. determine number elements ﬁnite boolean algebra. 2. supply boolean homomorphism f pj4 pj3 image pj4 4 elements. 3. provedisprove number boolean homomorphisms pj4 pj3 less number lattice homomorphisms pj4 pj3. 4. show lattice homomorphism boolean algebra preserves 0 1 boolean homomorphism. 5. consider class functions f r π e. deﬁne operations class make boolean algebra 6. show ﬁnite boolean algebra must least one atom. ﬁnite necessary 7. positive integer called squarefree divisible square prime. let bn k n kn. a b bn take operations b lcma b b gcda b a na. show bn boolean algebra n 1 squarefree. 8. show set subsets n either ﬁnite ﬁnite complement countable inﬁnite boolean algebra. find atoms. isomorphic boolean algebra ﬁnite length formulae involving variables p1 p2 9. let b boolean algebra xi b 1 2 . . . know that n n expression n w i1 xi meaningful boolean algebra due associativity. w i1 xi necessarily meaningful expression 10. provedisprove let f b1 b2 boolean homomorphism b1 atom. then fa atom b2. 11. fill blank number boolean homomorphisms pj4 pj3 . 12. fill blank number boolean homomorphisms pj4 onto pj3 . 13. many atoms divides 30030 boolean algebra has many elements have 14. b1 b2 boolean algebras size k k 100 must isomorphic must k isomorphisms them. 15. give examples two countably inﬁnite nonisomorphic boolean algebras. 16. give examples two uncountably inﬁnite nonisomorphic boolean algebras.
chapter 6 counting discussion 6.0.1. previous chapters learnt two sets say b cardinality exists oneone onto function f b. also learnt following two rules counting play basic role development subject. 1. multiplication rule task n compulsory parts say a1 a2 . . . ith part completed mi ai ways 1 . . . n task completed m1m2 mn ways. mathematical terms a1 a2 an a1 a2 an. 2. addition rule task consists n alternative parts say a1 a2 . . . an ith part done ai mi ways 1 . . . n task completed m1 m2 mn ways. mathematical terms a1 a2 an a1 a2 an whenever ai aj 1 i j n. deﬁnition 6.0.2. use notation n 1 2 n. convention take 0 1. 6.1 permutations combinations example 6.1.1. many three digit natural numbers formed using digits 0 1 9 identify number parts task type parts compulsory alternative. rule applies here ans task three compulsory parts. part 1 choose digit leftmost place. part 2 choose digit middle place. part 3 choose digit rightmost place. multiplication rule applies. ans 900. example 6.1.2. many three digit natural numbers distinct digits formed using digits 1 9 digit odd digit even identify number parts task type parts compulsory alternative. rule applies here 89
90 chapter 6. counting ans task two alternative parts. part 1 form three digit number distinct numbers 1 3 5 7 9 using odd digits. part 2 form three digit number distinct numbers 2 4 6 8 using even digits. observe part 1 task three compulsory subparts. view 6.1.1 see part 1 done 60 ways. part 2 task three compulsory subparts. view 6.1.1 see part 2 done 24 ways. since task alternative parts addition rule applies. ans 84. deﬁnition 6.1.3. rsequence rsequence elements x sequence length r elements x. may viewed word length r alphabets x function f r x. write an rsequence s mean an rsequence elements s. theorem 6.1.4. number rsequences number rsequences n nr. proof. task r compulsory parts. choose ﬁrst element sequence second element on. exercise 6.1.5. 1. many ways r distinguishabledistinct balls put n distinguishabledistinct boxes 2. many distinct ways make 5 letter word using english alphabet a restriction b consonants c vowels d consonant ﬁrst letter vowel second letter e vowels appear odd positions 3. determine total number possible outcomes a two coins tossed b coin die tossed c two dice tossed d three dice tossed e k dice tossed k n f ﬁve coins tossed 4. many 5letter words using as bs cs ds contain word cad deﬁnition 6.1.6. rpermutation nset nset mean set containing n elements. rpermutation nset arrangement r distinct elements row. rpermutation may viewed oneone mapping f r s. npermutation nset simply called permutation.
6.1. permutations combinations 91 example 6.1.7. many oneone maps f 4 a a b . . . z there ans task 4 compulsory parts select f1 select f2 select f3 select f4. note f2 cannot f1 f3 cannot f1 f2 on. apply multipli cation rule. ans 26 25 24 23 26 22. theorem 6.1.8. number rpermutations number rpermutation nset pn r n nr. proof. let us view rpermutation oneone map f r s. task r compulsory tasks select f1 select f2 . . . select fr condition 2 k r fk f1 f2 . . . fk 1. multiplication rule applies. hence number rpermutations equals nn 1 n r 1 n nr. deﬁnition 6.1.9. p n r denote number rpermutations n. convention pn 0 1. books use notation nr call falling factorial n. thus r n pn r nr 0 n r pn r nr n. exercise 6.1.10. 1. many distinct ways make 5 letter words using english alphabet letters must diﬀerent 2. many distinct ways arrange 5 letters word roy al 3. determine number ways place 4 couples row couple seats together. 4. many distinct ways 8 persons including ram shyam sit row ram shyam sitting next other proposition 6.1.11. principle disjoint preimages equal size let a b ﬁnite sets f b function pair b1 b2 b f 1b1 k f 1b2 recall f 1b1 f 1b2 . then a kb. discussion 6.1.12. consider word aabab. give subscripts three two bs complete following list. notice give us aabab erase subscripts. a1a2b1a3b2 a1a2b1b2a3 a1a2a3b1b2 a1a2a3b2b1 a1a2b2b1a3 a1a2b2a3b1 b2a3b1a1a2 b2a3b1a2a1 example 6.1.13. many words size 5 use three as two bs ans put arrangements a1 a2 a3 b1 b2 b words size 5 use three as two bs. arrangement a deﬁne fa word b obtained erasing subscripts. then function f b satisﬁes for b c b b c f 1b f 1c 32 f 1b f 1c . thus proposition 6.1.11 b a 32 5 32.
92 chapter 6. counting remark 6.1.14. let us ﬁx n k n 0 k n ask question how many words size n uses k many as n k many bs ans put arrangements a1a2 . . . akb1b2 . . . bnk b words size n uses k many as n k many bs proceed get b a kn k n kn k required answer. observe argument implies n knk z. denote number pn k. note pn k pn n k also per convention pn k 0 whenever k 0 n k. idea generalized below. deﬁnition 6.1.15. multiset collection objects object appear once. so set multiset. note a a b c d a b a c d 5multisets. theorem 6.1.16. arrangements let us ﬁx n k n 1 k n let multiset containing ni n objects ith type 1 . . . k n k p i1 ni. then n1 nk n1n2 nk n n1n2 nk arrangements objects s. proof. assume consists ni copies ai 1 . . . k. put a11 . . . a1n1 a21 . . . a2n2 b words size made using elements . arrangement a deﬁne fa word b obtained erasing right subscripts objects a. then function f b satisﬁes for b c b b c f 1b f 1c f 1b f 1c . thus proposition 6.1.11 b a n1nk n1nk n1nk n n1n2nk. theorem 6.1.17. allocation i distinct locations identical objects ni type i one per place fix positive integer k 1 i k let gis boxes containing ni n identical objects. objects distinct boxes nonidentical n k p i1 ni then number allocations objects n distinct locations l1 . . . ln location receiving one object n n1nknp ni. proof. consider new group gk1 nk1 n k p 1 ni objects new type. notice allocation objects g1 . . . gk n distinct places location receives one object gives unique arrangement elements g1 . . . gk1.1 thus number 1take allocation objects g1 . . . gk n distinct places location receives one object. nk1 locations empty. supply object gk1 locations. created arrangement elements g1 . . . gk1. conversely take arrangement elements g1 . . . gk1. view allocation elements g1 . . . gk1 n distinct places. empty places received elements gk1. created allocation elements g1 . . . gk n distinct places location receives one object.
6.1. permutations combinations 93 allocations objects g1 . . . gk n distinct places location receives one object number arrangements elements g1 . . . gk1. theorem 6.1.16 number n n1nknp ni. deﬁnition 6.1.18. let n n1 n2 . . . nk n. then number n n1 nkn p ni de noted p n n1 . . . nk. thus p6 1 1 1 p6 3. convention pn n1 . . . nk 0 whenever either ni 0 i 1 i k k p i1 ni n. many texts use cn n1 nk mean pn n1 nk. shall interchangeably use them. deﬁnition 6.1.19. rcombination rcombination nset rsubset s. number rsubsets nset denoted cn r. thus natural number n cn 0 cn n 1. theorem 6.1.20. combination cn r pn r n rnr. proof. theorem 6.1.17 number allocations r identical objects n distinct places p1 . . . pn place receiving 1 pn r. note allocation uniquely corresponds rsubset n namely i pi receives object a. thus cn r pn r n rnr. example 6.1.21. many ways allocate 3 identical passes 10 students student receives one ans c10 3 theorem 6.1.22. pascal cn r cn r 1 cn 1 r 1. proof. theorem 6.1.20 cn r n rnr. verify identity get result. experiment complete following list ﬁlling left list 3subsets 5 right list 3subsets 4 well 2subsets 4 shown below. c5 3 1 2 3 2 3 4 1 2 5 3 4 5 1 2 3 2 3 4 c4 3 1 2 3 4 c4 2 theorem 6.1.23. alternate proof pascals theorem 6.1.22 supply combi natorial proof i.e. by associating numbers objects. let n 1 r 1subset s. then cn 1 r 1 sets either n 1 a n 1 a. note n 1 a n 1 rsubset n. so number r 1subsets n 1 contain element n 1 is deﬁnition cn r.
94 chapter 6. counting also n 1 a r 1subset n. so set contain n 1 formed cn r 1 ways. hence r 1subset formed deﬁnition cn r cn r 1 ways. thus required result follows. experiment consider subsets 4. complete following list using 0s 1s xs ys x commuting xy yx symbols. 0000 yyyy y4 1 1000 xyyy xy3 2 0100 yxyy xy3 3 0010 yyxy xy3 4 0001 yyyx xy3 1 2 1100 xxyy x2y2 1 2 3 4 1111 xxxx x4 remark 6.1.24. another alternate proof pascals theorem 6.1.22 supply another combinatorial proof. r 1 subset n 1 may viewed string word size n 1 made n r many 0s r 1 many 1s. number strings end 1 cn r. number strings end 0 cn r 1. so required conclusion follows. practice 6.1.25. give combinatorial proof cn r cn n r whenever n r n 0 r n. theorem 6.1.26. allocation ii distinct locations distinct objects ni place i number ways allocating objects o1 . . . pockets p1 . . . pk pocket pi contains ni objects pn n1 . . . nk. proof. task k compulsory parts select n1 pocket p1 on. so answer cn n1cn n1 n2 cn n1 nk1 nk pn n1 . . . nk. alternate. take allocation o1 . . . pockets p1 . . . pk pocket pi gets ni objects. allocation n1 copies p1 nk copies pk locations o1 . . . location gets exactly one. hence answer pn n1 . . . nk. exercise 6.1.27. 1. class 17 girls 20 boys. committee 5 students formed represent class. a determine number ways forming committee consisting 5 students.
6.1. permutations combinations 95 b suppose committee also needs choose two diﬀerent people among them selves act spokesperson treasurer. case determine number ways forming committee consisting 5 students. note two com mittees diﬀerent i. either members diﬀerent ii. even members same diﬀerent students spokesperson andor treasurer. c due certain restrictions felt committee least 3 girls. case determine number ways forming committee consisting 5 students no one designated spokesperson andor treasurer. 2. combinatorially prove following identities a kcn k ncn 1 k 1. b newtons identity cn rcr k cn kcn k r k. c cn r cr rcn r 0 cr r 1cn r 1 cr 0cn r r. d cn 02 cn 12 cn n2 c2n n. 3. determine number ways selecting committee people group consisting n1 women n2 men n1 n2 m. 4. determine number ways arranging letters word a abracadabaraarcada. b kagarthalamnagarthalam. 5. many anagrams mississippi two adjacent 6. many rectangles n n square many squares there 7. show product n consecutive natural numbers always divisible n. 8. show mn divides mn. 9. n points placed circumference circle lines connecting joined largest number points intersection lines inside circle obtained 10. prove cpn pn n multiple p two ways. hint newtons identity. 11. many ways form word mathematician starting side moving horizontal vertical directions h h e h h e e h h e e h h e e h h e e h h e c e h h e c c e h h e c c e h h e c n c e h
96 chapter 6. counting 12. a many ways one arrange n diﬀerent books diﬀerent boxes kept row books inside boxes also kept row b box empty 13. prove induction 2nn 1 2n. 6.1.1 multinomial theorem deﬁnition 6.1.28. let x z commuting symbols. then algebraic expansion1 x zn mean expansion term form αxiyjzk two terms diﬀer degree least one x y z. word expansion2 x zn mean expansion term word length n using symbols x y z. expansions x1 xrn whenever xis commuting symbols may deﬁned similar way. example 6.1.29. 1. x3 3xy2 y3 3yx2 algebraic expansion x y3 xxx xxy xyx xyy yxx yxy yyx yyy word expansion x y3. 2. take word expansion x z9. term exactly two xs exactly three s nothing arrangement two xs three s four zs. so coeﬃcient x2y 3z4 algebraic expansion x z9 p9 2 3 4. 3. consider xyzn x z x z x z z n times . then expression need choose say a places n possible places x i 0 b j places remaining n i places j 0 c n i j left places z with n i j 0. thus get x zn x ij0ijn cn icn i jxiyjznij x ij0ijn pn i jxiyjznij. theorem 6.1.30. multinomial theorem fix positive integer n let x1 x2 . . . xn collection commuting symbols. then n n1 nk coeﬃcient xn1 1 xn2 2 xnk k algebraic expansion x1 xkn pn n1 nk. x1 xkn x n1 . . . nk 0 n1 nk n pn n1 nk xn1 1 xnk k . proof. proof left exercise reader. special case famous binomial theorem. corollary 6.1.31. binomial theorem x yn n p k0 cn kxnkyk. 1nonstandard notion 2nonstandard notion
6.2. circular permutations 97 example 6.1.32. form words size 5 using letters mathematician including multiplicity is may use twice. many there ans p k1k85 k12k23k32k41k51k62k71k81 c5 k1 k8. exercise 6.1.33. 1. show pn 2n following ways. a using binomial theorem. b using select subset task n compulsory parts. c associating subset 01 string length n evaluating values base2. d arguing line a subset n 1 either contains n 1 not using induction. 2. let set size n. then prove two diﬀerent ways number subsets odd size number subsets even size equivalently p k0 cn 2k p k0 cn 2k 1 2n1. 3. prove following identities binomial coeﬃcients. a n p kℓ ck ℓcn k cn ℓ2nℓ. b cm n ℓ ℓ p k0 cm k cn ℓk. c cn ℓ p k0 ct k cnt ℓk n p k0 ct k cnt ℓk t 0 t n. d cn r 1 r r p ℓ0 cn ℓ ℓ. e cn 1 r 1 n p ℓr cℓ r. 4. evaluate n p k0 2k 1 cn 2k 1 n p k0 5k 3 cn 2k 1 whenever n 3. 5. generalized pascal assume k1 km n. show cn k1 . . . km cn 1 k1 1 . . . km cn 1 k1 . . . km 1. 6. p k1.kmn cn k1 . . . km 7. put l m 2 . p k1.kmn 1k2k4k2lcn k1 . . . km 6.2 circular permutations deﬁnition 6.2.1. circular permutationarrangement circular permutation ar rangement n distinct objects circle. two circular arrangements element clockwise adjacent element. s n write a circular arrange ment s mean a circular arrangement elements s. x1 x2 . . . xn x1 shall denote circular arrangement keeping anticlockwise direction picture.
98 chapter 6. counting example 6.2.2. exactly two pictures figure 6.1 represent circular permutation. a1 a2 a3 a4 a5 a1 a2 a3 a4 a5 a1 a3 a4 a5 a1 a2 a5 a4 a3 a2 a1 figure 6.1 circular permutations example 6.2.3. determine number circular permutations x a1 a2 a3 a4 a5 ans 4. proof. let b circular permutations x permutations x. now deﬁne f b fa b obtained breaking cycle b gap following anticlockwise direction. example break leftmost circular permutation figure 6.1 gap b get a2 a3 a4 a5 a1. notice f 1b 5 b b. b c b f 1b f 1c why1. thus principle disjoint preimages equal size number circular permutations 55. theorem 6.2.4. circular permutations number circular permutations n n1. proof. proof may obtained line previous example. give alternate proof. put circular permutations 5. put b permutations 4. deﬁne f b f5 x1 x2 x3 x4 5 x1 x2 x3 x4. deﬁne g b a gx1 x2 x3 x4 5 x1 x2 x3 x4 5. then g fa a a f gb b b b. hence bijection principle see theorem 2.3.8 f bijection. example 6.2.5. find number circular arrangements a b b c c d d e e. ans one a. cutting circular arrangement get unique arrangement b b c c d d e e. so required answer 8 24. deﬁnition 6.2.6. rotation orbit size 1. given arrangement x1 . . . xn rotation r1x1 . . . xn short r1x1 . . . xn mean x2 . . . xn x1 r2x1 . . . xn mean x3 . . . xn x1 x2. sim ilar lines deﬁne ri n put r0x1 . . . xn x1 . . . xn. thus k n r0x1 . . . xn rknx1 . . . xn x1 . . . xn. 2. orbit size arrangement x1 . . . xn smallest positive integer satisﬁes rix1 . . . xn x1 . . . xn. case call n r0x1 . . . xn r1x1 . . . xn . . . ri1x1 . . . xn orbit x1 . . . xn. 1think creating circular permutation given permutation.
6.2. circular permutations 99 example 6.2.7. 1. r1abcabcabc bcabcabca r2abcabcabc cabcabcab r3abcabcabc abcabcabc. thus orbit size abcabcabc 3. 2. arrangement a a b b c c orbit size 6 aabcbc. arrange ment orbit size 3 acbacb. 3. arrangement a a b b c c orbit size 2. fact x1x2 x6 arrangement orbit size 2 then x1x2x3x4x5x6 x3x4x5x6x1x2. thus x1 x3 x5 possible. 4. arrangement a a b b c c orbit size 1 2 4 5. 5. 3 arrangements a a b b c c orbit size 3. 6. take arrangement a a b b c c orbit size 3. make circular arrangement joining ends. many distinct arrangements generate breaking circular arrangement gaps ans 3. elements orbit. 7. take arrangement a a b b c c orbit size 6. make circular arrangement joining ends. many distinct arrangements generate breaking circular arrangement gaps ans 6. elements orbit. 8. take arrangement n elements orbit size k. make circular arrangement joining ends. many distinct arrangements generate breaking circular arrangement gaps ans k. elements orbit. 9. take set arrangements ﬁnite multiset group orbits notice orbit gives us exactly one circular arrangement number orbits number circular arrangements. example 6.2.8. find number circular arrangements a a b b c c d d e e. ans two types arrangements s one orbit size 10 orbit size 5. number arrangements orbit size 5 5. so generate 4 distinct circular arrangements. number arrangements orbit size 10 10 22222 5. hence generate 10 2222210 5 10 distinct circular arrangements. thus total number circular arrangements 4 10 2222210 5 10. example 6.2.9. suppose given arrangement x1 . . . x10 ﬁve as ﬁve bs. orbit size 3 ans no. see assume its orbit size 3. then x1 . . . x10 r3x1 . . . x10 r6x1 . . . x10 r9x1 . . . x10 r2x1 . . . x10.
100 chapter 6. counting since 3 least positive integer r3x1 . . . x10 x1 . . . x10 arrive contradiction. hence orbit size cannot 3. proposition 6.2.10. orbit size arrangement nmultiset divisor n. proof. suppose orbit size x1 . . . xn k n kp r r 0 r k. then rkx1 . . . xn r2kx1 . . . xn rkpx1 . . . xn rkrx1 . . . xn. thus rkrx1 . . . xn x1 . . . xn contradicting minimality k. hence contradic tion therefore r 0. equivalently k divides n. proposition 6.2.11. let s1 pi1 pi2 . . . pik s2 pj1 pj2 . . . pjl two orbits certain arrangements nmultiset. then either s1 s2 or s1 s2. proof. s1 s2 nothing prove. so let exists arrangement pt s1 s2. then deﬁnition exist rotations r1 r2 r1pi1 pt r2pj1 pt. thus r1 2 pt pj1 hence r1 2 r1pi1 r1 2 pt pj1. therefore see arrangement pj1 s1 hence s2 s1. similar argument implies s1 s2 hence s1 s2. deﬁnition 6.2.12. binary operation let x1 . . . xn y1 . . . yn two arrangements nmultiset. then remainder section 1. shall consider expressions like x1 . . . xn y1 . . . yn. 2. ri rjx1 . . . xn mean expression rix1 . . . xn rjx1 . . . xn. 3. rix1 . . . xny1 . . . yn denote expression rix1 . . . xnriy1 . . . yn. example 6.2.13. think arrangements p1 . . . pn n 6 33 three as three bs. many copies abcabc r0 r5p1 pn ans course 6. see this note r0 takes abcabc itself r1 take cabcab abcabc r2 take bcabca abcabc on. example 6.2.14. let p x1 . . . x12 arrangement 12multiset orbit size 3. since orbit size p 3 set p r1p r2p forms orbit p. thus rotations r0 r3 r6 r9 ﬁx element s i.e. rirjp rjp 0 3 6 9 j 0 1 2. words r0 r11p accounts 4 counts circular arrangement 4 nothing number rotations ﬁxing p. thus see r0 r1 r11p r1p r2p r0 r1 r11p r0 r1 r11r1p r0 r1 r11r2p 4p r1p r2p 4p r1p r2p 4p r1p r2p 12p r1p r2p
6.2. circular permutations 101 proof next result similar idea example hence omitted. proposition 6.2.15. let p1 . . . pn arrangements mmultiset. then r0 rm1p1 pn mp1 pn. let p arrangement mmultiset orbit size k. then proposition 6.2.10 k divides m. now understanding obtained example note r0 rm1p accounts k counts circular arrangement k nothing the number rotations ﬁxing p. also proposition 6.2.11 know two orbits either disjoint hence next two results immediate. therefore readers supposed provide proof following results. discussion 6.2.16. let p1 . . . pn arrangements mmultiset. then x pi number rotations ﬁxing pi x pi r0 rm1pi mp1 pn mthe number circular arrangements. discussion 6.2.17. let p1 . . . pn arrangements mmultiset r0 r1 . . . rm1 set rotations. then x pi number rotations ﬁxing pi x pi rj rjpi pi pi rj rjpi pi x rj pi rjpi pi x rj number pis ﬁxed rj. hence using discussion 6.2.16 number circular arrangements 1 x rj rotation number pis ﬁxed rj. example 6.2.18. 1. many circular arrangements a a a b b b c c c there ans r0 ﬁxes 9 333 arrangements none r1 r2 r4 r5 r7 andr8 ﬁxes arrange ment r3 r6 ﬁxes 3 arrangements namely 3 arrangements x y z x aaa bbb z ccc. thus number circular arrangements 1 9 h 9 333 3 3 567812 9 564 3 188. 2. determine number circular arrangements size 5 using alphabets a b c. ans case r0 ﬁxes 35 arrangements. rotations r1 r2 r3 r4 ﬁxes arrangements aaaaa bbbbb ccccc. hence required number 1 5 35 4 3 51. verify answer 8 two alphabets b.
102 chapter 6. counting exercise 6.2.19. 1. n girls n boys number ways making sit around circular table way two girls adjacent two boys adjacent 2. persons p1 . . . p100 seating circle facing center talking. pi talks lie a person right talks truth. so minimum number persons talking truth . b second person right talks truth so minimum number persons talking truth . c next two persons right talk truth so minimum number persons talking truth . 3. let us assume two garlands one obtained rotation. then determine number distinct garlands formed using 6 ﬂowers ﬂowers a 2 colors say red blue. b 3 diﬀerent colors. c k diﬀerent colors k n. d red color 2 blue color 4. 6.3 solutions nonnegative integers deﬁnition 6.3.1. solution nonnegative integers recall n0 n 0. point p p1 . . . pk nk 0 p1 pk n called solution equation x1 xk n nonnegative integers solution x1 xk n n0. two solutions p1 . . . pk q1 . . . qk said pi qi 1 . . . k. thus 5 0 0 5 0 0 5 5 two diﬀerent solutions x z 10 n0. example 6.3.2. determine number 1. words uses 3 as 6 bs. 2. arrangements 3 as 6 bs. 3. distinct strings formed using 3 as 6 bs. 4. solutions equation x1 x2 x3 x4 6 xi n0 0 xi 6. 5. ways placing 6 indistinguishable balls 4 distinguishable boxes. 6. 3 subsets 9set. solution observe problems correspond forming strings using s or s bars 1s or balls dots place aa bs respectively
6.3. solutions nonnegative integers 103 abbbababb abbbbbaab bbabbbaba 111 1 11 0 3 1 2 11111 1 0 5 0 1 11 111 1 2 3 1 0 . . . . . . . . . . . . . . . . . . figure 6.2 understanding three problems note as indistinguishable among holds bs. thus need ﬁnd 3 places 9 3 6 places as. hence answer c9 3. answer remain need replace as s or s bs 1s or balls string 3 as 6 bs. see figure 6.2 note four numbers added using 3 s four adjacent boxes created putting 3 vertical lines s. general following result. theorem 6.3.3. solutions n0 number solutions x1 xr n n0 cn r 1 n. proof. solution x1 . . . xr may viewed arrangement n dots r 1 bars. put x1 many dots put bar put x2 many dots put another bar continue end putting xr many dots. example 0 2 1 0 0 associated . . . viceversa. thus cn r 1 r 1 arrangements n dots r 1 bars. theorem 6.3.4. a number solutions x1 xr n nonnegative integers cn r n. b number terms algebraic expansion x1 xrn cn r 1 n. proof. a solution x1 xr n uniquely corresponds solution x1 xry n nonnegative integers. b note term algebraic expansion x1 xrn form xi1 1 xi2 2 xir r i1i2 ir n. thus term uniquely corresponds solution i1i2 ir n nonnegative integers. theorem 6.3.5. rmultiset number rmultisets elements n cnr1 n1. proof. let rmultiset. let di number copies a. then solution d1 dn r nonnegative integers gives uniquely. hence conclusion. alternate. put arrangements n 1 dots r bars. put b rmultisets n. a deﬁne fa multiset fa di 1 di number dots left ith bar. example . gives us 1 1 3 4 4. easy deﬁne g b a fgb b b b gfa a a. thus bijection principle see theorem 2.3.8 a b. also know a cn r 1 n 1 hence required result follows.
104 chapter 6. counting example 6.3.6. 1. 5 kinds icecreams available market complex. many ways buy 15 party ans suppose buy xi icecreams ith type. then problem ﬁnding number solutions x1 x5 15 nonnegative integers. 2. many solutions n0 x z 60 x 3 4 z 5 ans x y z solution x3 y4 z5 solution xyz 48 n0. so answer c50 2. 3. many solutions n0 x z 60 20 x 3 30 y 4 40 z 5 ans looking solution n0 x z 48 x 17 26 z 35. let x y z n3 0 xy z 48 ax x y z n3 0 xy z 48 x 18 ay x y z n3 0 x z 48 27 az x y z n3 0 x z 48 z 36. know a c50 2. answer c50 2 ax ay az. soon learn ﬁnd value ax ay az. exercise 6.3.7. 1. determine number solutions x z 7 x y z n 2. find number allocations n identical objects r distinct locations location gets least pi 0 elements 1 2 r. 3. many ways pick integers x1 x2 x3 x4 x5 20 xi xi1 3 2 3 4 5 solve three diﬀerent ways. 4. find number solutions nonnegative integers b c e 11. 5. room 2 distinct book racks 5 shelves each. shelf capable holding 10 books. many ways place 10 distinct books two racks 6. many 4letter words with repetition letters alphabetical order 7. determine number nondecreasing sequences length r using numbers 1 2 . . . n. 8. many ways indistinguishable balls put n distinguishable boxes restriction box empty. 9. many 26letter permutations english alphabets 2 vowels together 10. many 26letter permutations english alphabets least two consonants two vowels 11. many ways select 10 integers set 1 2 . . . 100 positive diﬀerence two 10 integers least 3. 12. many 10element subsets english alphabets pair consecutive letters 13. many 10element subsets english alphabets pair consecutive let ters
6.4. set partitions 105 14. many ways distribute 50 balls 5 persons ram shyam together get 30 mohan gets least 10 15. many arrangements letters kagarthalamnagarthalam 2 vowels adjacent 16. many arrangements letters recurrencerelation 2 vowels adjacent 17. many ways arrange letters abracadabaraarcada ﬁrst a precedes ﬁrst b b b precedes ﬁrst ﬁrst precedes ﬁrst c c b precedes ﬁrst ﬁrst precedes ﬁrst c 18. many ways arrange letters kagarthalamnagarthatam ﬁrst a precedes ﬁrst t b precedes ﬁrst g ﬁrst h precedes ﬁrst a c precedes ﬁrst g ﬁrst precedes ﬁrst g 19. many ways pick 20 letters 10 as 15 bs 15 cs 20. determine number ways sit 10 men 7 women 2 women sit next other 21. many ways 8 persons including ram shyam sit row ram shyam sitting next other 22. evaluate n p i11 i1 p i21 i2 p i31 ik1 p ik1 1. 6.4 set partitions deﬁnition 6.4.1. set partition partition set collection pairwise disjoint nonempty subsets whose union s. example 6.4.2. a 1 2 3 4 5 6 1 3 2 4 5 6 1 2 3 4 5 6 partitions 6 3 subsets. b 2n1 1 partitions n n 2 two subsets. see this observe nontrivial subset pn set a ac partition n two subsets. since total number nontrivial subsets pn equals 2n 2 required result follows. c number allocations 7 students 7 diﬀerent project groups group one student 7 c7 1 1 1 1 1 1 1 number partitions set 7 students 7 subsets 1.
106 chapter 6. counting d many ways write n 1 2 3 4 5 6 7 8 9 10 11 12 piece paper condition sets written row increasing size ans let us write ﬁrst. n 1 2 3 4 5 6 7 8 9 10 11 12 correct n 2 1 3 4 5 6 7 8 9 10 11 12 correct n 5 6 3 4 1 2 10 11 12 9 7 8 correct n 2 3 1 4 5 6 7 8 9 10 11 12 incorrect partition n 2 1 3 4 7 8 9 5 6 10 11 12 incorrect satisfying condition 323 232 ways. notice written partition remove brackets get arrangement elements 12. e many arrangements generate partition pi subsets size ni n1 nk ans p1n1p1 pknkpk k i1 pinipi. theorem 6.4.3. set partition number partitions n pi subsets size ni n1 nk n n1p1p1 nkpkpk. proof. note partition generates k q i1 pinipi arrangement elements n. conversely arrangement elements n easily construct partition type generate arrangement. thus proof complete. deﬁnition 6.4.4. stirling numbers second kind denoted sn r number partitions n rsubsets rparts. convention sn r 1 n r 0 whenever either n 0 r 0 n r. theorem 6.4.5. recurrence sn r sn 1 r sn r 1 rsn r. proof. write rpartition n 1 erase n 1 it. is n 1 element rpartition number partitions become sn r 1 else n 1 appears one element rpartition n gives number rsn r. example 6.4.6. determine number ways putting n distinguishabledistinct balls r indistinguishable boxes restriction box empty. ans let set n distinct balls let balls ith box bi 1 i r. 1. since box nonempty bi nonempty. 2. also ball box hence r i1 bi a.
6.4. set partitions 107 3. boxes indistinguishable arrange boxes nonincreasing order i.e. b1 br. thus b1 b2 . . . br partition rparts. hence required number ways given sn r stirling number second kind. proceed further consider following example. example 6.4.7. let a b c d e 1 2 3. deﬁne onto function f s fa fb fc 1 fd 2 fe 3. then f gives partition b1 a b c b2 d b3 e 3parts. also let a1 a d a2 b e a3 c partition 3parts. then partition gives 3 onto functions s onetoone function a1 a2 a3 s namely f1a f1d 1 f1b f1e 2 f1c 3 f1a1 1 f1a2 2 f1a3 3 f2a f2d 1 f2b f2e 3 f2c 2 f2a1 1 f2a2 3 f2a3 2 f3a f3d 2 f3b f3e 1 f3c 3 f3a1 2 f3a2 1 f3a3 3 f4a f4d 2 f4b f4e 3 f4c 1 f4a1 2 f4a2 3 f4a3 1 f5a f5d 3 f5b f5e 1 f5c 2 f5a1 3 f5a2 1 f5a3 2 f6a f6d 3 f6b f6e 2 f6c 1 f6a1 3 f6a2 2 f6a3 1. lemma 6.4.8. total number onto functions f r n nsr n. proof. f onto means for n exists x r fx y. therefore number onto functions 0 whenever r n. so assume r n. then 1. n f 1i x r fx i nonempty set f onto. 2. f 1i f 1j whenever 1 i j n f function. 3. n i1 f 1i r domain f r. therefore f 1is give partition r nparts. also note function f gives onetoone function f 11 . . . f 1r n. conversely partition a1 a2 . . . r nparts get n onetoone function a1 a2 . . . an n. hence f r n f onto g a1 a2 . . . an n g onetoone partition r nparts n sr n. thus required result follows. lemma 6.4.9. let r n n ℓ minr n. then nr ℓ x k1 cn kksr k. 6.1
108 chapter 6. counting proof. let f f r n. compute a two diﬀerent methods. method 1 theorem 6.1.4 a nr. method 2 let f0 r n function. then f0 onto function r im f0 f0r. moreover k 1 k ℓ minr n. thus ℓ k1 ak ak f r n fr k ak aj whenever 1 j k ℓ. now using theorem 6.1.20 subset n size k selected cn k ways. thus 1 k ℓ ak k k n k k f r k f onto cn kksr k. therefore a ℓ k1 ai ℓ x k1 ak ℓ x k1 cn kksr k. hence using two counting methods required result follows. remark 6.4.10. 1. following two problems equivalent. a count number onto functions f r n. b count number ways put r distinguishabledistinct balls n distinguish abledistinct boxes box empty. 2. numbers sr k recursively calculated using equation 6.1. example show sm 1 1 1. ans take n 1 r 1 equation 6.1 get n n1 p1 k1 cn kks1 k cn 11s1 1 ns1 1. thus s1 1 1. take n 1 r 2 equation 6.1 get 1 1r p1 k1 c1 kksr k sr 1. 3. exercise verify s5 2 15 s5 3 25 s5 4 10 s5 5 1. exercise 6.4.11. 1. determine number ways a selecting r distinguishable objects n distinguishable objects n r. b distributing 20 distinct toys among 4 children children gets 5 toys c placing r distinguishable balls n indistinguishable boxes box empty d placing r distinguishable balls n indistinguishable boxes 2. n n let bn denote number partitions n. then bn n p r0 sn r called nth bell number. deﬁnition b0 1 b1. determine bn 2 n 5. 3. fix n n. then composition n expression n sum positive integers. example n 4 distinct compositions 4 3 1 1 3 2 2 2 1 1 1 1 2 1 2 1 1 1 1 1. let skn denote number compositions n k parts. then s14 1 s24 3 s34 3 s44 1. determine skn 1 k n p k1 skn.
6.4. set partitions 109 4. let f f r n. compute s two ways prove n 1r r p k0 cr knk. 5. suppose 13 people get lift level . people get level say 1 2 3 4 5 then calculate number ways getting least one person gets level. deﬁnition 6.4.12. partition number let n k n. partition n k parts tuple x1 xk nk written nonincreasing order x1 xk n. may viewed kmultiset n sum n. πnk denote number partitions n exactly k parts πn number partitions n. conventionally π0 1 πnk 0 whenever k n. remark 6.4.13. π74 3 partitions 7 4parts 4 1 1 1 3 2 1 1 2 2 2 1. verify π72 3 π73 4. example 6.4.14. determine number ways placing r indistinguishable balls n indistinguishable boxes 1. restriction box empty. ans balls indistinguishable need count number balls box. boxes indistinguishable arrange number balls inside boxes nonincreasing order. also box nonempty hence answer πrn. 2. restriction. ans let us place one ball box. placing r indistinguishable ball n indistinguishable boxes restriction placing r n indistinguishable balls n indistinguishable boxes box empty. therefore required answer πmnn. exercise 6.4.15. 1. calculate πn n 1 2 3 . . . 8. 2. prove π2rr πr r n. 3. ﬁxed n n determine recurrence relation numbers πnrs 1 r n. deﬁnition 6.4.16. stirling number ﬁrst kind stirling number ﬁrst kind denoted sn k coeﬃcient xk xn xn called falling factorial equals xx1x2 xn1. rising factorial xn deﬁned xx1x2 xn1. exercise 6.4.17. prove induction 1. sn m1nm coeﬃcient xm xn sn m sn m1nm. 2. let an k denote number permutations n k disjoint cycles. example a4 2 11 corresponds permutations 1234 1324 1423 1234 1243 1342 1432 1243 1423 1234 1324. con vention a0 0 1 an 0 0 a0 n whenever n 1. determine prove numbers an ks satisfy an k n 1an 1 k an 1 k 1.
110 chapter 6. counting 3. prove an m sn m n n0. 6.5 lattice paths catalan numbers consider lattice integer lines r2 let m n m n 0 1 . . . said points lattice. pair points say m1 n1 b m2 n2 m1 m2 n1 n2 deﬁne lattice path b subset e1 . . . ek ei x y ei1 either x 1 y x 1 1 i k 1. is step move either one unit right denoted r one unit up denoted u see figure 6.3. 0 0 2 3 8 7 r right u figure 6.3 lattice lattice path 2 3 8 7 example 6.5.1. 1. determine number lattice paths 0 0 m n. ans step unit increase either r u need take n many r steps many u steps reach m n 0 0. so arrangement n many rs many us give path uniquely. hence answer cm n m. 2. use method lattice paths prove p ℓ0 cn ℓ ℓ cn 1 m. ans observe cnm1 m number lattice paths 0 0 m n1 left hand side number lattice paths 0 0 ℓ n 0 ℓm. fix ℓ 0 ℓm let p lattice path 0 0 ℓ n. then path p q q urr r r appearing ℓtimes gives lattice path 0 0 m n 1 namely 0 0 p ℓ n u ℓ n 1 q m n 1. lattice paths 0 ℓm distinct hence result follows. exercise 6.5.2. 1. give bijection the solution set x0 x1 x2 xk n nonnegative integers the number lattice paths 0 0 n k. 2. use lattice paths construct proof n p k0 cn k 2n.
6.5. lattice paths catalan numbers 111 3. use lattice paths construct proof n p k0 cn k2 c2n n. hint cn k number lattice paths 0 0 n k k well n k k n n. discussion 6.5.3. observed earlier number lattice paths 0 0 n n c2n n. suppose wish take paths step number us exceeds number rs. then number paths ans call arrangement n many us n many rs bad path number us exceeds number rs least once. example path rruuurru bad path. arrangement correspond another arrangement n 1 many us n 1 many rs following way spot ﬁrst place number us exceeds rs bad path. then next letter onwards change r u u r. example bad path rruuurru corresponds path rruuuuur. notice oneone correspondence. thus number bad paths c2n n 1. so answer question c2n n c2n n 1 c2n n n 1 . deﬁnition 6.5.4. catalan number nth catalan number denoted cn number diﬀerent representations product a1 an1 n 1 square matrices size using n pairs brackets. convention c0 1. theorem 6.5.5. catalan number prove cn c2nn n1 n n. proof. claim n kth least k 2 many as. see pick substring starting right n kth till face k 1 many s. substring represents product matrices. so must contain k 2 many ais. given one representation product replace ai a. drop right brackets sequence n many s n 1 many as. thus number as used till n kth n 1 k 2 n k 1. so number as never exceeds number . conversely given arrangement put back s ﬁnd two consecutive letters last put right bracket them treat aa letter repeat process. example aaaaa aaaaa aaaaa aaaaa aaaaa previous example number arrangements c2nn n1 . readers interested knowing catalan numbers look book enumerative combinatorics stanley 11. exercise 6.5.6. 1. give recurrence relation cns i.e. formula cn involving c0 . . . cn1. hence show cn c2n nn 1. 2. give arithmetic proof fact n 1 divides c2n n. 3. man standing edge swimming pool facing it holding bag containing n blue n red balls. randomly picks one ball time discards it. ball blue takes step back ball red takes step forward. probability falling swimming pool
112 chapter 6. counting 4. consider regular polygon vertices 1 2 n. many ways divide polygon triangles using n 3 noncrossing diagonals 5. many arrangements n blue n red balls position arrangement number blue balls till position one number red balls till position 6. want write matrix size 10 2 using numbers 1 . . . 20 number ap pearing exactly once. then determine number matrices numbers a increase left right b increase down c increase left right down 6.6 generalizations 1. let n k n 0 k n. then theorem 6.1.20 saw cn k n kn k. hence think cn k n n 1 n k 1 k . understanding generalize cn k n r k n0 follows cn k 0 k 0 0 n 0 n k 1 n k n n 1 n k 1 k otherwise. 6.2 notations above give generalized binomial theorem without proof. theorem 6.6.1. generalized binomial theorem let n real number. then 1 xn 1 cn 1x cn 2x2 cn rxr . particular 1 x1 1 x x2 x3 a b r a b a bn bn 1 b n bn x r0 cn r a b r x r0 cn rarbnr. let us understand theorem 6.6.1 following examples. a let n 1 2. case k 1 equation 6.2 gives c1 2 k 1 2 1 2 1 1 2 k 1 k 1 1 3 2k 2kk 1k12k 2 22k1k 1k . thus 1 x12 x k0 c1 2 kxk 1 1 2x 1 23 x2 1 24 x3 x k4 1k12k 2 22k1k 1k xk.
6.6. generalizations 113 expression also obtained using taylor series expansion fx 1 x12 around x 0. recall taylor series expansion fx around x 0 equals fx f0 f 0x f0 2 x2 p k3 fk0 k xk f0 1 f 0 1 2 f 0 1 22 general f k0 1 2 1 2 1 1 2 k 1 k 3. b let n r r n. then k 1 equation 6.2 gives cr k r r 1 r k 1 k 1kcr k 1 k. thus 1 xn 1 1 xr 1 rx cr 1 2x2 x k3 cr k 1 kxk. 2. let n n. recall identity nm p k0 cn kksm k n p k0 cn kksm k equation 6.1. note n identity equals x ay x 0m 1m 2m 3m . . . nm c0 0 0 0 0 c1 0 c1 1 0 0 c2 0 c2 1 c2 2 0 . . . . . . . . . . . . . cn 0 cn 1 cn 2 cn n 0sm 0 1sm 1 2sm 2 . . . nsm n . lower triangular deta 1 inverse entry a1 similar form. so a1x a1 c0 0 0 0 0 0 c1 0 c1 1 0 0 0 c2 0 c2 1 c2 2 0 0 c3 0 c3 1 c3 2 c3 3 0 . . . . . . . . . . . . . . . . 1ncn 0 1n1cn 1 1n2cn 2 1n3cn 3 cn n . hence n n sm n 1 n x k0 1kcn kn km. 6.3 3. matrix inversion implies n n0 identity an x k0 cn kbk holds bn x k0 1kcn kak holds. end chapter another set exercises. exercise 6.6.2. 1. prove exists bijection two following sets. a set words length n alphabet consisting letters.
114 chapter 6. counting b set maps nset mset. c set distributions n distinct objects distinct boxes. d set ntuples letters. 2. prove exists bijection two following sets. a set n letter words distinct letters alphabet consisting letters. b set oneone functions nset mset. c set distributions n distinct objects distinct boxes subject if object put box object put box. d set ntuples letters without repetition. e set permutations symbols taken n time. 3. prove exists bijection two following sets. a set increasing words length n ordered letters. b set distributions n nondistinct objects distinct boxes. c set combinations symbols taken n time repetitions permitted.
chapter 7 advanced counting principles 7.1 pigeonhole principle discussion 7.1.1. pigeonhole principle php php1 n 1 pigeons stay n holes hole least two pigeons. php2 kn 1 pigeons stay n holes hole least k 1 pigeons. php3 p1 pn 1 pigeons stay n holes hole least pi 1 pigeons. example 7.1.2. 1. consider tournament n 1 players pair plays exactly player wins least once. then two players number wins. ans number wins vary 1 n 1 n players. 2. bag contains 5 red 8 blue 12 green 7 yellow marbles. least number marbles chosen ensure a least 4 marbles color 13 b least 7 marbles color 24 c least 4 red least 7 color 22. 3. group 6 people prove three mutual friends three mutual strangers. ans let person group. let f set friends set strangers a. clearly s f 5. php either f 3 s 3. case 1 f 3. two f friends two along three mutual friends. else f set mutual strangers size least 3. case 2 s 3. pair strangers two along three mutual strangers. else becomes set mutual friends size least 3. 4. 7 points chosen inside unit circle pair points distance 1. 115
116 chapter 7. advanced counting principles ans see divide circle 6 equal cone type parts creating angle 60o center. php part containing least two points. distance two 1. 5. n 1 integers selected 2n pair property one divides other. ans number form 2ko odd number. n odd numbers. select n 1 numbers s php two say x y odd part is x 2io 2jo. j xy otherwise yx. 6. a let r1 r2 rmn1 sequence mn1 distinct real numbers. then prove subsequence m1 numbers increasing subsequence n 1 numbers decreasing. ans deﬁne li maximum length increasing subsequence starting ri. li m 1 nothing prove. so let 1 li m. since li sequence mn 1 integers php one number repeats least n 1 times. let li1 li2 lin1 s i1 i2 in1. notice ri1 ri2 ri1 ri2 ri1 together increasing sequence length starting ri2 gives increasing sequence length 1. similarly ri2 ri3 rin1 hence required result holds. alternate. let r1 r2 rmn1 deﬁne map f z z fai s t 1 i mn 1 equals length largest increasing subsequence starting ai equals length largest decreasing sub sequence ending ai. now either m 1 n 1 done. not note 1 s m 1 t n. so number tuples s t mn. thus mn 1 distinct numbers mapped mn tuples hence php two numbers ai aj fai faj. now proceed previous case get required result. b statement hold every collection mn distinct numbers no. consider sequence n n1 1 2n 2n1 . . . n1 3n 3n1 2n1 mn mn1 mnn1. 7. given 1010 integers prove pair either diﬀer by sum to multiple 2017. true replace 1010 1009 ans let numbers n1 n2 . . . n1010 n1 nk n1 nk k 2 . . . 1010. then s 2018 hence least two remainder di vided 2017. then consider diﬀerence. later part consider 0 1 2 . . . 1008. 8. let qc. then inﬁnitely many rational numbers p q a p q 1 q2. ans enough show inﬁnitely many p q z2 qa p 1 q. note every n 0 ia ia 1 1 . . . 1. hence php exist
7.1. pigeonhole principle 117 i j j j ia jaia 1 1 j i. then tuple p1 q1 jaia j i satisﬁes required property. generate another tuple ﬁnd m2 1 m2 a p1 q1 proceed get p2 q2 q2ap2 1 m2 1 q2. since ap2 q2 1 m2 a p1 q1 p1 q1 p2 q2 . use induction get required result. 9. prove exist two powers 3 whose diﬀerence divisible 2017. ans let 1 30 3 32 33 . . . 32017. then s 2018. remainders integer divided 2017 0 1 2 . . . 2016 php pair remainder. hence 2017 divides 3j 3i i j. 10. prove exists power three ends 0001. ans let 1 30 3 32 33 . . . now divide element 104. s 104 php exist j remainders 3i 3j divided 104 equal. gcd104 3 1 thus 104 divides 3ℓ1. is 3ℓ1 104 positive integer s. is 3ℓ 104 1 hence result follows. exercise 7.1.3. 1. consider poset x p4 . write 6 maximal chains p1 . . . p6 need disjoint pi x. let a1 . . . a7 7 distinct subsets 4. use php prove exist i j ai aj pk k. is a1 . . . a7 cannot antichain. conclude holds width poset 6. 2. let x1 . . . x9 n 9 p i1 xi 30. then exist i j k 9 xi xj xk 12. 3. pick 6 integers 10 exists pair odd sum. 4. 14subset 46 four elements a b c b c d. 5. row 12 chairs 9 ﬁlled. then 3 consecutive chairs ﬁlled. 8 work 6. every nsequence integers consecutive subsequence sum divisible n. 7. let n 3 n size n2 2 1. then exist a b c s b c. 8. let a b n b. given half integers set a b pair diﬀer either b. 9. consider chess board two diagonally opposite corners removed. possible cover board pieces rectangular dominos whose size exactly two board squares 10. mark centers squares 8 8 chess board. possible cut board 13 straight lines passing center every piece 1 center 11. fifteen squirrels 100 nuts. then two squirrels equal number nuts.
118 chapter 7. advanced counting principles 12. suppose fx polynomial integer coeﬃcients. a fx 2 three distinct integers integer x fx equal 3. b fx 14 three distinct integers integer x fx equal 15. c fx 11 ﬁve distinct integers integer x fx equal 9. 13. choose 5 points random inside equilateral triangle side 1 unit exists pair distance 0.5 units. 14. prove among 55 integers 1 x1 x2 x3 x55 100 pair diﬀerence 9 pair diﬀerence 10 pair diﬀerence 12 pair diﬀerence 13. surprisingly need pair diﬀerence 11. 15. let x1 x2 . . . xn z. prove exist 1 i j n a xi xi1 xj1 xj multiple 2017 whenever n 2017. b xj xi xj xi multiple 2017 whenever n 1009. 16. let b two discs 2n equal sectors. disc a n sectors colored red n colored blue. sectors disc b colored arbitrarily red blue colors. show way putting two discs one other least n corresponding sectors colors. 17. 7 distinct real numbers. possible select two them say x 0 xy 1xy 1 3 18. n odd permutation p n product n q i1 i pi even. 19. fix positive α qc. then m nα m n z dense r. 20. take 25 points plane satisfying among three pair distance less 1. then circle unit radius contains least 13 given points. 21. five points chosen nodes square lattice view z z. certain midpoint two lattice point 22. given 9 lines cuts given square two quadrilaterals whose areas ratio 2 3. prove least three lines pass point. 23. half subsets n selected two selected subsets property one subset other. 24. given ten 4subsets 11 two least 2 elements common. 25. person takes least one aspirin day 30 days. takes 45 aspirin altogether sequence consecutive days takes exactly 14 aspirins. 26. 58 entries 14 14 matrix 1 2 2 submatrix whose entries 1. exercise 7.1.4. 1. point circle colored either red blue show exists isosceles triangle vertices color. 2. point plane colored red blue prove following.
7.2. principle inclusion exclusion 119 a exist two points color distance 1 unit. b equilateral triangle whose vertices color. c rectangle whose vertices color. 3. let 100 10set. then two disjoint subsets equal sum. 4. n n prove exists ℓn n divides 2ℓ1. 5. exist multiple 2017 formed using digits a 2 justify answer. b 2 3 number 2s 3s equal justify answer. 6. natural number multiple form 9 90 0 least one 9. 7.2 principle inclusion exclusion start section following example. example 7.2.1. many natural numbers n 1000 divisible 2 3 ans let a2 n n n 1000 2n a3 n n n 1000 3n. then a2 a3 a2 a3 a2 a3 500 333 166 667. so required answer 1000 667 333. generalize idea whenever 3 sets. theorem 7.2.2. principle inclusion exclusion let a1 ﬁnite subsets set u. then n i1 ai n x k1 1k1 x 1i1ikn ai1 aik . 7.1 equivalently number elements u none a1 a2 . . . equals u n i1 ai u n x k1 1k x 1i1ikn ai1 aik . proof. let x n i1 ai. then show inclusion x ai contributes increases value 1 sides equation 7.1. so assume x included sets a1 ar. then contribution x ai1 aik 1 i1 . . . ik r. hence contribution x p 1i1ikn ai1 aik cr k. thus contribution x right hand side equation 7.1 r cr 2 cr 3 1r1cr r 1. element x clearly contributes 1 left hand side equation 7.1 hence required result follows. proof equivalent condition left readers.
120 chapter 7. advanced counting principles example 7.2.3. many integers 1 10000 divisible none 2 3 5 7 ans 2 3 5 7 let ai n n n 10000 in. therefore required answer 10000 a2 a3 a5 a7 2285. deﬁnition 7.2.4. euler totient function ﬁxed n n eulers totient function deﬁned ϕn k n k n gcdk n 1. theorem 7.2.5. let n k q i1 pαi factorization n distinct primes p1 . . . pk. then ϕn n 1 1 p1 1 1 p2 1 1 pk . proof. 1 i k let ai m n n pim. then ϕn n ai n h 1 k x i1 1 pi x 1ijk 1 pipj 1k 1 p1p2 pk n 1 1 p1 1 1 p2 1 1 pk ai n pi ai aj n pipj on. thus required result follows. deﬁnition 7.2.6. derangementa derangement objects ﬁnite set permuta tionarrangement σ x σx x. example 2 1 4 3 derangement 1 2 3 4. number derangements 1 2 . . . n denoted dn. convention d0 1. also use b mean b approximate value a. theorem 7.2.7. n n dn n n x k0 1k k . thus dn n 1 e. proof. i 1 i n let ai set arrangements σ σi i. then verify ai n 1 ai aj n 2 on. thus ai n.n 1 cn 2n 2 1n1cn n0 n n x k1 1k1 k . so dn n ai n n p k0 1k k . furthermore lim n dn n 1 e. example 7.2.8. n n many squarefree integers exceed n ans let p p1 ps set primes exceeding n 1 i s let ai set integers 1 n multiples p2 . easy see ai n p2 ai aj n p2 p2 j on. so number squarefree integers greater n n i1 ai n x i1 n p2 x 1ijs n p2 p2 j x 1ijks n p2 p2 jp2 k n 100 p 2 3 5 7. so number squarefree integers exceeding 100 100 100 4 100 9 100 25 100 49 100 36 100 100 61.
7.2. principle inclusion exclusion 121 exercise 7.2.9. 1. let m n n gcdm n 1. then ϕmn ϕmϕn. 2. let n n. then use inclusionexclusion prove sn r 1 r r p i0 1icr ir in. 3. school 12 students take art course a 20 take biology course b 20 take chemistry course c 8 take dance course d. 5 students take b 7 students take c 4 students take d 16 students take b c 4 students take b 3 students take take c d. 3 take a b c 2 take a b d 3 take a c d 2 take b c d. finally 2 four courses 71 students taken courses. find total number students. 4. find number nonnegative integer solutions b c 27 1 a 5 2 b 7 3 c 9 4 d 11. 5. determine integers n satisfying ϕn 13. 6. determine integers n satisfying ϕn 12. 7. ﬁxed n n use mathematical induction prove p dn ϕd n. 8. function f n n said multiplicative fnm fnfm whenever gcdn m 1. a let f g n n functions satisfying fn p dn gd f1 g1 1. f multiplicative use induction show g also multiplicative. b imagine fractions 1 n 2 n . . . n n. cancel common factors regroup show n p dn ϕd. c conclude ϕ multiplicative. 9. show n 1 dn n e 1 2. 10. prove combinatorially n p i0 cn idni n. 11. show p k0 1kcm km kn n n 0 n. 12. determine number 10letter words using english alphabets contain vowels. 13. determine number ways put a 30 indistinguishable balls 4 distinguishable boxes 10 balls box. b 30 distinguishable balls 10 distinguishable boxes least 1 box empty. c r distinguishable balls n distinguishable boxes least 1 box empty. d r distinguishable balls n distinguishable boxes box empty.
122 chapter 7. advanced counting principles 14. determine number ways arrange 10 digits 0 1 . . . 9 digit never followed immediately 1. 15. determine number strings length 15 consisting 10 digits 0 1 . . . 9 string contains 10 digits. 16. determine number ways permuting 26 letters english alphabets none patterns lazy run show pet occurs. 17. let x positive integer less equal 9999999. a find number xs sum digits x equals 30. b many solutions obtained ﬁrst part consist 7 digits 7.3 generating functions one strongest tools combinatorics. start deﬁnition formal power series q develop theory generating functions. used get closed form expressions known recurrence relations used get binomial identities. deﬁnition 7.3.1. 1. formal power series algebraic expression form fx p n0 anxn q n 0 called formal power series indeterminate x q. px denote set formal power series x cfxn f coeﬃcient xn f e.g. cf xn p n0 anxn an. 2. equality two formal power series two elements f g px said equal cfxn f cfxn g n 0. 3. sum product px let fx p n0 anxn gx p n0 bnxn px. then a sumaddition deﬁned cfxn f g cfxn f cfxn g. b product called cauchy product deﬁned cfxn f g cn n p k0 akbnk. proceeding further consider following examples. example 7.3.2. 1. many words size 8 formed 6 copies 6 copies b ans 6 p k2 c8 k need choose k places a 2 k 6. alternate. word need many as n many bs n 8 6 n 6. also number words many as n many bs 8 mn. identify number 8xmyn mn note term degree 8 8 h 1 x x2 2 x3 3 x4 4 x5 5 x6 6 ih 1 y2 2 y3 3 y4 4 y5 5 y6 6 .
7.3. generating functions 123 replace x answer 8cf h x8 1 x x2 2 x3 3 x4 4 x5 5 x6 6 1 x x2 2 x3 3 x4 4 x5 5 x6 6 8cf h x8 x2 2 x3 3 x4 4 x5 5 x6 6 x2 2 x3 3 x4 4 x5 5 x6 6 8cf h x8 x2 2 x3 3 x2 2 x3 3 8cf x8 ex 1 x2 e2x 1 x2 2xex 2ex 2x 8 28 8 2 7 2 8 238. 2. many anagrams word mississippi ans using basic counting answer 11 442. another understanding readers also note 11 442 11 cf x11 1 x 1 x x2 2 x3 3 x4 4 21 x x2 2 11 cf x11 x x2 2 x4 4 x5 5 2x2 2 x3 3 need x x4 4 x4 4 x2 2 alphabets m i p respectively. 3. prove number nonnegative integer solutions u v w 10 equals cf x10 1 x x2 4 . ans note u take value 0 10 corresponds 1 x x10. hence using theorem 6.6.1 required answer cf x10 f 1 x x2 4 1 x4 c13 10 4 5 13 10 . deﬁnition 7.3.3. generating functions let br 0 sequence integers. then 1. ordinary generating function ogf formal power series b0 b1x b2x2 b3x3 2. exponential generating function egf formal power series b0 b1x b2 x2 2 b3 x3 3 . exists n br 0 r m generating functions ﬁnitely many terms. example 7.3.4. number nonnegative integer solutions 2a 3b 5c r r n0 ans note n0 hence 2a corresponds formal power series 1 x2 x4 . thus need consider ogf 1 x2 x4 1 x3 x6 1 x5 x10 1 1 x21 x31 x5. hence required answer cf xr 1 1 x21 x31 x5 .
124 chapter 7. advanced counting principles remark 7.3.5. 1. let fx p n0 xn n gx p n0 bn xn n px. then case egf product equals p n0 dn xn n dn n p k0 n k akbnk n 0. 2. note eex1 px ey p n0 yn n implies eex1 p n0 ex 1n n cf xm eex1 cf xm x n0 ex 1n n x n0 cf xm ex 1n n . 7.2 is 0 cf xm eex1 sum ﬁnite number rational numbers. whereas expression eex px requires inﬁnitely many computation cf xm eex 0. algebraic operations deﬁned deﬁnition 1.3 checked px forms commutative ring identity identity element given formal power series fx 1. ring element fx p n0 anxn said reciprocal exists another element gx p n0 bnxn px fxgx 1. so question arises conditions cfxn f ﬁnd gx px fxgx 1. answer question given following proposition. proposition 7.3.6. reciprocal f px exists cf x0 f 0. proof. let gx p n0 bnxn px reciprocal fx p n0 anxn. then fxgx 1 cf x0 f g 1 cfxn f g 0 n 1. but deﬁnition cauchy product cf x0 f g a0b0. hence a0 cf x0 f 0 cf x0 f g 0 thus f cannot reciprocal. however a0 0 coeﬃcients cfxn g bns recursively obtained follows b0 1 a0 1 c0 a0b0 b1 1 a0 a1b0 0 c1 a0b1 a1b0 b2 1 a0 a2b0 a1b1 0 c2 a0b2 a1b1 a2b0 general computed bk k r using 0 cr1 ar1b0 arb1 a1br a0br1 br1 1 a0 ar1b0 arb1 a1br. hence required result follows. note that proposition 7.3.6 bn q a0 q. look composition formal power series. recall that fx p n0 anxn gx p n0 bnxn px composition f gx fgx x n0 angxn x n0 an x m0 bmxmn may deﬁned just compute constant term composition one may look inﬁnite sum rational numbers. example let fx ex gx x1. note g0 1 0. here f gx fgx fx 1 ex1. so function f g well deﬁned formal procedure write ex1 p k0 akxk px i.e. ak q hence ex1 formal power series q. next result gives condition composition f gx well deﬁned.
7.3. generating functions 125 proposition 7.3.7. let f g px. then composition f gx px either f polynomial cf x0 gx 0. moreover cf x0 fx 0 exists g px cf x0 gx 0 f gx x. furthermore g fx px g fx x. proof. f gx px let f gx fgx p n0 cnxn suppose either f polynomial cf x0 gx 0. then compute ck cf xk f gx k 0 one needs consider terms k p n0 akgxn whenever fx p n0 anxn. hence ck q thus f gx px. completes proof ﬁrst part. leave proof part reader. proposition 7.3.8. basic tricks recall following statements binomial theorem theorem 6.6.1. 1. cf xn 1 xr 1 x x2 r cn r 1 n. 2. 1 xmn 1 cn 1xm cn 2x2m 1nxnm. 3. 1 x x2 xm1n 1 xm 1 x n 1 xmn1 x x2 n. deﬁne formal diﬀerentiation px give important results. proof left reader. deﬁnition 7.3.9. diﬀerentiation let fx p n0 anxn px. then formal diﬀerenti ation fx denoted f x deﬁned f x a1 2a2x nanxn1 x n1 nanxn1. proposition 7.3.10. ogf tricks let gx hx ogfs sequences ar 0 br 0 respectively. then following true. 1. agx bhx ogf aar bbr 0 . 2. 1 xgx ogf sequence a0 a1 a0 a2 a1 . 3. 1xx2 gx 1x1gx ogf mr 0 mr ar ar1 a0. 4. gxhx ogf cr 0 cr a0br a1br1 a2br2 arb0. 5. xf x ogf rar 1 . proof. example prove 3 note gx a0 a1x a2x2 coeﬃcient x2 1 x x2 a0 a1x a2x2 a2 a1 a0. example 7.3.11. 1. let ar 1 r 0. then ogf sequence ar 0 equals 1 x x2 1 x1 fx. so r 0 ogf a ar r xf x b ar r2 x f x xf x . c ar 3r 5r2 3xf x 5 xf x x2f x 8x1 x2 10x21 x3.
126 chapter 7. advanced counting principles 2. determine number ways distribute 50 coins among 30 students student gets 4 coins equals cf x50 1 x x2 x3 x430 cf x50 1 x5301 x30 c79 50 30c74 45 c30 2c69 40 10 x i0 1ic30 ic79 5i 50 5i. 3. n r n determine number solutions y1 yn r yi n0 1 i n. ans recall number equals cr n 1 r see theorem 6.3.3. alternate. think problem follows system interpreted coming monomial xr r y1 yn. is problem reduces ﬁnding coeﬃcients xyk formal power series yk 0. now recall cf yxk 1 y1 1. hence question reduces computing cf xr 1 1 y1 y 1 y cf yr 1 1 yn cr n 1 r. 4. evaluate p k0 1 2k k. put fx 1 x1. then required sum 1 2f 12 2. alternately rearranging terms absolutely convergent series 1 2 1 4 1 4 1 8 1 8 1 8 . . . 1 1 2 2. 5. determine closed form expression p n0 nxn px. ans 1 x1 p n0 xn one 1 x2 1 x1 p n0 xn p n0 nxn1. thus closed form expression x 1 x2 . alternate. let p n0 nxn x 2x2 3x3 . then xs x2 2x3 3x4 . hence 1 xs p k1 xk x p k0 xk x 1 x. thus x 1 x2 . 6. determine sum ﬁrst n positive integers. ans using previous example note k cf xk1 1 x2 . therefore propo sition 7.3.10 one n p k1 k cf xn1 1 x1 1 x2 hence n x k1 k cf xn1 1 x3 cn 1 n 1 nn 1 2 .
7.3. generating functions 127 7. determine sum squares ﬁrst n positive integers. ans recall p n0 nxn x 1x2 . thus p n0 n2xn x p n0 nxn x x 1x2 x1 x 1 x3 . hence n x k1 k2 cf xn 1 1 x x1 x 1 x3 cf xn1 1 1 x4 cf xn2 1 1 x4 cn 2 n 1 cn 1 n 2 nn 12n 1 6 . exercise 7.3.12. 1. n r n determine number solutions x12x2 nxn r xi n0 1 i n. 2. determine p k0 1 2k cn k 1 k. 3. find number nonnegative integer solutions b c e 27 satisfying a 3 a 8 b 3 a b c 8 c c multiple 3 e multiple 4. 4. determine number ways 100 voters cast 100 votes 10 candi dates candidate gets 20 votes. 5. determine closed form expression n p k1 k3. 6. determine closed form expression p n0 n2 n 6 n . 7. verify following table formal power series. table formal power series ex p k0 xk k 1 xn p r0 cn kxk n n0 cosx p r0 1rx2r 2r sinx p r0 1rx2r1 2r 1 coshx p r0 x2r 2r sinhx p r0 x2r1 2r 1 radius convergence x 1 log1 x p k1 xk k 1 1 x p k0 xk 1 1 xn p k0 cn k 1 kxk n n 1 xn xr p kr cn r kxk xn 1 xn1 p k0 ck nxk n n0 radius convergence x 1 4 1 1 4x p k0 c2k kxk 1 1 4x 2x p k0 1 k 1c2k kxk
128 chapter 7. advanced counting principles . . . . . . . . . . . . . . . 5 3 3 2 1 1 . . . . . . . . . . . . . . . 6 4 3 1 1 . . . . . . . . . . . . . . . . . . . 5 5 4 3 2 9 ihook 7 iihook 3 iiihook figure 7.1 ferrers diagram its conjugate deﬁnition 7.3.13. ferrers diagram n k n let n1 n2 nk partition n n k parts. then ferrers diagram n1 n2 nk pictorial representation pattern using dots following way place n1 dots ﬁrst row. n2 dots second row placed way cover ﬁrst n2 dots ﬁrst row see figure 7.1. example 7.3.14. 1. 1 1 1 1 2 2 2 1 1 partitions 4. 2. ferrers diagram 5 3 3 2 1 1 3. let λ partition µ its ferrers diagram. then diagram µ obtained interchanging rows columns µ called conjugate λ denoted λ. thus conjugate partition 5 3 3 2 1 1 6 4 3 1 1 another partition 15. deﬁnition 7.3.15. self conjugate partition λ said self conjugate ferrers diagram λ λ same. example 7.3.16. find oneone correspondence self conjugate partitions parti tions n distinct odd terms. ans let λ self conjugate partition k diagonal dots. 1 i k deﬁne ni number dots ith hook dotted lines figure 7.1. conversely given partition say x1 . . . xk odd terms get self conjugate partition putting x1 dots ﬁrst hook x2 dots second hook on. since xi odd hook symmetric xi xi1 2 2 i k implies corresponding diagram dots indeed ferrers diagram hence result follows. theorem 7.3.17. euler partition n generating function πn εx 1xx2 1x2 x4 1xn x2n 1 1 x1 x2 1 xn. proof. note partition λ n m1 copies 1 m2 copies 2 till mn copies n mi n0 1 i n n p i1 mi n. hence λ uniquely corresponds x1m1x2m2 xnmn wordexpansion 1 x x2 1 x2 x4 1 xn x2n . thus πn cfxn εx.
7.3. generating functions 129 example 7.3.18. let fn number partitions n part 1. then note ogf fn 1 xεx. hence fn πn πn1. alternate. let λ n1 . . . nk partition n nk 1. then λ gives partition n 1 namely n1 . . . nk1. conversely µ t1 . . . tk partition n 1 t1 . . . tk 1 partition n last part 1 hence required result follows. next result idea theorem 7.3.17 hence proof omitted. theorem 7.3.19. number partitions n entries r cf xn r q i1 1 1xi . theorem 7.3.20. ogf πnr fix n r n. then ogf πnr number partitions n r parts xr 1x1x21xr. proof. let λ partition n r parts. then λ corresponds partition n entries r. now add column dots height r left ferrers diagram λ. then new ferrers diagram corresponds partition n r r parts. conversely given partition n r r parts inverse map gives partition n r parts. thus theorem 7.3.19 get πnr cf xnr 1 1 x1 x2 1 xr . hence ogf πnr xr 1 x1 x2 1 xr. exercise 7.3.21. 1. n r n prove πnr number partitions ncr 2 r unequal parts. 2. let p n fn number partitions n parts p multiplicities m. find generating function numbers fn. theorem 7.3.22. suppose k types objects. 1. unlimited supply object egf number rpermutations ekx. 2. mi copies ith object egf number rpermutations 1 x x2 2 xm1 m1 1 x x2 2 xmk mk . 3. moreover nsr n coeﬃcient xr r ex 1n. proof. part 1 since unlimited supply object egf object corresponds ex 1 x xn n . hence required result follows. part 2 argument similar part 1 omitted. part 3 recall nsr n number surjections r s1 sn. surjection viewed word length r elements s si appearing least once. thus need selection ki n copies si n p i1 ki r. also theorem 6.1.26 number equals cr k1 kn. hence nsr n rcf xr x x2 2 x3 3 n cf xr r ex 1n .
130 chapter 7. advanced counting principles example 7.3.23. 1. many ways get rs 2007 using denominations 1 10 100 1000 only ans cf x2007 1 1 x1 x101 x1001 x1000 . 2. use 9 denomination a number cf x2007 9 x i1 xi 9 x i1 x10i 9 x i1 x100i 9 x i1 x1000i cf x2007 1 x10000 1 x 1. 3. every natural number unique baser representation r 2. note item 2 corresponds case r 10. 4. consider n integers k1 k2 kn gcdk1 . . . kn 1. then number natural numbers partition using k1 . . . kn ﬁnite. since gcdk1 . . . kn 1 exist αi z p αiki 1. let maxα1 . . . αn k minki n kmk1 kn. notice n n k n 2k . . . represented p βiki βi km. 1 r k n r kmk1 kn r p αiki pkm rαiki. thus integer greater n represented using k1 . . . kn. determining largest integer frobenius number coin problem money changing problem. general problem nphard. closed form formula known n 3. notice times way obtain recurrence relation generating function. important hence study next example carefully. example 7.3.24. 1. suppose f 1 1 x1 x101 x1001 x1000 a0 a1x anxn . then taking log diﬀerentiating get f f 1 1 x 10x9 1 x10 100x99 1 x100 1000x999 1 x1000 . so nan cf xn1 f cf xn1 f 1 1 x 10x9 1 x10 100x99 1 x100 1000x999 1 x1000 n x k1 ankbk bk cf xk1 1 1 x 10x9 1 x10 100x99 1 x100 1000x999 1 x1000 1 10 k 11 10k 100 k 111 10k 100k 1000 k 1111 else. 2. know lim n n p k1 1 k . lim n n p k1 1 pk pk kth prime
7.3. generating functions 131 ans n 1 let sn n p k1 1 k. then note sn 1 1 2 1 4 1 1 3 1 9 1 1 pn 1 p2 n n k1 1 1 pk 1. thus log sn log n k1 1 1 pk 1 n x k1 log1 1 pk 1 n x k1 1 pk 1 1 n1 x k1 1 pk . sn see lim n n p 1 pi as lim nlog sn . 3. let set natural numbers prime divisors 2 3 5 7. then 1 x ns 1 n 1 1 2 1 4 1 1 3 1 9 1 1 7 1 49 2 1 3 2 5 4 7 6. exercise 7.3.25. 1. let σn p dn d n n. then prove nπn n p k1 πnkσk. 2. durfee square largest square ferrers diagram. find generating function number self conjugate partitions n ﬁxed size k durfee square. hence show 1 x1 x3 1 x k1 xk2 1 x21 x4 1 x2k. 3. show number partitions n distinct terms term distinct number partitions n odd terms each term odd. 4. find number rdigit binary numbers formed using even number 0s even number 1s. 5. find egf number words size r using a b c d e word a letters letter appears even many times. b letters ﬁrst letter word appears even number times. 6. permutation σ n said connected exist k 1 k n σ takes k itself. let cn denote number connected permutations n put c0 0 show n x k1 ckn k n. hence derive relationship generating functions n cn. 7. let fn r number partitions n part repeats less r times. let gn r number partition n part divisible r. show fn r gn r. 8. find number 9sequences formed using 0 1 2 3 case. a sequence even number 0s. b sequence odd number 1s even number 0s. c digit appears exactly twice.
132 chapter 7. advanced counting principles 7.4 recurrence relation deﬁnition 7.4.1. recurrence relation recurrence relation way recursively deﬁning terms sequence function preceding terms together certain initial conditions. example 7.4.2. 3 2an1 n 1 initial condition a0 1 recurrence relation. note completely determines sequence an 1 5 13 29 61 . . . deﬁnition 7.4.3. diﬀerence equation sequence an ﬁrst diﬀerence dan an1. kth diﬀerence dkan dk1an dk1an1. diﬀerence equation equation involving diﬀerences. example 7.4.4. 1. d2an 5 diﬀerence equation. but note doesnt give recurrence relation dont initial conditions. 2. every recurrence relation expressed diﬀerence equation. diﬀerence equa tion corresponding recurrence relation 3 2an1 3 2an dan. deﬁnition 7.4.5. solution solution recurrence relation function fn satisfying recurrence relation. example 7.4.6. 1. fn 2n2 3 solution 3 2an1 a0 1. 2. fibonacci sequence given an1 an2 n 2 a0 1 a1 1. deﬁnition 7.4.7. lnhrrcclhrrcc recurrence relation linear nonhomogeneous recurrence relation constant coeﬃcients lnhrrcc order r if known func tion f c1an1 cranr fn ci r 1 i r cr 0. 7.3 f 0 equation 7.3 homogeneous called associated linear homogeneous recurrence relation constant coeﬃcients lhrrcc. theorem 7.4.8. k n let fi 1 i k known functions. consider k lnhrrcc c1an1 cranr fin 1 . . . k 7.4 set initial conditions. gi solution ith recurrence then c1an1 cranr k x i1 αifin 7.5 set initial conditions k p i1 αigin its solution. proof. proof left exercise reader. deﬁnition 7.4.9. characteristic equation consider lhrrcc c1an1 cranr cr 0. xn solution either x 0 x root xr c1xr1 cr 0. 7.6
7.4. recurrence relation 133 equation 7.6 called characteristic equation given lhrrcc. x1 . . . xr roots equation 7.6 xn and hence r p i1 αixn αi r solution given lhrrcc. theorem 7.4.10. general solution distinct roots roots xi 0 . . . r 1 equa tion 7.6 distinct every solution hn linear combination xn . moreover solution unique given r consecutive initial conditions. proof. let hn solution. then note exists α0 . . . αr1 h0 . . . hr 1 1 1 x0 xr1 . xr1 0 xr1 r1 α0 . . . αr1 r r matrix invertible matrix. is every αi r hn r1 p i0 αixn 0 n r 1. hence proved result ﬁrst r values hn. so let us assume result true n k. then deﬁnition hk r x j1 cjhk j r x j1 cj r1 x i0 αixkj r1 x i0 αi r x j1 cjxkj r1 x i0 αixk n k xk solution equation 7.6. thus pmi hn r1 p i0 αixn n. uniqueness left exercise reader. example 7.4.11. 1. solve 4an2 0 n 2 a0 1 a1 1. ans note 2 roots characteristic equation x2 4 0. roots distinct general solution α2n β2n α β r. initial conditions give α β 1 2β 2α 1. hence α 1 4 β 3 4. thus unique solutions 2n23 1n . 2. solve 3an1 4an2 n 2 a0 1 a1 c constant. ans note 1 4 roots characteristic equation x2 3x 4 0. roots distinct general solution α1n β4n α β r. now initial conditions imply α 4c 5 andβ 1c 5 . thus unique general solution a 4 c1n 5 1 c4n 5 c 4. b 4n c 4. 3. solve fibonacci recurrence an1 an2 initial conditions a0 a1 1. ans case note roots characteristic equation x2 x 1 0 1 5 2 . roots distinct general solution α 1 5 2 n β 1 5 2 n α β r. now using initial conditions get α 5 5 10 β 5 5 10 . hence required solution α 1 5 2 n β 1 5 2 n 1 5 1 5 2 n1 1 5 2 n1 .
134 chapter 7. advanced counting principles theorem 7.4.12. general solution multiple roots let root equation 7.6 multiplicity s. then un tnα1 nα2 ns1αs solution basic solution. general ti root equation 7.6 multiplicity si 1 . . . k every solution sum k basic solutions. proof. given zero polynomial f xr c1xr1 cr multiplicity s. put g0 xnrf xn c1xn1 crxnr g1 xg 0 g2 xg 1 . . . gs1 xg s2. then g0 g1 . . . gs1 zero t. is 0 1 . . . 1 git tnni c1tn1n 1i . . . crtnrn ri 0. now take un tnpn pn s1 p i0 niαi ﬁxed polynomial αi r 0 i s 1. then s1 x i0 αigit tnpn c1tn1pn 1 crtnrpn r 0. hence 0 i s 1 αi r un solution lhrrcc. part proof left reader. example 7.4.13. suppose lhrrcc roots 2 2 3 3 3. then general solution given 2nα1 nα2 3nβ1 nβ2 n2β3. theorem 7.4.14. lnhrrcc consider lnhrrcc equation 7.3 let un general solution associated lhrrcc. vn particular solution lnhrrcc un vn general solution lnhrrcc. proof. proof left reader. notice general algorithm solve lnhrrcc. fn nk linear combination these particular solution obtained easily. obtaining particular solution knowledge characteristic roots. 1. fn root equation 7.3 vn can. 2. fn root equation 7.3 multiplicity t vn cntan. 3. fn nk 1 root equation 7.3 use vn c0 c1n cknk. 4. fn nk 1 root equation 7.3 multiplicity t vn ntc0 c1n cknk. example 7.4.15. 1. let 3an1 2n n 1 a0 1. ans observe 3 characteristic root associated lhrrcc an 3an1. thus general solution lhrrcc un 3nα. note 1 characteristic
7.5. generating function recurrence relation 135 root hence particular solution nb b computed using nb 3a n 1b 2n. gives 3 2 b 1. hence 3nα n 3 2. using a0 1 check α 5 2. 2. let 3an1 2an2 35n n 3 a1 1 a2 2. ans observe 1 2 characteristic roots associated lhrrcc an 3an1 2an2. thus general solution lhrrcc un α1n β2n. note 5 characteristic root thus vn c5n particular solution lnhrrcc c5n 3c5n1 2c5n2 35n. is c 254. hence general solution lnhrrcc equals α β2n 2545n compute α β using initial conditions. 3. take fn 32n. then see c2n choice particular solution 4c 6c2c12 absurd statement. but choice cn2n 4nc 6n 1c 2n 2c 12 implying c 6. hence general solution lnhrrcc α β2n 6n2n compute α β using initial conditions. 7.5 generating function recurrence relation sometimes ﬁnd solution recurrence relation using generating function an. example 7.5.1. 1. consider 2an1 1 a0 1. ans let fx a0 a1x generating function ai. then f 1 x i1 aixi 1 x i1 2ai1 1xi x i0 xi 2x x i0 aixi 1 1 x 2xf. hence f 1 1x12x 2 12x 1 1x. thus cfxn f 2n1 1. 2. find ogf f fibonacci recurrence relation an1 an2 a0 0 a1 1. ans f x i0 aixi x x i2 ai2xi x i2 ai1xi x x2 x i0 aixi x x i1 aixi x x2 xf. thus f x 1 x x2 x x αx β α 1 5 2 β 1 5 2 . so f x x αx β 1 5 α x α β x β 1 5 x i0 xi αi xi βi . hence using α β 1 cfxn f 1n 5 βn αn 1 5n 1 5n 2n 5 . next result follows using small calculation hence proof left reader.
136 chapter 7. advanced counting principles theorem 7.5.2. obtaining generating function recurrence relation generating function rth order lhrrcc c1an1 cranr initial conditions ai ai 0 1 . . . r 1 r1 p i0 aixi c1x r2 p i0 aixi c2x2 r3 p i0 aixi cr1xr1a0 1 c1x crxr . example 7.5.3. 1. find ogf catalan numbers cns. ans let gx 1 p n1 cnxn cn c2nn n1 c0 1. then gx 1 x n1 cnxn x n1 1 n 1 2n nnxn x n1 22n 1 n 1 cn1xn x n1 4n 4 n 1 cn1xn x n1 6 n 1cn1xn 4xgx 6 x x z 0 tgtdt. so gx 1 4xgxx 6 x r 0 tgtdt. now diﬀerentiate respect x get gx1 4x g1 2x 1. solve ode ﬁrst observe z 1 2x x1 4x z 1 x 2 1 4x ln x 1 4x . thus integrating factor given ode x 14x hence ode rewritten gx x 1 4x gx 1 2x 1 4x32 1 1 4x32 d dx gx x 1 4x 1 1 4x32 . hence gx x 14x 1 214x c c r. or equivalently gx 1 2c1 4x 2x . 7.7 note c0 lim x0 gx 1 hence c 1 2. thus gx 1 1 4x 2x . alternate. recall cn number representations product n1 square matrices size using n pairs brackets. representation remove leftmost rightmost brackets obtain product two representations form a1a2 an1 a1a2a3 an1 a1 akak1 an1 a1 anan1. hence see cn c0cn1 c1cn2 cn1c0. 7.8
7.5. generating function recurrence relation 137 thus deﬁne gx p n0 cnxn n 1 cf xn1 gx2 cf xn1 x n0 cnxn 2 n1 x i0 cicn1i cn using equation 7.8. is cf xn xgx2 cn. hence gx 1 xgx2. solving gx get gx 1 2 1 x r 1 x2 4 x 1 1 4x 2x . function g continuous being power series domain convergence lim x0 gx c0 1 follows gx 1 1 4x 2x . 2. fix r n let an sequence a0 1 n p k0 akank cn r r n 1. determine an. ans let gx p n0 anxn. then note cn r r cn r 1 1 n. hence gx2 x n0 n x k0 akank xn x n0 cn r rxn x n0 cn r nxn 1 1 xr1 . hence cf h xn 1 1xr12 . example r 2 1nc32 n 3 5 7 2n 1 2n n 2n 1 22nnn . 3. determine sequence fn m n n0 satisﬁes fn 0 1 n 0 f0 m 0 0 fn m fn 1 m fn 1 1 n m 0 0. 7.9 ans deﬁne fnx p m0 fn mxm. then n 1 equation 7.9 gives fnx x m0 fn mxm x m0 fn 1 m fn 1 1 xm x m0 fn 1 mxm x m0 fn 1 1xm fn1x xfn1x 1 xfn1x 1 xnf0x. now using initial conditions f0x 1 hence fnx 1 xn. thus fn m cfxm 1 xn cn m 0 m n 0 n.
138 chapter 7. advanced counting principles alternate. deﬁne gmy p n0 fn myn. then 1 equation 7.9 gives gmy x n0 fn myn x n0 fn 1 m fn 1 1 yn x n0 fn 1 myn x n0 fn 1 1yn ygmy ygm1y. therefore gmy 1 ygm1y. using initial conditions g0y 1 1 y. hence gmy ym 1 ym1 . thus fn m cf yn ym 1 ym1 cf ynm 1 1 ym1 cn m 0 m n 0 n. 4. determine sequence sn m n n0 satisfy s0 0 1 sn m 0 either 0 n 0 sn m msn 1 m sn 1 1 n m 0 0. 7.10 ans deﬁne gmy p n0 sn myn. then 1 equation 7.10 gives gmy x n0 sn myn x n0 msn 1 m sn 1 1 yn x n0 sn 1 myn x n0 sn 1 1yn mygmy ygm1y. therefore gmy 1 mygm1y. using initial conditions g0y 1 hence gmy ym 1 y1 2y 1 my ym x k1 αk 1 ky 7.11 αk 1mkkm k m k 1 k m. thus sn m cf yn ym x k1 αk 1 ky x k1 cf ynm αk 1 ky x k1 αkknm x k1 1mkkn k m k 1 m x k1 1mkkncm k 1 m x k1 1km kncm k. 7.12 therefore sn m 1 m p k1 1km kncm k. identity generally known stirlings identity.
7.5. generating function recurrence relation 139 observation. a let us consider hnx p m0 sn mxm. then verify hnx x xdn 1 h0x 1. therefore h1x x h2x x x2 . thus dont single expression hnx gives value sn ms. but helps showing sn m ﬁxed n n ﬁrst increase decrease commonly called unimodal. holds sequence binomial coeﬃcients cn m 0 1 . . . n. b restriction n.m n0 equation 7.12 also valid n m. but know sn m 0 whenever n m. hence get following identity x k1 1mk kn1 k 1 m k 0 whenever n m. 5. bell numbers n n nth bell number denoted bn number partitions n. thus bn n p m1 sn m n 1 b0 1. hence n 1 bn n x m1 sn m x m1 sn m x m1 x k1 1mk kn1 k 1 m k x k1 kn k x mk 1mk m k 1 e x k1 kn k 1 e x k0 kn k 0n 0 n 0. 7.13 thus equation 7.13 valid even n 0. bn terms form kn k compute egf. thus bx p n0 bnxn n then bx 1 x n1 bnxn n 1 x n1 1 e x k1 kn k xn n 1 1 e x k1 1 k x n1 kn xn n 1 1 e x k1 1 k x n1 kxn n 1 1 e x k1 1 k ekx 1 1 1 e x k1 exk k 1 k 1 1 e eex 1 e 1 eex1. 7.14 recall eex1 valid formal power series see remark 7.3.5.taking logarithm equation 7.14 get log bx ex 1. hence bx exbx equivalently x x n1 bnxn1 n 1 xex x n0 bnxn n x x m0 xm m x n0 bnxn n .
140 chapter 7. advanced counting principles thus bn n 1 cf xn x n1 bnxn n 1 cf xn1 x m0 xm m x n0 bnxn n n1 x m0 1 n 1 m bm m . hence get bn n1 p m0 cn 1 mbm n 1 b0 1. exercise 7.5.4. 1. find number binary words without subword 00 111. 2. find number subsets 1 . . . n containing consecutive integers. 3. prove fn divides fnm n positive integers. objects distinct places distinct places nonempty relate number onto functions rsn r r1 p i0 1icr ir in n functions rn n rpartition set sn r n n partitions set bn r p i1 sn i n positive integer solutions cn 1 r 1 n n nonnegative integer solutions cn r 1 r 1 n n rpartition n πnr cf h xnr 1 1x1x21xr n n n partitions n length r r p i1 πni exercise 7.5.5. 1. find number circular permutations a a b b c c c c. 2. let n1 n2 n3 ni n p ni 15. evaluate x n1n2n3s 15 n1n2n3. 3. 9 senior students said the number junior students want help exactly one. 4 junior students a b c d wanted help. allocation done randomly. probability either exactly two seniors help b exactly 3 seniors help c seniors help him 4. particular semester 6 students took admission phd programme. 9 professors willing supervise students. rule a student
7.5. generating function recurrence relation 141 either one two supervisors. many ways allocate supervisors students willing professors allocated additional condition exactly one supervisor gets supervise two students 5. many lattice paths 0 0 9 9 cross dotted line 0 0 9 9 6. a prove combinatorially that n 2 dn n 1dn1 dn2. b use part a show exponential generating function dn ex 1 x. 7. friend says n 2 subsets 14 size 6. give value n guarantee some two subsets 3 elements common without seeing collection smallest possible value n 8. find number words size 12 made using letters a b c bca appear as consecutive subword. example abcabcccccba appearance bca bccabccabcca not. 9. find number 8 letter words made using alphabets a b c d 3 consecutive letters allowed same. 10. evaluate 9 p i11 i1 p i21 i2 p i31 i8 p i91 i2 9. 11. 3 blue bags 4 red bags 5 green bags. many balls colors blue red green. fill blank smallest positive integer. distribute balls without seeing colors bags one following must happen a blue bag contains 3 blue balls 4 red balls 5 green balls b red bag contains 3 blue balls 5 red balls 7 green balls c green bag contains 3 blue balls 6 red balls 9 green balls 12. integer polynomial fx. fill blank smallest positive integer. fx 2009 many distinct integer roots fx 9002 cannot integer root. 13. many ways one distribute a 10 identical chocolates among 10 students
142 chapter 7. advanced counting principles b 10 distinct chocolates among 10 students c 10 distinct chocolates among 10 students receives one d 15 distinct chocolates among 10 students receives least one e 10 15 distinct chocolates among 10 students receives one f 15 distinct chocolates among 10 students receives three g 15 distinct chocolates among 10 students receives least one three h 15 identical chocolates among 10 students receives three 14. many ways one carry a 15 distinct objects 10 identical bags answer using sn r. b 15 distinct objects 10 identical bags empty bag answer using sn r. c 15 distinct objects 10 identical bags bag containing three objects d 15 identical objects 10 identical bags e 15 identical objects 10 identical bags empty bag f 15 identical objects 20 identical bags empty bag 15. number integer solutions x z 10 x 1 2 z 3 16. number solutions xyz 10 nonnegative multiples 1 2 x y z allowed 0 12 1 32 . . . four times number nonnegative integer solutions x z 10 17. many words length 8 formed using english alphabets letter appear twice give answer using generating function. 18. let p1 . . . pn n 2 distinct prime numbers. consider set p1 . . . pn p2 1 . . . p2 n. many ways partition set subsets size two prime subset containing square 19. value 15 p k0 1kc15 k15 k5 20. number partitions a n entries r give answers using generating function. b n r parts give answers using generating function. c πnr n exactly r parts give answers using generating function. d πnr n cr 2 r distinct parts give answers using generating func tion. e n distinct entries give answers using generating function. f n entries odd give answers using generating function. g n distinct odd entries give answers using generating function. h n self conjugate give answers using generating function.
7.5. generating function recurrence relation 143 21. many words length 15 using letters abcde letter must appear word appears even number times give answers using generating function. 22. characteristic roots lhrrcc 2 2 2 3 3. form general solution 23. consider lnhrrcc c1an1 cranr 5n. give particular solution. 24. obtain ogf an 2an1 an2 2n a0 0 a1 1. 25. solve recurrence relation 2an1 an2 2n 5 a0 0 a1 1. 26. class n cse msc r mc students. suppose copies book distributed branch gets least s. many ways done student gets one many ways done without previous restriction answer using generating function. exercise 7.5.6. 1. class n cse msc r mc students. suppose distinct books distributed branch gets least s. many ways done student gets one many ways done without previous restriction answer using generating function. 2. class n students. assume that conduct exam identical answer scripts. many ways distribute answer scripts student gets least 2. answer using generating function. 3. class n students. assume that exam questions student answers questions order decided himher for example one follow 1 2 another follow m 1 1. many ways happen three students followed order answer using generating function. 4. freshers welcome organized 11 teachers went attend. 4 types soft drinks available. many ways total 18 glasses soft drinks served them general answer using generating function.
144 chapter 7. advanced counting principles
chapter 8 graphs 8.1 basic concepts experiment start dot. move line exactly once. draw it. following pictures drawn want starting dot ﬁnishing dot b b b b b b b b b b b b b b b b b b b b b b b b later shall see theorem euler addressing question. deﬁnition 8.1.1. pseudograph vertex set edge set pseudograph general graph g pair v e v nonempty set e multiset unordered pairs points v . set v called vertex set elements called vertices. set e called edge set elements called edges. example 8.1.2. g 4 1 1 1 2 2 2 3 4 3 4 pseudograph. discussion 8.1.3. pseudograph represented picture following way. 1. put diﬀerent points paper vertices label them. 2. u v appears e k times draw k distinct lines joining points u v. 3. loop u drawn u u e. example 8.1.4. picture pseudograph example 8.1.2 given figure 8.1. deﬁnition 8.1.5. loop end vertex incident vertexedge 1. edge u v sometimes denoted uv. edge uu called loop. vertices u v called end vertices edge uv. let e edge. say e incident u mean u end vertex e. 145
146 chapter 8. graphs b b b b 1 2 3 4 figure 8.1 pseudograph 2. multigraph simple graph multigraph pseudograph without loops. multigraph simple graph edge appears twice.1 3. henceforth graphs book simple ﬁnite vertex set unless stated oth erwise. 4. use v g or simply v eg or simply e denote vertex set edge set g respectively. number v g order graph g. sometimes denoted g. gwe denote number edges g. graph n vertices edges called n m graph. 1 0 graph trivial graph. 5. neighbor independent set uv edge g say u v adjacent g u neighbor v. write u v denote u adjacent v. two edges e1 e2 adjacent common end vertex. set vertices edges independent two adjacent. 6. isolated pendant vertex v v g nv ngv denote set neighbors v g nv called degree v. usually denoted dgv dv. vertex degree 0 called isolated. vertex degree one called pendant vertex. discussion 8.1.6. note graph algebraic structure namely pair sets satisfying conditions. however easy describe carry arguments pictorial representation graph. henceforth pictorial representations used describe graphs provide arguments whenever required. loss generality this. example 8.1.7. consider graph g figure 8.2. vertex 12 isolated vertex. n1 2 4 7 d1 3. set 9 10 11 2 4 7 independent vertex set. set 1 2 8 10 4 5 independent edge set. vertices 1 6 adjacent. deﬁnition 8.1.8. complete graph path graph cycle graph bipartite graph let g v e graph n vertices say v v1 . . . vn. then g said 1. complete graph denoted kn pair vertices g adjacent. 2. path graph denoted pn e vivi1 1 i n 1. 1a simple graph hypergraph v e e collection nonempty subsets v .
8.1. basic concepts 147 b b b b b b b b b b b b b b b 11 8 10 9 1 2 3 4 5 6 7 12 13 figure 8.2 graph g. 3. cycle graph denoted cn e vivi1 1 i n 1 vnv1. 4. complete bipartite graph denoted krs e vivj 1 i r r 1 j n r n. importance labels vertices depends context. point time even interchange labels vertices still call complete graph path graph cycle complete bipartite graph. b b b b b 1 2 3 n 1 n pn b b b b b 1 2 3 n 1 n cn figure 8.3 pn cn. quiz 8.1.9. maximum number edges possible simple graph order n1 lemma 8.1.10. hand shaking lemma graph g p vv dv 2e. thus number vertices odd degree even. proof. edge contributes 2 sum p vv dv. hence p vv dv 2e. note 2e x vv dv x dv odd dv x dv even dv even. so p dv odd dv even. hence number vertices odd degree even. quiz 8.1.11. party 27 persons prove someone must even number friends friendship mutual. 2 proposition 8.1.12. graph g n g 2 two vertices equal degree. 1cn 2. 2otherwise p dv odd.
148 chapter 8. graphs 1 p1 1 2 p2 2 1 3 p3 1 4 2 3 p4 3 2 4 1 5 p5 1 k1 1 2 k2 2 1 3 k3 1 4 2 3 k4 1 2 3 4 5 k5 2 1 3 c3 1 4 2 3 c4 1 2 3 4 5 c5 1 5 4 2 6 3 c6 1 2 k11 2 1 3 k12 1 4 2 3 k22 1 2 3 4 5 k23 figure 8.4 well known family graphs proof. g two isolated vertices done. so suppose g exactly one isolated vertex. then remaining n 1 vertices degree 1 n 2 hence php result follows. g isolated vertex g n vertices whose degree lie 1 n 1. now apply php get required result. example 8.1.13. graph figure 8.5 called petersen graph. shall use example many places. b b b b b b b b b b b b 1 2 3 4 5 6 7 8 9 10 figure 8.5 petersen graphs exercise 8.1.14. 1. let x v e graph vertex v v odd degree. then prove exists vertex u v path v u degu
8.1. basic concepts 149 also odd. 2. let x v e graph exactly two vertices say u v odd degree. then prove path x connecting u v. deﬁnition 8.1.15. regular graph cubic graph minimum degree vertex g denoted δg maximum degree vertex g denoted g. graph g called kregular dv k v v g. 3regular graph called cubic. example 8.1.16. 1. graph kn regular. 2. graph k4 cubic. 3. graph c4 2regular. 4. graph p4 regular. 5. petersen graph cubic. 6. consider graph g figure 8.2. δg 0 g 3. quiz 8.1.17. cubic graph 5 vertices1 deﬁnition 8.1.18. subgraph induced subgraph spanning subgraph kfactor graph h subgraph g v h v g eh eg. u v g subgraph induced u denoted u u e edge set e uv eg u v u. subgraph h g spanning subgraph v g v h. kregular spanning subgraph called kfactor. example 8.1.19. 1. consider graph g figure 8.2. a let h1 graph v h1 6 7 8 9 10 12 eh1 6 7 9 10 . then h1 subgraph g. b let h2 graph v h2 6 7 8 9 10 12 eh2 6 7 8 10 . then h2 subgraph induced subgraph g. c let h3 induced subgraph g vertex set 6 7 8 9 10 12. then verify eh3 6 7 8 9 8 10 . d graph g 1factor. 2. complete graph 1factor even order. 3. petersen graph many 1factors. one obtained selecting edges 1 6 2 7 3 8 4 9 5 10. quiz 8.1.20. consider k8 vertex set 8. many 1factors have2 deﬁnition 8.1.21. vertexedge deleted graph let g graph v vertex. then graph g v obtained deleting v edges incident v. e eg graph g e v eg e. u v v g u v g uv v eg uv. 1no p dv 15 even. 2824.
150 chapter 8. graphs example 8.1.22. consider graph g figure 8.2. let h2 graph v h2 6 7 8 9 10 12 eh2 6 7 8 10 . consider edge e 8 9. then h2 e induced subgraph 6 7 8 9 10 12and h2 8 6 7 9 10 12. deﬁnition 8.1.23. complement graph complement g graph g deﬁned v g e e uv u v uv eg. example 8.1.24. 1. see graphs figure 8.6. 1 4 2 3 c4 1 4 2 3 c4 1 2 3 4 5 c5 1 2 3 4 5 c5 c5 figure 8.6 complement graphs 2. complement k3 contains 3 isolated points. 3. graph g g g cg 2. 4. graph g order n dgv dgv n 1. thus g g n 1. quiz 8.1.25. 1. characterize graphs g g g n 1.1 2. graph g g g n 3. show kregular simple graph n vertices exists kn even n k 1. deﬁnition 8.1.26. intersection union disjoint union intersection two graphs g h denoted gh deﬁned v gv h egeh. union two graphs g h denoted gh deﬁned v g v h eg eh. disjoint union two graphs union treating vertex sets disjoint sets. example 8.1.27. two graphs g h shown below. graphs g h g h also shown below. b b b 1 2 3 g b bb b 1 2 4 h b b b b b 1 2 4 3 g h b b 1 2 g h disjoint union g g h g1 figure 8.7. 1if dgu dgv dgu n 1 dgu. hence g g dgv n 1 dgu dgv n 1 dgv n. thus answer regular graphs.
8.2. connectedness 151 b b b 1 2 3 b b b b b 1 2 4 3 g1 b b b b b b b b b b b b 1 2 b c k2 k3 k2 k3 b b b b b 1 b 2 k2 k2 figure 8.7 disjoint union join graphs deﬁnition 8.1.28. join two graphs v g v g join g g deﬁned g g vv v v v v . ﬁrst means join two graphs second means adding set edges given graph. example 8.1.29. a k2 k3 k5. b k2 k2 c4. quiz 8.1.30. complement disjoint union g h1 deﬁnition 8.1.31. cartesian product two graphs let g v e g v e two graphs. then cartesian product g g denoted g g v1 e1 graph v1 v v whose edge set consists elements u1 u2 v1 v2 either u1 v1 u2 v2 e u2 v2 u1 v1 e. example 8.1.32. see graphs figure 8.8. b x 1 2 3 1a 2a 3a 1b 2b 3b x 11 21 31 12 22 32 13 23 33 figure 8.8 cartesian product graphs 8.2 connectedness deﬁnition 8.2.1. walk trail path cycle circuit length internal vertex uv walk g ﬁnite sequence vertices u v1 v2 vk v vivi1 e 1 k 1. length walk number edges it. walk called trail edges walk repeated. vu walk called path vertices involved distinct except v u may same. path length 0. walk trail path 1g h.
152 chapter 8. graphs called closed u v. closed path called cyclecircuit. thus simple graph cycle length least 3. cycle walk path length k also written kcycle kwalk kpath. p uv path u v sometimes call u v end vertices p remaining vertices p internal vertices. example 8.2.2. a take g k5 vertex set 5. . then 1 2 3 2 1 2 5 4 3 8walk g 1 2 2 1 walk. . walk 1 2 3 4 5 2 4 1 closed trail. . walk 1 2 3 5 4 1 closed path is 5cycle. . maximum length cycle g 5 minimum length cycle g 3. . 10 c5 3 many 3cycles g. . verify number 4cycles g c5 4. b let g petersen graph. . 9cycle g namely 6 8 10 5 4 3 2 7 9 6. . 10cycles g. shall see discuss eulerian graphs. proposition 8.2.3 technique. let g graph u v v g u v. let w u u1 . . . uk v walk. then w contains uvpath. proof. vertex w repeats w path. so let ui uj j. now consider walk w1 u1 . . . ui1 uj uj1 . . . uk. also uv walk shorter length. thus using induction length walk desired result follows. deﬁnition 8.2.4. distance diameter radius center girth distance du v two vertices g shortest length uv path g. path exists distance taken . greatest distance two vertices graph g called diameter g. shall use diamg denote diameter g. let distv max ug dv u. radius min vg distv center consists vertices v distv radius. girth denoted gg graph g minimum length cycle contained g. g cycle put gg . example 8.2.5. let g petersen graph. diameter 2. radius 2. vertex center. girth 5. practice 8.2.6. determine diameter radius center girth following graphs pn cn kn knm kn km. exercise 8.2.7. let g graph. then show distance function du v metric v g. is satisﬁes 1. du v 0 u v v g du v 0 u v 2. du v dv u u v v g 3. du v du w dw u u v w v g.
8.2. connectedness 153 proposition 8.2.8 technique. let g graph g1 dv 2 vertex except one say v1. then g cycle. proof. consider longest path v1 . . . vk g as v g ﬁnite path exists. dvk 2 must adjacent vertex v2 . . . vk2 otherwise extend longer path. let 2 smallest vi adjacent vk. then vi vi1 . . . vk vi cycle. proposition 8.2.9 technique. let p q two diﬀerent uv paths g. then p q contains cycle. proof. imagine signal sent u v via p returned back v u via q. call edge dead signal passed twice. notice vertex receives signal many times sends signal. ep eq no otherwise p q graphs. so alive edges. get alive edge v1v2. must alive edge v2v3.1 similarly get v3v4 on. stop ﬁrst instance repetition vertex v1 v2 vi vi1 vj vi. then vi vi1 vj vi cycle. alternate. consider graph h v p v q epeq is symmetric diﬀerence. notice eh otherwise p q. degree vertex multigraph p q even h obtained deleting pairs multiple edges vertex h even degree. hence proposition 8.2.8 h cycle. proposition 8.2.10. every graph g containing cycle satisﬁes gg 2 diamg 1. proof. let c v1 v2 . . . vk v1 shortest cycle diamg r. k 2r 2 consider path p v1 v2 . . . vr2. since length p r 1 diamg r vr2v1 path r length r. note p r diﬀerent v1vr2 paths. proposition 8.2.9 closed walk p r length 2r 1 contains cycle. hence length cycle 2r1 contradiction c smallest length k 2r2. deﬁnition 8.2.11. chord chordal acyclic graphs let c v1 . . . vk v1 cycle. edge vivj called chord c edge c. graph called chordal cycle length least 4 chord. graph acyclic cycles. example 8.2.12. complete graphs chordal acyclic graphs. petersen graph chordal. quiz 8.2.13. 1. many acyclic graphs vertex set 32 2. many chordal graphs vertex set 43 1otherwise v2 incident one alive edge dead edges. means v2 received signal sent. 27 3 edges put 23 ways. one cycle. 361 6 edges put 26 ways. three 4cycles.
154 chapter 8. graphs deﬁnition 8.2.14. 1. maximal minimal graph graph g said maximal respect property p g property p proper supergraph g property p. similarly deﬁne term minimal. notice class graphs property poset here. so maximality minimality deﬁned naturally. 2. clique clique number connected graph complete subgraph g called clique. maximum order clique called clique number g. denoted ωg. graph g called connected uv path u v v g. 3. disconnected graph component graph graph connected called disconnected. g disconnected graph maximal connected subgraph called component sometimes connected component. example 8.2.15. consider graph g shown figure 8.2. then 1. cliques g 8 10 2 1 2 4. ﬁrst last maximal cliques. notice every vertex clique. similarly edge clique. ωg 3. 2. graph g connected. four connected components namely 8 9 10 11 1 2 3 4 5 6 7 12and 13. quiz 8.2.16. ωg petersen graph1 proposition 8.2.17. δg 2 g path length δg cycle length least δg 1. proof. let v1 vk longest path g. dvk 2 vk adjacent vertex v vk1. v path path longer v1 vk path. contradiction. let smallest positive integer vi adjacent vk. thus δg dvk vi vi1 vk1. hence cycle c vi vi1 vk vi length least δg 1 length path p vi vi1 vk least δg. deﬁnition 8.2.18. edge density edge density denoted εg deﬁned number eg v g. observe εg also graph invariant. quiz 8.2.19. 1. deletion vertex reduce edge density2 2. δg 2 lower bound εg3 12. 2put h g v. then h εgn dv εh εgndv n1 εg εgdv n1 . so choose vertex v degree εg. 3yes.
8.3. isomorphism graphs 155 3. suppose εg δg. vertex v εg dv1 proposition 8.2.20. let g graph g1. then g subgraph h δh εh εg. proof. εg δg take h g. otherwise vertex v εg dv. put g1 g v. then easily veriﬁed εg1 εg. εg1 δg1 take h g1. otherwise vertex v g1 εg1 dv. put g2 g1 v. then εg2 εg1 εg. continuing above note initially εg 0. ith stage obtained subgraph gi satisfying v gi g i εgi εgi1. is reducing number vertices corresponding edge densities nondecreasing. hence process must stop reach single vertex edge density 0. so let us assume process stops h. then εh δh must true else process would stop h hence required result follows. 8.3 isomorphism graphs deﬁnition 8.3.1. isomorphic graphs two graphs g v e g v e said isomorphic bijection f v v u v g fu fv g u v v . words isomorphism bijection vertex sets preserves adjacency. write g g mean g isomorphic g. example 8.3.2. consider graphs figure 8.9. then note b b b b b b b f 1 2 3 4 6 5 b b b b b b b b g 1 2 3 4 5 6 b b b b b b b b h 1 2 3 4 5 6 figure 8.9 f isomorphic g f isomorphic h 1. graph f isomorphic h independence number denoted αf f the maximum size independent vertex set 3 whereas αh 2. alternately h 3cycle whereas f not. 2. graph f isomorphic g map f v f v g deﬁned f1 1 f2 5 f3 3 f4 4 f5 2 f6 6 gives isomorphism. 1yes. otherwise εg dv v. particular εg δg contradiction.
156 chapter 8. graphs check adjacency f g 1 2 4 6 f1 1 f2 5 f4 4 f6 6 3 2 4 6 f3 3 f2 5 f4 4 f6 6 5 2 4 6 f5 2 f2 5 f4 4 f6 6 edges covered need check further. thus f isomorphism. discussion 8.3.3. isomorphism let f g isomorphic f v f v g. take f. relabel vertex v f fv. call new graph f . then f g. so v f v g ef eg due isomorphic nature function f. practice 8.3.4. take graphs f g figure 8.9. take isomorphism f1 1 f2 5 f3 3 f4 4 f5 2 f6 6. obtain f described discussion 8.3.3. list v f ef . list v g eg. notice same. deﬁnition 8.3.5. selfcomplementary graph g called selfcomplementary g g. example 8.3.6. 1. note cycle c5 0 1 2 3 4 0 self complimentary. iso morphism g g described fi 2i mod 5. 2. g n g g g nn 14. thus n 4k n 4k 1. exercise 8.3.7. 1. construct selfcomplementary graph order 4k. 2. construct selfcomplementary graph order 4k 1. deﬁnition 8.3.8. graph invariant function assigns value output isomorphic graphs. example 8.3.9. observe graph invariants are g g g δg multiset dv v v g ωg αg. exercise 8.3.10. many graphs vertex set 1 2 . . . n ﬁnd easy ask nonisomorphic graphs try n 4 proposition 8.3.11 technique. let f g h isomorphism v v g. then g v h fv. proof. consider bijection g v g v v h fv described g fv gv. deﬁnition 8.3.12. isomorphism g g called automorphism. example 8.3.13. 1. identity map always automorphism graph. 2. permutation sn automorphism kn. 3. two automorphisms path p8. proposition 8.3.14. let g graph let γg denote set automorphisms g. then γg forms group composition functions.
8.4. trees 157 proof. let v g n σ µ γg two automorphisms. then ij eg µiµj eg σ µiσ µj eg. thus σ µ automorphism. moreover µ1 σ1 indeed automorphisms. example 8.3.15. determine γc5. ans consider c5 1 . . . 5 1. note σ 2 3 4 5 1 automorphism. hence e σ σ2 . . . σ4 γc5 σ5 e. now let µ automorphism µ1 i. put τ σ6iµ. then τ automorphism τ1 1. τ2 2 adjacency structure implies τj j j 3 4 5. hence case σ6iµ e thus µ σi6 σi1. τ2 2 τ2 5 τ3 4 τ 2 53 4 reﬂection ﬁxes 1. let us denote permutation 2 53 4 ρ. then γc5 group generated σ ρ hence γc5 10 elements. example 8.3.16. notice γc5 subgroup γ1 e σ σ2 . . . σ4 σ5 e order 5. let g subgraph c5 obtained deleting zero allowed edges. g 5 γg 10. g 0 γg s5 5. g 4 γg 2. g 3 γg 2 4. g 2 γg 4 8. g 1 γg 2 3. thus subgraph g whose automorphism group γ1. exercise 8.3.17. 1. determine graphs g γg sn group permu tations 1 . . . n. 2. compute γg graphs small order. 3. let g subgraph h order. explore relationship γg γh. 8.4 trees deﬁnition 8.4.1. tree forest connected acyclic graph called tree. forest graph whose components trees. proposition 8.4.2. let tree u v v t. then unique uvpath t. proof. contrary assume two uvpaths t. then proposition 8.2.9 cycle contradiction. proposition 8.4.3. let g graph property between pair vertices unique path. then g tree. proof. clearly g connected. g cycle v1 v2 vk v1 v1 v2 . . . vk1 v1 vk1 two v1vk1 paths. contradiction. deﬁnition 8.4.4. cut vertex let g connected graph. vertex v g called cut vertex g v disconnected. thus g v connected v cut vertex.
158 chapter 8. graphs proposition 8.4.5. let g connected graph g 2. v v g dv 1 g v connected. is vertex degree 1 never cut vertex. proof. let u w v gv u w. g connected uw path p g. vertex v cannot internal vertex p internal vertex degree least 2. hence path p available g v. so g v connected. proposition 8.4.6 technique. let g connected graph g 2 let v v g. g v connected either dv 1 v cycle. proof. assume gv connected. dgv 1 nothing show. so assume dv 2. need show v cycle g. let u w two distinct neighbors v g. g v connected path say u u1 . . . uk w g v. then u u1 . . . uk w v u cycle g containing v. quiz 8.4.7. let g graph v vertex cycle. g v disconnected1 deﬁnition 8.4.8. cut edge let g graph. edge e g called cut edge bridge g e connected components g. proposition 8.4.9 technique. let g connected e u v cut edge. then g e two components one containing u containing v. proof. g e disconnected deﬁnition e cannot cut edge. so g e least two components. let gu respectively gv component containing vertex u respectively v. claim components. let w v g. then g connected graph hence path say p w u. moreover either p contains v internal vertex p doesnt contain v. ﬁrst case w v gv latter case w v gu. thus every vertex g either v gv v gu hence required result follows. proposition 8.4.10 technique. let g graph e edge. then e cut edge e cycle. proof. suppose e u v cut edge g. let f component g contains e. then proposition 8.4.9 f e two components namely fu contains u fv contains v. let possible c u v v1 . . . vk u cycle containing e u v. then v v1 . . . vk u uv path f e. hence f e still connected. contradiction. hence e cannot cycle. conversely let e u v edge cycle. now suppose f component g contains e. need show f e disconnected. let possible uvpath say u u1 . . . uk v f e. then v u u1 . . . uk v cycle containing e. contradiction e lying cycle. hence e cut edge f. consequently e cut edge g. 1yes. take g 4 1 2 1 3 1 4 3 4 v 1.
8.4. trees 159 proposition 8.4.11. center tree always consists set two vertices. proof. let tree radius k. since center contains least one vertex let u vertex center t. now let v another vertex center. claim u adjacent v. suppose u v. then exists path u v denoted pu v least one internal vertex say w. let x pendant dx 1 vertex t. then either v px w v px w. latter case check px w px vk. b b b b b b b b u w v x b b b b b b b b u w v x v px w u px w px w px uk. is distance w pendant vertex less k. hence k radius contradiction. thus uv t. cannot another vertex center else c3 t contradiction. theorem 8.4.12. let g graph v g n. then following equivalent. 1. g tree. 2. g minimal connected graph n vertices. 3. g maximal acyclic graph n vertices. proof. ab. suppose g tree. minimal connected graph n vertices edge u v gu v connected. then theorem 8.4.10 u v cycle g. contradiction. bc. suppose g minimal connected graph n vertices. g cycle say γ select edge e γ. thus theorem 8.4.10 g e still connected graph n vertices contradiction fact g minimal connected graph n vertices. hence g acyclic. since g connected new edge e graph g e contains cycle hence g maximal acyclic graph. ca. suppose g maximal acyclic graph n vertices. g connected let g1 g2 two components g. select v1 g1 v2 g2 note g v1 v2 acyclic graph n vertices. contradicts g maximal acyclic graph n vertices. thus g connected acyclic hence tree. exercise 8.4.13. 1. show graph g tree pair vertices g unique path. 2. draw tree 8 vertices. label v t 1 . . . 8 vertex 2 adjacent exactly one element i 1. proposition 8.4.14. let tree. then graph g δg t 1 subgraph h t. proof. prove result induction n t. result trivially true n 1 2. so let result true every tree n 1 vertices take tree n vertices. also suppose g graph δg t 1.
160 chapter 8. graphs let v v t dv 1. take u v t uv et. now consider tree t1 v. then δg t 1 n 1 n 2. hence induction hypothesis g subgraph h h t1 map say φ. let h v h φh u. since δg t 1 h neighbor say h1 h1 vertex h vertex g. now map vertex v get required result. proposition 8.4.15. let tree n vertices. then n 1 edges. proof. proceed induction. take tree n 2 vertices delete edge e. then get two subtrees t1 t2 order n1 n2 respectively n1 n2 n. so et et1et2 e. induction hypothesis t t1t21 n1 1n2 11 n1 n2 1 n1. proposition 8.4.16. let g connected graph n vertices n 1 edges. then g acyclic. proof. contrary assume g cycle say γ. now select edge e γ note g e connected. go selecting edges g lie cycles keep removing them get acyclic graph h. since edges removed lie cycle graph h still connected. so deﬁnition h tree n vertices. thus proposition 8.4.15 eh n 1. but argument deleted least one edge hence eg n. gives contradiction eg n 1. proposition 8.4.17. let g acyclic graph n vertices n 1 edges. then g connected. proof. let possible g disconnected components g1 . . . gk k 2. g acyclic deﬁnition gi tree on say ni 1 vertices p 1kni n. thus g k p i1 ni 1 n k n 1 g k 2. contradiction. theorem 8.4.18. following equivalent graph order n. a g tree. b g minimal connected. c g maximal acyclic. d g acyclic g n 1. e g connected g n 1. proof. left exercise. exercise 8.4.19. let g graph n 2 vertices. g cn 1 2 g necessarily connected give if if condition connectedness graph exactly cn 1 2 edges. proposition 8.4.20. tree n 2 vertices least two pendant vertices.
8.4. trees 161 proof. let tree n vertices. then p vv t dv 2et 2n 1 2n 2. hence php least two vertices degree 1. deﬁnition 8.4.21. let tree vertices labeled n integers say n. pr ufer code pt sequence x size n 2 created following way. 1. find largest pendant vertex say v1. let u1 neighbor v1. put x1 u1. 2. let t1 v1 ﬁnd x2. 3. repeat procedure obtain x3 . . . xn 2. example 8.4.22. example consider tree figure 8.10. b b b b b b b 1 2 3 4 5 6 figure 8.10 tree 6 vertices then process proceeds follows. step pendant vi neighbor ui pt x1 x2 . . . ti vi 1 5 2 2 b b b b b b 1 2 3 4 6 2 4 2 22 b b bb b 1 2 3 6 3 3 2 222 b b b 1 2 6 4 2 6 2226 b b 1 6 exercise 8.4.23. process prove uj i j di 2. example 8.4.24. get back original tree sequence 2 2 2 6 ans yes. process getting back original tree follows. 1. plot points 1 2 . . . 6. 2. since ui either 2 6 implies 2 6 pendant vertices. hence pendant vertices must 1 3 4 5. thus algorithm implies largest pendant 5 must adjacent the ﬁrst element sequence 2. 3. step 1 vertex 5 deleted. hence v t1 1 2 3 4 6 given sequence 2 2 6. so pendants t1 1 3 4 vertex 4 largest pendant adjacent 2.
162 chapter 8. graphs 4. now v t2 1 2 3 6 sequence 2 6. so 3 adjacent 2. 5. now v t3 1 2 6 sequence 6. so pendants current 1 2 2 adjacent 6. 6. lastly v t4 1 6. process ends k2 two vertices left must adjacent. corresponding set ﬁgures follows. 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 proposition 8.4.25. let tree vertex set n. then dv 2 v appears pr ufer code pt . thus v v pt precisely pendant vertices t. proof. let dv 2. since process ends edge stage say i dv decreases strictly. thus till i 1th stage v adjacent pendant vertex w ith stage v deleted thus v appears sequence. conversely let v appear sequence kth stage ﬁrst time. then tree tk pendant vertex w highest label adjacent v. note tk w tree least two vertices. thus dv dtkv 2. exercise 8.4.26. prove pr ufer code vertex v appears exactly dv 1 times. hint v largest pendant adjacent w v pt w pt . proposition 8.4.27. let two trees vertex set integers. pt pt . proof. statement trivially true t 3. assume statement holds 3 t n. now let two trees vertex set n pt pt . pt pt set pendants. further largest labeled pendant w adjacent vertex x1 trees. thus ptw pt w hence induction hypothesis w w. thus pmi . proposition 8.4.28. let set n 3 integers x sequence length n 2 elements form s. then tree v t pt x. proof. verify statement t 3. now let statement hold trees n 3 vertices consider set n 1 integers sequence x length n 1 elements s.
8.4. trees 163 let v maxx s x x s v x x2 . . . xn 1. note v xi i x sequence elements s length n 2. s n induction hypothesis tree pt x. let tree obtained adding new pendant v vertex x1 . vertices xi 2 available pendants vertex x1 also available pendant. here xis may same. let r x s x x pendants . then set pendants r v x1 equals x s x x. thus v pendant maximum label. hence pt x. theorem 8.4.29. a. cayley 1889 quart. j. math let n 3. then nn2 diﬀerent trees vertex set n. proof. let f class trees vertex set n let g class n 2sequences n. note function f f g deﬁned ft pt pr ufer code oneone onto mapping. g nn2 required result follows. exercise 8.4.30. 1. find nonisomorphic trees order 7 less. 2. show every automorphism tree ﬁxes vertex edge. 3. give class trees γt 6. 4. let tree σ γt u v t σ2u u. edge u v t σu v 5. let tree center u radius r. let v satisfy du v r. show r pendant. 6. let tree t 2. let obtained deleting pendant vertices t. show center center . 7. let tree center u σ γt. show σu u. 8. possible tree γt 7 9. construct tree vertices 1 2 3 6 7 8 9 pt 6 3 7 1 2. 10. practice examples get pr ufer code tree get tree given code vertex set. 11. many trees following forms vertex set 100 b b b b b b . . . b b b b b b . . . b b b b b b . . . b b b b b 12. show tree least t leaves pendant edges. 13. let tree t1 t2 t3 subtrees t1 t3 t2 t3 and t1 t2 t3 . show t1 t2 .
164 chapter 8. graphs 14. let set subtrees tree t. assume trees nonempty pairwise intersection. show overall intersection nonempty. true replace graph g 15. recall connected graph g said unicyclic g exactly one cycle its subgraph. prove g g g unicyclic graph. 8.5 connectivity proposition 8.5.1. let g connected graph vertex set n. then vertices labeled way induced subgraph set i connected 1 i n. proof. n 1 nothing prove. assume statement true n k let g connected graph vertex set k. g tree pick pendant vertex label k. g cycle pick vertex cycle label k. case g k connected. now use induction hypothesis get required result. deﬁnition 8.5.2. separating set let g graph. then set x v g eg called separating set g x connected components g. let x separating set g. then there exists u v v g lie component g lie diﬀerent components g x. u v g separating set g u cut vertex. e eg separating set g bridgecut edge. example 8.5.3. 1. tree edge bridge nonpendant vertex cut vertex. true forest 2. graph k7 separating set vertices. k7 separating set edges must contain least 6 edges. deﬁnition 8.5.4. vertex connectivity graph g said kconnected g k g connected even deletion k 1 vertices. vertex connectivity κg non trivial graph g largest k g kconnected. convention κk1 0. example 8.5.5. 1. connected graph order one 1connected. 2. 2connected graph also 1connected graph. 3. disconnected graph κg 0 n 1 κkn n 1. 4. graph g figure 8.11 2connected 3connected. thus κg 2. b b b b b b b b b b b b figure 8.11 graph vertex connectivity 2 5. petersen graph 3connected.
8.5. connectivity 165 deﬁnition 8.5.6. edge connectivity graph g called ledge connected g 1 g f connected every f eg f l. greatest integer l g ledge connected edge connectivity g denoted λg. convention λk1 0. example 8.5.7. 1. note λpn 1 λcn 2 λkn n 1 whenever n 1. 2. let tree n vertices. then λt 1. 3. graph g figure 8.11 λg 3. 4. petersen graph g λg 3. exercise 8.5.8. let g 1. show κg g 1 g kn. say λg theorem 8.5.9. h. whitney 1932 graph g κg λg δg. proof. g disconnected g 1 nothing prove. so let g connected graph g 2. then vertex v dv δg. delete edges incident v graph disconnected. thus δg λg. suppose λg 1 g uv disconnected components cu cv. cu cv 1 g k2 κg 1. cu 1 delete u see κg 1. λg k 2 set edges say e1 . . . ek whose removal disconnects g. notice g e1 . . . ek1 connected graph bridge say ek uv. e1 . . . ek1 select end vertex u v. deletion vertices g results graph h uv bridge connected component. note κh 1. hence κg λg. exercise 8.5.10. give lower bound number edges graph g n vertices vertex connectivity κg k. theorem 8.5.11. chartrand harary 1968 integers a b c 0 b c exists graph κg a λg b δg c. proof. omitted scope book. theorem 8.5.12. mader 1972 every graph g average degree least 4k kconnected subgraph. proof. k 1 assertion trivial. so let k 2. note n g g 4k 2k 1 8.1 g 2kn 2k 3n k 1 1. 8.2 shall use induction show g satisﬁes equations 8.1 8.2 g k connected subgraph. n 2k1 2k3nk11 n2n1 2 1 nn1 2 . so g graph n vertices least nn1 2 edges hence g kn. thus kk1 kn g. assume n 2k. v vertex dv 2k 3 apply induction hypothesis g v get result. so let dv 2k 2 vertex v. g kconnected then
166 chapter 8. graphs nothing prove. assume possible g kconnected. then g g1 g2 g1 g2 k g1 g2 n. thus g1 v g2 g2 v g1 least one vertex edge them. degree vertices least 2k 2 g1 g2 2k 1. g1 g2 satisﬁes equation 8.2 using induction hypothesis result follows. otherwise gi2k 3gi k 1 1 2 hence gg1 g22k 3g1 g2 2k 2 2k 3n k 1 contradiction equation 8.2 hence required result follows. theorem 8.5.13. menger graph kedgeconnected k edge disjoint paths pairs vertices. graph kconnected k internally vertex disjoint paths pairs vertices. proof. omitted. 8.6 eulerian graphs deﬁnition 8.6.1. eulerian graph eulerian tour graph g closed walk v0 v1 . . . vk v0 edge graph appears exactly walk. graph g said eulerian eulerian tour. note deﬁnition disconnected graph eulerian. section graphs loops multiple edges. graphs closed walk traversing edge exactly named eulerian graphs due solution famous k onigsberg bridge problem euler 1736. problem follows city k onigsberg the present day kaliningrad divided 4 land masses river pregel. land masses joined 7 bridges see figure 8.12. question required one answer is way start land mass passes seven bridges figure 8.12 return back starting land mass euler rephrased problem along following lines let four land masses denoted vertices a b c graph let 7 bridges correspond 7 edges graph. then asked does graph closed walk traverses edge exactly once gave necessary suﬃcient condition graph closed walk thus giving negative answer k onigsberg bridge problem. one also relate problem problem starting certain point draw given ﬁgure pencil neither pencil lifted paper line repeated drawing ends initial point. theorem 8.6.2. euler 1736 connected graph g eulerian dv even v v g. proof. let g eulerian tour say v0 v1 . . . vk v0. then dv 2r v v0 v appears r times tour. also dv0 2r 1 v0 appears r times tour. hence dv always even.
8.6. eulerian graphs 167 b c figure 8.12 k onigsberg bridge problem conversely let g connected graph vertex even degree. let w v0v1 vk longest walk g without repeating edge it. vk even degree follows vk v0 otherwise w extended. w eulerian tour exists edge say e viw w vi1 vi1. case wvi vk v0v1 vi1vi longer walk contradiction. thus edge lying outside w hence w eulerian tour. proposition 8.6.3. let g connected graph exactly two vertices odd degree. then eulerian walk starting one vertices ending other. proof. let x two vertices odd degree let v symbol v v g. then graph h v h v g v eh eg xv yv vertex even degree hence theorem 8.6.2 h eulerian. let γ v v1 x . . . vk y v eulerian tour. then γ v eulerian walk required properties. exercise 8.6.4. let g connected eulerian graph e edge. show g e connected. ﬁnd eulerian tour algorithm start vertex v0 move via edge taken go deleting them. take edge whose deletion creates non trivial component containing v0. exercise 8.6.5. find eulerian tours following graphs. b b b b b b b b b b b b bb b b b b b b b b b bb b b b b b b b b b bb b b 8 1 2 7 3 6 4 5 9 16 12 15 13 11 14 10 b b b b b b b b b b b b b b b b b 3 2 1 4 9 8 7 6 5 13 11 12 10 theorem 8.6.6. finding eulerian tour previous algorithm correctly gives eulerian tour whenever given graph eulerian. proof. let algorithm start vertex say v0. now assume u h current graph c non trivial component h. thus dhu 0. assume
168 chapter 8. graphs deletion edge uv creates non trivial component containing v0. let cu cv components c uv containing u v respectively. ﬁrst claim u v0. fact u v0 h must vertices even degree dhv0 2. so c eulerian. hence c uv cannot disconnected contradiction c uv two components cu cv. thus u v0. moreover note vertices odd degree c u v0. now claim cu non trivial component. suppose cu trivial. then v0 cv contradiction assumption deletion edge uv creates nontrivial component containing v0. so cu non trivial. finally claim v0 cv. possible let v0 cu. then vertices c uv odd degree v cv v0 cu. hence c uv v0v connected graph vertex even degree. so theorem 8.6.2 graph c uv v0v eulerian. but cannot true vv0 bridge. thus v0 cv. hence cu newly created non trivial component containing v0. also vertex cu even degree hence theorem 8.6.2 cu eulerian. means take edge e incident u complete eulerian tour cu. so u take edge e place edge e create non trivial component containing v0. thus stage algorithm either u v0 path u v0. moreover non trivial connected component. algorithm ends must u v0. because seen above condition u v0 gives existence edge incident u traversed as dhu odd. hence u v0 algorithm cannot stop. thus algorithm stops u v0 components trivial. exercise 8.6.7. apply algorithm graphs exercise 8.6.5. also create connected graphs vertex even degree apply algorithm. deﬁnition 8.6.8. bipartite graph graph g v e said bipartite v v1 v2 v1 v2 1 v1 v2 and v1 0 v2. complete bipartite graph kmn shown below. notice kmn km kn. possible edges km kn exercise 8.6.9. 1. give necessary suﬃcient condition n kmn eulerian. 2. 8 persons room shake hands every person per following rules a handshakes take place sequentially.
8.7. hamiltonian graphs 169 b handshake except ﬁrst involve someone previous hand shake. c person involved 3 consecutive handshakes. way sequence handshakes conditions met 3. let g connected graph. then g eulerian graph edge set g partitioned cycles. 8.7 hamiltonian graphs deﬁnition 8.7.1. hamiltonian cycle g said hamiltonian contains vertices g. g hamiltonian cycle g called hamiltonian graph. finding nice characterization hamiltonian graph unsolved problem. example 8.7.2. 1. positive integer n 3 cycle cn hamiltonian. b b b b b b bb b b b b b b b b b b dodecahedron graph b b b b b b b b b b b b petersen graph figure 8.13 hamiltonian nonhamiltonian graph 2. graphs corresponding platonic solids hamiltonian. 3. petersen graph nonhamiltonian graph the proof appears below. proposition 8.7.3. petersen graph hamiltonian. proof. suppose petersen graph say g hamiltonian. also vertex g degree 3 hence g c10 m set 5 chords vertex appears endpoint. assume c10 1 2 . . . 10 1. now consider vertices 1 2 3. b bb bb bb bb bb bb bb bb bb b 1 2 3 4 5 6 7 8 9 10
170 chapter 8. graphs since gg 5 vertex 1 adjacent one vertices 5 6 7. hence 1 adjacent 5 third vertex adjacent 10 creates cycles length 3 4. similarly 1 adjacent 7 choice third vertex adjacent 2. so let 1 adjacent 6. then 2 must adjacent 8. case note choice third vertex adjacent 3. thus petersen graph nonhamiltonian. theorem 8.7.4. let g hamiltonian graph. then v g graph g s s components. proof. note removing k vertices cycle one create k connected components. hence required result follows. theorem 8.7.5. dirac 1952 let g graph g n 3 dv n2 v v g. then g hamiltonian. proof. let possible g disconnected. then g component say h v h k n2. hence dv k 1 n2 v v h. contradiction dv n2 v v g. now let p v1 v2 vk longest path g. since p longest path neighbors v1 vk p. claim exists v1 vi vi1 vk. otherwise vi v1 must vi1 vk. then nvk k 1nv1. hence nv1nvk k 1 n contradiction dv n2 v v g. so claim valid hence cycle p v1vivi1 vkvi1 v1 length k. prove p gives hamiltonian cycle. suppose not. then exists v v g v outside p v adjacent vj. note case p cannot path longest length contradiction. thus required result follows. theorem 8.7.6. ore 1960 let g graph n 3 vertices du dv n every pair nonadjacent vertices u v. then g hamiltonian. proof. exercise. exercise 8.7.7. let u v two vertices du dv g whenever uv eg. prove g hamiltonian g uv hamiltonian. deﬁnition 8.7.8. closure graph closure graph g denoted cg obtained repeatedly choosing pairs nonadjacent vertices u v du dv n adding edges them. proposition 8.7.9. closure g unique. proof. let k closure obtained adding edges e1 u1v1 . . . ek ukvk sequentially f closure obtained adding edges f1 x1y1 . . . fr xryr sequentially. let ei ﬁrst edge esequence appear fsequence. put h g e1 ei1. then ei uivi implies ei eh dhuidhvi n. also h subgraph f
8.7. hamiltonian graphs 171 hence df ui df vi n. moreover ei uivi f ei appear fsequence. thus f cannot closure therefore required result follows. exercise 8.7.10. let g graph n 3 vertices. 1. g cut vertex prove cg kn. 2. then prove generalization diracs theorem closure cg kn g hamiltonian. theorem 8.7.11. let d1 dn vertex degrees g. suppose that k n2 dk k condition dnk n k holds. then prove g hamiltonian. proof. show condition h cg kn. contrary assume exist pair vertices u v v g uv eg dhu dhv n 1. among pairs choose pair u v v g uv eh dhu dhv maximum. assume dhv dhu k say. clearly k n2. now let sv x v h xv eh x v su w v h wu eh w u. therefore assumption dhu dhv maximum among pair vertices u v uv eh dhu dhv n 1 implies sv n 1 dhv dhu k x sv dhx dhu k. so least k vertices h elements sv degrees k. also w su note choice pair u v implies dhw dhv n 1 dhu n 1 k n k. moreover su n 1 k. further condition dhu dhv n 1 dhv dhu k u su implies dhu n 1 dhv n 1 k n k. so n k vertices h degrees less n k. therefore d 1 d n vertex degrees h observe exists k n2 d k k d nk n k. k n2 di d i get contradiction. exercise 8.7.12. complete alternate proof theorem 8.7.11. let r denote property r if dk k dnk n k k n2. know g property. 1. let e edge g. show g e also property. closure h cg g 2. assume maxdu dv u v h adjacent n 2. let e edge h. h e property r ch e h e 3. view previous observations assume g edge maximal graph property r hamiltonian. cg g show k vertices degree k n k vertices degree less n k. contradict r deﬁnition 8.7.13. line graph line graph h graph g graph v h eg e1 e2 v h adjacent h e1 e2 share common vertexendpoint.
172 chapter 8. graphs example 8.7.14. verify following 1. line graph c5 c5. 2. line graph p5 p4. 3. line graph graph g contains complete subgraph size g. exercise 8.7.15. 1. let g connected eulerian graph. then show line graph g hamiltonian. converse true 2. say clique number line graph theorem 8.7.16. connected graph g isomorphic its line graph g cn n 3. proof. g isomorphic line graph g g. thus g unicyclic graph. let v1 v2 . . . vk vk1 v1 form cycle g. then line graph g contains cycle p v1v2 v2v3 . . . vkv1. claim dgvi 2. suppose let dgv1 3. so exists vertex u v2 vk u v1. case line graph g contains triangle v1v2 v1vk v1u p t. thus line graph unicyclic contradiction. exercise 8.7.17. 1. determine closure g. 2. show h hamiltonian. b bb bb bb bb bb b g b b b b b b b b b b b b b b b b b b b b b b b b h 3. give necessary suﬃcient condition m n n kmn hamiltonian. 4. show graph g g 3 gcn 1 2 2 hamiltonian. 5. show n 3 graph h g cn 1 2 1 hamil tonian. but prove graphs h admit hamiltonian path a path containing vertices h. 8.8 bipartite graphs deﬁnition 8.8.1. 2colorable graphs graph 2colorable its vertices colored two colors way adjacent vertices get diﬀerent colors. lemma 8.8.2. let p q two vwpaths g length p odd length q even. then g contains odd cycle.
8.9. matching 173 proof. suppose p q inner vertex x a vertex v w common. then one pv x px w odd length even say pv x odd. lengths qv x qx w odd consider xwpaths px w qx w otherwise consider paths pv x qv x. view argument may assume p q inner vertex common. case clear p q odd cycle. theorem 8.8.3. let g connected graph least two vertices. then following statements equivalent. 1. g 2 colorable. 2. g bipartite. 3. g odd cycle. proof. part 1 part 2. let g 2colorable. let v1 set red vertices v2 set blue vertices. clearly g bipartite partition v1 v2. part 2 part 1. color vertices v1 red color v2 blue color get required 2 colorability g. part 2 part 3. let g bipartite partition v1 v2. let v0 v1 suppose γ v0v1v2 vk v0 cycle. follows v1 v3 v5 v2. since vk v1 see k even. thus γ even length. part 3 part 2. suppose g odd cycle. pick vertex v. deﬁne v1 w path even length v w v2 w path odd length v w. clearly v v1. also g odd cycle implies v1 v2 . g connected w either v1 v2. let x v1. then even path pv x v x. xy eg vywalk odd length. deleting cycles walk odd vypath. thus v2. similarly x v2 xy e v1. thus g bipartite parts v1 v2. exercise 8.8.4. 1. 15 women men room. man shook hands exactly 6 women woman shook hands exactly 8 men. many men room 2. test whether graph bipartite not 8.9 matching deﬁnition 8.9.1. matching graph matching graph g independent set edges. maximum matching matching maximum number edges. vertex v saturated matching edge e m incident v. matching perfect matching every vertex saturated. example 8.9.2. 1. figure 8.14 m1 u1u2 matching. so m2 e e edge. set m3 u3u2 u4u7 also matching. set m4 u1u2 u4u5 u6u7
174 chapter 8. graphs b b b b b b b b u1 u2 u3 u4 u5 u6 u7 figure 8.14 graph also matching maximum why. give another maximum matching 2. non trivial graph g maximum matching. 3. vertices saturated m3 u2 u3 u4 u7. 4. graph perfect matching must even order. figure 8.14 cannot perfect matching. deﬁnition 8.9.3. alternating path let matching g. path p called m alternating edges alternately g m. malternating path two unmatched vertices end points called maugmenting. convention path length 1 malternating. example 8.9.4. consider figure 8.14. 1. path u1 u2 m1alternating. path length 2 m1alternating u1 u2 u3. 2. path u1 u2 u4 u7 m3alternating. but u2 u3 u4 u7 m3alternating. 3. path p u1 u2 u3 u4 u7 u6 m3alternating m3augmenting. gives us way get larger in size matching m5 using m3 throw away even edges p m3 add odd edges i.e. m5 m3 u2u3 u4u7 u1u2 u3u4 u7u6. theorem 8.9.5. berge 1957 matching maximum m augmenting path g. proof. let u1v1 . . . ukvk maximum matching. maugmenting path p p m m p larger matching contradiction. conversely suppose maximum. let mbe maximum matching. consider graph h v m. note dhv 2 vertex h. thus h collection isolated vertices paths cycles. since cycle contains equal number edges m path p contains number edges mthan m. then p maugmenting path. contradiction. exercise 8.9.6. ﬁnd maximum matching graph g. example 8.9.7. ﬁnd matching saturates vertices graph given below
8.9. matching 175 1 2 3 ans no. let x given graph take 1 2 3. matching ns least s. case graph. question ns least s x theorem 8.9.8. hall 1935 let g xy e bipartite graph. then matching saturates vertices x x ns s. proof. matching obviously s ns subset x. conversely suppose ns s x. let possible mbe maximum matching saturate x x. nx x y xy m. since mcannot extended must matched x1 x. consider nx x1. vertex y1 adjacent either x x1 edge m. condition mcannot extended implies y1 must matched x2 x. continuing above see process never stops thus g inﬁnitely many vertices true. hence msaturates x x. corollary 8.9.9. let g kregular k 1 bipartite graph. then g perfect matching. proof. let x two parts. since g kregular x y . let x e set edges end vertex s. ks e p vns dv kns. hence see x s ns thus halls theorem required result follows. deﬁnition 8.9.10. covering graph let g graph. then v g called covering g edge least one end vertex s. minimum covering g covering g minimum cardinality. exercise 8.9.11. 1. show graph g size minimum covering nαg. 2. characterize g terms its girth size minimum covering g 2. proposition 8.9.12. let g graph. matching k covering g m k. m k maximum matching k minimum covering.
176 chapter 8. graphs proof. deﬁnition proof ﬁrst statement trivial. prove second statement suppose m k maximum matching. let mbe matching g m m. then using ﬁrst statement k m m. thus maximum. covering must least m elements see k minimum covering. exercise 8.9.13. let g kn n 3. then determine 1. cardinality maximum matching 2. cardinality minimum covering converse proposition 8.9.12 necessarily true guess class graphs converse proposition 8.9.12 true theorem 8.9.14. konig 1931 let mbe maximum matching bipartite graph g let kbe minimum covering. then m k. proof. let v x y bipartition v let maximum matching. let u vertices x saturated let z set vertices reachable u malternating path. put z x z y k x s. then u z x y . also every vertex saturated as maximum matching ns t. further vertex v x matched vertex t. thus k t x s m. k covering edge xy g x s t contradiction ns t. thus k covering hence using k m proposition 8.9.12 get k m. furthermore proposition 8.9.12 also see k minimum covering. exercise 8.9.15. many perfect matchings labeled k2n 8.10 ramsey numbers recall group 6 persons either see 3 mutual friends see 3 mutual strangers. expressed using graphs reads let g v e graph v 6. then either k3 g k3 g. deﬁnition 8.10.1. ramsey number ramsey number rm n smallest natural number k graph g k vertices either km kn its subgraph. example 8.10.2. c5 k3 k3 its subgraph r3 3 5. but using ﬁrst paragraph section get r3 3 6 hence r3 3 6. known r3 4 9 see text harary table. proposition 8.10.3. let g graph 9 vertices. then either k4 g k3 g. proof. assume v 9. then need consider three cases. case i. vertex dv 4. then nac 4. vertices nac pairwise adjacent k4 g. otherwise two nonadjacent vertices say b c nac. case a b c induces graph k3.
8.11. degree sequence 177 case ii. vertex da 6. nahas k3 done. otherwise r3 3 6 implies nahas k3 vertices say b c d. case a b c induces graph k4. case iii. vertex degree 5. case possible p dv even. exercise 8.10.4. draw graph 8 vertices k3 k4 it theorem 8.10.5. erdos szekeres 1935 let m n n. then rm n rm 1 n rm n 1. proof. let p rm 1 n q rm n 1. now take graph g p q vertices take vertex a. da p nahas either subgraph km1 and km1 together gives km subgraph kn. otherwise nac q. case nachas either subgraph km subgraph kn1 kn1 together gives kn. 8.11 degree sequence deﬁnition 8.11.1. degree sequence degree sequence graph order n tuple d1 . . . dn d1 dn. nondecreasing sequence d1 . . . dn nonnegative integers graphic graph whose degree sequence d. exercise 8.11.2. show 1 1 3 3 graphic. theorem 8.11.3. fix n 1 natural numbers d1 dn. then d1 . . . dn degree sequence tree n vertices p di 2n 2. proof. d1 . . . dn degree sequence tree n vertices p di 2et 2n 1 2n 2. conversely let d1 dn sequence natural numbers p di 2n 2. use induction show d1 . . . dn degree sequence tree n vertices. n 1 2 result trivial. let result true n k let d1 dk k 2 natural numbers p di 2k 2. since p di 2k 2 must d1 1 dk 1. then note d 2 d2 d k1 dk1 d k dk 1 natural numbers p d 2k 12. hence induction hypothesis tree vertices 2 k 1 k degrees d is. now introduce new vertex 1 add edge 1 k get tree required degree sequence. theorem 8.11.4. havelhakimi 1962 degree sequence d1 . . . dn graphic sequence d1 d2 . . . dndn1 dndn 1 . . . dn1 1 graphic. proof. later sequence graphic introduce new vertex make adjacent vertices whose degrees dndn 1 . . . dn1 1. hence sequence d1 . . . dn graphic. now assume graphic g graph degree sequence d. let n vertex dn k suppose vertices 1 . . . k adjacent n g. also let deg1 minimum among deg1 . . . degk. deg1 degk 1 . . . degn 1
178 chapter 8. graphs g n required graph. so let degk 1 deg1. then k 1 neighbor v 1 n v 1. now consider graph g gk 1 v n k 1 1 v1 n. then g also its degree sequence better degree neighbors. note average increases least 1 k. obviously average cannot go beyond n 1. thus repeating procedure ﬁnite number times required result follows. exercise 8.11.5. 1. many diﬀerent degree sequences possible graph 5 vertices list degree sequences draw graph one. include connected disconnected graphs. 2. sequences graphic draw graph supply argument. a 2 2 3 4 4 5 b 1 2 2 3 3 4 c 22 36 42 2 2 3 3 3 3 3 3 4 4 3. two graphs degree sequence necessarily isomorphic 4. two graphs isomorphic necessary degree sequence 8.12 planar graphs deﬁnition 8.12.1. embedding planar graph graph said embedded surface drawn two edges intersect. graph said planar embedded plane. plane graph graph embedded plane. k5nonplanar k33nonplanar k4 k4 planar embedding figure 8.15 planar nonplanar graphs example 8.12.2. 1. tree embedable plane embedded one face exterior face. 2. cycle cn n 3 planar plane representation cn two faces. 3. planar embedding k4 given figure 8.15. 4. draw planar embedding k23. 5. draw planar embedding three dimensional cube. deﬁnition 8.12.3. face planar embedding consider planar embedding graph g. regions plane deﬁned embedding called facesregions g. unbounded faceregion called exterior face see figure 8.16.
8.12. planar graphs 179 15 3 4 5 1 2 6 8 9 10 11 12 7 13 14 f1 f2 f3 f4 planar graph x1 3 4 5 1 2 6 8 9 10 11 12 7 f1 f2 f3 f4 f5 f6 planar graph x2 figure 8.16 planar graphs labeled faces understand eulers theorem faces planar graph x1 corresponding edges listed below. face corresponding edges f1 9 8 8 9 8 2 2 1 1 2 2 7 7 2 2 3 3 4 4 6 6 4 4 5 5 4 4 12 12 4 4 11 11 10 10 13 13 14 14 10 10 8 8 9 f2 10 13 13 14 14 10 f3 4 11 11 10 10 4 f4 2 3 3 4 4 10 10 8 8 2 2 15 15 2 table observe edge x1 appears two faces. easily observed faces dont pendant vertices see faces f2 f3. faces f1 f4 edges incident pendant vertex. observe edges incident pendant vertex e.g. edges 2 15 8 9 1 2 etc. appear twice traversing particular face. observation leads proof eulers theorem planar graphs next result. theorem 8.12.4. euler formula let g connected plane graph f number faces. then g g f 2. 8.3 proof. use induction f. let f 1. then g cannot subgraph isomorphic cycle. if g subgraph isomorphic cycle planar embedding g f 2. therefore g tree hence g g f n n 1 1 2. so assume equation 8.3 true plane connected graphs 2 f n. now let g connected planar graph f n. now choose edge cutedge say e. then g e still connected graph. also edge e incident two separate faces hence its removal combine two faces thus g e n 1 faces. thus g g f g e g e 1 n g e g e n 1 2 using induction hypothesis. hence required result follows. lemma 8.12.5. let g plane bridgeless graph g2. then 2g3f. further g cycle length 3 then 2g4f.
180 chapter 8. graphs proof. edge put two dots either side edge. total number dots 2g. g cycle face least three edges. so total number dots least 3f. further g cycle length 3 2g4f. theorem 8.12.6. complete graph k5 complete bipartite graph k33 planar. proof. k5 planar consider plane representation it. equation 8.3 f 7. but lemma 8.12.5 one 20 2g3f 21 contradiction. k33 planar consider plane representation it. note c3. also eulers formula f 5. hence lemma 8.12.5 one 18 2g4f 20 contradiction. deﬁnition 8.12.7. subdivision homeomorphic let g graph. then subdivision edge uv g obtained replacing edge two edges uw wv w new vertex. two graphs said homeomorphic obtained graph sequence subdivisions. example m n n paths pn pm homeomorphic. similarly cyclic graphs homeomorphic cycle c3 study simple graphs. general one say cyclic graphs homeomorphic graph g v e v v e e e i.e. graph exactly one vertex loop. also note two graphs isomorphic also homeomorphic. figure 8.17 gives examples homeomorphic graphs diﬀerent path cycle. figure 8.17 homeomorphic graphs theorem 8.12.8. kuratowski 1930 graph planar subgraph homeomorphic k5 k33. proof. omitted. following observations directly follow kuratowski theorem. remark 8.12.9. 1. among simple connected nonplanar graphs a complete graph k5 minimum number vertices. b complete bipartite graph k33 minimum number edges. 2. nonplanar subgraph graph x x also nonplanar. deﬁnition 8.12.10. blocks graph let g graph. deﬁne relation edges g e1 e2 either e1 e2 cycle containing edges. note
8.12. planar graphs 181 equivalence relation. let ei equivalence class containing edge ei. also let vi denote endpoints edges ei. then induced subgraphs viare called blocks g. proposition 8.12.11. graph g planar blocks planar. proof. omitted. deﬁnition 8.12.12. maximal planar graph called maximal planar planar addition edges results nonplanar graph. maximal plane graph necessarily connected. proposition 8.12.13. g maximal planar graph edges n 3 vertices every face triangle 3n 6. proof. suppose face say f described cycle u1 . . . uk u1 k 4. then take curve joining vertices u1 u3 lying totally inside region f gu1u3 planar. contradicts fact g maximal planar. thus face triangle. follows 2m 3f. n m f 2 2m 3f 32 n m 3n 6. exercise 8.12.14. 1. suppose g plane graph n vertices face 4cycle. number edges g 2. show petersen graph subgraph homeomorphic k33. 3. show plane graph n 3 vertices 2n 5 bounded faces. 4. necessary plane graph g contain vertex degree less 5 5. let g plane graph n vertices edges f faces k components. prove induction n m f k 1. 6. g plane graph without 3cycles show δg 3. 7. show plane graph n 4 vertices least four vertices degree ﬁve. 8. produce planar embedding graph g appears figure 8.18. 1 2 3 8 6 5 4 7 figure 8.18 graph 8 vertices
182 chapter 8. graphs 8.13 vertex coloring deﬁnition 8.13.1. kcolorable graph g said kcolorable vertices assigned k colors way adjacent vertices get diﬀerent colors. chromatic number g denoted χg minimum k g kcolorable. theorem 8.13.2. every graph g χg g 1. proof. g 1 statement trivial. assume result true g n let g graph n 1 vertices. let h g 1. h g 1colorable d1 g vertex 1 given color neighbors. theorem 8.13.3. brooks 1941 every non complete graph odd cycle χg g. theorem 8.13.4. 5color theorem every planar graph 5colorable. proof. let g minimal planar graph n 6 vertices edges g 5colorable. then proposition 8.12.13 3n 6. so nδg 2m 6n 12 hence δg 2mn 5. let v vertex degree 5. note minimality g g v 5colorable. neighbors v use 4 colors v colored 5th color get 5coloring g. else take planar embedding neighbors v1 . . . v5 v appear clockwise order. let h gvi vj graph spanned vertices colored j. vi vj diﬀerent connected components h swap colors j component contains vi vertices v1 . . . v5 use 4 colors. thus above case graph g 5colorable. otherwise 1 3colored path v1 v3 similarly 2 4colored path v2 v4. possible graph g planar. hence every planar graph 5colorable. 8.14 adjacency matrix deﬁnition 8.14.1. adjacency matrix let g v e simple undirected graph vertices 1 . . . n. then adjacency matrix ag g or simply a described aij 1 i j e 0 otherwise. let h graph obtained relabeling vertices g. then note ah s1ags permutation matrix recall permutation matrix st s1. hence talk adjacency matrix graph worry labeling vertices g. give adjacency matrix say b. but
8.15. exercises 183 example 8.14.2. adjacency matrices 4cycle c4 path p4 4 vertices given below. ac4 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 ap4 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 . exercise 8.14.3. graph g connected exists permutation matrix p ag a11 0 0 a22 matrices a11 a22. theorem 8.14.4. i j entry b agk number ijwalks length k. proof. note deﬁnition matrix product bij x i1.ik1 aii1ai1i2 aik1ik. thus bij r r sequences i1 . . . ik1 aii1 aik1ik 1. is bij r r walks length k j. theorem 8.14.5. let g graph g order n. then g connected ag n1 entrywise positive. proof. put b let g connected. p ijpath length n 1 bn1 ij an1 ij 1. p i i1 . . . ik j ijpath length k n1 bii . . . biibii1 . . . bik1j 1 bii used n 1 k times. thus bn1 ij 0. conversely let bn1 ij 0. then corresponding summand bii1 . . . bin1j positive. throwing entries form bii 1 i n expression expression corresponds ijpath length n 1. bn1 entrywise positive follows g connected. want put vertexedge incidence matrix 1 0 1 incidence matrix so results matrix tree theorem stated. need give proofs. deﬁnition 8.14.6. vertexedge incidence matrix vertexedge incidence matrix g g gmatrix whose i jentry described mij 1 edge ej incident vi 0 else. 8.15 exercises exercise 8.15.1. 1. graph size minimum covering g 2. characterize g size minimum covering g 1. 3. relationship size minimum covering αg
184 chapter 8. graphs 4. necessary plane graph g contain vertex degree 5 5. k5 e planar e edge 6. k33 e planar e edge 7. true group 7 persons 3 mutual friends 4 mutual strangers 8. provedisprove two colorable graph necessarily planar. 9. draw tree vertex set 12 whose pr ufer code 9954449795. 10. game thief played following way coin. n participants. one participant takes coin passes whoever heshe wishes to. whoever coin must pass somebody other person heshe received it quickly possible. music stops person coin thief. class students there 115 total playing it. 2009 passes music stopped. guarantee person started thief. how able understand 11. many chordal graphs vertex set 4 12. count diameter many nonisomorphic trees order 7 13. list automorphisms following graph. b b b b b b b b 1 2 3 4 5 6
bibliography 1 g. agnarson r. greenlaw graph theory modelling applications algorithm pearson education. 2 r. b. bapat graphs matrices hindustan book agency new delhi 2010. 3 j. cofman catalan numbers classroom elem. math. 52 1997 108 117. 4 d. m. cvetkovic michael doob horst sachs spectra graphs theory applications academic press new york 1980. 5 d. i. a. cohen basic techniques combinatorial theory john wiley sons new york 1978. 6 william dunham euler master us all published distributed math ematical association america 1999. 7 f. harary graph theory addisonwesley publishing company 1969. 8 g. e. martin counting art enumerative combinatorics undergraduate texts mathematics springer 2001. 9 r. merris combinatorics 2th edition wileyinterscience 2003. 10 j. riordan introduction combinatorial analysis john wiley sons new york 1958. 11 r. p. stanley enumerative combinatorics vol. 2 cambridge university press 1999. 12 h. s. wilf generatingfunctionology academic press 1990. 185
index n m graph 146 cn r 93 cn 111 147 e 146 eg 146 g v 149 malternating 174 maugmenting 174 nv 146 ngv 146 pn r 91 pn n1 . . . nk 93 pn 146 sn r 106 v 146 v g 146 g 149 γg 156 γb30dgγb30d 146 haub 149 δg 149 dom f 7 αs bα 29 κg 164 λg 165 a 44 πn 109 πnk 109 rng f 7 αs bα 29 axxa 29 f1 . . . fn g 67 b 55 ab 55 dv 146 dgv 146 fa 8 kcolorable 182 kconnected 164 kfactor 149 kplace predicate 71 kregular 149 kth diﬀerence 132 ledge connected 165 nset 90 p implies q 59 rcombination 93 rpermutation 90 rsequence 90 rm n 176 sn k 109 uv walk 151 xbound part 72 xn 109 acyclic 153 addition rule 89 adequate 63 adjacency matrix 182 adjacent 146 algebraic 45 algebraic expansion 96 antichain 46 antisymmetric 46 argument 57 assignment 58 atom 85 atomic formula 58 72 atomic variable 58 186
index 187 automorphism 156 axiom choice 51 basic solution 134 bell numbers 139 bijection 9 bipartite 168 blocks 181 boolean algebra 83 boolean homomorphism 86 boolean isomorphism 86 bound 72 bounded lattice 81 bridge 158 cardinal numbers 44 cartesian product 5 151 catalan number 111 cauchy product 122 center 152 chain 46 characteristic equation 133 chord 153 chordal 153 chromatic number 182 circular permutation 97 clique 154 clique number 154 closed 152 closure 170 coprime 17 codomain set 7 coin problem money changing problem 130 commutative ring unity 25 comparable 46 complement 5 81 150 complemented 81 complete 81 146 complete bipartite graph 147 component 154 composite 19 composition 31 conclusion 67 congruent 21 conjugate 128 conjunctionand 58 conjunctive normal form 65 connected 154 connected component 154 connected permutation 131 connectives 60 contradiction f 61 contrapositive 60 converse 60 countable 43 covering 175 cubic 149 cut edge 158 cut vertex 157 cycle graph 147 cyclecircuit 152 degree 146 degree sequence 177 derangement 120 diameter 152 dictionary 47 diﬀerence equation 132 direct product 79 disconnected 154 disjoint 6 disjoint union 150 disjunctionor 58 disjunctive normal form 65 distance 152 distributive lattice 77 divide 17 divisor 17 domain f 7 domain set 7 dual 66 durfee square 131
188 index edge connectivity 165 edge density 154 edge set 145 edges 145 egf 123 embedded 178 end vertices 145 end vertices p 152 enumeration 43 equivalence class 33 equivalence relation 31 equivalent 9 61 73 euclids algorithm 17 euclids algorithm 18 euclids lemma 19 eulers totient function 120 eulerian 166 eulerian tour 166 exponential generating function 123 exterior face 178 facesregions 178 falling factorial 91 109 family ﬁnite character 52 family sets 29 ferrers diagram 128 fibonacci sequence 132 ﬁeld 23 ﬁnite 10 ﬁrst diﬀerence 132 forest 157 formal power series 122 diﬀerentiation 125 integration 125 formal power series 122 free 72 free boolean algebra 84 frobenius number 130 function identity 36 function 7 functionally complete 63 fundamental theorem arithmetic 20 general graph 145 girth 152 graph invariant 156 graphic 177 greatest common divisor 17 greatest lower bound glb 49 hamel basis 53 hamiltonian 169 hasse diagram 48 height 46 homeomorphic 180 homogeneous recurrence relation 132 hypergraph 146 hypothesispremise 67 identity relation 31 image 7 incident 145 independence number 155 independent 146 induced 149 induced partial order 52 85 inﬁnite 10 inﬁnite cardinal numbers 45 initial condition 132 initial segment a 50 injection 8 internal vertices 152 interpretation 72 interpretation f 72 intersection 6 29 150 inverse 7 83 isolated 146 isomorphic 155 join 151 lattice 77 lattice homomorphism 80
index 189 lattice isomorphism 80 lattice ntuples 0 1 79 lattice path 110 least common multiple 20 least upper bound lub 49 length 151 lexicographic 47 lhrrcc 132 line graph 171 linearcompletetotal order 46 linearly ordered set 46 literal 65 lnhrrcc 132 logical conclusion 67 loop 145 lower bound 49 matching 173 maximal 49 154 maximal planar 181 maximum 49 maximum matching 173 minimal 49 154 minimum 49 minimum covering 175 multigraph 146 multiplication rule 89 multiplicative 121 multiset 92 negation 58 neighbor 146 newtons identity 95 nonhomogeneous recurrence relation 132 ogf 123 oneone 8 onto 9 orbit 98 orbit size 98 order 146 order operations 67 ordinary generating function 123 partial order 46 partially ordered set 46 partition 105 partition n k parts 109 partition s 105 path 151 path graph 146 pendant 146 159 perfect matching 173 permutation 90 petersen graph 148 pigeonhole principle php 115 planar 178 plane graph 178 polish notation 67 poset 46 pr ufer code 161 preimage 7 prime 19 principal connective 66 principle duality 84 principle transﬁnite induction 50 product 51 propositional function 71 pseudograph 145 quantiﬁers 71 quotient 16 radius 152 ramsey number 176 range f 7 reciprocal 124 recurrence relation 132 reﬂexive 31 relation 7 relation x 31 relatively prime 17 remainder 16 residue 21
190 index restriction f a 8 rising factorial 109 rotation 98 satisﬁable 67 saturated matching 173 scope 72 self conjugate 128 selfcomplementary 156 separating set 164 set cartesian product 5 complement 5 composition relations 31 diﬀerence 6 equality 5 equivalence class 33 equivalence relation 31 identity relation 31 intersection 6 power set 6 subset 5 symmetric diﬀerence 6 union 6 set diﬀerence 6 simple graph 146 single valued 7 solution 132 spanning subgraph 149 squarefree 26 squarefree 88 standard representative 21 stirling number ﬁrst kind 109 stirling numbers second kind 106 stirlings identity 138 subdivision 180 subgraph 149 subset 5 substitution instance 62 surjection 9 symmetric 31 symmetric diﬀerence 6 tautology t 61 trail 151 transcendental 45 transitive 31 tree 157 trivial graph 146 truth function 61 truth table 59 truth value 58 uncountable 43 union 6 29 150 unit 19 unity 19 universe discourses 71 upper bound 49 usual partial order 46 valid 73 vertex connectivity 164 vertex set 145 vertexedge incidence matrix 183 vertices 145 well formed formulae wﬀ 60 well order 50 width 46 wilsons theorem 24 word expansion 96
multivariable calculus oliver knill math 21a fall 2012 notes contain condensed two pages per lecture notes essential information only. remaining space ﬁlled problems.
harvard multivariable calculus math 21a fall 2012
math 21a multivariable calculus oliver knill fall 2012 1 geometry distance point plane two coordinates p x y. point space de termined three coordinates p x y z. signs coordinates deﬁne 4 quadrants plane 8 octants space. regions intersect origin 0 0 0 0 0 separated coordinate axes y 0 x 0 coordinate planes x 0 y 0 z 0 . 1 describe location points p 1 2 3 q 0 0 5 r 1 2 3 words. possible answer p 1 2 3 positive octant space coordinates positive. point r 0 0 5 negative z axis. point 1 2 3 xyplane. projected onto xyplane ﬁrst quadrant. 2 problem. find midpoint p 1 2 5 q 3 4 7. answer. midpoint obtained taking average coordinate p q2 1 3 6. euclidean distance two points p x y z q a b c space deﬁned dp q q x a2 y b2 z c2. deﬁnition euclidean distance motivated pythagorean theorem. 1 3 find distance dp q points p 1 2 5 q 3 4 7 verify dp m dq m dp q. answer distance dp q 42 22 22 24. distance dp m 22 12 12 6. distance dq m 22 12 12 6. indeed dp m dm q dp q. circle radius r centered p a b collection points plane distance r p. sphere radius ρ centered p a b c collection points space distance ρ p. equation sphere xa2yb2zc2 ρ2. 4 point 3 4 5 outside inside sphere x22 y 62 z 22 16 answer distance point center sphere 1 4 9 smaller 4 radius sphere. point inside. 1it appears appendix geometry discours de la m ethode 1637 ren e descartes 1596 1650. descartes descartes secret notebook amir aczel.
completion square equation x2 bx c 0 idea add b22 c sides get x b22 b22 c. solving x gives solution x b2 q b22 c. 2 5 solve 2x2 10x 12 0. answer. equation equivalent x2 5x 6. adding 522 sides gives x 522 14 x 2 x 3. 6 find center sphere x2 5x y2 2y z2 1. answer complete square get x 522 254 y 12 1 z2 1 x 522 y 12 z2 522. see sphere center 52 1 0 radius 52. alkhwarizai rene descartes distance spheres 7 find set points p x y z space satisfy x2 y2 9. answer cylinder radius 3 around zaxes parallel axis. 8 x2 y2 z2. answer set points distance z axes equal distance xyplane. must cone. 9 find distances p 12 5 3 xyplane. answer 3. find distance p 12 5 0 z axes. answer 13. 10 describe x2 2x y2 16y z2 10z 54 0. answer complete square get sphere x 22 y 82 z 52 36 center 2 8 5 radius 6. 11 describe set xz 0. answer either must x 0 z 0. set union two coordinate planes. 12 find equation set points distance 1 1 1 0 0 0. answer x 12 y 12 z 12 x2 y2 z2 gives 2x 1 2y 1 2z 1 0 2x 2y 2z 3. equation plane. 13 find distance spheres x2 y122z2 1 x32y2z42 9. answerthe distance centers 32 42 122 13. distance spheres 13 3 1 9. 2due alkhwarizmi 780850 compendium calculation completion reduction book the mathematics egypt mesopotamiachina india islam source book ed victor katz contains translations work.
math 21a multivariable calculus oliver knill fall 2012 2 vectors dot product two points p a b c q x y z space deﬁne vector v x a b z c. points p q write also v pq. real numbers numbers p q r vector v v1 v2 v3are called components v. similar deﬁnitions hold two dimensions vectors two components. vectors drawn everywhere space two vectors components considered equal. 1 addition two vectors u v u1 u2 u3 v1 v2 v3 u1 v1 u2 v2 u3v3. scalar multiple λ u λu1 u2 u3 λu1 λu2 λu3. diﬀerence u v best seen addition u 1 v. addition scalar multiplication vectors satisfy laws know arithmetic. commutativity u v v u associativity u v w u v w r s v r s v well distributivity rs v vrs r v w r vr w is scalar multiplication. length v vector v pq deﬁned distance dp q p q. vector length 1 called unit vector. 1 3 4 5 3 4 12 13. examples unit vectors i j k 1 35 45and 313 413 1213. vector length 0 zero vector 0 0. dot product two vectors v a b cand w p q ris deﬁned v w ap bq cr. dot product determines distance distance determines dot product. proof lets write v v proof. using dot product one express length v v v v. hand v w v w v v w w 2v w allows solve v w v w v w2 v2 w22 . cauchyschwarz inequality tells v w v w. proof. assume w 1 scaling equation. plug v w equation 0 vawvaw get 0 vvwwvvww v2vw22vw2 v2vw2 means v w2 v2. cauchyschwarz inequality allows us deﬁne angle is. angle two nonzero vectors deﬁned unique α 0 π satisﬁes v w v w cosα. 1we cover 2300 years math pythagoras 500 bc al kashi 1400 cauchy 1800 hamilton 1850.
al kashis theorem triangle abc side lengths a b c angle α opposite c satisﬁes a2 b2 c2 2ab cosα. proof. deﬁne v ab w ac. c2 v w2 v w v w v2 w2 2 v w know v w v w cosα c2 v2 w2 2 v w cosα a2 b2 2ab cosα. triangle inequality tells u v u v proof u v2 u v u v u2 v22 u v u2 v22 u v u2 v22 u v u v2. two vectors called orthogonal perpendicular v w 0. zero vector 0 orthogonal vector. example v 2 3is orthogonal w 3 2. pythagoras theorem v w orthogonal v w2 v2 w2. proof v w v w v v w w 2 v w v v w w. quod erat demonstrandum. 2 vector p v v w w2 w called projection v onto w. scalar projection v w w plus minus length projection v onto w. vector b v p v vector orthogonal w. 2 find projection v 0 1 1onto w 1 1 0. ansser p v 12 12 0. 3 wind force f 2 3 1is applied car drives direction vector w 1 1 0. find projection f onto w force accelerates slows car. answer w f w w2 52 52 0. 4 visualize dot product answer diﬃcult task lets try. absolute value dot product length projection. dot product positive v w form acute angle negative angle obtuse. 5 given v 2 1 2and w 3 4 0. find vector plane deﬁned v w bisects angle two vectors. answer. normalize two vectors make unit vectors add get 13 17 1015. 6 given two vectors v w perpendicular. condition v w perpendicular v w answer find dot product v w v w set zero. 7 angle 1 2 3and 15 2 4acute obtuse answer dot product 1. cute 2short qed. proven pythagoras alkashi. distance angle deﬁned deduced.
math 21a multivariable calculus oliver knill fall 2012 3 cross product cross product two vectors v v1 v2and w w1 w2in plane scalar v1w2 v2w1 det v1 v2 w1 w2 . cross product two vectors v v1 v2 v3and w w1 w2 w3in space deﬁned vector v w v2w3 v3w2 v3w1 v1w3 v1w2 v2w1. 1 remember write product determinant j k v1 v2 v3 w1 w2 w3 v2 v3 w2 w3 j v1 v3 w1 w3 k v1 v2 w1 w2 iv2w3 v3w2 jv1w3 v3w1 kv1w2 v2w1. 1 cross product 1 2and 4 5is scalar 5 8 3. 2 cross product 1 2 3and 4 5 1is vector 13 11 3. cross product v w anticommutative. re sulting vector orthogonal v w. proof. verify example v v w 0 look deﬁnition. sin formula v w v w sinα. proof verify lagranges identity v w2 v2 w2 v w2 direct computation. now v w v w cosα. absolute value respectively length v w deﬁnes area parallelo gram spanned v w. 1it hamilton described 1843 ﬁrst multiplication of 4 vectors. contains intrinsically dot cross product 0 v1 v2 v3 0 w1 w2 w3 vw v w.
deﬁnition shot ﬁts intuition w sinα height parallelogram base length v. area change rotate vectors around space length angle stay same. area also linear vectors v w. make v twice long area gets twice large. v w zero exactly v w parallel v λ w real λ. proof. follows immediately sin formula fact sinα 0 α 0 α π. cross product therefore used check whether two vectors parallel not. note v v also considered parallel even sometimes one calls antiparallel. trigonometric sinformula a b c side lengths triangle α β γ angles opposite a b c a sinα b sinβ c sinγ. proof. area triangle ab sinγ bc sinα ac sinβ divide ﬁrst equation sinγ sinα get one identity. divide second equation sinα sinβ get second identity. 3 v a 0 0and w b cosα b sinα 0 v w 0 0 ab sinαwhich length ab sinα. scalar u v w u v w called triple scalar product u v w. number u v w de ﬁnes volume parallelepiped spanned u v w orientation three vectors sign u v w. deﬁnitions ﬁt intuition value h u n n height parallelepiped n v w normal vector ground parallelogram area n v w. volume parallelepiped ha u n n v w simpliﬁes u n u v w indeed absolute value triple scalar product. vectors v w v w form right handed coordinate system. ﬁrst vector v thumb second vector w pointing ﬁnger v w third middle ﬁnger right hand. 4 problem find volume cuboid width length b height c. answer. cuboid parallelepiped spanned a 0 00 b 0and 0 0 c. triple scalar product abc. 5 problem find volume parallelepiped vertices 1 1 0 p 2 3 1 q 4 3 1 r 1 4 1. answer ﬁrst see spanned vectors u 1 2 1 v 3 2 1 w 0 3 1. get v w 1 3 9and u v w 2. volume 2. 6 problem ﬁnd equation ax cz plane contains point p 1 2 3 well line passes q 3 4 4 r 1 1 2. ﬁnd vector n a b cnormal noting x op n 0. answer normal vector n 1 2 2 a b cof plane ax cz obtained cross product pq rq n op 1 2 2 1 2 3 3 get equation x 2y 2z 3.
math 21a multivariable calculus oliver knill fall 2012 4 lines planes point p p q r vector v a b cdeﬁne line l p q r ta b c r . line obtained adding multiple vector v vector op p q r. every vector contained line necessarily parallel v. think parameter time. 0 p 1 op v. restricted parameter interval s u l p q rta b c u line segment connecting rs ru. 1 problem. get line p 1 1 2 q 2 4 6. solution. v pq 1 3 4we get get l x y z 1 1 2t1 3 4 rt 1t 13t 24t. since x y z 1 1 2t1 3 4consists three equations x 12t 13t z 24t solve get x 12 y 13 z 24. line r op t v deﬁned p p q r vector v a b cwith nonzero a b c satisﬁes symmetric equations x p q b z r c . proof. expressions equal t. symmetric equations modiﬁed bit one two numbers a b c zero. 0 replace ﬁrst equation x p b 0 replace second equation q c 0 replace third equation z r. 2 find symmetric equations line two points p 0 1 1 q 2 3 4 solution. ﬁrst ﬁrst form parametric equations x y z 0 1 1 t2 2 3 x 2t 1 2t z 1 3t solve get x2 y 12 z 13. 3 problem find symmetric equation z axes. answer situation b 0 c 1. symmetric equations simply x 0 0. two numbers a b c zero coordinate plane. one numbers zero line contained coordinate plane. point p two vectors v w deﬁne plane σ op t v s w t real numbers . 4 example σ x y z 1 1 2 t2 4 6 s1 0 1. called para metric description plane.
plane contains two vectors v w vector n v w orthogonal v w. also vector pq oq op perpendicular n q p n 0. q x0 y0 z0 p x y z n a b c means ax cz ax0by0 cz0 d. plane therefore described single equation ax cz d. shown equation plane x x0 t v s w ax cz a b c v w obtained plugging x0. 5 problem find equation plane contains three points p 1 1 1 q 0 1 1 r 1 1 3. answer plane contains two vectors v 1 2 0and w 2 2 2. n 4 2 2and equation 4x 2y 2z d. constant obtained plugging coordinates point left. case 4x 2y 2z 4. 6 problem find angle planes x 1 x z 2. answer ﬁnd angle n 1 1 0and 1 1 1. arccos2 6. finally lets look distance functions. distance p σ n x containing q dp σ p q n n . proof. project pq onto n. distance p line l dp l p q u u . proof area parallelogram spanned pq u divided base length u. lines l rt q t u st p t v distance dl m p q u v u v . proof. project pq onto n u v. distance two planes n x n x e dσ π ed n . proof p ﬁrst q second plane distance scalar projection pq onto n. note pq n e. 7 regular tetrahedron vertices points p1 0 0 3p2 0 8 1 p3 6 2 1 p4 6 2 1. find distance two edges intersect.
math 21a multivariable calculus oliver knill fall 2012 lecture 5 functions function two variables fx y rule assigns two numbers x third number fx y. example function fx y x2y2x assigns 3 2 number 322 6 24. domain func tion set points f deﬁned range fx y x y d . graph fx y sur face x y fx y x y d space graphs allow visualize functions. 1 graph fx y q 1 x2 y2 domain x2 y2 1 half sphere. range interval 0 1. set fx y c const called contour curve level curve f. example fx y 4x2 3y2 level curves f c ellipses c 0. collection contour curves fx y c called contour map f. 2 fx y x2 y2 set x2 y2 0 union lines x x y. curve x2 y2 1 made two hyperbola noses point 1 0 1 0. curve x2 y2 1 consists two hyperbola noses 0 1 0 1. 3 fx y x2 y2ex2y2 ﬁnd explicit expressions con tour curves x2 y2ex2y2 c. draw curves com puter function three variables gx y z assigns three variables x y z real num ber gx y z. visualize contour surfaces gx y z c c constant. helpful look traces intersections surfaces coordinate planes x 0 0 z 0. 4 gx y z zfx y level surface g 0 graph z fx y function two variables. example gx y z z x2 y2 0 graph z x2 y2 function fx y x2 y2 paraboloid. surfaces gx y z c graphs.
5 fx y z polynomial fx x x quadratic x f c quadric. sphere paraboloid plane x2 y2 z2 1 x2 y2 c z ax cz one sheeted hyperboloid cylinder two sheeted hyperboloid x2 y2 z2 1 x2 y2 r2 x2 y2 z2 1 ellipsoid hyperbolic paraboloid elliptic hyperboloid x2a2 y2b2 z2c2 1 x2 y2 z 1 x2a2 y2b2 z2c2 1
math 21a multivariable calculus oliver knill fall 2012 lecture 6 curves parametrization planar curve map rt xt ytfrom parameter interval r a b plane. functions xt yt called coordinate functions. image parametrization called parametrized curve plane. parametrization space curve rt xt yt zt. image r parametrized curve space. rhtl always think parameter time. ﬁxed t vector xt yt ztin space. varies end point vector moves along curve. parametrization contains information curve curve. tells also fast direction trace curve. 1 parametrization rt 1 3 cost 3 sintis circle radius 3 centered 1 0 2 rt cos3t sin5tdeﬁnes lissajous curve example. 3 xt t yt ft curve rt t fttraces graph function fx. example fx x2 1 graph parabola. 4 xt 2 cost yt sint rt follows ellipse xt24 yt2 1. 5 space curve rt t cost sint ttraces helix increasing radius. 6 xt cos2t yt sin2t zt 2t curve parameteri zation changed. 7 xt cost yt sint zt t traced opposite direction. 8 rt cost sint 0.1cos17t sin17twe example epicycle circle turns circle. used ptolemaic geocentric system predated copernican system still using circular orbits modern keplerian system planets move ellipses derived newtons laws.
rt xt yt ztis curve r t xt yt zt x y zis called velocity time t. length rt called speed v v called direction motion. vector r t called acceleration. third derivative r called jerk. vector parallel r t called tangent curve rt. addition rule one dimension f g f g scalar multiplication rule cf cf leibniz rule fg f g fg chain rule fg f gg generalize vector valued functions component single variable rule. process diﬀerentiation curve reversed using fundamental theorem calculus. rt r0 known ﬁgure rt integration rt r0 r 0 rs ds. assume know acceleration at rt times well initial velocity position r0 r0. rt r0t r0 rt rt r 0 vs ds vt r 0 as ds. free fall case acceleration constant. particular rt 0 0 10 r0 0 1000 2 r0 0 0 h rt 0 1000t h 2t 10t22. rt f constant rt r0 t r0 ft22. 50 100 150 200 250 2 2 4 6 8 10
math 21a multivariable calculus oliver knill fall 2012 7 arc length curvature a b 7 rt curve velocity r t speed r t l r b r t dt called arc length curve. space length l r b q xt2 yt2 zt2 dt. 1 arc length circle radius r given rt r cost r sintparameterized 0 t 2π 2π speed rt constant equal r. answer 2πr. 2 helix rt cost sint thas velocity r t sint cost 1and constant speed r t sint cost 1 2. 3 arc length curve rt t logt t22 1 t 2 answer rt 1 1t t rt q 1 1 t2 t2 1 t l r 2 1 1 dt logt t2 2 2 1 log2 2 12. curve constructed way arc length computed call opportunity. 4 find arc length curve rt 3t2 6t t3from 1 3. 5 arc length curve rt cos3t sin3t 0 t 2π answer rt 3 q sin2t cos4t cos2t sin4t 32 sin2t. therefore r 2π 0 32 sin2t dt 6. 6 find arc length rt t22 t33for 1 t 1. cubic curve satisﬁes y2 x389 example elliptic curve. speed rt t2 t4. r x 1 x2 dx 1x2323 arc length integral evaluated r 1 1 t 1 t2 dx 2 r 1 0 1 t2 dt 21 t23231 0 22 2 13. 7 arc length epicycle rt t sint costparameterized 0 t 2π. rt q 2 2 cost. l r 2π 0 q 2 2 cost dt. substitution 2u gives l r π 0 q 2 2 cos2u 2du r π 0 q 2 2 cos2u 2 sin2u 2du r π 0 q 4 cos2u 2du 4 r π 0 cosu du 8. 8 find arc length catenary rt t cosht cosht et et2 hyperbolic cosine 1 1. cosh2t2 sinh2t 1 sinht et et2 hyperbolic sine. answer r 1 1 cosht dt 2 sinh1. parameter change ts corresponds substitution integration change integral immediately
arc length independent parameterization curve. 9 circle parameterized rt cost2 sint2on 0 2π velocity r t 2tsint cost speed 2t. arc length still r 2π 0 2t dt t2 2π 0 2π. 10 always closed formula arc length curve. length lissajous ﬁgure rt cos3t sin5tleads r 2π 0 q 9 sin23t 25 cos25t dt needs evaluated numerically. deﬁne unit tangent vector tt r t r t unit tangent vector. curvature curve point rt deﬁned κt t r t. curvature magnitude acceleration vector rt traces curve constant speed 1. large curvature point means curve turns sharply. unlike acceleration velocity curvature depend parameterization curve. see curvature feel acceleration. curvature depend parametrization. proof. st parametrization chain rule ddtt st stst ddtrst rstst. see s cancels r. especially curve parametrized arc length meaning velocity vector rt length 1 κt t t. measures rate change unit tangent vector. 11 curve rt t ft graph function f velocity r t 1 f t unit tangent vector tt 1 f t q 1 f t2. simpliﬁcation get κt t r t f t q 1 f t2 3 example ft sint κt sint q 1 cos2t 3 . rt curve nonzero speed t deﬁne tt r t r t unit tangent vector nt t t normal vector bt tt nt binormal vector. plane spanned n b called normal plane. perpendicular curve. plane spanned n called osculating plane. diﬀerentiate tt tt 1 get t tt 0 see nt perpendicular tt. b automatically normal n shown three vectors tt nt bt unit vectors orthogonal other. useful formula curvature κt r t r t r t3
math 21a multivariable calculus oliver knill fall 2012 8 polar spherical coordinates point x y plane polar coordinates r x2 y2 θ arctgyx. relation x y r cosθ r sinθ. θ r x formula θ arctgyx deﬁnes angle θ addition π. points x y x y θ value. get correct θ one choose arctanyx π2 π2 π2 x y positive axes add π left plane including negative axes. curve given polar coordinates rθ fθ called polar curve. cartesian coordinates described rt ft cost ft sint. 1 describe curve r θ cartesian coordinates. solution formal substitution gives x2 y2 arctanyx better. remember curve rt r cost r sint exactly relation r t. curve spiral. 2 curve r 2 sinθ solution lets ignore absolute value moment look r2 2r sinθ. written x2 y2 2y x2 y2 2y 1 1. curve circle radius 1 centered 0 1. since absolute value radius θ θ π add circle radius 1 centered 0 1. represent points space x y z r cosθ r sinθ z speak cylindrical coordinates. surfaces described cylindrical coordinates 3 r 1 cylinder 4 r z double cone 5 θ 0 half plane 6 r θ rolled sheet paper 7 r 2 sinz example surface revolution.
spherical coordinates use distance ρ origin well two angles θ φ. ﬁrst angle θ polar angle polar coordinates xy coordinates φ angle vector op zaxis. relation x y z ρ cosθ sinφ ρ sinθ sinφ ρ cosφ . two important ﬁgures see connection. distance z axes r ρ sinφ height z ρ cosφ read oﬀby left picture rzplane coordinates x r cosθ r sinθ seen right picture xyplane. x φ ρ r x ρ cosθ sinφ ρ sinθ sinφ z ρ cosφ x θ r level surfaces described spherical coordinates 8 ρ 1 sphere 9 surface φ π2 single cone 10 surface sinθ cosφ plane. 11 ρ φ apple shaped surface 12 ρ 2 cos3θ sinφ example bumpy sphere. 13 write x2 y2 5x z2 cylindrical coordinates answer r2 5r cosθ z2. 14 match surfaces ρ sin3φ ρ sin3θ spherical coordinates ρ θ φ. helps see rz plane xy plane.
math 21a multivariable calculus oliver knill fall 2012 9 parametrized surfaces beside implicit equation gx y z 0 parametrization second fundamentally diﬀerent way describe surface. parametrization surface vectorvalued function ru v xu v yu v zu v xu v yu v zu v three functions two variables. two parameters u v involved map r plane space also called uvmap. parametrized surface image uvmap. domain uvmap called parameter do main. ﬁrst parameter u kept constant v 7 ru v curve surface. similarly constant v map u 7 ru v traces curve surface. curves called grid curves. computer draws surfaces using grid curves. world parametric surfaces intriguing. explored help computer. keep mind following 4 important examples. cover wide range cases. planes. parametric rs t op s v t w implicit ax cz d. parametric implicit ﬁnd normal vector n v w. implicit parametric ﬁnd two vectors v w normal vector n. example ﬁnd three points p q r surface forming u pq v pr.
ii spheres parametric ru v a b c ρ cosu sinv ρ sinu sinv ρ cosv. implicit x a2 y b2 z c2 ρ2. parametric implicit read oﬀthe radius center implicit parametric ﬁnd center a b c radius r possibly completing square. iii graphs parametric ru v u v fu v implicit z fx y 0. parametric implicit think z fx y implicit parametric use x parameterizations. iv surfaces revolution parametric ru v gv cosu gv sinu v implicit x2 y2 r gz written x2 y2 gz2. parametric implicit read oﬀthe function gz distance zaxis. implicit parametric again function g key link. 1 describe surface ru v v5 cosu v5 sinu v u 0 2π v r. solu tion. surface revolution. r v5 z5. draw rzplane. see r small origin making surface pointy like needle tip. 2 find parametrization plane contains three points p 3 7 1q 6 2 1 r 0 3 4. solution. take rs t op qp rp. rs t 3 3s 3t 7 5s 4t 1 3t. 3 parametrize lower half ellipsoid x24 y29 z225 1 z 0. solution. one solution solve z write graph ru v u v q 25 25u24 25v29. also deform sphere rθ φ 2 sinφ cosθ 3 sinφ sinθ 5 cosφ. 4 parametrize upper half hyperboloid x2 y24 z2 1. solution. round hyperboloid r2 z2 1 given rθ z z2 1 cosθ z2 1 sinθ z. deform get rθ z z2 1 cosθ 2 z2 1 sinθ z. 5 describe surface ru v 2 sin13u sin17vcosu sinv sinu sinv cosv. solution. looks bit like hedgehog. example bumpy sphere. radius ρ function angles. spherical coordinates ρ 2 sin13θ sin17φ.
math 21a multivariable calculus oliver knill fall 2012 10 continuity function fx y domain r continuous point a b r fx y fa b whenever x y a b. function f continuous r f continuous every point a b r. 1 a fx y x2 y4 xy siny sin sin sin sinx2 continuous entire plane. built functions continuous using addition multiplication composition functions continuous everywhere. 2 fx y 1x2y2 continuous everywhere except origin deﬁned. 3 fx y sinxx continuous away x 0. every point 0 y discontin uous. f1n y y 1 f1n y y 1 n . 4 fx y sin1x y continuous except line x 0. 5 fx y x4 y4x2 y2 continuous 0 0. divide x2 y2 see function equivalent x2 y2 away 0 0. deﬁning f0 0 0 see function continuous. 6 three reasons function discontinuous jump diverge inﬁnity oscillate. example jump appears fx sinxx pole gx 1x leads vertical asymptote function going inﬁnity. example function discontinuous due oscillations hx sin1x. graph devils comb. prototypes one dimensions jump fx sinxx 5 5 1.0 0.5 0.5 1.0 diverge gx 1x 1.0 0.5 0.5 1.0 10 5 0 5 10
oscillate hx sin1x 1.0 0.5 0.5 1.0 1.0 0.5 0.5 1.0 one mixtures phenomena like function 3 sinxx sin1x jumps also oscillatory problem x 0. 1.0 0.5 0.5 1.0 4 2 0 4 two handy tools discover discontinuities 1 use polar coordinates coordinate center point analyze function. 2 restrict function one dimensional curves check continuity curve one function one variables. 7 determine whether function fx y sinx2y2 x2y2 continuous 0 0. solution use polar coordinates write sinr2r2 continuous 0 apply lhopital twice want verify this. 8 function fx y x2y2 x2y2 continuous 0 0 solution use polar coordinates see cos2θ. see value depends angle only. arbitrarily close 0 0 function takes value 1 1. 9 function fx y x2y x4y2 continuous solution. look line x2 get function x42x4 12x2. continuous 0. example real shocker continuous line origin ax fx ax ax3x4 a2x2 axx2 a2 goes zero x 0 long 0. 0 however 0 f 0x4 continuously extended x 0 too. 10 function fx y xy2y3 x2y2 solution. use polar coordinates write r3 sin2θcosθ sinθr2 r sin2θcosθ sinθ shows function con verges 0 r 0. 11 function fx y sinx2y2 x2y2 continuous everywhere solution. use polar coordinates see sinr2r2. function continuous 0 hˆ opitals theorem.
math 21a multivariable calculus oliver knill fall 2012 11 partial derivatives fx y function two variables xfx y deﬁned derivative function gx fx y considered constant. called partial derivative f respect x. partial derivative respect deﬁned similarly. also write fxx y xfx y. fyx x yf. 1 1 fx y x4 6x2y2 y4 fxx y 4x3 12xy2 fxx 12x2 12y2 fyx y 12x2y 4y3 fyy 12x2 12y2 see fxx fyy 0. function satisﬁes equation also called harmonic. equation fxx fyy 0 example partial diﬀerential equation unknown function fx y involving partial derivatives. vector fx fyis called gradient. clairauts theorem fxy fyx continuous fxy fyx. proof look equations without taking limits ﬁrst. extend deﬁnition say background planck constant h positive fxx y fx h y fx yh. h 0 deﬁne fx before. compare two sides ﬁxed h 0 hfxx y fx h y fx y h2fxyx y fxh yhfxh y h fx h y fx y hfyx y fx h fx y. h2fyxx y fx h h fx h y fx h fx y limits taken. established identity holds h 0 discrete derivatives fx fy satisfy fxy fyx. quantum clairaut theorem. classical derivatives fxy fyx continuous limit h 0 leads classical clairauts theorem. quantum clairaut theorem holds functions fx y two variables. even continuity needed. 2 find fxxxxxyxxxxx fx sinx x6y10 cosy. answer compute think. 3 continuity assumption fxy necessary. example fx y x3yxy3 x2y2 contradicts clairauts theorem fxx y 3x2y y3x2 y2 2xx3y xy3x2y22 fx0 y y fxy0 0 1 fyx y x3 3xy2x2 y2 2yx3y xy3x2 y22 fyx 0 x fyx0 0 1. equation unknown function fx y involves partial derivatives respect least two diﬀerent variables called partial diﬀerential equation. derivative respect one variable appears called ordinary diﬀerential equation. 1xf yf introduced carl gustav jacobi. josef lagrange used term partial diﬀerences.
examples partial diﬀerential equations. know ﬁrst 4 well. 4 wave equation fttt x fxxt x governs motion light sound. function ft x sinx t sinx t satisﬁes wave equation. 5 heat equation ftt x fxxt x describes diﬀusion heat spread epi demic. function ft x 1 tex24t satisﬁes heat equation. 6 laplace equation fxx fyy 0 determines shape membrane. function fx y x3 3xy2 example satisfying laplace equation. 7 advection equation ft fx used model transport wire. function ft x ext2 satisfy advection equation. 8 eiconal equation f 2 x f 2 1 used see evolution wave fronts optics. function fx y x2 y2 satisﬁes eiconal equation. 9 burgers equation ft ffx fxx describes waves beach break. function ft x x 1 ex24t 11 ex24t satisﬁes burgers equation. 10 kdv equation ft 6ffx fxxx 0 models water waves narrow channel. function ft x a2 2 cosh2 2x a2t satisﬁes kdv equation. 11 schr odinger equation ft i h 2mfxx used describe quantum particle mass m. function ft x eikx h 2m k2t solves schr odinger equation. here i2 1 imaginary h planck constant h 1034js. graphs solutions equations. match pdes
math 21a multivariable calculus oliver knill fall 2012 14 linearization linear approximation function fx point linear function lx fa f ax a . ylhxl yfhxl graph function l close graph f near a. generalize higher dimensions linear approximation fx y a b linear function lx y fa b fxa bx a fya by b . linear approximation function fx y z a b c lx y z fa b c fxa b cx a fya b cy b fza b cz c . using gradient fx y fx fyrsp. fx y z fx fy fz linearization written l x f x0f a x a. keeping second variable b ﬁxed get onedimensional situation variable x. fx b fa bfxa bxa linear approximation. similarly x x0 ﬁxed single variable fx0 y fx0 y0 fyx0 y0y y0. knowing linear approximations x variables get general linear approximation fx y fx0 y0 fxx0 y0x x0 fyx0 y0y y0. please avoid notion diﬀerentials. relict old times. 1 linear approximation function fx y sinπxy2 point 1 1 fxx y yfx y πy2 cosπxy2 2yπ cosπxy2 point 1 1 equal f1 1 π cosπ 2π cosπ π 2π. 2 linearization used estimate functions near point. previous example 0.00943 f10.01 10.01 l10.01 10.01 π0.012π0.013π 0.00942 . 3 find linear approximation fx y z xy yz zx point 1 1 1. since f1 1 1 3 fx y z y z x z x f1 1 1 2 2 2. lx y z f1 1 1 2 2 2 x 1 1 z 1 3 2x 1 2y 1 2z 1 2x 2y 2z 3. 4 estimate f0.01 24.8 1.02 fx y z exyz. solution take x0 y0 z0 0 25 1 fx0 y0 z0 5. solution.the gradient
fx y z exyz exz2y exy. point x0 y0 z0 0 25 1 gra dient vector 5 110 5. linear approximation lx y z fx0 y0 z0 fx0 y0 z0xx0 yy0 zz0 55 110 5x0 y25 z1 5xy105z2.5. approximate f0.01 24.8 1.02 55 110 50.01 0.2 0.02 50.050.02 0.10 5.13. actual value f0.01 24.8 1.02 5.1306 close estimate. 5 find tangent line graph function gx x2 point 2 4. solution tangent line level curve linearlization lx y fx y yx2 0 passes point. compute gradient a b f2 4 g2 1 4 1 forming ax 4x d 4 2 1 4 4. answer 4x 4 . 6 barth surface deﬁned level surface f 0 fx y z 3 5t1 x2 y2 z222 x2 y2 z22 8x2 t4y2t4x2 z2y2 t4z2x4 2x2y2 y4 2x2z2 2y2z2 z4 5 12 constant called golden ratio. replace 1t 5 12 see surface middle. 1 see right surface fx y z 8. find tangent plane later surface point 1 1 0. solution ﬁnd level curve linearization computing gradient f1 1 0 64 64 0. surface x constant d. plugging point 1 1 0 see x 2. 7 quartic surface fx y z x4 x3 y2 z2 0 called piriform. equation tan gent plane point p 2 2 2 pair shaped surface solution. get a b c 20 4 4and equation plane 20x 4y 4z 56 obtained constant right plugging point x y z 2 2 2.
math 21a multivariable calculus oliver knill fall 2012 15 chain rule f g functions one variable t single variable chain rule tells dtfgt f gtgt . example ddt sinlogt coslogtt. proven linearizing functions f g verifying chain rule linear case. chain rule useful ﬁnd derivatives like arccosx write 1 ddx cosarccosx sinarccosx arccosx q 1 sin2arccosx arccosx 1 x2 arccosx arccosx 1 1 x2. 1 derive using implicit diﬀerentiation derivative ddx arctanx. solution. sin cos cos sin cos2x sin2x 1. follows 1 tan2x 1 cos2x. therefore ddx tanarctanx 1 cos2arctanx tanx x 1 cos2x 11 tan2x tanx 11 x2. deﬁne gradient fx y fxx y fyx y fx y z fxx y z fyx y z fzx y z. rt curve f function several variables build function 7f rt one variable. similarly rt parametrization curve plane f function two variables 7f rt function one variable. multivariable chain rule dtf rt f rt rt . proof. written two dimensions dtfxt yt fxxt ytxt fyxt ytyt . now identity fxthythfxtyt h fxthythfxtyth h fxtythfxtyt h holds every h 0. left hand side converges dtfxt yt limit h 0 right hand side fxxt ytxt fyxt ytyt using single variable chain rule twice. proof later diﬀerentiate f respect treated constant f xth fxt h f xt xthxt fxt xthxt xthxt h .
write ht xthxt ﬁrst part right hand side. fxt h fxt h fxt h fxt h xt h xt h . h 0 also h 0 ﬁrst part goes f xt second factor xt. 2 move circle rt cost sinton table temperature distribution fx y x2 y3. find rate change temperature fx y 2x 3y2 rt sint costddtf rt t rt rt 2 cost 3 sint2 sint cost 2 cost sint 3 sin2t cost. fx y 0 one express function x. dd fx yx f 1 yx fx fyy 0 obtain implicit diﬀerentation y fxfy. even so know yx compute derivative implicit diﬀerentiation works also three variables. equation fx y z c deﬁnes surface. near point fz zero surface described graph z zx y. compute derivative zx without actually knowing function zx y. so consider ﬁxed parameter compute using chain rule fxx y zx y1 fzx yzxx y 0. leads following implicit diﬀerentiation zxx y fxx y zfzx y z zyx y fyx y zfzx y z 3 surface fx y z x2 y24 z29 6 ellipsoid. compute zxx y point x y z 2 1 1. solution zxx y fx2 1 1fz2 1 1 429 18. 4 chain rule relate diﬀerentiation rules answer. chain rule universal implies single variable diﬀerentiation rules like addition product quotient rule one dimensions fx y x y x ut vt ddtx y fxu fyv u v. fx y xy x ut vt ddtxy fxu fyv vu uv. fx y xy x ut vt ddtxy fxu fyv uy vuv2. 5 one prove chain rule linearization verifying linear functions solution. yes one dimensions chain rule follows linearization. f linear function fx y ax c curve rt x0 tu y0 tvparametrizes line. dtf rt dtax0 tu by0 tv au bv dot product f a b r t u v. since chain rule refers derivatives functions agree point chain rule also true general functions. 6 mechanical systems determined energy function hx y function two variables. ﬁrst variable x position second variable momentum. equations motion curve rt xt ytare called hamilton equations xt hyx y yt hxx y homework verify energy hamiltonian system preserved every path rt xt ytsolving system hxt yt const.
math 21a multivariable calculus oliver knill fall 2012 16 gradient tangent gradient fx y fxx y fyx yor fx y z fxx y z fyx y z fzx y zin three dimensions important object. symbol is spelled nabla named egyptian harp. following theorem important provides crucial link calculus geometry. gradient theorem gradients orthogonal level curves level surfaces. proof. every curve rt level curve level surface satisﬁes dtf rt 0. chain rule f rt perpendicular tangent vector rt. true every curve gradient perpendicular surface. gradient theorem useful example allows get tangent planes tangent lines faster tangent plane x0 y0 z0 level surface fx y z axbycz d fx0 y0 z0 a b cand obtained plugging point. statement two dimensions completely analog. 1 find tangent plane surface 3x2y z2 4 0 point 1 1 1. solution fx y z 6xy 3x2 2z. f1 1 1 6 3 2. plane 6x3y2z constant. ﬁnd constant plugging point get 6x3y2z 11. 2 problem reﬂect ray rt 1 t t 1at surface x4 y2 z6 6. solution rt hits surface time 2 point 1 2 1. velocity vector ray v 1 1 0the normal vector point f1 2 1 4 4 6 n. reﬂected vector r v 2proj n v v . proj n v 8684 4 6. therefore reﬂected ray w 4174 4 6 1 1 0.
lecture 16 tangent spaces 1 lets compute tangent line π 0 curve sinx directly determining slope making sure line goes point. 2 look fx y sinx 0. find gradient fπ 0 a bof f π 0. ﬁnd tangent line again. 3 find tangent plane surface x2 y2 z2 1 point 2 3 2. 4 find line perpendicular surface x2 y2 z2 1 point 2 3 2
math 21a multivariable calculus oliver knill fall 2012 17 extrema important problem multivariable calculus extremize function fx y two vari ables. one dimensions order look maxima minima consider points derivative zero. point a b called critical point fx y fa b 0 0. critical points candidates extrema critical points directional derivatives d vf f v zero. increase value f moving direction. 1 1 find critical points fx y x4 y4 4xy 2. gradient fx y 4x3 y 4y3 xwith critical points 0 0 1 1 1 1. 2 fx y sinx2 y y. gradient fx y 2x cosx2 y cosx2 y 1. critical points must x 0 cosy 1 0 means π k2π. critical points .0 π 0 π 0 3π . 3 graph fx y x2 y2ex2y2 looks like volcano. gradient f 2x 2xx2 y2 2y 2yx2 y2ex2y2 vanishes 0 0 circle x2 y2 1. function inﬁnitely many critical points. 4 function fx y y22g cosx energy pendulum. variable g con stant. f y g sinx 0 0for x y . . . π 0 0 0 π 0 2π 0 . . . points angles pendulum rest. 5 function fx y x y diﬀerentiable ﬁrst quadrant. critical points there. function minimum 0 0 domain gradient f deﬁned. one dimension needed f x 0 f x 0 local minimum f x 0 f x 0 local maximum. f x 0 f x 0 critical point undetermined could maximum like fx x4 minimum like fx x4 ﬂat inﬂection point like fx x3. let fx y function two variables critical point a b. deﬁne fxxfyy f 2 xy. called discriminant critical point. 1this deﬁnition include points f derivative deﬁned. usually assume functions nice.
remark remembered better knowing determinant hessian matrix h fxx fxy fyx fyy . second derivative test. assume a b critical point fx y. 0 fxxa b 0 a b local minimum. 0 fxxa b 0 a b local maximum. 0 a b saddle point. case 0 need higher derivatives determine nature critical point. 6 function fx y x33xy33y graph looks like napkin. gradient fx y x21 y21. 4 critical points 1 11 11 1 1 1. hessian matrix includes partial derivatives h 2x 0 0 2y . 1 1 4 saddle point 1 1 4 fxx 2 local maximum 1 1 4 fxx 2 local minimum. 1 1 4 saddle point. function local maximum local minimum well 2 saddle points. determine maximum minimum fx y domain determine critical points interior domain compare values maxima minima boundary. see next time look extrema boundary. 7 find maximum fx y 2x2 x3 y2 1. fx y 4x 3x2 2y critical points 43 0 0 0. hessian hx y 4 6x 0 0 2 . 0 0 discriminant 8 saddle point. 43 0 discriminant 8 h11 43 43 0 local maximum. also look boundary 1 function gx fx 1 2x2 x3 1. since gx 0 x 0 43 0 local minimum 43 local maximum line 1. comparing f43 0 f43 1 shows 43 0 global maximum.
math 21a multivariable calculus oliver knill fall 2012 18 lagrange multipliers aim ﬁnd maxima minima function fx y presence constraint gx y 0. necessary condition critical point gradients f g parallel otherwise move along curve g increase f. directional derivative f direction tangent level curve zero tangent vector g perpendicular gradient f tangent vector. system equations fx y λgx y gx y 0 three unknowns x y λ called lagrange equations. variable λ lagrange multiplier. lagrange theorem extrema fx y curve gx y c either solutions lagrange equations critical points g. proof. condition f parallel g either means f λg f 0 g 0. case f 0 included lagrange equation case λ 0. 1 minimize fx y x2 2y2 constraint gx y x y2 1. solution lagrange equations 2x λ 4y λ2y. 0 x 1. 0 divide second equation get 2x λ 4 λ2 showing x 1. point x 1 0 solution. 2 find shortest distance origin curve x6 3y2 1. solution minimize function fx y x2 y2 constraint gx y x6 3y2 1. gradients f 2x 2y g 6x5 6y. lagrange equations f λg lead system 2x λ6x5 2y λ6y x6 3y2 1 0. get λ 13 x x5 either x 0 1 1. constraint equation g 1 obtain q 1 x63. so solutions 0 q 13 1 0 1 0. see minimum evaluate f points. see 0 q 13 minima.
3 cylindrical soda cans height h radius r minimal surface ﬁxed volume solution volume v r h hπr2 1. surface area ar h 2πrh 2πr2. x hπ r need optimize fx y 2xy 2πy2 constrained gx y xy2 1. calculate fx y 2y 2x 4πy gx y y2 2xy. task solve 2y λy2 2x 4πy λ2xy xy2 1. ﬁrst equation gives yλ 2. putting second one gives 2x 4πy 4x 2πy x. third equation ﬁnally reveals 2πy3 1 12π13 x 2π2π13. means h 0.54. r 2h 1.08. 4 curve gx y x2 y3 function fx y x obviously minimum 0 0. lagrange equations f λg solutions. case minimum solution gx y 0. remarks. 1 either two properties equated lagrange theorem equivalent f g 0 dimensions 2 3. 2 gx y 0 lagrange equations also written fx y λ 0 fx y λ fx y λgx y. 3 either two properties equated lagrange theorem equivalent g λf f critical point. 4 constrained optimization problems work also higher dimensions. proof same extrema f x constraint g x c either solutions lagrange equations f λg g c points g 0. 5 find extrema fx y z z sphere gx y z x2 y2 z2 1. solution compute gradients fx y z 0 0 1 gx y z 2x 2y 2z solve 0 0 1 f λg 2λx 2λy 2λz x2 y2 z2 1. case λ 0 excluded third equation 1 2λz ﬁrst two equations 2λx 0 2λy 0 give x 0 0. 4th equation gives z 1 z 1. minimum south pole 0 0 1 maximum north pole 0 0 1. 6 dice shows k eyes probability pk. introduce vector p1 p2 p3 p4 p5 p6 p1 p2 p3 p4 p5 p6 1. entropy p deﬁned f p p6 i1 pi logpi p1 logp1 p2 logp2 . p6 logp6. find distribution p maximizes entropy constrained g p p1 p2 p3 p4 p5 p6 1. solution f 1 logp1 . . . 1 logpn g 1 . . . 1. lagrange equations 1 logpi λ p1.p6 1 get pi eλ1. last equation 1 p expλ1 6 expλ1 ﬁxes λ log161 pi 16. fair dice maximal entropy. maximal entropy means least information content. unfair dice provides additional information allows cheating gambler casino gain proﬁt. 7 assume probability physical chemical system state k pk energy state k ek. nature minimizes free energy fp1 . . . pn p ipi logpi eipi energies ei ﬁxed. probability distribution pi satisfying p pi 1 minimizing free energy called gibbs distribution. find distribution general ei given. solution f 1 logp1 e1 . . . 1 logpn en g 1 . . . 1. lagrange equation logpi 1 λ ei pi expeic c exp1 λ. constraint p1 pn 1 gives cp expei 1 c 1 p eei. gibbs solution pk expek p expei. 1 1this example appears book rufus bowen lecture notes math 470 1978
math 21a multivariable calculus oliver knill fall 2012 20 global extrema determine maximum minimum fx y domain determine critical points interior domain compare values maxima minima boundary. solve extrema problems constraints without constraints. point a b called global maximum fx y fx y fa b x y. example point 0 0 global maximum function fx y 1 x2 y2. similarly call a b global minimum fx y fa b x y. 1 function fx y x4y42x22y2 global maximum global minimum yes ﬁnd them. solution function global maximum. seen restricting function xaxis fx 0 x4 2x2 function without maximum. function four global minima however. located 4 points 1 1. best way see note fx y x2 12 y 12 2 minimal x2 1 y2 1. 2 find maximum fx y 2x2 x3 y2 1. solution. fx y 4x 3x2 2y critical points 43 0 0 0. hessian hx y 4 6x 0 0 2 . 0 0 discriminant 8 saddle point. 43 0 discriminant 8 h11 43 43 0 local maximum. also look boundary 1 function gx fx 1 2x2 x3 1. since gx 0 x 0 43 0 local minimum 43 local maximum line 1. comparing f43 0 f43 1 shows 43 0 global maximum. 3 find extrema function fx y x3 y3 3x 12y 20 plane characterize them. ﬁnd global maximum global minimum among them solu tion. critical points satisfy fx y 0 0or 3x2 3 3y2 12 0 0. 4 critical points x y 1 2. discriminant fxxfyy f 2 xy 36xy fxx 6x. point fxx classiﬁcation value 12 72 6 maximum 38 1 2 72 6 saddle 6 1 2 72 6 saddle 34 1 2 72 6 minimum 2 global maxima global minima function takes arbitrarily large small values. 0 function gx fx 0 x3 3x 20 satisﬁes limxgx . ignore following qa safely. might answer questions. 1. global extrema always exist yes region compact meaning every sequence xn yn pick subsequence converges . equivalent domain closed bounded.
bolzanos extremal value theorem. every continuous function compact domain global maximum global minimum. 2. critical points important critical points relevant physics represent conﬁgurations lowest energy. many physical laws describe extrema. new ton equations mrt2 v rt 0 describing particle mass moving ﬁeld v along path γ 7 rt equivalent property path extremizes length sγ r b mrt22 v rt dt among paths. 3. second derivative test true assume fx y critical point 0 0 quadratic function satisfying f0 0 0. ax2 2bxy cy2 ax b ay2 c b2 y2 aa2 db2 x b ay b b2a2 discriminant d. see fxx 0 0 c b2a 0 function positive values x y 0 0. point 0 0 minimum. fxx 0 0 c b2a 0 function negative values x y 0 0 point x y local maximum. 0 f takes negative positive values near 0 0. general function approximate quadratic one. 4. something cool critical points yes assume fx y height island. assume ﬁnitely many critical points nonzero determi nant. label critical point 1 maximum minimum 1 saddle point. sum numbers 1 independent island. 1 5 avoid lagrange substitution extremize fx y constraint gx y 0 ﬁnd yx second equation extremize single variable problem fx yx. extremize fx y x2 y2 1 example need extremize 1 x2. diﬀerentiate get critical points also look cases x 1 x 1 actual minima maxima occur. general also substitution. 6 second derivative test lagrange second derivative test designed using second directional derivative direction tangent. instead make list critical points pick maximum minimum. 7 lagrange also work constraints two constraints constraint g c h deﬁnes curve. gradient f must plane spanned gradients g h otherwise could move along curve increase f. formula tion three dimensions. extrema fx y z constraint gx y z c hx y z either solutions lagrange equations f λg µh g c h solutions g 0 fx y z µh h solutions h 0 f λg g c solutions g h 0. 8 fxx appear second derivative test . natural. dis criminant determinant deth matrix h fxx fxy fyx fyy . 0 sign fxx sign trace fxx fyy coordinate independent too. deter minant product λ1λ2 eigenvalues h trace sum eigenvalues. 9 mean discriminant deﬁned also points critical point. number k d1 f22 called gaussian curvature surface. critical points k d. curvature remarkable quantity since depends intrinsic geometry surface way surface embedded space. 2 10 2. derivative test higher dimensions yes. one form second deriva tive matrix h look eigenvalues h. eigenvalues negative local maximum eigenvalues positive local minimum. general eigenvalues diﬀerent signs saddle point type. 1this follows poincarehopf theorem. 2this theorema egregia gauss.
math 21a multivariable calculus oliver knill fall 2012 21 double integrals integral r r r fx y dxdy deﬁned limit riemann sum 1 n2 x n j nr f n j n n . 1 integrate fx y xy unit square sum riemann sum ﬁxed jn get y2. perform integral get 14. example shows reduce double integrals single variable integrals. 2 fx y 1 integral area region r. integral limit lnn2 ln number lattice points in jn inside r. 3 integral r r r fx y dxdy signed volume solid graph f region r x y plane. volume xyplane counted negatively. fubinis theorem allows switch order integration rectangle function f continuous r b r c fx y dxdy r c r b fx y dydx. proof. every n quantum fubini identity x nab x j n cd f n j n x j ncd x n ab f n j n holds functions. divide sides n2 take limit n . type region form r x y x b cx y dx . integral region called type integral zz rf da z b z dx cx fx y dydx . b chxl dhxl
type ii region form r x y c y d ay x by . integral region called type ii inte gral zz rf da z c z by ay fx y dxdy . c ahyl bhyl 4 integrate fx y x2 region bounded sinx3 bounded graph sinx3 0 x π. value integral physical meaning. called moment inertia. z π13 0 z sinx3 sinx3 x2 dydx 2 z π13 0 sinx3x2 dx solved substitution 2 3 cosx3π13 0 4 3 . 5 integrate fx y y2 region bound x axes lines x 1 1 x. problem best solved type integral. would compute 2 diﬀerent integrals type integral. bounds x 1 x 1 y z 1 0 z 1y y1 y3 dx dy 2 z 1 0 y31y dy 21 4 1 3 1 10 . 6 let r triangle 1 x 0 0 y x. z z r ex2 dxdy type ii integral r 1 0 r 1 ex2 dxdy solved ex2 antiderivative terms elemen tary functions. type integral r 1 0 r x 0 ex2 dy dx however solved z 1 0 xex2 dx ex2 2 1 0 1 e1 2 0.316. .
math 21a multivariable calculus oliver knill fall 2012 lecture 21 polar integration 1 area disc radius r z r r z r2x2 r2x2 1 dydx z r r 2 r2 x2 dx . integral solved substitution x r sinu dx r cosu z π2 π2 2 q r2 r2 sin2ur cosu du z π2 π2 2r2 cos2u du . using double angle formula get r2 r π2 π2 2 1cos2u 2 du r2π. see better polar coordinates. polar region region bound simple closed curve given polar coordinates curve rt θt. cartesian coordinates parametrization boundary curve rt rt cosθt rt sinθt. especially interested regions bound polar graphs θt t. 2 polar region deﬁned r cos3θ belongs class roses rt cosnt also called rhododenea. names reﬂect polar regions model ﬂowers well. 3 polar curve rθ 1 sinθ called cardioid. looks like heart. special case limacon polar curve form rθ 1 b sinθ. 4 polar curve rθ q cos2t called lemniscate. looks like inﬁnity sign. encloses ﬂower two petals. 1.0 0.5 0.5 1.0 0.5 1.0 1.5 2.0 1.0 0.5 0.5 1.0 0.5 0.5 1.0 0.5 0.5 1.0 1.0 0.5 0.5 1.0
integrate polar coordinates evaluate integral zz rfx y dxdy zz rfr cosθ r sinθr drdθ 5 integrate fx y x2 x2 xy unit disc. fx y fr cosθ r sinθ r2 r2 cosθ sinθ r r rfx y dxdy r 1 0 r 2π 0 r2 r2 cosθ sinθr dθdr 2π4. 6 earlier computed area disc x2 y2 r2 using substitution. elegant integral polar coordinates r 2π 0 r r 0 r drdθ 2πr22r 0 πr2. include factor r move polar coordinates reason small rectangle r dimensions dθdr r θ plane mapped r θ 7 r cosθ r sinθ sector segment x y plane. area r dθdr. 7 integrate function fx y 1 θ rθ rθ cos3θ . z z r 1 dxdy z 2π 0 z cos3θ 0 r dr dθ z 2π 0 cos3θ2 2 dθ π2 . 8 integrate fx y yx2 y2 region r x y 1 x2 y2 4 0 . z 2 1 z π 0 r sinθr r dθdr z 2 1 r3 z π 0 sinθ dθdr 24 14 4 z π 0 sinθ dθ 152 integration problems region part annular region see function terms x2 y2 try use polar coordinates x r cosθ r sinθ. 9 belgian biologist johan gielis deﬁned 1997 family curves given polar coordinates rφ cos mφ 4 n1 sin mφ 4 n2 b 1n3 supercurve produce variety shapes like circles square triangle stars. also used produce supershapes. supercurve generalizes superellipse discussed 1818 lam e helps describe forms biology. 1 1gielis j. generic geometric transformation uniﬁes wide range natural abstract shapes. american journal botany 90 333 338 2003.
math 21a multivariable calculus oliver knill fall 2012 lecture 22 surface area surface ru v parametrized parameter domain r surface area z z r ruu v rvu v dudv . proof. vector ru tangent grid curve u 7 ru v rv tangent v 7 ru v. two vectors span parallelogram area ru rv. small rectangle u uduv vdv mapped r parallelogram spanned r r ru r r rv area ruu v rvu v dudv. 1 parametrized surface ru v 2u 3v 0is part xyplane. parameter region g gets stretched factor 2 x coordinate factor 3 coordinate. ru rv 0 0 6and see example area rg 6 times area g. planar region rs t p sv tw s t g surface area area g times v w. 2 map ru v l cosu sinv l sinu sinv l cosvmaps rectangle g 0 2π 0 π onto sphere radius l. compute ru rv l sinv ru v. so ru rv l2 sinv r r r 1 ds r 2π 0 r π 0 l2 sinv dvdu 4πl2 sphere radius l ru rv l2 sinv surface area 4πl2. 3 graphs u v 7u v fu v ru 1 0 fuu v rv 0 1 fvu v. cross product ru rv fu fv 1 length q 1 f 2 u f 2 v . area surface region g r r g q 1 f 2 u f 2 v dudv. graph z fx y parametrized g surface area z z g q 1 f 2 x f 2 dxdy . 4 lets take surface revolution ru v v fv cosu fv sinuon r 0 2π a b. ru 0 fv sinu fv cosu rv 1 f v cosu f v sinu ru rv fvf v fv cosu fv sinu fvf v cosu sinu. surface area r r ru rv dudv 2π r b fv q 1 f v2 dv.
surface revolution r fz z b surface area 2π z b fz q 1 f z2 dz . 5 gabriels trumpet surface revolution gz 1z 1 z . volume r 1 πgz2 dz π. compute class sur face area. 6 find surface area part paraboloid x y2 z2 inside cylinder y2 z2 9. solution. use polar coordinates yzplane. paraboloid parametrized u v 7v2 v cosu v sinu surface integral r 3 0 r 2π 0 ru rv dudv equal r 3 0 r 2π 0 v 1 4v2 dudv 2π r 3 0 v 1 4v2 dv π3732 16. 7 example derive distortion factor r polar coordinates. so parametrize region xy plane ru v u cosv u sinv 0. given region g uv plane like rectangle 0 π 1 2 obtain region xy plane image. factor ru rv equal radius u. example surface area r π 0 r 2 1 u dudv π4 1 3π. area half annulus s. could used polar coordinates directly xy plane compute r π 0 r 2 1 r dr dθ 3π. thing changed names variables. 8 surface parametrized ru v 2v cosu2 cosu 2v cosu2 sinu v sinu2 g 0 2π 1 1 called m obius strip. surface area solution. calculation ru rv2 4 3v24 4v cosu2 v2 cosu2 straightforward bit tedious. integral 0 2π 1 1 evaluated numerically result 25.413.
math 21a multivariable calculus oliver knill fall 2012 lecture 24 triple integrals fx y z function three variables e solid region space r r r e fx y z dxdydz deﬁned n limit riemann sum 1 n3 x injnkne f n j n k n . two dimensions triple integrals evaluated iterated 1d integral computations. simple example 1 assume e box 0 1 0 1 0 1 fx y z 24x2y3z. r 1 0 r 1 0 r 1 0 24x2y3z dz dy dx . compute integral start core r 1 0 24x2y3z dz 12x3y3 integrate middle layer r 1 0 12x3y3 dy 3x2 ﬁnally ﬁnally handle outer layer r 1 0 3x2dx 1. calculate inner integral ﬁx x y. integral integrating fx y z along line intersected body. completing middle integral computed integral plane z const intersected r. outer integral sums two dimensional sections. two important methods triple integrals washer method sandwich method. washer method single variable calculus reduces problem directly one dimensional integral. new sandwich method reduces problem two dimensional integration problem. washer method slices solid along zaxes. gz double in tegral along two dimensional slice r b a r r rz fx y z dxdy dz. sandwich method sees solid sandwiched graphs two functions gx y hx y common two dimensional region r. integral becomes r r r r hxy gxy fx y z dz dxdy. 2 important special case sandwich method volume z r z fxy 0 1 dzdxdy . graph function fx y region r. integral r r r fx y da. actually computed triple integral
3 find volume unit sphere. solution sphere sandwiched graphs two functions. let r unit disc xy plane. use sandwich method get v z z r z 1x2y2 1x2y2 1dzda . gives double integral r r r 21 x2 y2 da course best solved polar coordinates. r 2π 0 r 1 0 1 r2r drdθ 4π3. washer method case also called disc method slice along z axes get disc radius 1 z2 area π1 z2. method suitable single variable calculus get directly r 1 1 π1 z2 dz 4π3. 4 mass body density ρx y z deﬁned r r r r ρx y z dv . bodies constant density ρ mass ρv v volume. compute mass body bounded parabolic cylinder z 4 x2 planes x 0 0 6 z 0 density body 1. solution z 2 0 z 6 0 z 4x2 0 dz dy dx z 2 0 z 6 0 4 x2 dydx 6 z 2 0 4 x2 dx 64x x332 0 32 5 solid region bound x2 y2 1 x z z 0 called hoof archimedes. historically signiﬁcant one ﬁrst examples archimedes probed riemann sum integration technique. appears every calculus text book. find volume. solution. look situation picture x y plane. see half disc r. ﬂoor solid. roof function z x. integrate r r r x dxdy. got double integral problems best done polar coordinates r π2 π2 r 1 0 r2 cosθ drdθ 23. 6 finding volume solid region bound three cylinders x2 y2 1 x2 z2 1 y2 z2 1 one famous volume integration problems. solution look 116th body given cylindri cal coordinates 0 θ π4 r 1 z 0. roof z 1 x2 one eighth disc r cylinder x2 z2 1 matters. polar integration problem 16 z π4 0 z 1 0 q 1 r2 cos2θr drdθ inner rintegral 1631 sinθ3 cos2θ. integrating θ done integrating 1 sinx3 sec2x parts using tanx sec2x leading anti derivative cosxsecxtanx. result 16 8 2.
math 21a multivariable calculus oliver knill fall 2012 lecture 25 spherical integration cylindrical coordinates coordinates space polar coordinates chosen xy plane zcoordinate left untouched. surface revolution described cylindrical coordinates r gz. coordinate change transformation tr θ z r cosθ r sinθ z produces integration factor r polar coordinates. zz trfx y z dxdydz zz rgr θ z r drdθdz remember also spherical coordinates use ρ distance origin well two angles θ polar angle φ angle vector z axis. coordinate change x y z ρ cosθ sinφ ρ sinθ sinφ ρ cosφ . integration factor seen measuring volume spherical wedge dρ ρ sinφ dθ ρdφ ρ2 sinφdθdφdρ. zz trfx y z dxdydz zz rgρ θ z ρ2 sinφ dρdθdφ 1 sphere radius r volume z r 0 z 2π 0 z π 0 ρ2 sinφ dφdθdρ . inner integral r π 0 ρ2 sinφdφ ρ2 cosφπ 0 2ρ2. next layer is φ appear r 2π 0 2ρ2 dφ 4πρ2. ﬁnal integral r r 0 4πρ2 dρ 4πr33. moment inertia body g respect axis l deﬁned triple integral r r r g rx y z2 dzdydx rx y z r sinφ distance axis l.
2 sphere radius r obtain respect zaxis z r 0 z 2π 0 z π 0 ρ2 sin2φρ2 sinφ dφdθdρ z π 0 sin3φ dφ z r 0 ρ4 dr z 2π 0 dθ z π 0 sinφ1 cos2φ dφ z r 0 ρ4 dr z 2pi 0 dθ cosφcosφ33π 0l552π 4 3r5 5 2π 8πr5 15 . sphere rotates angular velocity ω iω22 kinetic energy sphere. example moment inertia earth 81037kgm2. angular velocity ω 2πday 2π86400s. rotational energy 8 1037kgm27464960000s2 1029j 2.51024kcal. 3 find volume center mass diamond intersection unit sphere cone given cylindrical coordinates z 3r. solution use spherical coordinates ﬁnd center mass x z 1 0 z 2π 0 z π6 0 ρ3 sin2φ cosθ dφdθdρ 1 v 0 z 1 0 z 2π 0 z π6 0 ρ3 sin2φ sinθ dφdθdρ 1 v 0 z z 1 0 z 2π 0 z π6 0 ρ3 cosφ sinφ dφdθdρ 1 v 2π 32v 4 find r r r r z2 dv solid obtained intersecting 1 x2 y2 z2 4 double cone z2 x2 y2. solution since result double cone twice result single cone work diamond shaped region r z 0 multiply result end 2. spherical coordinates solid r given 1 ρ 2 0 φ π4. z ρ cosφ z 2 1 z 2π 0 z π4 0 ρ4 cos2φ sinφ dφdθdρ 25 5 15 5 2πcos3φ 3 π4 0 2π31 5 1 232 . result double cone 4π3151 1 2 3 .
math 21a multivariable calculus oliver knill fall 2012 lecture 26 vector ﬁelds vector ﬁeld plane map assigns point x y vector fx y px y qx y. vector ﬁeld space map assigns x y z space vector fx y z px y z qx y z rx y z. example fx y x1 yx12y232 x1 yx12y232 electric ﬁeld positive negative point charge. called dipole ﬁeld. shown picture below fx y function two variables fx y fx y called gradient ﬁeld. gradient ﬁelds space form fx y z fx y z. vector ﬁeld gradient ﬁeld fx y px y qx y fx y implies qxx y pyx y. hold point f gradient ﬁeld. clairaut test qxx y pyx y zero point fx y px y qx yis gradient ﬁeld. see next week condition curlf qx py 0 also necessary f gradient ﬁeld. class see examples construct potential f gradient ﬁeld f. 1 vector ﬁeld fx y p q 3x2y 2 x3 x 1a gradient ﬁeld solution clairot test shows qx py 0. integrate equation fx p 3x2y 2 get fx y 2x xy x3y cy. take derivative respect get x x2 cy compare x3 x 1. see cy 1 cy y c. see solution x3y xy y 2x .
2 vector ﬁeld fx y xy 2xy2a gradient ﬁeld solution no qx py 2y2 x zero. vector ﬁelds important diﬀerential equations. motivation comes also mechanics 3 class vector ﬁelds important mechanics hamiltonian ﬁelds hx y func tion two variables hyx y hxx yis called hamiltonian vector ﬁeld. example harmonic oscillator hx y x2y2. vector ﬁeld hyx y hxx y y x. ﬂow lines hamiltonian vector ﬁelds located level curves h as shown th homework chain rule. 4 newtons law m r f relates acceleration r body force f acting point. example xt position mass point 1 1 attached two springs mass 2 point experiences force x x 2x mx 2x xt xt. introduce yt xt t xt yt yt xt. course velocity mass point pair x y thought initial condition describes system nature knows future evolution system given data. 5 vector ﬁeld fx y px y qx y xy x2a gradient ﬁeld no. vector ﬁeld fx y px y qx y sinx y cosy xa gradient ﬁeld yes. function fx y cosx siny xy. 6 spot following vector ﬁelds pictures fx y y 0 fx y y x x y fx y y x fx y y x x y. ones conservative
math 21a multivariable calculus oliver knill fall 2012 lecture 28 fundamental theorem line integrals recall f vector ﬁeld plane space c 7 rt curve deﬁned interval a b z b f rt r t dt called line integral f along curve c. following theorem generalizes fundamental theorem calculus higher dimensions fundamental theorem line integrals f f z b f rt rt dt f rb f ra . proof fundamental theorem uses chain rule second equality funda mental theorem calculus third equality following identities z b f rt rt dt z b f rt rt dt z b dtf rt dt f rb f ra . 1 let fx y 2xy2 3x2 2yx2. find potential f f p q. solution potential function fx y satisﬁes fxx y 2xy2 3x2 fyx y 2yx2. integrating second equation gives fx y x2y2 hx. partial diﬀerentiation re spect x gives fxx y 2xy2hx 2xy23x2 take hx x3. potential function fx y x2y2x3. find g h fx y r x 0 pt y dthy fyx y gx y. 2 olivers last birthday relaxed jacuzzi boston harbor hotel. moved along curve c given part curve x10 y10 1 ﬁrst quadrant oriented counter clockwise. hot water tub velocity fx y x y4. calculate line integral r c f dr energy gain ﬂuid force.
math 21a multivariable calculus oliver knill fall 2012 lecture 29 greens theorem curl vector ﬁeld fx y px y qx yis scalar ﬁeld curlfx y qxx y pyx y . curlf measures vorticity vector ﬁeld. one write f curl f two dimensional cross product x y f p qis scalar qx py. 1 fx y y xwe curlfx y 2. 2 fx y f gradient ﬁeld curl zero px y fxx y qx y fyx y curlf qx py fyx fxy 0 clairauts theorem. greens theorem fx y px y qx yis vector ﬁeld r region boundary c parametrized r to left z c f dr z z g curlf dxdy . proof. integral f along boundary g x xǫy yǫ r ǫ 0 pxt ydt r ǫ 0 qx ǫ t dt r ǫ 0 px t ǫ dt r ǫ 0 qx t dt. qx ǫ y qx y qxx yǫ px ǫ px y pyx yǫ qx pyǫ2 r ǫ 0 r ǫ 0 curlf dxdy. identities hold limit ǫ 0. general region g cut small squares size ǫ. summing line integrals around boundaries gives line integral around boundary interior line integrals cancel. sum ming vorticities squares riemann sum approximation double integral. boundary integrals converge line integral c. george green lived 1793 1841. physicist selftaught mathematician miller. 3 f gradient ﬁeld sides greens theorem zero r c f dr zero fundamental theorem line integrals. r r g curlf da zero curlf curlgradf 0.
already established clairaut identity curlgradf 0 also remembered f noting cross product two identical vectors 0. treating as vector nabla calculus. 4 find line integral fx y x2y2 2xy p qalong boundary rectangle 0 20 1. solution curl f qxpy 2y2y 4y r c f dr r 2 0 r 1 0 4y dydx 2y21 0x2 0 4. 5 find area region enclosed rt sinπt2 t2 1 1 t 1. so use greens theorem vector ﬁeld f 0 x. 6 important application green compute area. vector ﬁelds fx y p q y 0or fx y 0 xhave vorticity curl fx y 1. fx y 0 x right hand side greens theorem area g areag z c0 xt xt ytdt . 7 let g region graph function fx a b. line integral around boundary g 0 a 0 b 0 fx y 0 0there. line integral also zero b 0 b fb a fa a 0 n 0. line integral along curve t ft r b ayt 01 f tdt r b ft dt. greens theorem conﬁrms area region graph. consequence fundamental theorem line integrals f gradient ﬁeld curlf 0 everywhere. converse true answer region r called simply connected every closed loop r pulled together point r. curl f 0 simply connected region g f gradient ﬁeld. proof. given closed curve c g enclosing region r. greens theorem assures r c f dr 0. f closed loop property g line integrals path independent f gradient ﬁeld.
math 21a multivariable calculus oliver knill fall 2012 lecture 30 divergence curl curl vector ﬁeld f p q ris vector ﬁeld curlp q r ry qz pz rx qx py. 1 curl vector ﬁeld x2 y5 z2 x2 z2is 2z 2x 5y4. write curl f f. two dimensions curl vector ﬁeld f p qis qx py scalar ﬁeld. ﬁeld zero curl everywhere ﬁeld called irrotational. curl often visualized using paddle wheel. place wheel ﬁeld direction v rotation speed wheel measures quantity f v. see soon. consequently direction wheel turns fastest direction curl f. angular velocity length curl. wheel could actually used measure curl vector ﬁeld point. situations large vorticity like tornado one see direction curl near vortex center. two dimensions two derivatives gradient curl. three dimensions three fundamental derivatives gradient curl divergence. divergence f p q ris scalar ﬁeld divp q r f px qy rz. divergence also deﬁned two dimensions fundamental. divergence f p qis divp q f px qy. two dimensions divergence curl 90 degrees rotated ﬁeld g q p div g qx py curl f. divergence measures expansion ﬁeld. ﬁeld zero divergence everywhere ﬁeld called incompressible. vector x y z write curl f f div f f.
f divgradf fxx fyy fzz . laplacian f. one also writes f 2f f divgradf. f 0 f 0 get divcurl f 0 curlgrad f 0. 2 question vector ﬁeld g f x y z y2 curl g answer no div f 1 incompatible divcurl g 0. 3 show simply connected region every irrotational incompressible ﬁeld written vector ﬁeld f gradf f 0. proof. since f irrotational exists function f satisfying f gradf. now divf 0 implies divgradf f 0. 4 find example ﬁeld incompressible irrotational. solution. find f satisﬁes laplace equation f 0 like fx y x3 3xy2 look gradient ﬁeld f f. case gives fx y 3x2 3y2 6xy. 5 rotate vector ﬁeld f p qby 90 degrees π2 get new vector ﬁeld g q p. integral r c f ds becomes ﬂux r γ g dn g boundary r dn normal vector length rdt. div f px qy see curl f div g greens theorem r r r div g dxdy r c g dn. dnx y normal vector x y orthogonal velocity vector r x y x y. bother this greens theorem disguise. divergence point x y ﬂux ﬁeld small circle radius r around point limit radius circle goes zero curl point x y work done along small circle radius r around point limit radius circle goes zero derivatives together. dimension d fundamental derivatives. 1 1 grad 1 1 grad 2 curl 1 1 grad 3 curl 3 div 1
math 21a multivariable calculus oliver knill fall 2012 lecture 31 flux integrals surface parametrized ru v xu v yu v zu vover domain g uvplane f vector ﬁeld ﬂux integral f z z g f ru v ru rv dudv . short hand notation ds ru rv dudv representing inﬁnitesimal normal vector surface written r r f ds. interpretation f ﬂuid velocity ﬁeld r r f ds amount ﬂuid passing unit time. n ru rv ru rv unit vector normal surface surface f n normal component vector ﬁeld respect surface. one could write therefore also r r f ds r r f n ds ds surface element know computed surface area. function f n scalar projection f normal direction. whereas formula r r 1 ds gave area surface ds ru rvdudv ﬂux integral weights area element ds normal component vector ﬁeld f ru v n ru v. use formula computations computing n gives additional work. determine vectors f ru v ru rv integrate dot product domain. 1 compute ﬂux fx y z 0 1 z2through upper half sphere parametrized ru v cosu sinv sinu sinv cosv. solution. ru rv sinv r f ru v 0 1 cos2vso z 2π 0 z π 0 0 1 cos2v cosu sin2v sinu sin2v cosv sinvdudv . ﬂux integral r 2π 0 r π π2 sin2v sinucos3v sinv dudv r π π2 cos3 v sinv dv cos4v4π2 0 14.
2 calculate ﬂux vector ﬁeld fx y z 1 2 4zthrough paraboloid z x2 y2 lying region x2 y2 1. solution parametrize surface rr θ r cosθ r sinθ r2where rr rθ 2r2 cosθ 2r2 sinθ rand f ru v 1 2 4r2. get r f ds r 2π 0 r 1 0 2r2 cosv 4r2 sinv 4r3 drdθ 2π. 3 compute ﬂux fx y z 2 3 1through torus parameterized ru v 2 cosv cosu 2 cosv sinu sinv u v range 0 2π. solution. computation needed. think ﬂux means. 4 evaluate ﬂux integral r r s0 0 yz ds surface parametric equation x uv u v z u v r u2 v2 1. solution ru v 1 1 rv u 1 1so ru rv 2 u v u v. ﬂux integral r r r0 0 u2 v2 2 u v u vdudv r r rv2u u3 v3 u2v dudv best evaluated using polar coordinates r 1 0 r 2π 0 r4sin2θ cosθ cos3θ sin3θ cos2θ sinθ dθdr 0. 5 evaluate ﬂux integral r r scurlf ds fx y z xy yz zx part paraboloid z 4 x2 y2 lies square 0 1 0 1 upward orientation. solution curlf y z x. parametrization ru v u v 4u2v2gives ru rv 2u 2v 1and curlf ru v v u2v24 u. ﬂux integral r 1 0 r 1 0 2uv2vu2v24udvdu 121312412 256. 6 relation ﬂux vector ﬁeld f gg surface g 1 gx y z x6 y4 2z8 surface area s solution. surface area equal ﬂux f ru rv ru rv. 7 find ﬂux vector ﬁeld g g 0 0 1through surface s. solution. ﬂux zero vector ﬁeld tangent surface. 8 compute ﬂux fx y z 2 3 1through torus parameterized ru v 2 cosv cosu 2 cosv sinu sinv u v range 0 2π. solution. computation needed. think ﬂux means. 9 walk rain velocity v along x axis. rain treated ﬂuid velocity f v 0 w w speed rain drops. find ﬂux front top assume rectangular box 0 12 0 1 14 13. solution. ﬂux surface consisting front top amount water soak per second. parameterize front ru v 0 u vin yzplane. front shirt trousers catch unit time ﬂux rain front surface. head gets rest. know ru rv 0 0 1for top ru rv 1 0 0for front. ﬂux r r f ds z 14 0 z 13 0 v 0 w 0 0 1dudv z 12 0 z 1 0 v 0 w 1 0 0dudv w12 v2 . walk ﬁxed distance l time lv total ﬂux lvw12 v2 l2 lw2v. part soaked front depends l speeds. ﬂux caught head shoulder less walk fast. better run fast
math 21a multivariable calculus oliver knill fall 2012 lecture 32 stokes theorem boundary surface curve oriented surface left normal vector surface pointing up. words velocity vector v vector w pointing towards surface normal vector n surface form right handed coordinate system. stokes theorem let surface bounded curve c f vector ﬁeld. z z curl f ds z c f dr . 1 let fx y z y x 0and let upper semi hemisphere curl fx y z 0 0 2. surface parameterized ru v cosu sinv sinu sinv cosvon g 0 2π 0 π2 ru rv sinv ru v curl fx y z ru rv cosv sinv2. integral r 2π 0 r π2 0 sin2v dvdu 2π. boundary c parameterized rt cost sint 0so dr r tdt sint cost 0dt f rt r tdt sint2 cos2t 1. line integral r c f dr along boundary 2π. 2 surface xyplane f p q 0has zero z component curl f 0 0 qx pyand curl f ds qx py dxdy. case stokes theorem seen consequence greens theorem. vector ﬁeld f induces vector ﬁeld surface 2d curl normal component curlf. reason third component qx py curl fry qz pz rx qx pyis two dimensional curl f ru v 0 0 1 qx py. c boundary surface r r f ru v 0 0 1dudv r c f rt r tdt. 3 calculate ﬂux curl fx y z y x 0through surface parameterized ru v cosu cosv sinu cosv cos2v cosv sin2u π2. surface boundary upper half sphere integral 2π example. surface closed bounds solid. ﬂux curl vector ﬁeld closed surface zero.
4 electric ﬁeld e magnetic ﬁeld b linked maxwell equation curl e 1 c b. take closed wire c bounds surface consider r r b ds ﬂux magnetic ﬁeld s. ﬂux change related voltage using stokes theorem ddt r r bds r r bds r r c curl e ds c r c e dr u u voltage measured cutup wire. ﬂux changed turn around magnet around wire wire inside magnet get electric voltage. stokes theorem explains generate electricity motion. george gabriel stokes andr e marie ampere 5 find r c f dr fx y z x2y x33 xyand c curve intersection hyperbolic paraboloid z y2 x2 cylinder x2 y2 1 oriented counter clockwise viewed above. solution. curl f curlf x y 0. parametrize hyperbolic paraboloid ru v u cosv u sinv u2 cos2v. ru rv 2u2 cosv 2u2 sinv u. f ru v u cosv u sinv 0. f ru v ru rv 2u3. integral r 1 0 r 2π 0 2r3 dθdr π. 6 evaluate ﬂux fx y z xey2z3 2xyzex2z x z2ex2z yex2z zexthrough part ellipsoid x2 y24 z 12 2 z 0 oriented normal vector points upwards. solution. stokes theorem assures ﬂux integral equal line integral along boundary surface. boundary ellipse rt cost 2 sint 0 t 2π. vector ﬁeld xyplane z 0 fx y 0 0 x yex2. compute line integral vector ﬁeld along boundary curve compute rt sint 2 cost 0and f rt 0 cost 2 sintesin2t. dot product power 2 cos2t. integrating 0 2π gives 2π.
math 21a multivariable calculus oliver knill fall 2012 lecture 32 stokes theorem understand stokes like anything mathematics ﬁrst make sure deﬁnitions clear. already quite tough stokes theorem involves lot concepts . parametrized curves boundary surface curve . parametrized surfaces surface usually given . line integral r f rt r t dt . ﬂux integral r g f ru v ru rv dudv . dot cross product appear. ﬂux integral see triple scalar product . result relates single variable integral double integral theorem stokes theorem let surface bounded curve c f vector ﬁeld. z z curl f ds z c f dr . stokes theorem assigned exam problem 1854 george stokes. george maxwell the one maxwell equations took exam. clearly inﬂuenced him. see exhibit course website. stokes theorem important because . crucial prototype theorem. . crowns multivariable calculus reinforcing older concepts. 1 . understanding it demonstrate ability abstract thought. . explains concepts physics especially ﬂuid dynamics electromagnetism. . explains curl is. knowing deﬁnitions go theorem statement theorem use theorem tool. ﬁrst important worry meaning things much learn use theorem tool example solve complicated line integrals avoid complex ﬂux integrals. two typical examples use theorem 1as general principle always good idea learn subject actually need.
1 problem a find ﬂux curl fx y z y sinxz x z3through surface x2 y24 z2 z4x2 1 z 0. solution. problem ﬂux integral would tough compute. would parametrize surface would succeed compute integral. much easier use stokes theorem compute line integral f along boundary surface ellipse xy plane. 2 problem b find line integral r c f dr c circle radius 3 xzplane oriented counter clockwise looking point 0 1 0 onto plane f vector ﬁeld fx y z 2x2z x5 cosey 2xz2 sinsinz. use convenient surface c boundary. solution line integral computed directly. curl f 0 2x2 2z2 0. r 3 0 r 2π 0 2r3 dθdr. answer 81π. two conceptual problems 3 surface x6 y6 z6 1 assume f smooth vector ﬁeld. explain r r scurl f ds 0. solution. ﬂux curlf closed surface zero stokes theorem fact surface boundary. one see also cutting surface two pieces apply stokes pieces. 4 assume look surface boundary doughnut. cut doughnut two pieces. verify ﬂux curl one part surface opposite ﬂux other. solution. one understand stokes theorem. ﬂux ﬁrst part equal line integral along boundary. ﬂux second line integral along boundary opposite direction. 5 simply connectedness region consideration needed verify vector ﬁeld zero curl gradient ﬁeld problem appears movie a beautiful mind. solution. given closed loop c space. simply connectedness means one pull together loop point. pulling together deﬁnes surface curve c boundary. ﬂux curl f zero. stokes theorem line integral zero. f closed loop property therefore gradient ﬁeld. region simply connected like complement zaxes ﬁnd vector ﬁelds f zero curl everywhere region gradient ﬁelds. example fx y z yx2 y2 xx2 y2 0.
math 21a multivariable calculus oliver knill fall 2012 lecture 33 divergence theorem three integral theorems three dimensions. seen already fundamental theorem line integrals stokes theorem. divergence theorem completes list integral theorems three dimensions divergence theorem. let e solid boundary surface oriented normal vector points outside. let f vector ﬁeld. z z z e div f dv z z f ds . prove this one look small box x x dx y dyz z dz. ﬂux f p q rthrough faces perpendicular xaxes fx dx y z 1 0 0 fx y z 1 0 0dydz px dx y z px y z px dxdydz. similarly ﬂux yboundaries py dydxdz ﬂux two z boundaries pz dzdxdy. total ﬂux faces cube px py pz dxdydz div f dxdydz. general solid approximated union small cubes. sum ﬂuxes cubes con sists ﬂux faces without neigh boring faces. ﬂuxes adjacent sides can cel. sum ﬂuxes cubes ﬂux boundary union. sum div f dxdydz riemann sum approximation integral r r r g div f dxdydz. limit dx dy dz goes zero obtain divergence theorem. theorem explains divergence means. average divergence small cube equal ﬂux ﬁeld boundary cube. positive ﬁeld exists cube entering cube. ﬁeld generated inside. divergence measures expansion ﬁeld. 1 let fx y z x y zand let sphere. divergence f constant function div f 3 r r r g div f dv 3 4π3 4π. ﬂux boundary r r r ru rv dudv r r ru v2 sinv dudv r π 0 r 2π 0 sinv dudv 4π also. see divergence theorem allows us compute area sphere volume enclosed ball compute volume surface area.
2 ﬂux vector ﬁeld fx y z 2x 3z2 y sinxthrough solid g 0 3 0 3 0 3 0 3 1 2 1 2 1 2 0 3 1 2 0 3 0 3 1 2 cube three perpendicular cubic holes removed solution use divergence theorem div f 2 r r r g div f dv 2 r r r g dv 2volg 227 7 40. note ﬂux integral would complicated surface dozens rectangular planar regions. 3 find ﬂux curlf torus f yz2 z sinx y cosxand torus parametrization rθ φ 2 cosφ cosθ 2 cosφ sinθ sinφ. solution answer 0 divergence curlf zero. divergence theorem ﬂux zero. 4 similarly greens theorem allowed calculate area region passing along boundary volume region computed ﬂux integral take example vector ﬁeld fx y z x 0 0which divergence 1. ﬂux vector ﬁeld boundary solid region equal volume solid r r δgx 0 0 ds volg. 5 heavy we distance r center earth solution law gravity formulated div f 4πρ ρ mass density. as sume earth ball radius r. rotational symmetry gravitational force normal sur face f x fr x x. ﬂux f ball radius r r r sr fx ds 4πr2 fr. di vergence theorem 4πmr 4π r r r br ρx dv mr mass material inside sr. 4π2ρr33 4πr2 fr r r 4π2ρr33 4πr2 fr r r. inside earth gravitational force fr 4πρr3. outside earth satisﬁes fr mr2 4πr3ρ3. 0.5 1.0 1.5 2.0 0.2 0.4 0.6 0.8 1.0
math 21a multivariable calculus oliver knill fall 2012 lecture 34 overview integral theorems incarnations fundamental theorem multivariable calculus z g df z δg f df derivative f δg boundary g. 1 1 ftc 1 1 ftl 2 green 1 1 ftl 3 stokes 3 gauss 1 fundamental theorem line integrals c curve boundary a b f function z c f dr fb fa remarks. 1 closed curves line integral r c f dr zero. 2 gradient ﬁelds path independent f f line integral two points p q depend path connecting two points. 3 theorem justiﬁes name conservative gradient vector ﬁelds. 4 term potential coined george green lived 17931841. 1 let fx y z x2 y4 z. find line integral vector ﬁeld fx y z fx y z along path rt cos5t sin2t t2from 0 2π. solution. r0 1 0 0and r2π 1 0 4π2and f r0 1 f r2π 1 4π2. fundamental theorem line integral gives r c f dr fr2π fr0 4π2. greens theorem. r region boundary c f vector ﬁeld z z r curl f dxdy z c f dr . remarks. 1 curve oriented way region left. 2 boundary curve consist piecewise smooth pieces. 3 c 7 rt xt yt line integral r b apxt yt qxt yt xt ytdt. 4 greens theorem found george green 17931841 1827 mikhail ostrogradski 18011862. 5 curl f 0 everywhere plane ﬁeld gradient ﬁeld. 7 fx y 0 xwe get area formula.
2 find line integral vector ﬁeld fx y x4 sinx y x y3along path rt cost 5 sint log1 sint runs 0 π. solution. curl f 0 implies line integral depends end points 0 1 0 1 path. take simpler path rt t 0 1 t 1 veloc ity r t 1 0. line integral r 1 1t4sint t1 0dt t551 1 25. stokes theorem. surface boundary c f vector ﬁeld z z curl f ds z c f dr . remarks. 1 stokes theorem allows derive greens theorem f zindependent surface contained xyplane one obtains result green. 2 orientation c walk along c head direction normal vector ru rv surface left. 3 stokes theorem found andr e amp ere 17751836 1825 rediscovered george stokes 18191903. 4 ﬂux curl f depends boundary s. 5 ﬂux curl closed surface like sphere zero boundary empty. 3 compute line integral fx y z x3xy y zalong polygonal path c connecting points 0 0 0 2 0 0 2 1 0 0 1 0. solution. path c bounds surface ru v u v 0parameterized r 0 2 0 1. stokes theorem line integral equal ﬂux curl fx y z 0 0 x s. normal vector ru rv 1 0 0 0 1 0 0 0 1so r r curl f ds r 2 0 r 1 0 0 0 u 0 0 1dudv r 2 0 r 1 0 u dudv 2. divergence theorem boundary region e space f vector ﬁeld z z z b div f dv z z f ds . remarks. 1 divergence theorem also called gauss theorem. 2 helpful determine ﬂux vector ﬁelds surfaces. 3 discovered 1764 joseph louis lagrange 17361813 later rediscovered carl friedrich gauss 17771855 george green. 4 divergence free vector ﬁelds f ﬂux closed surface zero. ﬁelds f also called incompressible source free. 4 compute ﬂux vector ﬁeld fx y z x y z2through boundary rectangular box 0 3 1 2 1 2. solution. gauss theorem ﬂux equal triple integral divf 2z box r 3 0 r 2 1 r 2 1 2z dxdydz 3 02 14 1 27.
napiers ideal construction logarithms denis roegel 12 september 2012 1 introduction today john napier 15501617 renowned inventor loga rithms.1 conceived general principles logarithms 1594 be fore spent next twenty years developing theory 108 p. 63 33 pp. 103104. description logarithms miriﬁci logarithmorum ca nonis descriptio published latin edinburgh 1614 131 161 considered one greatest scientiﬁc discoveries world seen 83. several mathematicians anticipated properties correspondence arithmetic geometric progression napier jost bürgi 15521632 constructed tables purpose simplifying calculations. bürgis work however published incomplete form 1620 six years napier published descriptio 26.2 napiers work quickly translated english mathematician cartographer edward wright3 15611615 145 179 published posthu mously 1616 132 162. second edition appeared 1618. wright friend henry briggs 15611630 turn may led briggs visit napier 1615 1616 develop decimal logarithms. this document part locomat project loria collection mathe matical tables 1among many activities interests napier also devoted lot time com mentary saint johns revelation published 1593. one author went far writing napier invented logarithms order speed calculations number beast. 40 2it possible napier knew bürgis work computation sines ursus fundamentum astronomicum 1588 149. 3in 1599 prior napier logarithms actually used implicitely wright without wright realizing done so without using simplify calculations 33. therefore wright and not lay claim prior discovery. see wedemeyers article 201 additional information. 1
briggs ﬁrst table logarithms logarithmorum chilias prima appeared 1617 20 contained logarithms base 10 ﬁrst 1000 chil ias integers 14 places. followed arithmetica logarithmica 1624 21 trigonometria britannica 1633 22. details napiers construction logarithms published posthumously 1619 miriﬁci logarithmorum canonis constructio 133 134 136. descriptio constructio several editions.4 stressed napiers logarithms logarithms known today neperian or natural hyperbolic logarithms related construction made time integral diﬀerential calculus yet exist. napiers construction wholly geometrical based theory proportions. recomputed tables published 1614 1616 according napiers rules5 using gnu mpfr library 54. computations done napier made error. 4latin editions descriptio published 1614 131 1619 1620 134 google books id qz4aaaacaaj 1658 1807 110 1857. english editions published 1616 1618 1857 by filipowski 135. latin editions constructio pub lished 1619 133 1620 134 google books id vukhaqaaiaaj uj4aaaacaaj 1658. copies edition published 1620 lyon carry year 1619. 1658 edition remainder 16191620 editions title page preliminary matter reset 108. english edition constructio published 1889 136. addition archibald cites 1899 edition two latin texts 4 p. 187 probably refers gravelaars description napiers works 71 matter fact latter contains excerpts descriptio constructio. recently w. f. hawkins also gave english translations works 78 thesis. then jean peyroux published french translation descriptio 1993 96. finally ian bruce made new transla tions available web extensive details various editions napiers works known 1889 see macdonald 136 pp. 101169. 5throughout article consider idealization napiers calculations assuming values computed exactly. fact napiers values rounded present results would slightly diﬀerent presentation napiers actual construction. detailed analysis problems given fischer 49. might provide table based napiers rounding methods future. 2
2 napiers construction napier interested simplifying6 computations introduced new notion numbers initially called artiﬁcial numbers. name used miriﬁci logarithmorum canonis constructio published 1619 written long descriptio 1614. latter work napier introduced word logarithm greek roots logos ratio arith mos number. number authors interpreted name meaning number ratios7 g. a. gibson w. r. thomas believe word logarithm merely signiﬁes rationumber number connected ratio 188 p. 195. moreover thomas believes napier found expression number connected ratio archimedes sand reckoner known napier good greek scholar number princeps editions archimedes book available england early 17th century 188 86. case napier meant provide arithmetic measure geo metric ratio least deﬁne continuous correspondence two progressions. napier took origin value 107 deﬁned logarithm 0. smaller value x given logarithm corresponding ratio 107 x. 2.1 first ideas likely napier ﬁrst idea correspondence geometric series arithmetic one. correspondences exhibited before particular archimedes nicolas chuquet 1484 long napier stifel 184. according cantor napiers words negative numbers stifel leads think acquainted stifels work 35 p. 703. bürgi also 6it seems napier heard method prosthaphaeresis simpliﬁ cation trigonometric computations john craig obtained paul wittich frankfurt end 1570s 62 pp. 1112. wittich also inﬂuenced tycho brahe jost bürgi came close discovering logarithms 166. connection john craig misinterpreted meaning longomontanus invented logarithms would copied napier 33 p. 99101. unfairness several authors towards napier see cajori 33 pp. 107108. perhaps extreme case unfairness jacomyrégnier claims napier calculating machine made bürgi return shy bürgi told invention logarithms 88 p. 53. 7for example carslaw misled apparent meaning logarithm although observes napiers construction absolutely agree meaning 37 p. 82. 3
indirectly inﬂuenced stifel and fact extensive correspondence arithmetic geometric progression probably bürgi 26. but see napier could impossibly go whole way explicit correspondence needed another deﬁnition. 2.2 deﬁnition logarithm precise deﬁnition logarithm given napier follows.8 considered two lines ﬁgure 1 two points moving left right diﬀerent speeds. ﬁrst line point α moving arithmetically left right is constant speed equal 107. ﬁgure shows positions point instant t 2t etc. unit time. ﬁrst interval traversed 107t. modern terms xαt 107t. α β 0 107 2t 3t 4t figure 1 progressions α β. second line which length 107 point β moving geomet rically sense distances left traverse instants t 2t etc. form geometrical sequence. let us analyze napiers deﬁni tion modern terms. di remaining distance instant ti di1 di c c constant. xβt distance traversed since beginning 107 xβn 1t 107 xβnt c set yβt 107 xβt yβn 1t cyβnt therefore yβnt cnyβ0. follows yβt cttyβ0 aat equal multiples t. initially t 0 point β left line therefore 107. follows xβt 1071 at 1071 et ln a again normally deﬁned kt. 8we borrowed notations michael lexas introduction 103. 4
napier set initial speed 107 determines motion entirely. indeed y βt 107at ln a easily obtain e1. initial speed therefore determines a. moreover xβt depend t means napiers construction deﬁnes correspondence unit plays role. motion deﬁned napier therefore corresponds equation xβt 1071 et remembered napier nowhere uses notations exponentials derivatives construction. one par ticular consequence which napiers assumption deﬁnition speed β equal distance left traverse x βt 107at 107 xβt yβt. another consequence point β travels equal time increments two numbers equally proportioned used later computing logarithms second table constructed napier. now napiers deﬁnition logarithm denote λn following time β position x α position y λn107 x y. instance beginning x 0 λn107 0. using kinematic approach theory proportions9 napier thus able deﬁne correspondence two continua lines α β. modern expression napiers logarithms easily obtained. yβt napier associates xαt words x 107et napier associates λnx 107t hence λnx 107ln107 ln x. 1 particular λnx 107 ln107 modern logarithm base 1 e but contrary written various authors λnx loga rithm base 1 e. however instead factor 107 take 1 λnx indeed modern logarithm base 1 e.10 9many works concerned proportions napier mention particular portuguese alvarus thomas 1509 used geometric progression divide line 187 33. 10for overview various bases ascribed napiers logarithms see matzka 111 112 cantor 35 p. 736 tropfke 190 p. 150 mautz 113 pp. 67. many authors addressed question reader consult references cited end article. 5
also write λnx 107 ln x 107 view 107 scaling factor λnx x. words scale sines logarithms napiers tables 107 obtain correspondence essentially logarithm base 1 e. although logarithms precisely deﬁned napiers procedure computation obvious. this per se already particularly interesting napier managed deﬁne numerical concept separate con cept actual computation. next napier deﬁnes geometric divisions use approximate values logarithms. 2.3 geometric divisions napier constructed three geometric sequences. sequences start 107 use diﬀerent multiplicative factors. tables given sections 10.1 10.2 10.3.11 ﬁrst sequence ai uses ratio r1 0.9999999 ai1 ai r1 a0 107. napier computes 100 terms sequence using exactly 7 decimal places a100 9999900.0004950 136 p. 13. values ai given section 10.1. napiers last value correct. second sequence bi also starts 107 uses ratio r2 0.99999. bi1 bi r2. napier computes 50 terms reaches b50 9995001.224804 using exactly 6 decimal places. napiers value actually b 50 9995001.222927 136 p. 14 must made computation error error much larger one due rounding.12 error together conﬁdence napier displays calculations certainly indicates alone computations.13 ﬁrst second sequences related b1 a100 follows 0.9999999100 0.99999. however see napier identify a100 b1 λna100 λnb1. 11some notations sequences appear identical nearly identical carslaw 37 pp. 7981 mere coincidence. reason carslaw name values third table except ﬁrst line ﬁrst column. 12even without details napiers calculations even without napiers manuscript probably possible pinpoint error precisely assuming one error made made attempt aware analysis error. stress used napiers rounding would obtained b50 9995001.224804 b50 9995001.224826 explained fischer 49. rounded computation given future document. 13in 60 constructio napier observes inconsistencies table suggests way improve it without noticing least part problem lies computation error. 6
third sequence c actually made 69 subsequences. subsequences uses ratio r3 0.9995. ﬁrst subsequences c00 c10 c20 . . . c200 starts c00 107. second subsequence c01 c11 c21 . . . c201. 69th subsequence c068 c168 c268 . . . c2068. 1 i 20 0 j 68 cij ci1j r3. particular c10 9995000 b50 0.9999950 0.9995. relationship one subsequence next one con stant ratio r4 0.99 c0j1 c0j r4. moreover c20i c0i1 0.999520 0.99. napier computed ﬁrst column third table using 5 decimal places. then used values compute values rows 4 decimal places. napier found c200 9900473.57808 136 p. 14 exact 9900473.57802314 c201 9801468.8423 136 p. 15 exact 9801468.842243 c202 9703454.1539 136 p. 16 exact 9703454.153821 . . . c068 5048858.8900 136 p. 16 exact 5048858.887871 c2068 4998609.4034 136 p. 16 exact 4998609.401853. napier therefore divided interval 107 5 106 69 20 1380 subintervals constant ratio although junctions 69 subsequences. then 1380 in tervals could divided 50 intervals corresponding ratio 0.99999 intervals could turn divided 100 intervals whose ratio 0.9999999. sequences therefore provide approximation subdivision interval 107 5 106 many small intervals corresponding ratio 0.9999999. logarithm 0.9999999 could computed sequences would provide means compute approxima tions logarithms values sequence. values third table dense enough used con junction sines angles 90to 30 whose sine 5000000 sinus totus 107. ideally napier would divided whole interval 1075 106 using ratio r1 impossible task. using ratio 0.9999999 total 100 50 20 69 6900000 times would obtained last value 5015760.517. would fact needed go little 5 106. proceeded way iterative divisions would introduced important rounding errors. contrary three sequences napier able keep rounding errors minimum 14again work consider ideal computations postpone examination rounded calculations separate work. using napiers rounding actually obtained c200 9900473.57811 values table also slightly diﬀerent. 7
since never 100 multiplications row fact mostly 20. 2.4 approximation logarithms napiers next task compute mere approximations logarithms value sequences interval approximations15 is bounds logarithm taking particular account fact various sequences seamless. using interval approximations napier knows exactly accuracy computations. 2.4.1 computing ﬁrst logarithm α β e f c b figure 2 bounds λncb. napier ﬁrst tries compute logarithm a1 9999999. considers conﬁguration ﬁgure 2 b endpoints β c point between. e origin line α f point α corresponding c. β line extended left time taken go equal time taken go c. this explained above entails cb ab ab db . 2 since ef logarithm cb since speed β equal distance β b since distance decreases speed β also decreases naturally ac ef. similarly da ef. two bounds logarithm cb ac λncb da 15what napier actually interval arithmetic. ﬁeld developed ex tensively 20th century starting youngs seminal work 204. popular introduction interval arithmetic see hayes article american scientist 79. 8
using equation 2 compute bounds ac ab cb 107 cb da db ab ab db ab 1 ab ab cb 1 ab ab cb cb ab ac cb practice gives 107 cb λncb 107 107 cb cb 3 taking cb 9999999 have 1 λn9999999 107 9999999 1 107 constructio napier gives upper bound 1.00000010000001 136 p. 21. napier decided take average two bounds set λn9999999 1.00000005 136 p. 21. exact value λn9999999 107ln107 ln9999999 1.000000050000003 . . . napiers approximation therefore excellent.16 napier compute logarithms ﬁrst sequence ai since approximations ﬁrst two values sequence λna0 0 1 λna1 107 9999999 also λnai1 λnai λna1 λna0 λna1. hence λnai iλna1 λnai 107 9999999 napier takes approximation λnai λnai i 1.00000005 16some authors write incorrectly napier took logarithm 9999999 1. instance case delambre 42 vol. 1 p. xxxv. logarithm 9999999 may 1 napiers ﬁrst experiments actual descriptio. 9
ﬁlls table section 10.1. order distinguish theoretical values logarithms values corresponding napiers process denote latter l1ai ﬁrst table l2bi second table l3cij third table. moreover necessary use l iaj napiers actual table values diﬀer recomputed ones.17 always λnai l1ai λnbi l2bi λncij l3cij given excellent approximation λna1 values ﬁrst table also excellent approximations. reconstruction given approximations napier would obtained computations correct rounded. words approximation take approximation ﬁrst logarithm arithmetic mean bounds. last value ﬁrst table λna100 bounds reasoning with exact computation 100 λna100 100.000010000001 . . . 2.4.2 computing ﬁrst logarithm second table next napier tackles computation logarithm b1 9999900. napier already knows logarithm a100 b1 consider logarithms a100 b1 equal. instead napier consider diﬀerence λnb1 λna100 shows diﬀerence bounded. proceeds follows. first observes b c d λnaλnb λncλnd consequence napiers deﬁnition. then considers ﬁgure 3. let ab 107 cb db two logarithms whose diﬀerence considered. points e f deﬁned ea ab cd db af ab cd cb 17l slightly diﬀers l consequence napiers errors rounding. l diﬀers λn napier took arithmetic mean approximating values logarithms. 10
β e f c b figure 3 bounding diﬀerence two logarithms. follows eb ab abcd db ab ab ab cd db 1 ab cd db db cb db fb ab ab fa ab ab ab cd cb ab cb cd cb db cb therefore λndb λncb λnfb λnab λnfb seen above since fb ab ab eb af λnfb ea therefore abcd cb λndb λncb abcd db 4 taking cb a100 db b1 107 a100 b1 a100 λnb1 λna100 107 a100 b1 b1 numerically gives with exact values 0.00049500333301 . . . λnb1 λna100 0.00049500333303 . . . since two bounds close napier took λnb1 λna100 0.0004950 approximation λna100 found above ob tained 136 pp. 2930 100 0.0004950 . . . λna100 λnb1 λna100 100.0000100 . . . 0.0004950 . . . 100.0004950 . . . λnb1 100.0005050 . . . accurate bounds are 100.00049500333301 . . . λnb1 100.00050500333403 . . . 11
napier chose take average two bounds namely l2b1 100.0005 excellent approximation real value λnb1 100.00050000333 . . . napier computes logarithms second sequence using approximation λnbi i 100.0005 bounds last value λnb50 50 100.0004950 . . . λnb50 50 100.0005050 . . . napier chose l2b50 5000.0250000 also excellent approxi mation real value λnb50 5000.0250001 . . . 2.4.3 computing two logarithms third table order complete third table napier compute approximations two logarithms λnc10 λn9995000 λnc01 λn9900000. might ﬁrst think using fact b50 close c10 consider diﬀerence λnc10 λnb50. bounds found using equation 4 107b50 c10 b50 λnc10 λnb50 107b50 c10 c10 is using exact value b50 1.225416581 . . . λnc10 λnb50 1.225416731 . . . fact napier obtains even better bounds goes back ﬁrst table dense one. napier ﬁrst computes x x 107 c10 b50 gives x 9999998.774583418771 . . . napier actually found x 9999998.7764614 136 p. 31.18 then λnc10λnb50 λnxλn107 λnx. express bounds λnxλna1 closest value x ﬁrst table a1 9999999 18the diﬀerence essentially due error b50 constructing sequence bi. 12
107a1 x a1 λnx λna1 107a1 x x 0.2254166037 . . . λnx λna1 0.2254166088 . . . 1 λna1 1.000000100 . . . hence 1.2254166037 . . . λnx 1.2254167088 . . . exact value 1.225416656 . . . bounds accurate ones obtained using second table only ﬁrst bounds also good. napier actually found 1.2235386 . . . λnx 1.2235387 . . . diﬀerence real bounds consequence napiers error b50 translated error 0.002 x. this obtain using exact values bounds 5000.0247501 . . . 1.2254166 . . . λnc10 5000.0252501 . . . 1.2254167 . . . using exact values 5001.2501667 . . . λnc10 5001.2506668 . . . average bounds 5001.2504168 napier found 5001.2485387 see 136 p. 31. take l3c10 5001.2504168. diﬀerence napiers value exact one still due error b50. error propagate increase building third table. exact value λnc10 5001.25041682 . . . napiers ideal19 approximation excellent. napier compute approximations ci0 20. c200 exact bounds 100025.003335 . . . λnc200 100025.013337 . . . napier uses value 100024.9707740 136 p. 32 instead better 100025.008 . . . still consequence error b50. however rounded one decimal values identical see 136 p. 34. 19ideal napier hadnt made error. 13
next napier observes c200 c01. similarly napier bounds diﬀerence λnc01 λnc200. used ﬁrst column third table would obtained 107c200 c01 c200 λnc01 λnc200 107c200 c01 c01 is using exact value c200 478.33875779 . . . λnc01 λnc200 478.36163968 . . . but napier did use second table instead ﬁrst col umn third table. napier found x x 107 c01 c200 gives x 9999521.66124220 . . . napier actually found x 9999521.6611850 136 p. 33. value outside range ﬁrst table within range second table. then λnc01 λnc200 λnx λn107 λnx. express bounds λnb5 λnx closest value x ﬁrst table b5 9999500.009999900 . . . 107x b5 x λnb5 λnx 107x b5 b5 21.6522780 λnb5 λnx 21.6523249 . . . 21.6523250 λnx λnb5 21.6522780 500.0024750 λnb5 500.0025251 hence 478.3501501 λnx 478.3502471 bounds much accurate ones obtained using third table only. napier actually found 478.3502290 λnx 478.3502812. this obtain using exact values bounds 100025.003335 . . . 478.350150 . . . λnc01 100025.013337 . . . 478.350247 . . . using exact values 100503.353485 λnc01 100503.363585 14
average two bounds 100503.358535 exact value λnc01 100503.35853501 . . . napier instead found 100503.3210291 136 p. 33 diﬀerence values still due error b50. approximations λnc200 λnc01 napier obtain bounds values third table. first obtains bounds λnc0j λnc0j λnc00 j λnc01 λnc00 hence λnc0j j λnc01 necessary bounds λnc01 used ﬁnd bounds λnc0j. finally λncij λnc0j λnc1j λnc0j λnc10 λnc00 λnc10 λncij λnc0j λnc11 j λnc01 λnc10 logarithms third table computed mere linear combination logarithms λnc10 λnc01. 2.5 finding logarithms sines shown sole purpose ﬁrst second tables build third table. now third table used ﬁnd logarithms integers 0 107 rather sine values radius taken equal 107. use following notation denote sine values n standing napier sinn x 107 sin x time sine considered ratio length semichord circle certain radius. napier sine 90is 107. napier course exponent notation. 15
2.5.1 source sines first napier needed sines cosines every minute 0and 45. napiers constructio somewhat misleading matter. would like construct table napier suggests use table reinholds table accurate one 136 p. 45. table mind must one published 1554 150. reinholds tables contain table tangents table sine latter every minute. appears sine values used napier agree reinhold 0 89 except typos. value sinus total also 107 reinholds tables. however number diﬀerences 89to 90 showing napier necessarily used least another diﬀerent source. also easy see napier cannot used rheticus 1551 canon 151 gives trigonometric functions every 10 minutes. could however used rheticus opus palatinum 1596 152 gave sines radius 1010 every 10 seconds napier would merely cut oﬀthree digits round. comparison rheticus opus palatinum shows diﬀerences suggesting probably napiers source. moreover napier probably started work copy opus palatinum reached him. actual source found 1990 glowatzki göttsche investigating regiomontanus tables 69. concluded napier either used table published fincke 1583 48 one published lansberge 1591 192. consulted finckes table found values lansberges 1591 edition20 napiers tables. fincke lansberges tables go back regiomontanus 14361476 perhaps even bianchini 1410c1469 168 p. 421.21 case whatever source napier obtain logarithm every sine. total 90 60 5400 diﬀerent sine values values could obtained others see later. computing initial tables logarithms took napier 20 years. 20one careful 1604 edition lansberge sine table 1591 edition. 1591 edition agrees napier. 21detailed accounts history trigonometric tables found particular works braunmühl 199 zeller 205 van brummelen 191. see also lüneburgs section sine tables 105 pp. 148162. 16
2.5.2 computing logarithm within range third table seen earlier computing ﬁrst logarithm 107 n λnn 107107 n n . setting x 107 n napiers procedure amounts approximate λnn 1 2 x x 1 x 107 x x2 2107. therefore x2 2107 neglected good approximation λnn x 107 n. napier uses approximation n 9996700 is x2 2107 0.5445 . . . napier chose limit found using equation 3 3300 λn9996700 3301 136 p. 36 is smallest value logarithm bounded two consecutive integers. correct bounds actually 3300 λn9996700 3301.0893 . . . napier slightly wrong case napier takes lower bound approximation λnn n 9996700 considers error less 0.5. exact computation shows error smaller 0.5 n 9996837. even computation slightly incorrect assuming limit case constructio limit used construction table problem actually concerns two sine values table log. sines namely 8832 8833. 5 106 n 9996700 n third table napier proceeds follows. first i j found ci1j n cij c0j1 n c20j. two numbers x y x n know λnx λny. bound either λnx λnn λnn λny. former case 107 n x x λnn λnx 107 n x n latter case 107 n λnn λny 107 y n n also bounds λnx λny adding them would obtain bounds λnn. however forget bounds use approximations λnnλnx λnnλny λnx λny. 17
example n 5 106 take x c2068 4998609.401853 c1968 5001109.956832 ﬁnd bounds λnn λny since n closest y. λny 6929252.1. therefore 107 1109.956832 5001109.956832 λnn λny 1071109.956832 5 106 2219.420 . . . λnn λny 2219.913 . . . λnn 6929252.1 2219.7 6931471.8 n 8 106 take x c522 7996285.161399 c422 8000285.304051 ﬁnd bounds λnn λny since n closest y. λny 2231078.9. therefore 107 285.304051 8000285.304051 λnn λny 107285.304051 8 106 356.61 . . . λnn λny 356.63 . . . λnn 2231078.9 356.6 2231435.5 napier actually found λn5000000 6931469.22 λn8000000 2231434.68. exact values 6931471.805599 2231435.513142 computations show napier could found much accurate values made error second table computed correctly ﬁrst logarithms third table. 2.5.3 napiers short table constructio 133 136 napier gives table sums values diﬀerences logarithms two numbers numbers correspond given ratio. table used ﬁnd logarithms numbers outside range 1075 106. napier shown earlier b c d λna λnb λnc λnd. diﬀerence λnp λnq therefore depends p q actual values p q. also important realize λnp λnq 107 lnpq λnpq 107 ln107 therefore λnp λnq λnpq. set ρpq λnq λnp. order ﬁll table napier uses value λn5000000 computed earlier. obtains ρ2 λn5000000 λn10000000 λn5000000 6931469.22. dif ference corresponding ratio 2. then napier computes λnx4 λnx2 λnx2λnx ρ2 hence ρ4 λnx4λnx 2λnx2 λnx 2ρ2. similarly obtains ρ8 3ρ2 20794407.66. 18
then napier uses value λn8000000 also computed earlier writes λn1000000 λn8000000 ρ8 hence λn1000000 λn8000000 ρ8 23025842.34. finally ρ10 λn1000000 λn10000000 λn1000000. iterat ing process napier computes ρ20 ρ40 etc. ρ10000000. general ρab ρa ρb ρab bρa. napier complete short table ﬁgure 4. noticed ρ10 107 ln 10 so way napier ac cidentally computed ln 10 also ln 2 ln 4 ln 8 etc. accurate calculations judged accuracy ln 2 ln 10 respectively ﬁve six correct places. authors wondered value λn1.22 short table readily gives approximation λn1 since λn1 λn1 λn107 ρ107 napiers table small diﬀerences beginning table accumulate produce greater diﬀerences λn1. using modern expression seen previously ﬁnd course λn1 161180956.509 . . . using table ﬁnd therefore λn1 161180955.81 napier λn1 161180896.38 136 p. 39. however value little importance construction logarithms. 2.5.4 computing logarithm outside range third table n 5106 determine k 2 4 8 10 . . . 5106 kn 107. two values choose one like. then know λnn λnkn ρk λnn λnkn ρk. example let us compute λn87265 87265 approximation sinn 30. exact value sinn 30 87265.354 . . . case take k 100. ﬁrst ﬁnd approximation λn8726500 using technique seen above. setting n 8726500 have c1213 n c1113 c1113 8727067.052058 c1213 8722703.518532 λnc1113 22bower shows instance λn1 computed using methods set forth 1614 1616 editions descriptio. napiers slight errors tables approximations use napiers original tables would give slightly diﬀerent value explained bower 18. see also sommerville 178. 19
given proportions sines. corresponding diﬀerences logarithm. given proportions sines. corresponding diﬀerences logarithm. di di 2 one 6931469.22 8000 one 89871934.68 4 13862938.44 10000 92103369.36 8 20794407.66 20000 99034838.58 10 23025842.34 40000 105966307.80 20 29957311.56 80000 112897777.02 40 36888780.78 100000 115129211.70 80 43820250.00 200000 122060680.92 100 46051684.68 400000 128992150.14 200 52983153.90 800000 135923619.36 400 59914623.12 1000000 138155054.04 800 66846092.34 2000000 145086523.26 1000 69077527.02 4000000 152017992.48 2000 76008996.24 8000000 158949461.70 4000 82940465.46 10000000 161180896.38 given proportions sines. corresponding diﬀerences logarithm. given proportions sines. corresponding diﬀerences logarithm. di di 2 one 6931471.77 8000 one 89871967.80 4 13862943.54 10000 92103403.32 8 20794415.31 20000 99034875.09 10 23025850.83 40000 105966346.86 20 29957322.60 80000 112897818.63 40 36888794.37 100000 115129254.15 80 43820266.14 200000 122060725.92 100 46051701.66 400000 128992197.69 200 52983173.43 800000 135923669.46 400 59914645.20 1000000 138155104.98 800 66846116.97 2000000 145086576.75 1000 69077552.49 4000000 152018048.52 2000 76009024.26 8000000 158949520.29 4000 82940496.03 10000000 161180955.81 figure 4 napiers short table above ideal values below. ﬁrst table copied constructio whereas second table napier obtained made error rounding. table diﬀers exact values ﬁgure 5 napier takes average bounds. 20
given proportions sines. corresponding diﬀerences logarithm. given proportions sines. corresponding diﬀerences logarithm. di di 2 one 6931471.81 8000 one 89871968.21 4 13862943.61 10000 92103403.72 8 20794415.42 20000 99034875.53 10 23025850.93 40000 105966347.33 20 29957322.74 80000 112897819.14 40 36888794.54 100000 115129254.65 80 43820266.35 200000 122060726.46 100 46051701.86 400000 128992198.26 200 52983173.67 800000 135923670.07 400 59914645.47 1000000 138155105.58 800 66846117.28 2000000 145086577.39 1000 69077552.79 4000000 152018049.19 2000 76009024.60 8000000 158949521.00 4000 82940496.40 10000000 161180956.51 figure 5 exact values ratios rounded two places. 1361557.4. therefore λnn λnc1113 107 c1113 n n 1361557.4 649.80 1362207.20 λn87265 λn100 87265 ρ100 1362207.20 46051701.66 47413908.86 napier found λn87265 47413852 compute λn87265 way. however used method would still error ﬁrstly consequemce incorrect value c1113 third table consequence error b50 secondly error ρ100. moreover napier would 5400 computations. however easier compute logarithms loga rithms. napier actually used following equation 136 p. 42 λn 1 2 sinn 90 λnsinn α λn sinn α 2 λn sinn 90 α 2 . immediate consequence sin α 2 sin α 2 cos α 2 . 21
formula suﬃcient compute logarithms sines 45and 90and therefore make use half third table. previous equation also yields λnsinn α λn 1 2 sinn 90 λnsinn 2α λnsinn90 α 5 2230 α 45 λnsinn α expressed using already known values. process iterated values logarithms sinn α α 2230 known values logarithms sinn α 1115 α 2230 obtained 136 p. 43. example take α 1230. using equation 5 ﬁnd λnsinn 1230 λn 1 2 sinn 90 λnsinn 25 λnsinn7730 λn5000000 λn4226183 λn9762960 6931469 8612856 239895 15304430 value napiers table. general however resulting value exact errors add up. ﬁrst approach value transposed standard interval would better assuming basic tables correctly computed values ρ known suﬃcient accuracy. figure 6 shows complete computation λnsinn 30 obtained values computed earlier. calculation appears results coincide napier. instance using equation 5 napier found λnsinn 32 6350304 6350305. three seven computations show discrepancy start values tables napiers value λn5000000. likely explanation napier computation decimal places rounded results. 3 napiers 1614 tables napiers tables span 90 pages one page covering half degree ﬁgure 7. sines arranged semiquadrantically line approx imations of sinnα λnsin α λnsin α λnsin90 α λnsin90 α sin90 α order. instance α 030 have 22
λnsinn 32 λn 1 2 sinn 90 λnsinn 64 λnsinn58 λn5000000 λn8987946 λn8480481 6931469 1067014 1648179 6350304 table 6350305 λnsinn 16 λn 1 2 sinn 90 λnsinn 32 λnsinn74 λn5000000 λnsinn 32 λn9612617 6931469 6350305 395086 12886688 table 12886689 λnsinn 8 λn 1 2 sinn 90 λnsinn 16 λnsinn82 λn5000000 λnsinn 16 λn9902681 6931469 12886689 97796 19720362 table 19720362 λnsinn 4 λn 1 2 sinn 90 λnsinn 8 λnsinn86 λn5000000 λnsinn 8 λn9975640 6931469 19720362 24390 26627441 table 26627442 λnsinn 2 λn 1 2 sinn 90 λnsinn 4 λnsinn88 λn5000000 λnsinn 4 λn9993908 6931469 26627442 6094 33552817 table 33552817 λnsinn 1 λn 1 2 sinn 90 λnsinn 2 λnsinn89 λn5000000 λnsinn 2 λn9998477 6931469 33552817 1523 40482763 table 40482764 λnsinn 30 λn 1 2 sinn 90 λnsinn 1 λnsinn8930 λn5000000 λnsinn 1 λn9999619 6931469 40482764 381 47413852 table 47413852 figure 6 computation λnsinn 30 using values computed previously. case table values exact values used compute new value. 23
sinnα 87265 λnsinn α 47413852 λnsinn α λnsinn90 α 47413471 λnsinn90 α 381 sinn90 α 9999619 deﬁne σnx λnsinn x τnx diﬀerential σnxσn90 x τnx simple expression τnx λnsinn x λncosn x 107ln107 lnsinn x 107ln107 lncosn x 107 lntann x τn positive 0to 45and negative afterwards indicated signs top middle column. 4 wrights 1616 tables translated descriptio wright actually reset tables 132. conspicuous change probably translation heading gr. becoming deg. fact wright reduced sines logarithms one ﬁgure 18 p. 14. instance 030 napier sinn030 87265 λnsinn030 47413852 ﬁgure 7 whereas wright sinw030 8726 λwsinw030 4741385 ﬁgure 8. according oughtred change made make interpolation easier 67 pp. 179. feature reproduced reconstruction 1616 table. another diﬀerence use decimal point portion table ﬁgure 8. change introduced wright naturally also changes modern ex pression logarithm tables. deﬁnition have λwx 1 10λn10x 106ln107 ln10x particular λw1 13815510.557 . . . article bower shows values λn1 λw1 computed using methods set forth napiers descriptio trans lation 18 pp. 1516. 24
figure 7 excerpt napiers table 1620 reprint. table almost identical 1614 table current table reset. compare page wrights version ﬁgure 8. 25
figure 8 excerpt wrights translation. compare page orig inal version ﬁgure 7. 26
expression diﬀerential remains similar τwx λwsinw x λwcosw x 106ln10 cosw x ln10 sinw x 106 ln tanw x therefore 1614 values diﬀerential could also divided 10. however process seems erred ﬁrst values table.23 second edition wrights translation published 1618 also con tained anonymous appendix probably written william oughtred ﬁrst time socalled radix method computing logarithms used 67. method described detail briggs arithmetica logarithmica 21. 5 decimal fractions decimal fractions know today used napier recent introduction. stevin made decisive step forward 1585 work de thiende 35 pp. 615617 169. fractions considered before concept position. several mathematicians either came close it24 even propounded used decimal fraction notation25 stevin ﬁrst devote book speciﬁc matter introduce notation fractional digits level integer digits speciﬁc notation indicated weight fractional digit. stevin therefore admittedly clumsy index notation fractional digits although use full index notation integer part. integer part considered index 0 0 could viewed position marker equivalent fractional dot. stevin wrote 318 0 9 1 3 2 7 3 for 318.937. stevin extended four fundamental operations numbers proved validity rules. late introduction concepts although position concept existed sumerian mathematics decimal numeration traced back egypt fourth millenium b.c. partly due resistance introduction socalled hinduarabic numerals popular 1500. 23for instance wright gives λwsinw 2 7449419 λwsinw 8958 .2 diﬀer ential 7449421. three ﬁrst lines table show idiosyncrasies perhaps printing errors. 24one earliest example john murs quadripartitum numerorum com pleted 1343. 25for instance viète used decimal fractions universalium inspectionum 1579. 27
dots used notation numbers 1500 dots decimal dots. usually used separate groups digits. writers also written two numbers next other one integer part one fractional part separating symbol for instance without grasping full signiﬁcance juxtaposition. clavius instance used point decimal separator 1593 still wrote decimal fractions common fractions 1608 grasp decimal notation open doubt 169 p. 177. jost bürgi however put small zero last integral ﬁgure unpublished coss called arithmetica cantor completed around end 16th century kepler ascribed new kind decimal notation bürgi. pitiscus also used decimal points trigonometria published 1608 1612 however use lacked consistency.26 occasional systematic 169 p. 181. napier fact main introducer decimal fractions common practice new mathematical instrument became best vehicle decimal idea. descriptio actually contains rare instances decimal fractions. decimal fractions napiers 1614 table27 within tables wrights 1616 translation decimal fractions occur angles 89 90 degrees.28 instance sine 8930 degrees given 999961.9 logarithm 38.1 ﬁgure 8. decimal fractions appear full force constructio written number years descriptio subtends it. several examples values found constructio particular values contained three tables progressions short table already given earlier document. one ﬁrst sentences constructio following 136 p. 8 in numbers distinguished thus period midst whatever written period fraction denominator unity many cyphers ﬁgures period. 26cantor one authors wrote pitiscus 1608 1612 tables exhibited use dot separator decimal part 35 pp. 617619. this however true. even cursory examination two tables shows contain indeed dots dots separating groups digits separate decimal part course ascribed sole meaning. 27it noted maseres reprint descriptio course retypeset it layout slightly diﬀerent. maseres main change group ﬁgures introduction commas appear original version 110. maseres gives instance sinm 4330 6883546 λmsinm 4330 7734510. 28wright must used decimal points logarithms small felt rounding values integer would produce great loss accuracy. 28
6 computing napiers logarithms 6.1 basic computations napiers logarithms used basic multiplications divisions transformed additions subtractions. easy see λnab 107ln107 ln ln b λna λnb 107 ln107 λna λnb λn1 λnab 107ln107 ln ln b λna λnb 107 ln107 λna λnb λn1 previous computations normally involve addition subtraction large number λn1 161180896.38 napiers constructio. however many cases one dispense constant. in stance wish compute λnabc λnabc λnab λnc λn1 λna λnb λn1 λnc λn1 λna λnb λnc equations easily transposed wrights version loga rithms λwab λwa λwb λw1 λwab λwa λwb λw1 λwabc λwa λwb λwc may come surprise however napier give simple examples descriptio. napiers simplest examples appear chapter 5 end book descriptio. considers four cases. ﬁrst case a b c c b b a values b given c sought. napier shows λnc 2λnb λna c easily obtained. second case proportions c b b a c given b sought. napier explains square root b ac replaced division two λnb 1 2λna λnc. third case four numbers a b c considered b c b c. knowing ﬁrst three fourth sought. λnd λnb λnc λna. 29
fourth case proportions third case given b c sought. λnc λnd 1 3λnaλnd λnb λna 1 3λnd λna. 6.2 oughtreds radix method appendix second edition wrights translation probably au thored william oughtred radix method given calculating logarithm number. method actually involves extension napiers short table. values new table values ρw1 ρw2 ρw3 etc. equivalent ρ1 ρ2 ρ3 etc. one digit less. order compute λwn oughtreds idea ﬁnd 980000 1000000 wrights translation λw106 0 product simple factors 1 2 3 . . . 10 20 . . . 90 100 . . . 1.1 1.2 1.3 . . . 1.01 1.02 . . . 1.09. then λwn λwan ρwa ρwa computed using oughtreds table relation ρwab ρwa ρwb. λwan computed either sight near 1000000 using closest value table interpolation. interestingly appendix also makes use oughtreds multiplication abreviations sine tangent cosine cotangent appear one ﬁrst time formulæ. full analysis appendix given glaisher 67 give details radix method study briggs arithmetica logarithmica 160. 6.3 negative numbers descriptio book 1 chapter 1 napier explains numbers greater sinus total negative logarithm logarithmes numbers greater whole sine lesse nothing . logarithmes lesse nothing cal defective wanting setting marke before 132 p. 6. 6.4 scaling notation descriptio napier introduced primitive exponent notation ma nipulating numbers need scaled ﬁt within table. in stance logarithm 137 sought ﬁnds 1371564 among sines. value table 19866327 exact 19866333.98 napier writes 198663270000 meaning four ﬁgures must removed sine. napier manipulates logarithms calls impure 30
number zeros exponents. logarithms without 000. 000 . . . called pure. napier impure logarithm 230258420 fact equal 0 added impure logarithm .0. order render pure. napier really merely move exponent man tissa. napier manipulating decimal characteristic transposed logarithms. apart glaisher 1920 68 p. 165 napiers nota tion apparently hasnt much described yet easy under stand transpose modern notations. x pure logarithm x 000 . . . .0 z n merely notation x nρ10. x000 . . . .0 z n notation x nρ10. given ρ10 λn1 λn10 easy see λna λn10na nρ10. previous example λn137.1564 λn1371564 4ρ10 198663270000. so 230258420 actually represents 23025842 ρ10 0. notation napier add subtract zeros exponents really adds subtracts ρ10. cumbersome notation certainly one main reasons explained move decimal logarithms characteristics become mere integers. problem napiers logarithms and also natural logarithms base equal base numeration. 6.5 interpolation 1616 briggs supplied chapter introduced interpolation meth ods. 6.6 trigonometric computations 6.6.1 first example main purpose napiers table logarithms simplify trigono metric calculations. descriptio napier gives number rules speciﬁc triangle problems. instance proposition 4 chapter 2 book 2 reads triangle summe logarithmes angle side inclosing same equall summe logarithmes side angle opposite them. 132 p. 35 napier writes the logarithm angle implicitely means logarithm sine angle. words considering ﬁgure 9 given napier proposition means instance λnsinn a λnc λna λnsinn c 31
c b 57955 58892 b 26302 c 26 75 79 figure 9 application logarithms trigonometry. follows easily law sines. sin sin b b sin c c . therefore lnsin a lnsin c ln ln c λnx λn1 107 ln x hence 107 lnsin a 107 lnsin c 107 ln 107 ln c λnsinn aλn1 λnsinn cλn1λnaλn1λncλn1 reduces λnsinn a λnsinn c λna λnc λnsinn a λnc λna λnsinn c napiers result. previous example napier in 1614 obtains λna 545470700 λnsinn c 8246889 λnc 1335492100 computes λnsinn a 346684 resulting logarithm pure nearly λnsinn 75 therefore 75. napier adds result would 105if angle appeared obtuse. one checks napiers table value λnsinn c 8246889 found. two values c ﬁnd approaching values 32
α sinnα λnsinnα 1514 2627505 13365493 1515 2630312 13354817 3525 5795183 5455577 3526 5797553 5451488 100a lies sinn3525 sinn3526 100c lies sinn1514 sinn1515. isnt clear napiers values obtained interpolation procedure gives diﬀerent values 1072630312 2630200 2630312 λn2630200 λn2630312 1072630312 2630200 2630200 425.804 . . . λn2630200 λn2630312 425.823 . . . λn2630200 λn2630312 425.8 13354817 425.8 13355243 diﬀers napiers 13354921. 1075795500 5795183 5795500 λn5795183 λn5795500 1075795500 5795183 5795183 546.97 . . . λn5795183 λn5795500 547.00 . . . λn5795500 λn5795183 547 5455577 547 5455030 diﬀers napiers 5454701. wrights example exactly same logarithm values ten times smaller ensures napiers proposition still valid.29 6.6.2 second example third proposition book 2 descriptio involves diﬀerential corresponds logarithm tangent. proposition reads thus 132 p. 33 right angled triangle logarithme legge equall summe diﬀerentiall opposite angle logarithme leg remaining. take example ﬁgure 10 napiers example. given triangle three sides. like before set σnx λnsinn x 29so wright λwa 5454710 λwsinw c 824689 λwc 13354920 deduces λwsinw a 34668. wrights translation mistakenly writes 346680. 33
c b 9385 137 b 9384 c figure 10 computation rightangled triangle not scale. τnx diﬀerential σnx σn90 x napiers proposition amounts to λnb λnc τnb λnc σnb σn90 b λnc λnsinn b λncosn b since λnx 107ln107 ln x λn1 107 ln x reduces ln b ln c lnsin b lncos b ln c lntan b therefore b c tan b correct. napier uses proposition order ﬁnd angle b. approximating 137000 sinn47 136714 938400 sinn6947 9383925 in 1614 τnb λnb λnc 42924534000 635870000 42288664 34
since diﬀerential 42304768 050 42106711 051 napier concludes b 050. wright triangle dimensions values logarithms λwb 429245300 λwc 6358700 ﬁnds τwb 4228866 obtains b 05011. isnt clear wright obtained latter value incorrect. wrights table τw050 4230477 τw051 4210571 linear interpolation gives 0505 050.08. napier gave examples also considered use logarithms resolution spherical triangles. napier devised rule called circular parts useful triangles 87 104 128. 7 napiers scientiﬁc heritage 7.1 napiers suggestions napier suggested several improvements tables. first construc tio 136 p. 46 suggested ﬁner grain construction three funda mental tables ensure greater accuracy. new table 1 contain 100 steps new table 2 also 100 steps compared 50 initial scheme new table 3 contain 100 columns 35 steps instead 69 columns 20 steps. total number intervals would therefore 35 106 instead 6.9 106. then also constructio suggested better kind logarithms logarithm 1 0 logarithm either 10 1 10 equal 10. explained logarithms computed. gave particular example whereby decimal logarithm 5 computed repeated square root extractions constructing sequence approximating sought result. starting log 10 1 log 1 0 compute log 1 10 0.5 log q 10 10 0.75 getting closer log 5 using square roots products numbers greater smaller 5 136 pp. 51 97100 35 pp. 736737 190 pp. 168169. 7.2 decimal logarithms napiers work taken adapted briggs published ﬁrst table soon 1617. ﬁrst publish tables decimal logarithms trigonometric functions gunter 1620 75. briggs main tables published 1624 1633 adriaan vlacq published less accuratebut 35
extensivetables 1628 1633. tables used basis almost later tables beginning 20th century. 7.3 natural neperian logarithms logarithms introduced speidell 1622 1623 67 pp. 175 176 although table looked like one natural logarithms already appeared appendix second edition wrights translation. table fact table extending napiers short table number simple ratios. values table values ρ1 ρ2 ρ3 etc. equal 106 ln1 106 ln2 106 ln3 etc. table never thought table logarithms. table diﬀerences napierian logarithms historically interpreted otherwise although glaisher views ﬁrst publication natural logarithms. full analysis appendix thought william oughtred given glaisher 67. 7.4 tables 1620 also saw publication bürgis tables albeit without description use.30 kepler familiar napiers logarithms bürgis work also published tables own. time benjamin ursinus john speidell also pub lished tables extending napiers tables see particular cantor 35 p. 739 743. speidells table logarithms reproduced maseres 110. 7.5 slide rules important consequence napiers invention development slide rule. first came gunters scale. figure 11 shows logarithmic scale one scales found scale named edmund gunter 15811626 invented 1620. multiplications divisions could done rule using pair dividers. around 1622 william oughtred 15741660 idea using two logarithmic scales putting side side ﬁgure 12 made possible dispense dividers. oughtreds initial design used circular scales 144 200 pairs straight scales introduced later. 30for analysis bürgis tables see reconstruction 166. although bürgis tables viewed tables logarithms bürgi reach abstract notion developed napier considered coinventor logarithms. 36
1 2 3 4 5 6 7 8 9 10 10 20 30 40 50 60 70 80 90100 figure 11 logarithmic scale 1 100. easy see scales used work. ﬁg ure 11 scale goes 1 100 positions numbers proportional logarithm. words distance n 1 proportional logn. distance 1 10 10 100 logarithms equidiﬀerent. given di vider opening corresponds every ratio. multiplying 2 corresponds distance 1 2 also distance 2 4 4 8 10 20 etc. pair divider therefore easily used perform multiplications divisions. ﬁgure 12 two scales put parallel 1 ﬁrst rule put position second scale 2.5. since given ratio corresponds linear distance scales value c facing certain value b ﬁrst scale instance 6 c b 1 therefore c ab. arrangement used divisions dividers longer needed. 1 2 3 4 5 6 7 8 9 10 10 20 30 40 50 60 70 80 90100 1 2 3 4 5 6 7 8 9 10 10 20 30 40 50 60 70 80 90100 2.5 15 2.5 6 figure 12 two logarithmic scales showing computation 2.5 6 15. history slide rule refer reader cajoris articles books 30 31 3431 stolls popular account scientiﬁc american 185. construction logarithmic lines scale see also robertson 154 nicholson 142. good source recent information history slide rules journal oughtred society. important remember many popular encyclopædias cover ing large subject bound contain many errors even written reputed mathematicians. two interesting articles worth reading context mautz 113 miller 123. 31in 1909 book cajori ﬁrst attributes invention slide rule wingate corrects addenda. 37
8 note recalculation tables 8.1 auxiliary tables tables closing document computed ai bi cij exactly logarithms using napiers averaging method. hence took l1a1 1.00000005 l2b2 100.0005 l3c10 5001.2504168 l3c01 100503.358535. 8.2 main tables three volumes accompanying study. ﬁrst32 gives ideal recomputation napiers logarithms values sinn α λnα exact computation. two volumes33 provide approximations napiers tables as suming napier make mistake auxiliary tables assuming round value. two volumes con structed using napiers radix table well method interpolation. latter values two tables diﬀer ideal table. diﬀerences napiers actual table tables due errors sines rounding errors auxiliary tables errors interpolation. seemed pointless try mimick errors. also noted use trigonometric relations compute logarithms angles 45 napier would probably used them aware propagation errors time exact computations. would seemed strange correct errors napiers auxiliary tables time use errorprone trigonometric computation since combined result procedure would still far napiers actual table. rounded sines taking logarithms seems napier used accurate values sines small angles given table. may used rounded values larger angles. might try produce faithful approximation napiers table future. approximation would particular take rounding account computation sequences ai bi etc. 32volume napier1614idealdoc. 33volumes napier1614doc napier1616doc. 38
reconstruction 1616 table obtained rounding re construction 1614 table. wright didnt recompute value table. 9 acknowledgements pleasure thank ian bruce provided useful translations napiers descriptio constructio helped clarify details napiers work. 39
references following list covers important references34 related napiers tables. items list mentioned text sources seen marked so. added notes contents articles certain cases. 1 juan abellan. henry briggs. gaceta matemática 4 1st series3941 1952. this article contains many incorrect statements. 2 frances e. andrews. romance logarithms. school science mathematics 282121130 february 1928. 3 anonymous. ﬁrst introduction words tangent secant. philosophical magazine series 3 28188382387 may 1846. 4 raymond claire archibald. napiers descriptio constructio. bulletin american mathematical society 224182187 1916. 5 raymond claire archibald. william oughtred 15741660 table ln x. 1618. mathematical tables aids computation 325372 1949. 6 gilbert arsac. histoire de la découverte des logarithmes. bulletin de lassociation des professeurs de mathématiques de lenseignement public 299281298 1975. 7 raymond ayoub. napierian logarithm american mathematical monthly 1004351364 april 1993. 8 raymond ayoub. napier invention logarithms. journal oughtred society 32713 september 1994. not seen 9 évelyne barbin et al. editors. histoires de logarithmes. paris ellipses 2006. 34note titles works original titles come many idiosyncrasies features line splitting size fonts etc. often reproduced list references. therefore seemed pointless capitalize works according conventions relation original work also restore title entirely. following list references title words except german therefore left uncapitalized. names authors also homogenized initials expanded much possible. reader keep mind list meant facsimile original works. original style information could doubt added note done here. 40
10 margaret e. baron. john napier. charles coulston gillispie editor dictionary scientiﬁc biography volume 9 pages 609613. new york 1974. 11 yu. a. belyi. johannes kepler development mathematics. vistas astronomy 18643660 1975. 12 volker bialas. о вычислении неперовых и кеплеровых логарифмов и их различии über die berechnung der neperschen und keplerschen logarithmen und ihren unterschied. н. и. невская n. i. nevskaia editor иоганн кеплер сборник 2. johannes kepler collection articles n. 2 volume работы о кеплере в россии и германии the works kepler russia germany pages 97101. санктпетербург saint petersburg борейарт boreiart 2002. in russian german summary article p. 146 article brief description logarithms kepler napier diﬀerences 13 j. p. biester. decreasing logarithms according contrivance two authors great fame viz. neper kepler greatest use trigonometrical calculations taken impression copies wanting. present state republick letters 689107 1730. this preface biesters book. 14 jeanbaptiste biot. review mark napiers memoir john napier ﬁrst part. journal des savants pages 151162 march 1835. followed 15 reprinted 16. 15 jeanbaptiste biot. review mark napiers memoir john napier second part. journal des savants pages 257270 may 1835. sequel 14 reprinted 16. 16 jeanbaptiste biot. mélanges scientiﬁques et littéraires. paris michel lévy frères 1858. volume 2. pages 391425 reproduce articles 14 15. 17 nathaniel bowditch. application napiers rules solving cases rightangled spheric trigonometry several cases obliqueangled spheric trigonometry. memoirs american academy arts sciences 313337 1809. 18 william r. bower. note napiers logarithms. mathematical gazette 101441416 january 1920. 41
19 carl benjamin boyer. history mathematics. john wiley sons 1968. 20 henry briggs. logarithmorum chilias prima. london 1617. the tables reconstructed d. roegel 2010. 156 21 henry briggs. arithmetica logarithmica. london william jones 1624. the tables reconstructed d. roegel 2010. 160 22 henry briggs henry gellibrand. trigonometria britannica. gouda pieter rammazeyn 1633. the tables reconstructed d. roegel 2010. 159 23 ian bruce. napiers logarithms. american journal physics 682148154 february 2000. 24 evert marie bruins. history logarithms bürgi napier briggs de decker vlacq huygens. janus 674241260 1980. 25 david j. bryden. scotlands earliest surviving calculating device robert davenports circles proportion c. 1650. scottish historical review 551595460 april 1976. 26 jost bürgi. arithmetische und geometrische progress tabulen sambt gründlichem unterricht wie solche nützlich allerley rechnungen zugebrauchen und verstanden werden sol. prague 1620. these tables recomputed 2010 d. roegel 166 27 robert p. burn. alphonse antonio de sarasa logarithms. historia mathematica 28117 2001. 28 william d. cairns. napiers logarithms developed them. american mathematical monthly 3526467 february 1928. 29 florian cajori. history elementary mathematics hints methods teaching. new york macmillan company 1896. 30 florian cajori. notes history slide rule. american mathematical monthly 15115 1908. 31 florian cajori. history logarithmic slide rule allied instruments. new york engineering news publishing company 1909. 42
32 florian cajori. history exponential logarithmic concepts. american mathematical monthly 201514 january 1913. 33 florian cajori. algebra napiers day alleged prior inventions logarithms. knott 99 pages 93109. 34 florian cajori. history gunters scale slide rule seventeenth century. university california publications mathematics 19187209 1920. 35 moritz cantor. vorlesungen über geschichte der mathematik. leipzig b. g. teubner 1900. volume 2 pp. 702704 730743 napier 36 horatio scott carslaw. discovery logarithms napier merchiston. journal proceedings royal society new south wales 484272 1914. 37 horatio scott carslaw. discovery logarithms napier. mathematical gazette 81177684 may 1915. 38 horatio scott carslaw. discovery logarithms napier concluded. mathematical gazette 8118115119 july 1915. 39 horatio scott carslaw. napiers logarithms development theory. philosophical magazine series 6 32191476486 november 1916. 40 robert g. clouse. john napier apocalyptic thought. sixteenth century journal 51101114 april 1974. 41 frank cole. protologs complete logarithms john napier 15501617 realization. letchworth f. cole 1999. not seen 42 jeanbaptiste joseph delambre. histoire de lastronomie moderne. paris veuve courcier 1821. 2 volumes 43 charles henry edwards. historical development calculus. new york springerverlag 1979. 44 john ellis evans. logarithms base e justly called natural logarithms. national mathematics magazine 1429195 1939. 43
45 john fauvel. revisiting history logarithms. frank swetz john fauvel otto bekken bengt johansson victor katz editors learn masters pages 3948. mathematical association america 1995. 46 john fauvel. john napier 15501617. ems newsletter 382425 december 2000. reprinted pp. 1 68 cms notesnotes de la smc volume 33 issue 6 october 2001. 47 mordechai feingold. mathematicians apprenticeship science universities society england 15601640. cambridge cambridge university press 1984. 48 thomas fincke. geometria rotundi. basel henric petri 1583. not seen 49 joachim fischer. ein blick vor bzw. hinter den rechenschieber wie konstruierte napier logarithmen 1997. 50 joachim fischer. looking behind slide rule napier compute logarithms proceedings third international meeting slide rule collectors september 12 1997 fabercastell castle steinnürnberg pages 818 1997. 51 joachim fischer. napier computation logarithms. journal oughtred society 711116 1998. a corrected reprint available journal mentioned bob otnes note volume 72 fall 1998 p. 50. 52 alan fletcher jeﬀery charles percy miller louis rosenhead leslie john comrie. index mathematical tables. oxford blackwell scientiﬁc publications ltd. 1962. 2nd edition 1st 1946 2 volumes 53 dora m. forno. notes origin use decimals. mathematics news letter 3858 april 1929. 54 laurent fousse guillaume hanrot vincent lefèvre patrick pélissier paul zimmermann. mpfr multipleprecision binary ﬂoatingpoint library correct rounding. acm transactions mathematical software 332 2007. 55 jeanpierre friedelmeyer. contexte et raisons dune miriﬁque invention. barbin et al. 9 pages 3972. 44
56 jeanpierre friedelmeyer. linvention des logarithmes par neper et le calcul des logarithmes décimaux par briggs. activités mathématiques et scientiﬁques 61105122 february 2007. 57 anthony gardiner. understanding inﬁnity mathematics inﬁnite processes. mineola ny dover publications inc. 2002. 58 carl immanuel gerhardt. geschichte der mathematik deutschland volume 17 geschichte der wissenschaften deutschland. neuere zeit. münchen r. oldenbourg 1877. 59 george alexander gibson. napier invention logarithms. proceedings royal philosophical society glasgow 453556 1914. 60 george alexander gibson. napier invention logarithms. napier tercentenary celebration handbook exhibition pages 116. edinburgh 1914. reprint 59. 61 george alexander gibson. napiers logarithms change briggss logarithms. knott 99 pages 111137. 62 owen gingerich robert s. westman. wittich connection conﬂict priority late sixteenthcentury cosmology volume 78 7 transactions american philosophical society. american philosophical society 1988. 63 lynne gladstonemillar. john napier logarithm john. edinburgh national museums scotland publishing 2003. 64 james whitbread lee glaisher. report committee mathematical tables. london taylor francis 1873. also published part report fortythird meeting british association advancement science london john murray 1874. review r. radau published bulletin des sciences mathématiques et astronomiques volume 11 1876 pp. 727 65 james whitbread lee glaisher. logarithm. encyclopædia britannica 11th edition volume 16 pages 868877. new york encyclopædia britannica company 1911. 66 james whitbread lee glaisher. napier john. encyclopædia britannica 11th edition volume 19 pages 171175. new york encyclopædia britannica company 1911. 45
67 james whitbread lee glaisher. earliest use radix method calculating logarithms historical notices relating contributions oughtred others mathematical notation. quarterly journal pure applied mathematics 46125197 1915. 68 james whitbread lee glaisher. early tables logarithms early history logarithms. quarterly journal pure applied mathematics 48151192 1920. 69 ernst glowatzki helmut göttsche. die tafeln des regiomontanus ein jahrhundertwerk volume 2 algorismus. munich institut für geschichte der naturwissenschaften 1990. 70 herman heine goldstine. history numerical analysis 16th 19th century. new york springer 1977. 71 nicolaas lambertus willem antonie gravelaar. john napiers werken. amsterdam johannes müller 1899. 72 george john gray. edmund gunter. dictionary national biography volume 8 pages 793794. london smith elder co. 1908. volume 23 1890 ﬁrst edition 73 norman t. gridgeman. john napier history logarithms. scripta mathematica 29124965 1973. 74 detlef gronau. logarithms calculation functional equations. ii. österreichisches symposium zur geschichte der mathematik neuhofen der ybbs 22.28. oktober 1989 1989. also published notices south african mathematical society vol. 28 no. 1 april 1996 pp. 6066. 75 edmund gunter. canon triangulorum. london william jones 1620. recomputed 2010 d. roegel 158. 76 rafail samoilovich guter iurii leonovich polunov. dzhon neper 15501617. moscow nauka 1980. biography napier russian seen. 77 edmund halley. easie demonstration analogy logarithmick tangent meridian line sum secants various methods computing utmost exactness. philosophical transactions 19202214 16951697. 46
78 william francis hawkins. mathematical work john napier 15501617. phd thesis university auckland 1982. 3 volumes 79 brian hayes. lucid interval. american scientist 91484488 novemberdecember 2003. 80 j. henderson. methods construction earliest tables logarithms. mathematical gazette 15210250256 december 1930. 81 james henderson. bibliotheca tabularum mathematicarum descriptive catalogue mathematical tables. part i logarithmic tables a. logarithms numbers volume xiii tracts computers. london cambridge university press 1926. 82 christopher hill. intellectual origins english revolution revisited. oxford clarendon press 1997. 83 ernest william hobson. john napier invention logarithms 1614. cambridge university press 1914. 84 ellice martin horsburgh editor. modern instruments methods calculation handbook napier tercentenary exhibition. london g. bell sons 1914. 85 charles hutton. mathematical tables containing common hyperbolic logistic logarithms also sines tangents secants versedsines etc. london g. g. j. j. robinson r. baldwin 1785. 86 alex inglis. napiers educationa speculation. mathematical gazette 20238132134 may 1936. 87 james ivory. napiers rules circular parts. philosophical magazine series 1 58282255259 october 1821. 88 jacomyrégnier. histoire des nombres et de la numération mécanique. paris napoléon chaix et cie 1855. 89 graham jagger. making logarithm tables. martin campbellkelly mary croarken raymond flood eleanor robson editors history mathematical tables sumer spreadsheets pages 4877. oxford oxford university press 2003. 47
90 philip edward bertrand jourdain. john napier tercentenary invention logarithms. open court 289513520 september 1914. 91 louis charles karpinski. decimal point. science new series 451174663665 1917. issue dated 29 june 1917 92 wolfgang kaunzner. logarithms. ivor grattanguinness editor companion encyclopedia history philosophy mathematical sciences volume 1 pages 210228. london routledge 1994. 93 johannes kepler. chilias logarithmorum. marburg caspar chemlin 1624. reproduced 95. 94 johannes kepler. supplementum chiliadis logarithmorum. marburg caspar chemlin 1625. reproduced 95. 95 johannes kepler. gesammelte werke volume ix mathematische schriften. münchen c. h. becksche verlagsbuchhandlung 1960. this volume contains 93 94. 96 johannes kepler john napier henry briggs. les milles logarithmes etc. bordeaux jean peyroux 1993. french translation keplers tables nepers descriptio jean peyroux. 97 georg kewitsch. bemerkungen zu meinem aufsatze die basis der bürgischen und neperschen logarithmen nebst einigem anderen. zeitschrift für mathematischen und naturwissenschaftlichen unterricht 27577579 1896. 98 georg kewitsch. die basis der bürgischen logarithmen ist e der neperschen 1 e. ein mahnruf die mathematiker. zeitschrift für mathematischen und naturwissenschaftlichen unterricht 27321333 1896. 99 cargill gilston knott editor. napier tercentenary memorial volume. london longmans green company 1915. 100 john knox laughton. edward wright. dictionary national biography volume 21 pages 10151017. london smith elder co. 1909. volume 63 1900 ﬁrst edition 101 loïc le corre. john neper et la merveilleuse table des logarithmes. barbin et al. 9 pages 73112. 48
102 xavier lefort. histoire des logarithmes un exemple du développement dun concept en mathématiques. miguel hernández gonzález editor proyecto penélope pages 142151. tenerife españa fundaciõn canaria orotava de historia de la ciencia 2002. 103 michael lexa. remembering john napier logarithms. ieee potentials magazine 2002. 104 edgar odell lovett. note napiers rules circular parts. bulletin american mathematical society 410552554 july 1898. 105 heinz lüneburg. von zahlen und größen dritthalbtausend jahre theorie und praxis. basel birkhäuser 2008. 2 volumes 106 g. b. m. review johannes tropfke geschichte der elementarmathematik. nature 691792409410 1904. issue dated 3 march 1904 107 g. b. m. base napiers logarithms. nature 691799582 1904. issue dated 21 april 1904. 108 william rae macdonald. john napier. dictionary national biography volume 14 pages 5965. london smith elder co. 1909. volume 40 1894 ﬁrst edition 109 george mackenzie. lives characters eminent writers scots nation abstract catalogue works various editions judgment learnd concerning them volume 3. edinburgh james watson 1711. pp. 519526 napier 110 francis maseres. scriptores logarithmici volume 6. london r. wilks 1807. contains reprint descriptio 131 pp. 475624 followed observations maseres pp. 625710. 111 wilhelm matzka. beiträge zur höheren lehre von den logarithmen. archiv der mathematik und physik 15121196 1850. 112 wilhelm matzka. ein kritischer nachtrag zur geschichte und erﬁndung der logarithmen mit beziehung auf abh. iii. im 15. theil 2. heft seiten 121196. archiv der mathematik und physik 343341354 1860. 49
113 otto mautz. zur basisbestimmung der napierschen und bürgischen logarithmen. beilage zu den jahresberichten des gymnasiums der realschule und der töchterschule basel schuljahr 191819. basel kreis co. 1919. 114 erik meijering. chronology interpolation ancient astronomy modern signal image processing. proceedings ieee 903319342 march 2002. 115 nicolaus mercator. certain problems touching points navigation. philosophical transactions 1215218 1666. 116 george abram miller. postulates history science. proceedings national academy sciences 129537540 1916. 117 george abram miller. john napier invention logarithms. science progress 20307310 1926. 118 george abram miller. history logarithms. journal indian mathematical society 16209213 1926. 119 george abram miller. socalled naperian logarithms. bulletin american mathematical society 32585586 nov.dec. 1926. 120 george abram miller. note history logarithms. tôhoku mathematical journal 29308311 1928. 121 george abram miller. john napier invent logarithms science 7018049798 1929. 122 george abram miller. mathematical myths. national mathematics magazine 128388392 may 1938. see p. 389 myth surrounding confusion natural napierian logarithms 123 george abram miller. eleventh lesson history mathematics. mathematics magazine 2114855 septemberoctober 1947. 124 u. g. mitchell mary strain. number e. osiris 1476496 january 1936. 125 nobuo miura. applications logarithms trigonometry richard norwood. historia scientiarum international journal history science society japan 371730 1989. 50
126 jeanétienne montucla. histoire des mathématiques. paris charles antoine jombert 1758. two volumes see vol. 2 pp. 616 napier 127 augustus de morgan. almost total disappearance earliest trigonometrical canon. monthly notices royal astronomical society 615221228 1845. reprinted appendix london edinburgh dublin philosophical magazine journal science volume 26 number 175 june 1845 pp. 517526. 128 robert moritz. napiers fundamental theorem relating right spherical triangles. american mathematical monthly 227220222 september 1915. 129 conrad müller. john napier laird merchiston und die entdeckungsgeschichte seiner logarithmen. die naturwissenschaften 228669676 july 1914. 130 j. bass mullinger. william oughtred. dictionary national biography volume 14 pages 12501252. london smith elder co. 1909. volume 42 1895 ﬁrst edition 131 john napier. miriﬁci logarithmorum canonis descriptio. edinburgh andrew hart 1614. reprinted 110 pp. 475624 recomputed 2010 d. roegel 161. modern english translation ian bruce available web. 132 john napier. description admirable table logarithmes. london 1616. english translation 131 edward wright reprinted 1969 da capo press new york recomputed 2010 d. roegel 162. second edition appeared 1618. 133 john napier. miriﬁci logarithmorum canonis constructio. edinburgh andrew hart 1619. reprinted 134 translated 136. modern english translation ian bruce available web. 134 john napier. logarithmorum canonis descriptio miriﬁci logarithmorum canonis constructio. lyon barthélemi vincent 1620. reprint napiers descriptio constructio. least constructio reprinted a. hermann 1895. 135 john napier. wonderful canon logarithms. edinburgh william home lizars 1857. new english translation herschell filipowski. 51
136 john napier. construction wonderful canon logarithms. edinburgh william blackwood sons 1889. translation 133 william rae macdonald. 137 mark napier. memoirs john napier merchiston lineage life times history invention logarithms. edinburgh william blackwood 1834. 138 charles naux. histoire des logarithmes de neper à euler. paris a. blanchard 1966 1971. 2 volumes 139 juan navarroloidi josé llombart. introduction logarithms spain. historia mathematica 3583101 2008. 140 katherine neal. discrete continuous broadening number concepts early modern england. dordrecht kluwer academic publishers 2002. 141 new york times. logarithm tercentenary royal society edinburgh planning celebration 1914. new york times 1912. issue dated december 11 1912. short notice actually technical even mentions bürgi. 142 william nicholson. principles illustration advantageous method arranging diﬀerences logarithms lines graduated purpose computation. philosophical transactions royal society london 77246252 1787. 143 jack oliver. birth logarithms. mathematics school 295913 november 2000. 144 william oughtred. circles proportion horizontall instrument. london 1632. 145 e. j. s. parsons w. f. morris. edward wright work. imago mundi 36171 1939. 146 bartholomaeus pitiscus. thesaurus mathematicus sive canon sinuum ad radium 1. 00000. 00000. 00000. et ad dena quæque scrupula secunda quadrantis una cum sinibus primi et postremi gradus ad eundem radium et ad singula scrupula secunda quadrantis adiunctis ubique diﬀerentiis primis et secundis atque ubi res tulit etiam tertijs. frankfurt nicolaus hoﬀmann 1613. the tables reconstructed d. roegel 2010. 163 52
147 bertha porter. edmund wingate. dictionary national biography volume 21 pages 651652. london smith elder co. 1909. volume 62 1900 ﬁrst edition 148 bernd reifenberg. keplers logarithmen und andere marburger frühdrucke volume 123 schriften der universitätsbibliothek. marburg 2005. exhibition catalogue. 149 nicolas reimarus. fundamentum astronomicum id est nova doctrina sinuum et triangulorum. strasbourg bernhard jobin 1588. 150 erasmus reinhold. primus liber tabularum directionum discentibus prima elementa astronomiæ necessarius utilissimus. insertus est canon fecundus ad singula scrupula quadrantis propagatus. item nova tabula climatum parallelorum item umbrarum. appendix canonum secundi libri directionum qui regiomontani opere desiderantur. tübingen ulrich morhard 1554. 151 georg joachim rheticus. canon doctrinæ triangulorum. leipzig wolfgang gunter 1551. this table recomputed 2010 d. roegel 164. 152 georg joachim rheticus valentinus otho. opus palatinum de triangulis. neustadt matthaeus harnisch 1596. this table recomputed 2010 d. roegel 165. 153 v. frederick rickey philip m. tuchinsky. application geography mathematics history integral secant. mathematics magazine 533162166 may 1980. 154 john robertson. construction logarithmic lines gunters scale. philosophical transactions royal society london 4896103 17531754. 155 denis roegel. reconstruction adriaan vlacqs tables trigonometria artiﬁcialis 1633. technical report loria nancy 2010. this recalculation tables 195. 156 denis roegel. reconstruction briggss logarithmorum chilias prima 1617. technical report loria nancy 2010. this recalculation tables 20. 157 denis roegel. reconstruction de deckervlacqs tables arithmetica logarithmica 1628. technical report loria nancy 2010. this recalculation tables 194. 53
158 denis roegel. reconstruction gunters canon triangulorum 1620. technical report loria nancy 2010. this recalculation tables 75. 159 denis roegel. reconstruction tables briggs gellibrands trigonometria britannica 1633. technical report loria nancy 2010. this recalculation tables 22. 160 denis roegel. reconstruction tables briggs arithmetica logarithmica 1624. technical report loria nancy 2010. this recalculation tables 21. 161 denis roegel. reconstruction tables napiers descriptio 1614. technical report loria nancy 2010. this recalculation tables 131. 162 denis roegel. reconstruction tables napiers description 1616. technical report loria nancy 2010. this recalculation tables 132. 163 denis roegel. reconstruction tables pitiscus thesaurus mathematicus 1613. technical report loria nancy 2010. this recalculation tables 146. 164 denis roegel. reconstruction tables rheticuss canon doctrinæ triangulorum 1551. technical report loria nancy 2010. this recalculation tables 151. 165 denis roegel. reconstruction tables rheticuss opus palatinum 1596. technical report loria nancy 2010. this recalculation tables 152. 166 denis roegel. bürgis progress tabulen 1620 logarithmic tables without logarithms. technical report loria nancy 2010. this recalculation tables 26. 167 grażyna rosińska. tables trigonométriques de giovanni bianchini. historia mathematica 84655 1981. 168 grażyna rosińska. tables decimal trigonometric functions ca. 1450 ca. 1550. annals new york academy sciences 500419426 1987. 54
169 george sarton. ﬁrst explanation decimal fractions measures 1585. together history decimal idea facsimile no. xvii stevins disme. isis 231153244 june 1935. 170 robert schlapp. contribution scots mathematics. mathematical gazette 57399116 february 1973. 171 jürgen schönbeçk. thomas fincke und die geometria rotundi. ntm zeitschrift für geschichte der wissenschaften technik und medizin 1228099 june 2004. 172 francis shennan. flesh bones life passions legacies john napier. edinburgh napier polytechnic edinburgh 1989. 173 edwin roscoe sleight. john napier logarithms. national mathematics magazine 184145152 january 1944. 174 david eugene smith. napier tercentenary celebration. bulletin american mathematical society 213123127 december 1914. 175 david eugene smith. law exponents works sixteenth century. knott 99 pages 8191. 176 david eugene smith. review napier tercentenary memorial volume. bulletin american mathematical society 238372374 may 1917. 177 david eugene smith. history mathematics. new york dover 1958. 2 volumes 178 duncan maclaren young sommerville. note napiers logarithms. mathematical gazette 8124300301 july 1916. 179 thomas sonar. der fromme tafelmacher die frühen arbeiten des henry briggs. berlin logos verlag 2002. 180 thomas sonar. die berechnung der logarithmentafeln durch napier und briggs 2004. 181 john speidell. new logarithmes ﬁrst inuention whereof was honourable lo. iohn nepair baron marchiston printed edinburg scotland anno 1614 whose vse required knowledge algebraicall addition substraction according . 1619. 55
182 john speidell. new logarithmes ﬁrst inuention whereof was honourable lo iohn nepair baron marchiston printed edinburg scotland anno 1614. whose vse required knowledge algebraicall addition substraction according . london 1620. 2nd edition. least ﬁve editions 1625. 183 david stewart walter minto. account life writings inventions john napier merchiston. perth r. morison 1787. 184 michael stifel. arithmetica integra. nuremberg johannes petreius 1544. 185 cliﬀord stoll. slide rules ruled. scientiﬁc american 29458087 may 2006. 186 dirk jan struik. source book mathematics 12001800. cambridge massachusetts harvard university press 1969. 187 alvarus thomas. liber de triplici motu proportionibus annexis . philosophicas suiseth calculationes ex parte declarans. paris g. anabat 1509. 188 w. r. thomas. john napier. mathematical gazette 19234192205 1935. 189 simone trompler. lhistoire des logarithmes 2002. 33 pages 190 johannes tropfke. geschichte der elementarmathematik systematischer darstellung. leipzig veit comp. 19021903. 191 glen van brummelen. mathematics heavens earth early history trigonometry. princeton princeton university press 2009. 192 philippe van lansberge. triangulorum geometriæ libri quatuor. leiden franciscus raphelengius 1591. 193 rafael villarrealcalderon. chopping logs look history uses logarithms. montana mathematics enthusiast 523337344 2008. 194 adriaan vlacq. arithmetica logarithmica. gouda pieter rammazeyn 1628. the introduction reprinted 1976 olms tables reconstructed d. roegel 2010. 157 56
195 adriaan vlacq. trigonometria artiﬁcialis. gouda pieter rammazeyn 1633. the tables reconstructed d. roegel 2010. 155 196 kurt vogel. wittich paul. charles coulston gillispie editor dictionary scientiﬁc biography volume 14 pages 470471. new york 1976. 197 nicole vogel. la construction des logarithmes de neper. louvert journal de lapmep dalsace et de lirem de strasbourg 552841 june 1989. 198 anton von braunmühl. zur geschichte der prosthaphaeretischen methode der trigonometrie. abhandlungen zur geschichte der mathematik 91529 1899. 199 anton von braunmühl. vorlesungen über geschichte der trigonometrie. leipzig b. g. teubner 1900 1903. 2 volumes 200 p. j. wallis. william oughtreds circles proportion trigonometries. transactions cambridge bibliographical society 4372382 1968. 201 a. wedemeyer. die erste tafel der logarithmen und der antilogarithmen und die erste anwendung der mechanischen quadratur. astronomische nachrichten 2145120131134 1921. 202 derek thomas whiteside. patterns mathematical thought later seventeenth century. archive history exact sciences 1179388 1961. 203 thomas whittaker. henry briggs. dictionary national biography volume 2 pages 12341235. london smith elder co. 1908. volume 6 1886 ﬁrst edition 204 rosalind cecily young. algebra manyvalued quantities. mathematische annalen 1041260290 december 1931. 205 mary claudia zeller. development trigonometry regiomontanus pitiscus. phd thesis university michigan 1944. published 1946 57
10 napiers construction tables 10.1 first table number logarithm number logarithm ai l1ai ai l1ai 0 10000000.0000000 0.0000000 25 9999975.0000300 25.0000013 1 9999999.0000000 1.0000001 26 9999974.0000325 26.0000013 2 9999998.0000001 2.0000001 27 9999973.0000351 27.0000014 3 9999997.0000003 3.0000002 28 9999972.0000378 28.0000014 4 9999996.0000006 4.0000002 29 9999971.0000406 29.0000015 5 9999995.0000010 5.0000003 30 9999970.0000435 30.0000015 6 9999994.0000015 6.0000003 31 9999969.0000465 31.0000016 7 9999993.0000021 7.0000004 32 9999968.0000496 32.0000016 8 9999992.0000028 8.0000004 33 9999967.0000528 33.0000017 9 9999991.0000036 9.0000005 34 9999966.0000561 34.0000017 10 9999990.0000045 10.0000005 35 9999965.0000595 35.0000018 11 9999989.0000055 11.0000006 36 9999964.0000630 36.0000018 12 9999988.0000066 12.0000006 37 9999963.0000666 37.0000019 13 9999987.0000078 13.0000007 38 9999962.0000703 38.0000019 14 9999986.0000091 14.0000007 39 9999961.0000741 39.0000020 15 9999985.0000105 15.0000008 40 9999960.0000780 40.0000020 16 9999984.0000120 16.0000008 41 9999959.0000820 41.0000021 17 9999983.0000136 17.0000009 42 9999958.0000861 42.0000021 18 9999982.0000153 18.0000009 43 9999957.0000903 43.0000022 19 9999981.0000171 19.0000010 44 9999956.0000946 44.0000022 20 9999980.0000190 20.0000010 45 9999955.0000990 45.0000023 21 9999979.0000210 21.0000011 46 9999954.0001035 46.0000023 22 9999978.0000231 22.0000011 47 9999953.0001081 47.0000024 23 9999977.0000253 23.0000012 48 9999952.0001128 48.0000024 24 9999976.0000276 24.0000012 49 9999951.0001176 49.0000025 25 9999975.0000300 25.0000013 50 9999950.0001225 50.0000025 58
number logarithm number logarithm ai l1ai ai l1ai 50 9999950.0001225 50.0000025 75 9999925.0002775 75.0000038 51 9999949.0001275 51.0000026 76 9999924.0002850 76.0000038 52 9999948.0001326 52.0000026 77 9999923.0002926 77.0000039 53 9999947.0001378 53.0000027 78 9999922.0003003 78.0000039 54 9999946.0001431 54.0000027 79 9999921.0003081 79.0000040 55 9999945.0001485 55.0000028 80 9999920.0003160 80.0000040 56 9999944.0001540 56.0000028 81 9999919.0003240 81.0000041 57 9999943.0001596 57.0000029 82 9999918.0003321 82.0000041 58 9999942.0001653 58.0000029 83 9999917.0003403 83.0000042 59 9999941.0001711 59.0000030 84 9999916.0003486 84.0000042 60 9999940.0001770 60.0000030 85 9999915.0003570 85.0000043 61 9999939.0001830 61.0000031 86 9999914.0003655 86.0000043 62 9999938.0001891 62.0000031 87 9999913.0003741 87.0000044 63 9999937.0001953 63.0000032 88 9999912.0003828 88.0000044 64 9999936.0002016 64.0000032 89 9999911.0003916 89.0000045 65 9999935.0002080 65.0000033 90 9999910.0004005 90.0000045 66 9999934.0002145 66.0000033 91 9999909.0004095 91.0000046 67 9999933.0002211 67.0000034 92 9999908.0004186 92.0000046 68 9999932.0002278 68.0000034 93 9999907.0004278 93.0000047 69 9999931.0002346 69.0000035 94 9999906.0004371 94.0000047 70 9999930.0002415 70.0000035 95 9999905.0004465 95.0000048 71 9999929.0002485 71.0000036 96 9999904.0004560 96.0000048 72 9999928.0002556 72.0000036 97 9999903.0004656 97.0000049 73 9999927.0002628 73.0000037 98 9999902.0004753 98.0000049 74 9999926.0002701 74.0000037 99 9999901.0004851 99.0000050 75 9999925.0002775 75.0000038 100 9999900.0004950 100.0000050 59
10.2 second table number logarithm number logarithm bi l2bi bi l2bi 0 10000000.000000 0.0000000 25 9997500.299977 2500.0125000 1 9999900.000000 100.0005000 26 9997400.324974 2600.0130000 2 9999800.001000 200.0010000 27 9997300.350971 2700.0135000 3 9999700.003000 300.0015000 28 9997200.377967 2800.0140000 4 9999600.006000 400.0020000 29 9997100.405963 2900.0145000 5 9999500.010000 500.0025000 30 9997000.434959 3000.0150000 6 9999400.015000 600.0030000 31 9996900.464955 3100.0155000 7 9999300.021000 700.0035000 32 9996800.495950 3200.0160000 8 9999200.027999 800.0040000 33 9996700.527945 3300.0165000 9 9999100.035999 900.0045000 34 9996600.560940 3400.0170000 10 9999000.044999 1000.0050000 35 9996500.594935 3500.0175000 11 9998900.054998 1100.0055000 36 9996400.629929 3600.0180000 12 9998800.065998 1200.0060000 37 9996300.665922 3700.0185000 13 9998700.077997 1300.0065000 38 9996200.702916 3800.0190000 14 9998600.090996 1400.0070000 39 9996100.740909 3900.0195000 15 9998500.104995 1500.0075000 40 9996000.779901 4000.0200000 16 9998400.119994 1600.0080000 41 9995900.819893 4100.0205000 17 9998300.135993 1700.0085000 42 9995800.860885 4200.0210000 18 9998200.152992 1800.0090000 43 9995700.902877 4300.0215000 19 9998100.170990 1900.0095000 44 9995600.945868 4400.0220000 20 9998000.189989 2000.0100000 45 9995500.989858 4500.0225000 21 9997900.209987 2100.0105000 46 9995401.034848 4600.0230000 22 9997800.230985 2200.0110000 47 9995301.080838 4700.0235000 23 9997700.252982 2300.0115000 48 9995201.127827 4800.0240000 24 9997600.275980 2400.0120000 49 9995101.175816 4900.0245000 25 9997500.299977 2500.0125000 50 9995001.224804 5000.0250000 60
10.3 third table column 0 column 1 column 2 number logarithm number logarithm number logarithm ci0 l3ci0 ci1 l3ci1 ci2 l3ci2 0 10000000.000000 0.0 0 9900000.000000 100503.4 0 9801000.000000 201006.7 1 9995000.000000 5001.3 1 9895050.000000 105504.6 1 9796099.500000 206008.0 2 9990002.500000 10002.5 2 9890102.475000 110505.9 2 9791201.450250 211009.2 3 9985007.498750 15003.8 3 9885157.423762 115507.1 3 9786305.849525 216010.5 4 9980014.995001 20005.0 4 9880214.845051 120508.4 4 9781412.696600 221011.7 5 9975024.987503 25006.3 5 9875274.737628 125509.6 5 9776521.990252 226013.0 6 9970037.475009 30007.5 6 9870337.100259 130510.9 6 9771633.729257 231014.2 7 9965052.456272 35008.8 7 9865401.931709 135512.1 7 9766747.912392 236015.5 8 9960069.930044 40010.0 8 9860469.230743 140513.4 8 9761864.538436 241016.7 9 9955089.895079 45011.3 9 9855538.996128 145514.6 9 9756983.606167 246018.0 10 9950112.350131 50012.5 10 9850611.226630 150515.9 10 9752105.114364 251019.2 11 9945137.293956 55013.8 11 9845685.921017 155517.1 11 9747229.061806 256020.5 12 9940164.725309 60015.0 12 9840763.078056 160518.4 12 9742355.447275 261021.7 13 9935194.642946 65016.3 13 9835842.696517 165519.6 13 9737484.269552 266023.0 14 9930227.045625 70017.5 14 9830924.775169 170520.9 14 9732615.527417 271024.2 15 9925261.932102 75018.8 15 9826009.312781 175522.1 15 9727749.219653 276025.5 16 9920299.301136 80020.0 16 9821096.308125 180523.4 16 9722885.345044 281026.7 17 9915339.151486 85021.3 17 9816185.759971 185524.6 17 9718023.902371 286028.0 18 9910381.481910 90022.5 18 9811277.667091 190525.9 18 9713164.890420 291029.2 19 9905426.291169 95023.8 19 9806372.028257 195527.1 19 9708308.307975 296030.5 20 9900473.578023 100025.0 20 9801468.842243 200528.4 20 9703454.153821 301031.7 61
column 3 column 4 column 5 number logarithm number logarithm number logarithm ci3 l3ci3 ci4 l3ci4 ci5 l3ci5 0 9702990.000000 301510.1 0 9605960.100000 402013.4 0 9509900.499000 502516.8 1 9698138.505000 306511.3 1 9601157.119950 407014.7 1 9505145.548750 507518.0 2 9693289.435747 311512.6 2 9596356.541390 412015.9 2 9500392.975976 512519.3 3 9688442.791030 316513.8 3 9591558.363119 417017.2 3 9495642.779488 517520.5 4 9683598.569634 321515.1 4 9586762.583938 422018.4 4 9490894.958098 522521.8 5 9678756.770349 326516.3 5 9581969.202646 427019.7 5 9486149.510619 527523.0 6 9673917.391964 331517.6 6 9577178.218044 432020.9 6 9481406.435864 532524.3 7 9669080.433268 336518.8 7 9572389.628935 437022.2 7 9476665.732646 537525.5 8 9664245.893052 341520.1 8 9567603.434121 442023.4 8 9471927.399780 542526.8 9 9659413.770105 346521.3 9 9562819.632404 447024.7 9 9467191.436080 547528.0 10 9654584.063220 351522.6 10 9558038.222588 452025.9 10 9462457.840362 552529.3 11 9649756.771188 356523.8 11 9553259.203476 457027.2 11 9457726.611442 557530.5 12 9644931.892803 361525.1 12 9548482.573875 462028.4 12 9452997.748136 562531.8 13 9640109.426856 366526.3 13 9543708.332588 467029.7 13 9448271.249262 567533.0 14 9635289.372143 371527.6 14 9538936.478421 472030.9 14 9443547.113637 572534.3 15 9630471.727457 376528.8 15 9534167.010182 477032.2 15 9438825.340080 577535.5 16 9625656.491593 381530.1 16 9529399.926677 482033.4 16 9434105.927410 582536.8 17 9620843.663347 386531.3 17 9524635.226714 487034.7 17 9429388.874447 587538.0 18 9616033.241516 391532.6 18 9519872.909100 492035.9 18 9424674.180009 592539.3 19 9611225.224895 396533.8 19 9515112.972646 497037.2 19 9419961.842919 597540.6 20 9606419.612282 401535.1 20 9510355.416160 502038.4 20 9415251.861998 602541.8 62
column 6 column 7 column 8 number logarithm number logarithm number logarithm ci6 l3ci6 ci7 l3ci7 ci8 l3ci8 0 9414801.494010 603020.2 0 9320653.479070 703523.5 0 9227446.944279 804026.9 1 9410094.093263 608021.4 1 9315993.152330 708524.8 1 9222833.220807 809028.1 2 9405389.046216 613022.7 2 9311335.155754 713526.0 2 9218221.804197 814029.4 3 9400686.351693 618023.9 3 9306679.488176 718527.3 3 9213612.693295 819030.6 4 9395986.008517 623025.2 4 9302026.148432 723528.5 4 9209005.886948 824031.9 5 9391288.015513 628026.4 5 9297375.135358 728529.8 5 9204401.384004 829033.1 6 9386592.371505 633027.7 6 9292726.447790 733531.0 6 9199799.183312 834034.4 7 9381899.075320 638028.9 7 9288080.084566 738532.3 7 9195199.283721 839035.6 8 9377208.125782 643030.2 8 9283436.044524 743533.5 8 9190601.684079 844036.9 9 9372519.521719 648031.4 9 9278794.326502 748534.8 9 9186006.383237 849038.1 10 9367833.261958 653032.7 10 9274154.929339 753536.0 10 9181413.380045 854039.4 11 9363149.345327 658033.9 11 9269517.851874 758537.3 11 9176822.673355 859040.6 12 9358467.770655 663035.2 12 9264883.092948 763538.5 12 9172234.262019 864041.9 13 9353788.536769 668036.4 13 9260250.651402 768539.8 13 9167648.144888 869043.1 14 9349111.642501 673037.7 14 9255620.526076 773541.0 14 9163064.320815 874044.4 15 9344437.086680 678038.9 15 9250992.715813 778542.3 15 9158482.788655 879045.6 16 9339764.868136 683040.2 16 9246367.219455 783543.5 16 9153903.547260 884046.9 17 9335094.985702 688041.4 17 9241744.035845 788544.8 17 9149326.595487 889048.1 18 9330427.438209 693042.7 18 9237123.163827 793546.0 18 9144751.932189 894049.4 19 9325762.224490 698043.9 19 9232504.602245 798547.3 19 9140179.556223 899050.6 20 9321099.343378 703045.2 20 9227888.349944 803548.5 20 9135609.466445 904051.9 63
column 9 column 10 column 11 number logarithm number logarithm number logarithm ci9 l3ci9 ci10 l3ci10 ci11 l3ci11 0 9135172.474836 904530.2 0 9043820.750088 1005033.6 0 8953382.542587 1105536.9 1 9130604.888599 909531.5 1 9039298.839713 1010034.8 1 8948905.851316 1110538.2 2 9126039.586155 914532.7 2 9034779.190293 1015036.1 2 8944431.398390 1115539.4 3 9121476.566362 919534.0 3 9030261.800698 1020037.3 3 8939959.182691 1120540.7 4 9116915.828078 924535.2 4 9025746.669798 1025038.6 4 8935489.203100 1125541.9 5 9112357.370164 929536.5 5 9021233.796463 1030039.8 5 8931021.458498 1130543.2 6 9107801.191479 934537.7 6 9016723.179565 1035041.1 6 8926555.947769 1135544.4 7 9103247.290884 939539.0 7 9012214.817975 1040042.3 7 8922092.669795 1140545.7 8 9098695.667238 944540.2 8 9007708.710566 1045043.6 8 8917631.623460 1145546.9 9 9094146.319405 949541.5 9 9003204.856210 1050044.8 9 8913172.807648 1150548.2 10 9089599.246245 954542.7 10 8998703.253782 1055046.1 10 8908716.221245 1155549.4 11 9085054.446622 959544.0 11 8994203.902155 1060047.3 11 8904261.863134 1160550.7 12 9080511.919398 964545.2 12 8989706.800204 1065048.6 12 8899809.732202 1165551.9 13 9075971.663439 969546.5 13 8985211.946804 1070049.8 13 8895359.827336 1170553.2 14 9071433.677607 974547.7 14 8980719.340831 1075051.1 14 8890912.147423 1175554.4 15 9066897.960768 979549.0 15 8976228.981160 1080052.3 15 8886466.691349 1180555.7 16 9062364.511788 984550.2 16 8971740.866670 1085053.6 16 8882023.458003 1185557.0 17 9057833.329532 989551.5 17 8967254.996237 1090054.8 17 8877582.446274 1190558.2 18 9053304.412867 994552.7 18 8962771.368738 1095056.1 18 8873143.655051 1195559.5 19 9048777.760661 999554.0 19 8958289.983054 1100057.3 19 8868707.083224 1200560.7 20 9044253.371780 1004555.2 20 8953810.838063 1105058.6 20 8864272.729682 1205562.0 64
column 12 column 13 column 14 number logarithm number logarithm number logarithm ci12 l3ci12 ci13 l3ci13 ci14 l3ci14 0 8863848.717161 1206040.3 0 8775210.229990 1306543.7 0 8687458.127690 1407047.0 1 8859416.792803 1211041.6 1 8770822.624875 1311544.9 1 8683114.398626 1412048.3 2 8854987.084406 1216042.8 2 8766437.213562 1316546.2 2 8678772.841427 1417049.5 3 8850559.590864 1221044.1 3 8762053.994955 1321547.4 3 8674433.455006 1422050.8 4 8846134.311069 1226045.3 4 8757672.967958 1326548.7 4 8670096.238278 1427052.0 5 8841711.243913 1231046.6 5 8753294.131474 1331549.9 5 8665761.190159 1432053.3 6 8837290.388291 1236047.8 6 8748917.484408 1336551.2 6 8661428.309564 1437054.5 7 8832871.743097 1241049.1 7 8744543.025666 1341552.4 7 8657097.595409 1442055.8 8 8828455.307225 1246050.3 8 8740170.754153 1346553.7 8 8652769.046612 1447057.0 9 8824041.079572 1251051.6 9 8735800.668776 1351554.9 9 8648442.662088 1452058.3 10 8819629.059032 1256052.8 10 8731432.768442 1356556.2 10 8644118.440757 1457059.5 11 8815219.244503 1261054.1 11 8727067.052058 1361557.4 11 8639796.381537 1462060.8 12 8810811.634880 1266055.3 12 8722703.518532 1366558.7 12 8635476.483346 1467062.0 13 8806406.229063 1271056.6 13 8718342.166772 1371559.9 13 8631158.745105 1472063.3 14 8802003.025948 1276057.8 14 8713982.995689 1376561.2 14 8626843.165732 1477064.5 15 8797602.024435 1281059.1 15 8709626.004191 1381562.4 15 8622529.744149 1482065.8 16 8793203.223423 1286060.3 16 8705271.191189 1386563.7 16 8618218.479277 1487067.0 17 8788806.621811 1291061.6 17 8700918.555593 1391564.9 17 8613909.370037 1492068.3 18 8784412.218501 1296062.8 18 8696568.096316 1396566.2 18 8609602.415352 1497069.5 19 8780020.012391 1301064.1 19 8692219.812267 1401567.4 19 8605297.614145 1502070.8 20 8775630.002385 1306065.3 20 8687873.702361 1406568.7 20 8600994.965338 1507072.0 65
column 15 column 16 column 17 number logarithm number logarithm number logarithm ci15 l3ci15 ci16 l3ci16 ci17 l3ci17 0 8600583.546413 1507550.4 0 8514577.710949 1608053.7 0 8429431.933839 1708557.1 1 8596283.254640 1512551.6 1 8510320.422093 1613055.0 1 8425217.217872 1713558.3 2 8591985.113012 1517552.9 2 8506065.261882 1618056.2 2 8421004.609263 1718559.6 3 8587689.120456 1522554.1 3 8501812.229251 1623057.5 3 8416794.106959 1723560.8 4 8583395.275896 1527555.4 4 8497561.323137 1628058.7 4 8412585.709905 1728562.1 5 8579103.578258 1532556.6 5 8493312.542475 1633060.0 5 8408379.417050 1733563.3 6 8574814.026469 1537557.9 6 8489065.886204 1638061.2 6 8404175.227342 1738564.6 7 8570526.619455 1542559.1 7 8484821.353261 1643062.5 7 8399973.139728 1743565.8 8 8566241.356146 1547560.4 8 8480578.942584 1648063.7 8 8395773.153158 1748567.1 9 8561958.235468 1552561.6 9 8476338.653113 1653065.0 9 8391575.266582 1753568.3 10 8557677.256350 1557562.9 10 8472100.483786 1658066.2 10 8387379.478948 1758569.6 11 8553398.417722 1562564.1 11 8467864.433544 1663067.5 11 8383185.789209 1763570.8 12 8549121.718513 1567565.4 12 8463630.501328 1668068.7 12 8378994.196314 1768572.1 13 8544847.157653 1572566.6 13 8459398.686077 1673070.0 13 8374804.699216 1773573.4 14 8540574.734075 1577567.9 14 8455168.986734 1678071.2 14 8370617.296867 1778574.6 15 8536304.446708 1582569.1 15 8450941.402241 1683072.5 15 8366431.988218 1783575.9 16 8532036.294484 1587570.4 16 8446715.931539 1688073.7 16 8362248.772224 1788577.1 17 8527770.276337 1592571.6 17 8442492.573574 1693075.0 17 8358067.647838 1793578.4 18 8523506.391199 1597572.9 18 8438271.327287 1698076.2 18 8353888.614014 1798579.6 19 8519244.638003 1602574.1 19 8434052.191623 1703077.5 19 8349711.669707 1803580.9 20 8514985.015684 1607575.4 20 8429835.165527 1708078.7 20 8345536.813872 1808582.1 66
column 18 column 19 column 20 number logarithm number logarithm number logarithm ci18 l3ci18 ci19 l3ci19 ci20 l3ci20 0 8345137.614501 1809060.5 0 8261686.238356 1909563.8 0 8179069.375972 2010067.2 1 8340965.045694 1814061.7 1 8257555.395237 1914565.1 1 8174979.841284 2015068.4 2 8336794.563171 1819063.0 2 8253426.617539 1919566.3 2 8170892.351364 2020069.7 3 8332626.165889 1824064.2 3 8249299.904230 1924567.6 3 8166806.905188 2025070.9 4 8328459.852806 1829065.5 4 8245175.254278 1929568.8 4 8162723.501735 2030072.2 5 8324295.622880 1834066.7 5 8241052.666651 1934570.1 5 8158642.139985 2035073.4 6 8320133.475068 1839068.0 6 8236932.140318 1939571.3 6 8154562.818915 2040074.7 7 8315973.408331 1844069.2 7 8232813.674248 1944572.6 7 8150485.537505 2045075.9 8 8311815.421627 1849070.5 8 8228697.267410 1949573.8 8 8146410.294736 2050077.2 9 8307659.513916 1854071.7 9 8224582.918777 1954575.1 9 8142337.089589 2055078.4 10 8303505.684159 1859073.0 10 8220470.627317 1959576.3 10 8138265.921044 2060079.7 11 8299353.931317 1864074.2 11 8216360.392004 1964577.6 11 8134196.788084 2065080.9 12 8295204.254351 1869075.5 12 8212252.211808 1969578.8 12 8130129.689690 2070082.2 13 8291056.652224 1874076.7 13 8208146.085702 1974580.1 13 8126064.624845 2075083.4 14 8286911.123898 1879078.0 14 8204042.012659 1979581.3 14 8122001.592532 2080084.7 15 8282767.668336 1884079.2 15 8199939.991653 1984582.6 15 8117940.591736 2085085.9 16 8278626.284502 1889080.5 16 8195840.021657 1989583.8 16 8113881.621440 2090087.2 17 8274486.971360 1894081.7 17 8191742.101646 1994585.1 17 8109824.680629 2095088.4 18 8270349.727874 1899083.0 18 8187646.230595 1999586.3 18 8105769.768289 2100089.7 19 8266214.553010 1904084.2 19 8183552.407480 2004587.6 19 8101716.883405 2105090.9 20 8262081.445733 1909085.5 20 8179460.631276 2009588.8 20 8097666.024963 2110092.2 67
column 21 column 22 column 23 number logarithm number logarithm number logarithm ci21 l3ci21 ci22 l3ci22 ci23 l3ci23 0 8097278.682213 2110570.5 0 8016305.895390 2211073.9 0 7936142.836437 2311577.2 1 8093230.042871 2115571.8 1 8012297.742443 2216075.1 1 7932174.765018 2316578.5 2 8089183.427850 2120573.0 2 8008291.593572 2221076.4 2 7928208.677636 2321579.7 3 8085138.836136 2125574.3 3 8004287.447775 2226077.6 3 7924244.573297 2326581.0 4 8081096.266718 2130575.5 4 8000285.304051 2231078.9 4 7920282.451010 2331582.2 5 8077055.718585 2135576.8 5 7996285.161399 2236080.1 5 7916322.309785 2336583.5 6 8073017.190725 2140578.0 6 7992287.018818 2241081.4 6 7912364.148630 2341584.7 7 8068980.682130 2145579.3 7 7988290.875309 2246082.6 7 7908407.966556 2346586.0 8 8064946.191789 2150580.5 8 7984296.729871 2251083.9 8 7904453.762572 2351587.2 9 8060913.718693 2155581.8 9 7980304.581506 2256085.1 9 7900501.535691 2356588.5 10 8056883.261834 2160583.0 10 7976314.429215 2261086.4 10 7896551.284923 2361589.8 11 8052854.820203 2165584.3 11 7972326.272001 2266087.6 11 7892603.009281 2366591.0 12 8048828.392793 2170585.5 12 7968340.108865 2271088.9 12 7888656.707776 2371592.3 13 8044803.978596 2175586.8 13 7964355.938810 2276090.1 13 7884712.379422 2376593.5 14 8040781.576607 2180588.0 14 7960373.760841 2281091.4 14 7880770.023233 2381594.8 15 8036761.185819 2185589.3 15 7956393.573961 2286092.6 15 7876829.638221 2386596.0 16 8032742.805226 2190590.5 16 7952415.377174 2291093.9 16 7872891.223402 2391597.3 17 8028726.433823 2195591.8 17 7948439.169485 2296095.1 17 7868954.777790 2396598.5 18 8024712.070606 2200593.0 18 7944464.949900 2301096.4 18 7865020.300401 2401599.8 19 8020699.714571 2205594.3 19 7940492.717425 2306097.6 19 7861087.790251 2406601.0 20 8016689.364714 2210595.5 20 7936522.471067 2311098.9 20 7857157.246356 2411602.3 68
column 24 column 25 column 26 number logarithm number logarithm number logarithm ci24 l3ci24 ci25 l3ci25 ci26 l3ci26 0 7856781.408072 2412080.6 0 7778213.593991 2512584.0 0 7700431.458052 2613087.3 1 7852853.017368 2417081.9 1 7774324.487194 2517585.2 1 7696581.242323 2618088.6 2 7848926.590859 2422083.1 2 7770437.324951 2522586.5 2 7692732.951701 2623089.8 3 7845002.127564 2427084.4 3 7766552.106288 2527587.7 3 7688886.585226 2628091.1 4 7841079.626500 2432085.6 4 7762668.830235 2532589.0 4 7685042.141933 2633092.3 5 7837159.086687 2437086.9 5 7758787.495820 2537590.2 5 7681199.620862 2638093.6 6 7833240.507144 2442088.1 6 7754908.102072 2542591.5 6 7677359.021052 2643094.8 7 7829323.886890 2447089.4 7 7751030.648021 2547592.7 7 7673520.341541 2648096.1 8 7825409.224947 2452090.6 8 7747155.132697 2552594.0 8 7669683.581370 2653097.3 9 7821496.520334 2457091.9 9 7743281.555131 2557595.2 9 7665848.739580 2658098.6 10 7817585.772074 2462093.1 10 7739409.914353 2562596.5 10 7662015.815210 2663099.8 11 7813676.979188 2467094.4 11 7735540.209396 2567597.7 11 7658184.807302 2668101.1 12 7809770.140698 2472095.6 12 7731672.439291 2572599.0 12 7654355.714898 2673102.3 13 7805865.255628 2477096.9 13 7727806.603072 2577600.2 13 7650528.537041 2678103.6 14 7801962.323000 2482098.1 14 7723942.699770 2582601.5 14 7646703.272773 2683104.8 15 7798061.341839 2487099.4 15 7720080.728420 2587602.7 15 7642879.921136 2688106.1 16 7794162.311168 2492100.6 16 7716220.688056 2592604.0 16 7639058.481176 2693107.3 17 7790265.230012 2497101.9 17 7712362.577712 2597605.2 17 7635238.951935 2698108.6 18 7786370.097397 2502103.1 18 7708506.396423 2602606.5 18 7631421.332459 2703109.8 19 7782476.912349 2507104.4 19 7704652.143225 2607607.7 19 7627605.621793 2708111.1 20 7778585.673892 2512105.6 20 7700799.817153 2612609.0 20 7623791.818982 2713112.3 69
column 27 column 28 column 29 number logarithm number logarithm number logarithm ci27 l3ci27 ci28 l3ci28 ci29 l3ci29 0 7623427.143471 2713590.7 0 7547192.872036 2814094.0 0 7471720.943316 2914597.4 1 7619615.429899 2718591.9 1 7543419.275600 2819095.3 1 7467985.082844 2919598.6 2 7615805.622184 2723593.2 2 7539647.565963 2824096.5 2 7464251.090303 2924599.9 3 7611997.719373 2728594.4 3 7535877.742180 2829097.8 3 7460518.964758 2929601.1 4 7608191.720514 2733595.7 4 7532109.803308 2834099.0 4 7456788.705275 2934602.4 5 7604387.624653 2738596.9 5 7528343.748407 2839100.3 5 7453060.310923 2939603.6 6 7600585.430841 2743598.2 6 7524579.576533 2844101.5 6 7449333.780767 2944604.9 7 7596785.138126 2748599.4 7 7520817.286744 2849102.8 7 7445609.113877 2949606.2 8 7592986.745557 2753600.7 8 7517056.878101 2854104.0 8 7441886.309320 2954607.4 9 7589190.252184 2758601.9 9 7513298.349662 2859105.3 9 7438165.366165 2959608.7 10 7585395.657058 2763603.2 10 7509541.700487 2864106.5 10 7434446.283482 2964609.9 11 7581602.959229 2768604.4 11 7505786.929637 2869107.8 11 7430729.060340 2969611.2 12 7577812.157749 2773605.7 12 7502034.036172 2874109.0 12 7427013.695810 2974612.4 13 7574023.251671 2778606.9 13 7498283.019154 2879110.3 13 7423300.188962 2979613.7 14 7570236.240045 2783608.2 14 7494533.877644 2884111.5 14 7419588.538868 2984614.9 15 7566451.121925 2788609.4 15 7490786.610706 2889112.8 15 7415878.744598 2989616.2 16 7562667.896364 2793610.7 16 7487041.217400 2894114.0 16 7412170.805226 2994617.4 17 7558886.562416 2798611.9 17 7483297.696791 2899115.3 17 7408464.719824 2999618.7 18 7555107.119134 2803613.2 18 7479556.047943 2904116.5 18 7404760.487464 3004619.9 19 7551329.565575 2808614.4 19 7475816.269919 2909117.8 19 7401058.107220 3009621.2 20 7547553.900792 2813615.7 20 7472078.361784 2914119.0 20 7397357.578166 3014622.4 70
column 30 column 31 column 32 number logarithm number logarithm number logarithm ci30 l3ci30 ci31 l3ci31 ci32 l3ci32 0 7397003.733883 3015100.8 0 7323033.696544 3115604.1 0 7249803.359579 3216107.5 1 7393305.232016 3020102.0 1 7319372.179696 3120605.4 1 7246178.457899 3221108.7 2 7389608.579400 3025103.3 2 7315712.493606 3125606.6 2 7242555.368670 3226110.0 3 7385913.775110 3030104.5 3 7312054.637359 3130607.9 3 7238934.090985 3231111.2 4 7382220.818223 3035105.8 4 7308398.610040 3135609.1 4 7235314.623940 3236112.5 5 7378529.707813 3040107.0 5 7304744.410735 3140610.4 5 7231696.966628 3241113.7 6 7374840.442960 3045108.3 6 7301092.038530 3145611.6 6 7228081.118145 3246115.0 7 7371153.022738 3050109.5 7 7297441.492511 3150612.9 7 7224467.077586 3251116.2 8 7367467.446227 3055110.8 8 7293792.771764 3155614.1 8 7220854.844047 3256117.5 9 7363783.712504 3060112.0 9 7290145.875379 3160615.4 9 7217244.416625 3261118.7 10 7360101.820647 3065113.3 10 7286500.802441 3165616.6 10 7213635.794416 3266120.0 11 7356421.769737 3070114.5 11 7282857.552040 3170617.9 11 7210028.976519 3271121.2 12 7352743.558852 3075115.8 12 7279216.123264 3175619.1 12 7206423.962031 3276122.5 13 7349067.187073 3080117.0 13 7275576.515202 3180620.4 13 7202820.750050 3281123.7 14 7345392.653479 3085118.3 14 7271938.726944 3185621.6 14 7199219.339675 3286125.0 15 7341719.957152 3090119.5 15 7268302.757581 3190622.9 15 7195619.730005 3291126.2 16 7338049.097174 3095120.8 16 7264668.606202 3195624.1 16 7192021.920140 3296127.5 17 7334380.072625 3100122.0 17 7261036.271899 3200625.4 17 7188425.909180 3301128.7 18 7330712.882589 3105123.3 18 7257405.753763 3205626.6 18 7184831.696225 3306130.0 19 7327047.526148 3110124.5 19 7253777.050886 3210627.9 19 7181239.280377 3311131.2 20 7323384.002385 3115125.8 20 7250150.162361 3215629.1 20 7177648.660737 3316132.5 71
column 33 column 34 column 35 number logarithm number logarithm number logarithm ci33 l3ci33 ci34 l3ci34 ci35 l3ci35 0 7177305.325983 3316610.8 0 7105532.272723 3417114.2 0 7034476.949996 3517617.5 1 7173716.673320 3321612.1 1 7101979.506587 3422115.4 1 7030959.711521 3522618.8 2 7170129.814983 3326613.3 2 7098428.516833 3427116.7 2 7027444.231665 3527620.0 3 7166544.750076 3331614.6 3 7094879.302575 3432117.9 3 7023930.509549 3532621.3 4 7162961.477701 3336615.8 4 7091331.862924 3437119.2 4 7020418.544294 3537622.6 5 7159379.996962 3341617.1 5 7087786.196992 3442120.4 5 7016908.335022 3542623.8 6 7155800.306963 3346618.3 6 7084242.303894 3447121.7 6 7013399.880855 3547625.1 7 7152222.406810 3351619.6 7 7080700.182742 3452122.9 7 7009893.180914 3552626.3 8 7148646.295606 3356620.8 8 7077159.832650 3457124.2 8 7006388.234324 3557627.6 9 7145071.972459 3361622.1 9 7073621.252734 3462125.4 9 7002885.040207 3562628.8 10 7141499.436472 3366623.3 10 7070084.442108 3467126.7 10 6999383.597687 3567630.1 11 7137928.686754 3371624.6 11 7066549.399887 3472127.9 11 6995883.905888 3572631.3 12 7134359.722411 3376625.8 12 7063016.125187 3477129.2 12 6992385.963935 3577632.6 13 7130792.542550 3381627.1 13 7059484.617124 3482130.4 13 6988889.770953 3582633.8 14 7127227.146278 3386628.3 14 7055954.874815 3487131.7 14 6985395.326067 3587635.1 15 7123663.532705 3391629.6 15 7052426.897378 3492132.9 15 6981902.628404 3592636.3 16 7120101.700939 3396630.8 16 7048900.683929 3497134.2 16 6978411.677090 3597637.6 17 7116541.650088 3401632.1 17 7045376.233587 3502135.4 17 6974922.471252 3602638.8 18 7112983.379263 3406633.3 18 7041853.545471 3507136.7 18 6971435.010016 3607640.1 19 7109426.887574 3411634.6 19 7038332.618698 3512137.9 19 6967949.292511 3612641.3 20 7105872.174130 3416635.8 20 7034813.452389 3517139.2 20 6964465.317865 3617642.6 72
column 36 column 37 column 38 number logarithm number logarithm number logarithm ci36 l3ci36 ci37 l3ci37 ci38 l3ci38 0 6964132.180496 3618120.9 0 6894490.858691 3718624.3 0 6825545.950104 3819127.6 1 6960650.114405 3623122.2 1 6891043.613261 3723625.5 1 6822133.177129 3824128.9 2 6957169.789348 3628123.4 2 6887598.091455 3728626.8 2 6818722.110540 3829130.1 3 6953691.204454 3633124.7 3 6884154.292409 3733628.0 3 6815312.749485 3834131.4 4 6950214.358851 3638125.9 4 6880712.215263 3738629.3 4 6811905.093110 3839132.6 5 6946739.251672 3643127.2 5 6877271.859155 3743630.5 5 6808499.140564 3844133.9 6 6943265.882046 3648128.4 6 6873833.223226 3748631.8 6 6805094.890993 3849135.1 7 6939794.249105 3653129.7 7 6870396.306614 3753633.0 7 6801692.343548 3854136.4 8 6936324.351981 3658130.9 8 6866961.108461 3758634.3 8 6798291.497376 3859137.6 9 6932856.189805 3663132.2 9 6863527.627907 3763635.5 9 6794892.351627 3864138.9 10 6929389.761710 3668133.4 10 6860095.864093 3768636.8 10 6791494.905452 3869140.1 11 6925925.066829 3673134.7 11 6856665.816161 3773638.0 11 6788099.157999 3874141.4 12 6922462.104295 3678135.9 12 6853237.483252 3778639.3 12 6784705.108420 3879142.6 13 6919000.873243 3683137.2 13 6849810.864511 3783640.5 13 6781312.755866 3884143.9 14 6915541.372807 3688138.4 14 6846385.959079 3788641.8 14 6777922.099488 3889145.1 15 6912083.602120 3693139.7 15 6842962.766099 3793643.0 15 6774533.138438 3894146.4 16 6908627.560319 3698140.9 16 6839541.284716 3798644.3 16 6771145.871869 3899147.6 17 6905173.246539 3703142.2 17 6836121.514074 3803645.5 17 6767760.298933 3904148.9 18 6901720.659916 3708143.4 18 6832703.453317 3808646.8 18 6764376.418783 3909150.1 19 6898269.799586 3713144.7 19 6829287.101590 3813648.0 19 6760994.230574 3914151.4 20 6894820.664686 3718145.9 20 6825872.458039 3818649.3 20 6757613.733459 3919152.6 73
column 39 column 40 column 41 number logarithm number logarithm number logarithm ci39 l3ci39 ci40 l3ci40 ci41 l3ci41 0 6757290.490603 3919631.0 0 6689717.585697 4020134.3 0 6622820.409840 4120637.7 1 6753911.845358 3924632.2 1 6686372.726904 4025135.6 1 6619508.999635 4125639.0 2 6750534.889435 3929633.5 2 6683029.540541 4030136.8 2 6616199.245135 4130640.2 3 6747159.621990 3934634.7 3 6679688.025770 4035138.1 3 6612891.145513 4135641.5 4 6743786.042179 3939636.0 4 6676348.181757 4040139.3 4 6609584.699940 4140642.7 5 6740414.149158 3944637.2 5 6673010.007666 4045140.6 5 6606279.907590 4145644.0 6 6737043.942083 3949638.5 6 6669673.502663 4050141.8 6 6602976.767636 4150645.2 7 6733675.420112 3954639.7 7 6666338.665911 4055143.1 7 6599675.279252 4155646.5 8 6730308.582402 3959641.0 8 6663005.496578 4060144.3 8 6596375.441613 4160647.7 9 6726943.428111 3964642.2 9 6659673.993830 4065145.6 9 6593077.253892 4165649.0 10 6723579.956397 3969643.5 10 6656344.156833 4070146.8 10 6589780.715265 4170650.2 11 6720218.166419 3974644.7 11 6653015.984755 4075148.1 11 6586485.824907 4175651.5 12 6716858.057336 3979646.0 12 6649689.476762 4080149.3 12 6583192.581995 4180652.7 13 6713499.628307 3984647.2 13 6646364.632024 4085150.6 13 6579900.985704 4185654.0 14 6710142.878493 3989648.5 14 6643041.449708 4090151.8 14 6576611.035211 4190655.2 15 6706787.807054 3994649.7 15 6639719.928983 4095153.1 15 6573322.729693 4195656.5 16 6703434.413150 3999651.0 16 6636400.069019 4100154.3 16 6570036.068328 4200657.7 17 6700082.695944 4004652.2 17 6633081.868984 4105155.6 17 6566751.050294 4205659.0 18 6696732.654596 4009653.5 18 6629765.328050 4110156.8 18 6563467.674769 4210660.2 19 6693384.288268 4014654.7 19 6626450.445386 4115158.1 19 6560185.940932 4215661.5 20 6690037.596124 4019656.0 20 6623137.220163 4120159.3 20 6556905.847961 4220662.7 74
column 42 column 43 column 44 number logarithm number logarithm number logarithm ci42 l3ci42 ci43 l3ci43 ci44 l3ci44 0 6556592.205741 4221141.1 0 6491026.283684 4321644.4 0 6426116.020847 4422147.8 1 6553313.909639 4226142.3 1 6487780.770542 4326645.7 1 6422902.962837 4427149.0 2 6550037.252684 4231143.6 2 6484536.880157 4331646.9 2 6419691.511355 4432150.3 3 6546762.234057 4236144.8 3 6481294.611717 4336648.2 3 6416481.665600 4437151.5 4 6543488.852940 4241146.1 4 6478053.964411 4341649.4 4 6413273.424767 4442152.8 5 6540217.108514 4246147.3 5 6474814.937429 4346650.7 5 6410066.788054 4447154.0 6 6536946.999960 4251148.6 6 6471577.529960 4351651.9 6 6406861.754660 4452155.3 7 6533678.526460 4256149.8 7 6468341.741195 4356653.2 7 6403658.323783 4457156.5 8 6530411.687196 4261151.1 8 6465107.570324 4361654.4 8 6400456.494621 4462157.8 9 6527146.481353 4266152.3 9 6461875.016539 4366655.7 9 6397256.266374 4467159.0 10 6523882.908112 4271153.6 10 6458644.079031 4371656.9 10 6394057.638241 4472160.3 11 6520620.966658 4276154.8 11 6455414.756992 4376658.2 11 6390860.609422 4477161.5 12 6517360.656175 4281156.1 12 6452187.049613 4381659.4 12 6387665.179117 4482162.8 13 6514101.975847 4286157.3 13 6448960.956088 4386660.7 13 6384471.346527 4487164.0 14 6510844.924859 4291158.6 14 6445736.475610 4391661.9 14 6381279.110854 4492165.3 15 6507589.502396 4296159.8 15 6442513.607372 4396663.2 15 6378088.471299 4497166.5 16 6504335.707645 4301161.1 16 6439292.350569 4401664.4 16 6374899.427063 4502167.8 17 6501083.539791 4306162.3 17 6436072.704393 4406665.7 17 6371711.977349 4507169.0 18 6497832.998021 4311163.6 18 6432854.668041 4411666.9 18 6368526.121361 4512170.3 19 6494584.081522 4316164.8 19 6429638.240707 4416668.2 19 6365341.858300 4517171.5 20 6491336.789482 4321166.1 20 6426423.421587 4421669.4 20 6362159.187371 4522172.8 75
column 45 column 46 column 47 number logarithm number logarithm number logarithm ci45 l3ci45 ci46 l3ci46 ci47 l3ci47 0 6361854.860639 4522651.1 0 6298236.312032 4623154.5 0 6235253.948912 4723657.9 1 6358673.933208 4527652.4 1 6295087.193876 4628155.7 1 6232136.321938 4728659.1 2 6355494.596242 4532653.6 2 6291939.650279 4633157.0 2 6229020.253777 4733660.4 3 6352316.848944 4537654.9 3 6288793.680454 4638158.2 3 6225905.743650 4738661.6 4 6349140.690519 4542656.1 4 6285649.283614 4643159.5 4 6222792.790778 4743662.9 5 6345966.120174 4547657.4 5 6282506.458972 4648160.7 5 6219681.394382 4748664.1 6 6342793.137114 4552658.6 6 6279365.205743 4653162.0 6 6216571.553685 4753665.4 7 6339621.740545 4557659.9 7 6276225.523140 4658163.2 7 6213463.267908 4758666.6 8 6336451.929675 4562661.1 8 6273087.410378 4663164.5 8 6210356.536274 4763667.9 9 6333283.703710 4567662.4 9 6269950.866673 4668165.7 9 6207251.358006 4768669.1 10 6330117.061858 4572663.6 10 6266815.891240 4673167.0 10 6204147.732327 4773670.4 11 6326952.003327 4577664.9 11 6263682.483294 4678168.2 11 6201045.658461 4778671.6 12 6323788.527326 4582666.1 12 6260550.642052 4683169.5 12 6197945.135632 4783672.9 13 6320626.633062 4587667.4 13 6257420.366731 4688170.7 13 6194846.163064 4788674.1 14 6317466.319746 4592668.6 14 6254291.656548 4693172.0 14 6191748.739983 4793675.4 15 6314307.586586 4597669.9 15 6251164.510720 4698173.2 15 6188652.865613 4798676.6 16 6311150.432792 4602671.1 16 6248038.928464 4703174.5 16 6185558.539180 4803677.9 17 6307994.857576 4607672.4 17 6244914.909000 4708175.7 17 6182465.759910 4808679.1 18 6304840.860147 4612673.6 18 6241792.451546 4713177.0 18 6179374.527030 4813680.4 19 6301688.439717 4617674.9 19 6238671.555320 4718178.3 19 6176284.839767 4818681.6 20 6298537.595497 4622676.1 20 6235552.219542 4723179.5 20 6173196.697347 4823682.9 76
column 48 column 49 column 50 number logarithm number logarithm number logarithm ci48 l3ci48 ci49 l3ci49 ci50 l3ci50 0 6172901.409423 4824161.2 0 6111172.395329 4924664.6 0 6050060.671375 5025167.9 1 6169814.958718 4829162.5 1 6108116.809131 4929665.8 1 6047035.641040 5030169.2 2 6166730.051239 4834163.7 2 6105062.750726 4934667.1 2 6044012.123219 5035170.4 3 6163646.686213 4839165.0 3 6102010.219351 4939668.3 3 6040990.117158 5040171.7 4 6160564.862870 4844166.2 4 6098959.214241 4944669.6 4 6037969.622099 5045172.9 5 6157484.580439 4849167.5 5 6095909.734634 4949670.8 5 6034950.637288 5050174.2 6 6154405.838148 4854168.7 6 6092861.779767 4954672.1 6 6031933.161969 5055175.4 7 6151328.635229 4859170.0 7 6089815.348877 4959673.3 7 6028917.195388 5060176.7 8 6148252.970912 4864171.2 8 6086770.441203 4964674.6 8 6025902.736791 5065177.9 9 6145178.844426 4869172.5 9 6083727.055982 4969675.8 9 6022889.785422 5070179.2 10 6142106.255004 4874173.7 10 6080685.192454 4974677.1 10 6019878.340529 5075180.4 11 6139035.201877 4879175.0 11 6077644.849858 4979678.3 11 6016868.401359 5080181.7 12 6135965.684276 4884176.2 12 6074606.027433 4984679.6 12 6013859.967159 5085182.9 13 6132897.701433 4889177.5 13 6071568.724419 4989680.8 13 6010853.037175 5090184.2 14 6129831.252583 4894178.7 14 6068532.940057 4994682.1 14 6007847.610656 5095185.4 15 6126766.336956 4899180.0 15 6065498.673587 4999683.3 15 6004843.686851 5100186.7 16 6123702.953788 4904181.2 16 6062465.924250 5004684.6 16 6001841.265008 5105187.9 17 6120641.102311 4909182.5 17 6059434.691288 5009685.8 17 5998840.344375 5110189.2 18 6117580.781760 4914183.7 18 6056404.973942 5014687.1 18 5995840.924203 5115190.4 19 6114521.991369 4919185.0 19 6053376.771455 5019688.3 19 5992843.003741 5120191.7 20 6111464.730373 4924186.2 20 6050350.083070 5024689.6 20 5989846.582239 5125192.9 77
column 51 column 52 column 53 number logarithm number logarithm number logarithm ci51 l3ci51 ci52 l3ci52 ci53 l3ci53 0 5989560.064662 5125671.3 0 5929664.464015 5226174.6 0 5870367.819375 5326678.0 1 5986565.284629 5130672.5 1 5926699.631783 5231175.9 1 5867432.635465 5331679.3 2 5983572.001987 5135673.8 2 5923736.281967 5236177.1 2 5864498.919147 5336680.5 3 5980580.215986 5140675.0 3 5920774.413826 5241178.4 3 5861566.669688 5341681.8 4 5977589.925878 5145676.3 4 5917814.026619 5246179.6 4 5858635.886353 5346683.0 5 5974601.130915 5150677.5 5 5914855.119606 5251180.9 5 5855706.568410 5351684.3 6 5971613.830350 5155678.8 6 5911897.692046 5256182.1 6 5852778.715126 5356685.5 7 5968628.023434 5160680.0 7 5908941.743200 5261183.4 7 5849852.325768 5361686.8 8 5965643.709423 5165681.3 8 5905987.272328 5266184.6 8 5846927.399605 5366688.0 9 5962660.887568 5170682.5 9 5903034.278692 5271185.9 9 5844003.935905 5371689.3 10 5959679.557124 5175683.8 10 5900082.761553 5276187.1 10 5841081.933937 5376690.5 11 5956699.717346 5180685.0 11 5897132.720172 5281188.4 11 5838161.392970 5381691.8 12 5953721.367487 5185686.3 12 5894184.153812 5286189.6 12 5835242.312274 5386693.0 13 5950744.506803 5190687.5 13 5891237.061735 5291190.9 13 5832324.691118 5391694.3 14 5947769.134550 5195688.8 14 5888291.443204 5296192.1 14 5829408.528772 5396695.5 15 5944795.249983 5200690.0 15 5885347.297483 5301193.4 15 5826493.824508 5401696.8 16 5941822.852358 5205691.3 16 5882404.623834 5306194.7 16 5823580.577596 5406698.0 17 5938851.940931 5210692.5 17 5879463.421522 5311195.9 17 5820668.787307 5411699.3 18 5935882.514961 5215693.8 18 5876523.689811 5316197.2 18 5817758.452913 5416700.5 19 5932914.573703 5220695.0 19 5873585.427966 5321198.4 19 5814849.573687 5421701.8 20 5929948.116417 5225696.3 20 5870648.635252 5326199.7 20 5811942.148900 5426703.0 78
column 54 column 55 column 56 number logarithm number logarithm number logarithm ci54 l3ci54 ci55 l3ci55 ci56 l3ci56 0 5811664.141181 5427181.4 0 5753547.499769 5527684.7 0 5696012.024772 5628188.1 1 5808758.309111 5432182.6 1 5750670.726019 5532686.0 1 5693164.018759 5633189.3 2 5805853.929956 5437183.9 2 5747795.390656 5537687.2 2 5690317.436750 5638190.6 3 5802951.002991 5442185.1 3 5744921.492961 5542688.5 3 5687472.278031 5643191.8 4 5800049.527489 5447186.4 4 5742049.032215 5547689.7 4 5684628.541892 5648193.1 5 5797149.502726 5452187.6 5 5739178.007698 5552691.0 5 5681786.227621 5653194.3 6 5794250.927974 5457188.9 6 5736308.418695 5557692.2 6 5678945.334508 5658195.6 7 5791353.802510 5462190.1 7 5733440.264485 5562693.5 7 5676105.861840 5663196.8 8 5788458.125609 5467191.4 8 5730573.544353 5567694.7 8 5673267.808910 5668198.1 9 5785563.896546 5472192.6 9 5727708.257581 5572696.0 9 5670431.175005 5673199.3 10 5782671.114598 5477193.9 10 5724844.403452 5577697.2 10 5667595.959418 5678200.6 11 5779779.779041 5482195.1 11 5721981.981250 5582698.5 11 5664762.161438 5683201.8 12 5776889.889151 5487196.4 12 5719120.990260 5587699.7 12 5661929.780357 5688203.1 13 5774001.444207 5492197.6 13 5716261.429765 5592701.0 13 5659098.815467 5693204.3 14 5771114.443485 5497198.9 14 5713403.299050 5597702.2 14 5656269.266059 5698205.6 15 5768228.886263 5502200.1 15 5710546.597400 5602703.5 15 5653441.131426 5703206.8 16 5765344.771820 5507201.4 16 5707691.324101 5607704.7 16 5650614.410860 5708208.1 17 5762462.099434 5512202.6 17 5704837.478439 5612706.0 17 5647789.103655 5713209.3 18 5759580.868384 5517203.9 18 5701985.059700 5617707.2 18 5644965.209103 5718210.6 19 5756701.077950 5522205.1 19 5699134.067170 5622708.5 19 5642142.726499 5723211.8 20 5753822.727411 5527206.4 20 5696284.500137 5627709.7 20 5639321.655135 5728213.1 79
column 57 column 58 column 59 number logarithm number logarithm number logarithm ci57 l3ci57 ci58 l3ci58 ci59 l3ci59 0 5639051.904524 5728691.4 0 5582661.385479 5829194.8 0 5526834.771624 5929698.2 1 5636232.378572 5733692.7 1 5579870.054786 5834196.0 1 5524071.354238 5934699.4 2 5633414.262382 5738693.9 2 5577080.119759 5839197.3 2 5521309.318561 5939700.7 3 5630597.555251 5743695.2 3 5574291.579699 5844198.5 3 5518548.663902 5944701.9 4 5627782.256474 5748696.4 4 5571504.433909 5849199.8 4 5515789.389570 5949703.2 5 5624968.365345 5753697.7 5 5568718.681692 5854201.0 5 5513031.494875 5954704.4 6 5622155.881163 5758698.9 6 5565934.322351 5859202.3 6 5510274.979127 5959705.7 7 5619344.803222 5763700.2 7 5563151.355190 5864203.5 7 5507519.841638 5964706.9 8 5616535.130820 5768701.4 8 5560369.779512 5869204.8 8 5504766.081717 5969708.2 9 5613726.863255 5773702.7 9 5557589.594622 5874206.0 9 5502013.698676 5974709.4 10 5610919.999823 5778703.9 10 5554810.799825 5879207.3 10 5499262.691827 5979710.7 11 5608114.539823 5783705.2 11 5552033.394425 5884208.5 11 5496513.060481 5984711.9 12 5605310.482554 5788706.4 12 5549257.377728 5889209.8 12 5493764.803951 5989713.2 13 5602507.827312 5793707.7 13 5546482.749039 5894211.1 13 5491017.921549 5994714.4 14 5599706.573399 5798708.9 14 5543709.507665 5899212.3 14 5488272.412588 5999715.7 15 5596906.720112 5803710.2 15 5540937.652911 5904213.6 15 5485528.276382 6004716.9 16 5594108.266752 5808711.4 16 5538167.184084 5909214.8 16 5482785.512244 6009718.2 17 5591311.212618 5813712.7 17 5535398.100492 5914216.1 17 5480044.119487 6014719.4 18 5588515.557012 5818713.9 18 5532630.401442 5919217.3 18 5477304.097428 6019720.7 19 5585721.299234 5823715.2 19 5529864.086241 5924218.6 19 5474565.445379 6024721.9 20 5582928.438584 5828716.4 20 5527099.154198 5929219.8 20 5471828.162656 6029723.2 80
column 60 column 61 column 62 number logarithm number logarithm number logarithm ci60 l3ci60 ci61 l3ci61 ci62 l3ci62 0 5471566.423908 6030201.5 0 5416850.759669 6130704.9 0 5362682.252072 6231208.2 1 5468830.640696 6035202.8 1 5414142.334289 6135706.1 1 5360000.910946 6236209.5 2 5466096.225375 6040204.0 2 5411435.263122 6140707.4 2 5357320.910490 6241210.7 3 5463363.177263 6045205.3 3 5408729.545490 6145708.6 3 5354642.250035 6246212.0 4 5460631.495674 6050206.5 4 5406025.180717 6150709.9 4 5351964.928910 6251213.2 5 5457901.179926 6055207.8 5 5403322.168127 6155711.1 5 5349288.946446 6256214.5 6 5455172.229336 6060209.0 6 5400620.507043 6160712.4 6 5346614.301972 6261215.7 7 5452444.643222 6065210.3 7 5397920.196789 6165713.6 7 5343940.994821 6266217.0 8 5449718.420900 6070211.5 8 5395221.236691 6170714.9 8 5341269.024324 6271218.2 9 5446993.561689 6075212.8 9 5392523.626073 6175716.1 9 5338598.389812 6276219.5 10 5444270.064909 6080214.0 10 5389827.364260 6180717.4 10 5335929.090617 6281220.7 11 5441547.929876 6085215.3 11 5387132.450577 6185718.6 11 5333261.126072 6286222.0 12 5438827.155911 6090216.5 12 5384438.884352 6190719.9 12 5330594.495509 6291223.2 13 5436107.742333 6095217.8 13 5381746.664910 6195721.1 13 5327929.198261 6296224.5 14 5433389.688462 6100219.0 14 5379055.791577 6200722.4 14 5325265.233662 6301225.7 15 5430672.993618 6105220.3 15 5376366.263682 6205723.6 15 5322602.601045 6306227.0 16 5427957.657121 6110221.5 16 5373678.080550 6210724.9 16 5319941.299744 6311228.2 17 5425243.678293 6115222.8 17 5370991.241510 6215726.1 17 5317281.329094 6316229.5 18 5422531.056453 6120224.0 18 5368305.745889 6220727.4 18 5314622.688430 6321230.7 19 5419819.790925 6125225.3 19 5365621.593016 6225728.6 19 5311965.377086 6326232.0 20 5417109.881030 6130226.5 20 5362938.782219 6230729.9 20 5309309.394397 6331233.2 81
column 63 column 64 column 65 number logarithm number logarithm number logarithm ci63 l3ci63 ci64 l3ci64 ci65 l3ci65 0 5309055.429551 6331711.6 0 5255964.875256 6432214.9 0 5203405.226503 6532718.3 1 5306400.901836 6336712.8 1 5253336.892818 6437216.2 1 5200803.523890 6537719.6 2 5303747.701385 6341714.1 2 5250710.224372 6442217.4 2 5198203.122128 6542720.8 3 5301095.827535 6346715.3 3 5248084.869259 6447218.7 3 5195604.020567 6547722.1 4 5298445.279621 6351716.6 4 5245460.826825 6452219.9 4 5193006.218557 6552723.3 5 5295796.056981 6356717.8 5 5242838.096411 6457221.2 5 5190409.715447 6557724.6 6 5293148.158953 6361719.1 6 5240216.677363 6462222.4 6 5187814.510590 6562725.8 7 5290501.584873 6366720.3 7 5237596.569024 6467223.7 7 5185220.603334 6567727.1 8 5287856.334081 6371721.6 8 5234977.770740 6472224.9 8 5182627.993033 6572728.3 9 5285212.405914 6376722.8 9 5232360.281855 6477226.2 9 5180036.679036 6577729.6 10 5282569.799711 6381724.1 10 5229744.101714 6482227.5 10 5177446.660697 6582730.8 11 5279928.514811 6386725.3 11 5227129.229663 6487228.7 11 5174857.937366 6587732.1 12 5277288.550554 6391726.6 12 5224515.665048 6492230.0 12 5172270.508397 6592733.3 13 5274649.906278 6396727.8 13 5221903.407215 6497231.2 13 5169684.373143 6597734.6 14 5272012.581325 6401729.1 14 5219292.455512 6502232.5 14 5167099.530957 6602735.8 15 5269376.575034 6406730.3 15 5216682.809284 6507233.7 15 5164515.981191 6607737.1 16 5266741.886747 6411731.6 16 5214074.467879 6512235.0 16 5161933.723201 6612738.3 17 5264108.515804 6416732.8 17 5211467.430646 6517236.2 17 5159352.756339 6617739.6 18 5261476.461546 6421734.1 18 5208861.696930 6522237.5 18 5156773.079961 6622740.8 19 5258845.723315 6426735.3 19 5206257.266082 6527238.7 19 5154194.693421 6627742.1 20 5256216.300453 6431736.6 20 5203654.137449 6532240.0 20 5151617.596074 6632743.3 82
column 66 column 67 column 68 number logarithm number logarithm number logarithm ci66 l3ci66 ci67 l3ci67 ci68 l3ci68 0 5151371.174238 6633221.7 0 5099857.462496 6733725.0 0 5048858.887871 6834228.4 1 5148795.488651 6638222.9 1 5097307.533764 6738726.3 1 5046334.458427 6839229.6 2 5146221.090907 6643224.2 2 5094758.879998 6743727.5 2 5043811.291198 6844230.9 3 5143647.980361 6648225.4 3 5092211.500558 6748728.8 3 5041289.385552 6849232.1 4 5141076.156371 6653226.7 4 5089665.394807 6753730.0 4 5038768.740859 6854233.4 5 5138505.618293 6658227.9 5 5087120.562110 6758731.3 5 5036249.356489 6859234.6 6 5135936.365484 6663229.2 6 5084577.001829 6763732.5 6 5033731.231811 6864235.9 7 5133368.397301 6668230.4 7 5082034.713328 6768733.8 7 5031214.366195 6869237.1 8 5130801.713102 6673231.7 8 5079493.695971 6773735.0 8 5028698.759011 6874238.4 9 5128236.312246 6678232.9 9 5076953.949123 6778736.3 9 5026184.409632 6879239.6 10 5125672.194090 6683234.2 10 5074415.472149 6783737.5 10 5023671.317427 6884240.9 11 5123109.357993 6688235.4 11 5071878.264413 6788738.8 11 5021159.481768 6889242.1 12 5120547.803314 6693236.7 12 5069342.325280 6793740.0 12 5018648.902028 6894243.4 13 5117987.529412 6698237.9 13 5066807.654118 6798741.3 13 5016139.577577 6899244.6 14 5115428.535647 6703239.2 14 5064274.250291 6803742.5 14 5013631.507788 6904245.9 15 5112870.821379 6708240.4 15 5061742.113166 6808743.8 15 5011124.692034 6909247.1 16 5110314.385969 6713241.7 16 5059211.242109 6813745.0 16 5008619.129688 6914248.4 17 5107759.228776 6718242.9 17 5056681.636488 6818746.3 17 5006114.820123 6919249.6 18 5105205.349161 6723244.2 18 5054153.295670 6823747.5 18 5003611.762713 6924250.9 19 5102652.746487 6728245.4 19 5051626.219022 6828748.8 19 5001109.956832 6929252.1 20 5100101.420113 6733246.7 20 5049100.405912 6833750.0 20 4998609.401853 6934253.4 83
introduction fourier analysis fourier series partial diﬀerential equations fourier transforms notes prepared ma3139 arthur l. schoenstadt department applied mathematics naval postgraduate school code mazh monterey california 93943 august 18 2005 c 1992 professor arthur l. schoenstadt 1
contents 1 inﬁnite sequences inﬁnite series improper integrals 1 1.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 functions sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3 limits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.4 order notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.5 inﬁnite series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 1.6 convergence tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.7 error estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 1.8 sequences functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2 fourier series 25 2.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.2 derivation fourier series coeﬃcients . . . . . . . . . . . . . . . . . . 26 2.3 odd even functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 2.4 convergence properties fourier series . . . . . . . . . . . . . . . . . . . . 40 2.5 interpretation fourier coeﬃcients . . . . . . . . . . . . . . . . . . . . 48 2.6 complex form fourier series . . . . . . . . . . . . . . . . . . . . 53 2.7 fourier series ordinary diﬀerential equations . . . . . . . . . . . . . . . 56 2.8 fourier series digital data transmission . . . . . . . . . . . . . . . . . . 60 3 onedimensional wave equation 70 3.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 3.2 onedimensional wave equation . . . . . . . . . . . . . . . . . . . . . . 70 3.3 boundary conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 3.4 initial conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 3.5 introduction solution wave equation . . . . . . . . . . . . . . 82 3.6 fixed end condition string . . . . . . . . . . . . . . . . . . . . . . . . . 85 3.7 free end conditions problem . . . . . . . . . . . . . . . . . . . . . . . . 97 3.8 mixed end conditions problem . . . . . . . . . . . . . . . . . . . . . . 106 3.9 generalizations method separation variables . . . . . . . . . . . 117 3.10 sturmliouville theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 3.11 frequency domain interpretation wave equation . . . . . . . . . 132 3.12 dalembert solution wave equation . . . . . . . . . . . . . . . . 137 3.13 eﬀect boundary conditions . . . . . . . . . . . . . . . . . . . . . . . 141 4 twodimensional wave equation 147 4.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 4.2 rigid edge problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148 4.3 frequency domain analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 154 4.4 time domain analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 4.5 wave equation circular regions . . . . . . . . . . . . . . . . . . . . 159 4.6 symmetric vibrations circular drum . . . . . . . . . . . . . . . . . . 163
4.7 frequncy domain analysis circular drum . . . . . . . . . . . . . . . . 170 4.8 time domain analysis circular membrane . . . . . . . . . . . . . . . 171 5 introduction fourier transform 178 5.1 periodic aperiodic functions . . . . . . . . . . . . . . . . . . . . . . . . 178 5.2 representation aperiodic functions . . . . . . . . . . . . . . . . . . . . 179 5.3 fourier transform inverse transform . . . . . . . . . . . . . . . . . 182 5.4 examples fourier transforms graphical representation . . . . 185 5.5 special computational cases fourier transform . . . . . . . . . . . . 189 5.6 relations transform inverse transform . . . . . . . . . . . 192 5.7 general properties fourier transform linearity shifting scaling 195 5.8 fourier transform derivatives integrals . . . . . . . . . . . . . . 199 5.9 fourier transform impulse function implications . . . . . 203 5.10 extensions fourier transform . . . . . . . . . . . . . . . . . . 209 6 applications fourier transform 215 6.1 introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 6.2 convolution fourier transforms . . . . . . . . . . . . . . . . . . . . . . 215 6.3 linear shiftinvariant systems . . . . . . . . . . . . . . . . . . . . . . . . . 223 6.4 determining systems impulse response transfer function . . . . . . 228 6.5 applications convolution signal processing filters . . . . . . . . . . 234 6.6 applications convolution amplitude modulation frequency division multiplexing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237 6.7 dalembert solution revisited . . . . . . . . . . . . . . . . . . . . . . 242 6.8 dispersive waves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245 6.9 correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248 6.10 summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249 7 appendix bessels equation 251 7.1 bessels equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251 7.2 properties bessel functions . . . . . . . . . . . . . . . . . . . . . . . . . . 253 7.3 variants bessels equation . . . . . . . . . . . . . . . . . . . . . . . . . . 256 ii
list figures 1 zenos paradox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 2 black box function . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 3 natural logarithm function ln . . . . . . . . . . . . . . . . . . . . 3 4 graph sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 5 sampling continuous signal . . . . . . . . . . . . . . . . . . . . . . . . . 4 6 pictorial concept limit . . . . . . . . . . . . . . . . . . . . . . . . 6 7 pictorial concept limit inﬁnity . . . . . . . . . . . . . . . . . . . . . 6 8 estimating error partial sum . . . . . . . . . . . . . . . . . . . . . 17 9 sequence functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 10 general periodic function . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 11 piecewise continuous function example . . . . . . . . . . . . . . . 29 12 convergence partial sums fourier series . . . . . . . . . . . . . . 31 13 symmetries sine cosine . . . . . . . . . . . . . . . . . . . . . . . . . 35 14 integrals even odd functions . . . . . . . . . . . . . . . . . . . . . . 36 15 fx x 3 x 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 16 spectrum signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 17 typical periodic function . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 18 square wave . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 19 transmitted digital signal . . . . . . . . . . . . . . . . . . . . . . . . . . 60 20 simple circuit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 21 periodic digital test signal . . . . . . . . . . . . . . . . . . . . . . . . . . 61 22 undistorted distorted signals . . . . . . . . . . . . . . . . . . . . . . . . 65 23 first sample output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 24 second sample output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 25 elastic string . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 26 small segment string . . . . . . . . . . . . . . . . . . . . . . . . . . 72 27 free end conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 28 mixed end conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 29 initial displacement ux 0 . . . . . . . . . . . . . . . . . . . . . . . . 93 30 free end conditions problem . . . . . . . . . . . . . . . . . . . . . . . . 98 31 initial displacement fx . . . . . . . . . . . . . . . . . . . . . . . . . . 103 32 mixed end condition problem . . . . . . . . . . . . . . . . . . . . . . . 107 33 a2x cos 2πct l . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 34 various modes vibration . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 35 moving function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 36 constructing dalembert solution unbounded region . . . . . . . 141 37 dalembert solution boundary conditions . . . . . . . . . . . . . 144 38 boundary reﬂections via dalembert solution . . . . . . . . . . . . . . 145 39 modes vibrating rectangle . . . . . . . . . . . . . . . . . . . . . . . . . 155 40 contour lines modes vibrating rectangle . . . . . . . . . . . . . . . 156 41 spectrum rectangular drum . . . . . . . . . . . . . . . . . . . . 157 42 traveling plane wave . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159 iii
43 two plane waves traveling directions ˆ k k . . . . . . . . . . . . . 160 44 ordinary bessel functions j0r y0r . . . . . . . . . . . . . . . . 166 45 modes circular membrane . . . . . . . . . . . . . . . . . . . . . . . . 171 46 spectrum circular membrane . . . . . . . . . . . . . . . . . . . . . . . 172 47 approximation deﬁnite integral . . . . . . . . . . . . . . . . . . . . . 180 48 square pulse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 49 fourier transform square pulse . . . . . . . . . . . . . . . . . . . 187 50 function ht given 5.4.16 . . . . . . . . . . . . . . . . . . . . . . . . 188 51 alternative graphical descriptions fourier transform . . . . . . . . . 189 52 function et . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 53 relationship ht hf . . . . . . . . . . . . . . . . . . . . . . . 194 54 relationship ht hat . . . . . . . . . . . . . . . . . . . . . . . 196 55 relationship hf 1 ah f . . . . . . . . . . . . . . . . . . . . 198 56 relationship ht ht b . . . . . . . . . . . . . . . . . . . . . . 199 57 fourier transform computed using derivative rule . . . . . . . . . . 201 58 graphical interpretations δt . . . . . . . . . . . . . . . . . . . . . . 204 59 transform pair f δt 1 . . . . . . . . . . . . . . . . . . . . . . . 205 60 transform pair f cos2πf0t . . . . . . . . . . . . . . . . . . . . . 207 61 transform pair f sin2πf0t . . . . . . . . . . . . . . . . . . . . . . 207 62 transform pair periodic function . . . . . . . . . . . . . . . . . . 208 63 transform pair function sgnt . . . . . . . . . . . . . . . . . . 210 64 transform pair unit step function . . . . . . . . . . . . . . . . 211 65 relation ht function ht τ function τ . . . 216 66 graphical description convolution . . . . . . . . . . . . . . . . . . 218 67 graph gt ht example . . . . . . . . . . . . . . . . . 219 68 graphical description second convolution . . . . . . . . . . . . . . 220 69 graphical description system output transform domain . 228 70 sample rc circuit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 71 example impulse response transfer function . . . . . . . . . . . . . 230 72 example input output rc circuit . . . . . . . . . . . . . . . 234 73 transfer functions ideal filters . . . . . . . . . . . . . . . . . . . . . . . 235 74 real filters impulse responses transfer functions. top show rc ﬁlter lopassmiddle rc ﬁlter highpass bottom lrc ﬁlter bandpass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236 75 amplitude modulation time domain view . . . . . . . . . . . . . . . 237 76 amplitude modulation frequency domain view . . . . . . . . . . . . 238 77 single sideband modulation frequency domain view . . . . . . . . . . 239 78 frequency division multiplexing time frequency domain views . 240 79 recovering frequency division multiplexed signal . . . . . . . . . . . . . . 241 80 solutions dispersive wave equation diﬀerent times . . . . . . . . . 247 81 bessel functions jn a yn b. . . . . . . . . . . . . . . . . . . . . 254 iv
3139 fourier analysis partial diﬀerential equations introduction notes are least indirectly human eye human ear philosophy physical phenomena. now dont go asking money back yet really mathematics anatomy philosophy text. shall however develop fundamental ideas branch mathematics used interpret much way two intriguing human sensory organs function. terms actual anatomical descriptions shall need simple high schoollevel concept organs. branch mathematics consider called fourier analysis french mathematician jean baptiste joseph fourier1 17681830 whose treatise heat ﬂow ﬁrst introduced concepts. today fourier analysis is among things perhaps single important mathematical tool used call signal processing. represents fundamental procedure complex physical signals may decom posed simpler ones and conversely complicated signals may created simpler building blocks. mathematically fourier analysis spawned fundamental developments understanding inﬁnite series function approxima tion developments are unfortunately much beyond scope notes. equally important fourier analysis tool many everyday phenomena perceived diﬀerences sound violins drums sonic booms mixing colors better understood. shall come see fourier analysis es tablishing simultaneous dual view phenomena shall come call frequency domain time domain representations. shall also come argue later shall call time frequency domains immediately relate ways human ear eye interpret stimuli. ear example responds minute variations atmospheric pressure. cause ear drum vibrate and various nerves inner ear convert vibrations brain interprets sounds. eye contrast electromagnetic waves fall rods cones back eyeball converted brain interprets colors. fundamental diﬀerences way interpretations occur. speciﬁcally consider one great american pastimes watching television. speaker television vibrates producing minute compressions rarefactions increases decreases air pressure propagate across room viewers ear. variations impact ear drum single continuously varying pressure. however time result interpreted brain separated diﬀerent actors voices background sounds etc. is nature human ear take single complex signal the sound pressure wave decompose simpler components recognize simultaneous existence diﬀerent components. essence shall come view terms fourier analysis frequency domain analysis signal. 1see v
contrast ears response behavior eye. television uses electron guns illuminate groups three diﬀerent colored red green blue phosphor dots screen. illuminated dot emits light corresponding color. diﬀerent colored light beams propagate across room fall eye. the phosphor dots closely grouped screen one dot group three illuminated diﬀerent light beams seem come location. eye however interprets simultaneous reception diﬀerent colors exactly reverse manner ear. one pure color actually present eye perceives single composite color e.g. yellow. is nature human eye take multiple simple component signals the pure colors combine synthesize single complex signal. terms fourier analysis time domain interpretation signal. vi
1 inﬁnite sequences inﬁnite series improper in tegrals 1.1 introduction concepts inﬁnite series improper integrals i.e. entities represented symbols n n fnx fx dx central fourier analysis. we assume reader already least somewhat familiar these. however sake completeness clarity shall fully deﬁne later. concepts indeed almost concepts dealing inﬁnity mathematically extremely subtle. ancient greeks example wrestled totally successfully issues. perhaps bestknown example diﬃculty dealing concepts famous zenos paradox. example concerns tortoise ﬂeetfooted runner reputed achilles versions. tortoise assumed certain head start runner runner could travel twice fast tortoise. i know thats either awfully fast tortoise slow runner didnt make paradox. anyway paradox goes given signal runner tortoise start move. interval time runner reach tortoises original starting point. tortoise moved half far ahead runner start race. then time runner reaches tortoises new position tortoise moved ahead still lead runner albeit quarter original distance on figure 1 zenos paradox 1
paradox appears every time runner reaches tortoises old location tortoise continued still ahead even though distance closing. runner catch tortoise experience clearly tells us tortoise always small distance lead resolution paradox really requires concept inﬁnite series. ﬁrst must establish little notation. 1.2 functions sequences functions fundamental mathematics part mathematics involving calculus. probably many diﬀerent although basically equivalent ways deﬁne functions mathematics texts. shall actually use one although primary viewpoint function inputoutput relationship black box particular input value x produces corresponding output fx. input x domain f output fx range function figure 2 black box function allowable or valid inputs black box comprise domain function possible outputs range. example consider natural logarithm function implemented calculators. somewhere inside calculators circuitry chip whose function take numbers compute natural logarithm. actual algo rithm used totally hidden user calculator i.e. its opaque box whose inner workings mystery. user knows enter number example x 1.275 calculators display hit lnx key calculator return value 0.2429 . . . ln1.275 display. furthermore crucial concept function result produced every time input value. in real world numerical analysts design computer chips worry great deal problem i.e. ensure every diﬀerent manufacturers chips fact produce answers. domain lnx function 0 x range lnx . inputs domain e.g. x 1 produce calculator sort error message ﬂashing display word error etc. noted above however black boxes way functions may in terpreted. example occasions shall consider realvalued functions essentially equivalent graphs. thus natural logarithm function could equally well speciﬁed figure 3. graphical point view domain function consists points horizontal axis correspond points curve range equivalent points vertical axis. note deliberately labeled axes e.g. called 2
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 3 2 1 0 1 2 3 lnx 1.275.2429 figure 3 natural logarithm function ln horizontal axis x. emphasize essential independence function particular symbol used represent independent input variable i.e. lnx function x graph lnu function u. fact long replace symbol independent variable symbol throughout change graph. this really nothing old euclidean notion equals replaced equals remain equal. therefore lnx2 function x2 graph lnu function u even though lnx2 function x not calculus assume already quite familiar deals various additional properties relationships functions e.g. limits derivative integral. these turn may viewed diﬀerent ways. example derivative f x f dx lim h0 fx h fx h may viewed either instantaneous rate change function fx slope tangent line curve fx x deﬁnite integral b fxdx may considered representing net area curve fx interval points x x b. one special class functions whose domain consists integers the positive negative whole numbers plus zero subset integers. functions commonly referred sequences. furthermore discussions sequences independent variable likely represented one letters i j k l m n x y z etc. lastly although functions reasonably represented standard functional notation practice special nature emphasized writing independent variable subscript i.e. writing instead an. sequences speciﬁed several diﬀerent ways. among are listing terms explicitly e.g. a0 1 a1 1 a2 1 2 a3 1 6 a4 1 24 etc. 3
recurrence relation e.g. a0 1 an1 n n 1 2 3 . . . explicit formula e.g. 1n n n 0 1 2 . . . or graph 1 1 5 n figure 4 graph sequence note since sequences deﬁned integer values independent variable graphs consist straight lines individual points. addition se quences encounter deﬁned nonnegative integer subscripts. sequences arise number common applications outside scope text. nevertheless its worth mentioning point. 5 figure 5 sampling continuous signal sampling common occurrence signal processing conversion con tinuous analog signal say ft sequence discrete signal simply recording value continuous signal regular time intervals. thus 4
example sampling times would represented sequence ti resulting sample values second sequence fi fti. probability certain random events example arrival ships port messages communications site occur discrete quantities. thus example sequence pn might denote probability exactly n ar rivals ships messages facility single unit time. approximation signiﬁcant number mathematical problems solved process successive approximations. so number nonmathematical problems adjusting artillery ﬁre process sequence the oretically better better approximations desired solution gener ated. perhaps familiar mathematical instance newtons method most time sequence successively better solutions equation form fx 0 generated algorithm xn1 xn fxn f xn . sequences simply particular class functions inherit nor mal properties operations valid functions. includes property variable subscript replaced expression value sides relation without altering validity relation. thus example if 2n n ak 2k k a2n1 22n1 2n 1 1 2 4n 2n 1 a4 24 4 16 24 . 1.3 limits concepts limits limit processes also central dealing sequences as calculus. assume reader already fairly solid introduction concepts shall brieﬂy review present notation. function single real variable limit shall use notation lim xx0 fx statement fx converges x approaches x0. shall treat limits limiting behavior intuitive pictorial concepts mathematical deﬁnitions 5
although aware concepts deﬁned rigorously if somewhat abstractly calculus. view limit exists provided fx assured suﬃciently close whenever x suﬃciently close but strictly speaking equal x0. considered black box concept function whether limit exists depends whether holding input within tolerances enough guarantee output within speciﬁcations. pictorially limit exits when values horizontal axis become arbitrarily close x0 corresponding points curve become arbitrarily close a. x0 fx figure 6 pictorial concept limit x0 ﬁnite i.e. consider lim xfx situation slightly changed. consider limit exist provided fx guaranteed suﬃciently close whenever x suﬃciently large. look corresponding ﬁgure figure 7 shows limit inﬁnity or negative inﬁnity precisely condition referred calculus horizontal asymptote. fx figure 7 pictorial concept limit inﬁnity limits play central role study improper integrals one particular class integral especially important studies. improper integrals ones whose 6
values may may even exist either integrands discontinuous length interval integration inﬁnite or both e.g. 1 0 1 xdx 0 1 x2 1dx recall calculus treats integrals terms limits integrals exist said converge whenever appropriate limits exist. thus example second integral converges exists lim r r 0 1 x2 1dx lim rtan1x r 0 lim rtan1r π 2 0 1 x2 1dx given improper integral appropriate limit exist would say improper integral converge diverged. sequences however one important diﬀerence emerges. sequences deﬁned integer values argument therefore ﬁnite n0 notation lim nn0 generally really quite silly since n never closer n0 one unit away. sequences sense talking limits inﬁnity. concept however lim nan perfectly reasonable simply means assured arbitrarily close n suﬃciently large. more colloquially means point sequence values terms change eﬀectively point on. purists exact deﬁnition is lim nan arbitrary δ 0 exists n0 a δ whenever n n0 . again functions shall use statement an converges a interchangeably limnan a. shall also often use shorthand a denote existence limit. limit exists use fairly standard convention saying sequence diverges. several standard techniques may used determine actual existence non existence limit given sequence. common perhaps either inspection application lhopitals rule sequence after replacing n x. 7
examples n3 1 3n 13 converges 1 27 1 n 0 1n diverges n diverges . note using lhopitals rule existence lhopitals limit x implies existence limit sequence necessarily conversely. one case sequence limit exists lhopitals limit sinnπ. able explain example mathematically graphically statement sequence converges suﬃcient guarantee eventually terms sequence become arbitrarily close limit value. however knowing series converges fails convey one crucial fact speed convergence i.e. many terms must evaluated limit reasonably well approximated. illustrated following table values two diﬀerent sequences n3 1 3n 13 bn n3 3 3n3 converge limit 1 3 n bn 5 0.1944 0.3253 10 0.2508 0.3323 20 0.2880 0.3332 100 0.3235 0.3333 500 0.3313 0.3333 note that every subscript value bn contains least full signiﬁcant digit accuracy an. shall come see course trivial consideration. like computation evaluating terms sequence free therefore cases costs more work slowly converging sequences. next section consider notation signiﬁcantly simplify discussion fast certain sequences converge. 1.4 order notation order notation used describe limiting behavior sequence or function. speciﬁ cally sequences order notation relates behavior two sequences one αn usually 8
fairly complicated other βn fairly simple expresses concept that limit either 1 sequences αn βn essentially identical 2 sequence βn dominates sequence αn sense βn serve worst case approximation αn . order notation useful discussing sequences and later discussing inﬁnite series convergence divergence really determined behavior large n. yet frequently n large terms sequenceseries conveniently approximated much simpler expressions. example n large αn 7n2 1 cosnπ 3n4 n3 19 close βn 7 3n2 . ﬁrst notation introduce used convey information relative behavior sequences is αn oβn if constant c αn cβn n . colloquially shall say case either αn order βn or slightly precisely αn big oh βn. one drawback however order notation described above. draw back that notation involves less equal relation instances statements made using notation correct somewhat unenlight ening. example since 1 n2 1 n then strictly speaking 1 n2 1 n . couple ways cleaned up. one introduce second type order sometimes called little oh small oh. speciﬁcally say αn oβn lim n αn βn 0 . note αn oβn implies immediately αn oβn necessarily converse. thus convey two sequences αn βn involve terms about size saying αn oβn αn oβn . somewhat similar slightly stronger statement conveyed notation asymp totic equivalence. speciﬁcally say αn asymptotically equivalent βn lim n αn βn 1 . 9
asymptotic equivalence generally denoted αn βn . standard rules simplify determining order sequence. include 1 polynomial n n highest power dominate and limit polynomial approximated highest power term computation involving later subtraction another polynomial degree. thus example unless added subtracted another quartic polynomial limit 2n4 20n3 500n2 n 1000 simply replaced 2n4 . 2 values n x sinnπx 1 cosnπx 1 . 3 large n n n e n 2πn . thus example sequence αn 1 cosnπ n2 1π2 following order notation statements true αn 1 n2 αn 1 n2 αn 1 n αn 2 π2n2 n 0 2 4 . . . . last important point would emphasize leaving section order notation simply convenient shorthand expressing information growth decay terms sequence. not itself prove behavior actually occurs. information actual rates growth decay sequence must still obtained basic calculus techniques. order notation simply allows express information found concise form. 10
1.5 inﬁnite series again shall assume prior familiarity topic part reader cover high points. deﬁnition inﬁnite series formed adding terms sequence. note general theoretically requires adding together inﬁnite number terms. example 2 3 n n 0 1 2 . . . inﬁnite series formed n0 a0 a1 a2 a3 n0 2 3 n 1 2 3 4 9 8 27 vernacular fortran inﬁnite series thus equivalent loop whose upper limit unbounded. clearly evaluation inﬁnite series direct addition i.e. brute force impractical even physically impossible. heart zenos paradox. posed ancient greeks paradox hinges adding together inﬁnite number time intervals half previous one. logical ﬂaw greeks analysis implicit assumption sum inﬁnite number nonzero terms necessarily inﬁnite. correctly understanding paradox requires recognition zenos paradox matter inﬁnite series involves sum inﬁnite number terms therefore valid method analysis terms limits. mathematical analysis inﬁnite series starts recognition every inﬁnite series fact involves two sequences 1 sequence terms an 2 sequence partial sums sn a0 a1 n n0 sequence partial sums limit i.e. lim nsn 11
exists meaningfully talk adding together inﬁnite number terms series yet getting ﬁnite answer. case shall say inﬁnite series converges. if limit doesnt exist say series diverges. fundamental deﬁnition understanding ultimately convergence divergence series depends terms an primary quantity analyzed deciding whether series converges diverges sequence partial sums sn. furthermore frequently causes confusion students ﬁrst encounter inﬁnite series since two sequences involved always possibility one converge not. precise inﬁnite series sequence partial sums cannot converge sequence terms not convergence sequence terms series limit even limit zero guarantee inﬁnite series converge. many examples exist sequence terms converges series not. one series given by 1 n 0 1 2 . . . sn n n0 1 1 1 1 n 1 . simple example illustrates another fundamental point inﬁnite series determining convergence divergence series directly deﬁnition requires ﬁrst or ﬁnding formula partial sums. few few classic nontrivial cases possible. wellknown example geometric series sn n n0 rn 1 r r2 rn 1 rn1 1 r r 1 r real number. formula one show quite easily n0 rn 1 1r r 1 diverges otherwise coincidentally formula also resolves zenos paradox. observed before time interval paradox exactly half previous interval. therefore assuming ﬁrst interval i.e. time race starts runner reaches tortoises initial position one unit time sequence times given by tn 1 2 n . hence time runner catches tortoise is n0 1 2 n 1 1 1 2 2 exactly common sense tells us be. 12
1.6 convergence tests unfortunately majority inﬁnite series seldom behave fortuitously last example previous section. almost never least cases practical interest explicit formula partial sums. yet importance inﬁnite series applications cannot cases simply throw hands say dont explicit formula test limit dont know whether series converges not. real need methods determine whether series converges without explicit formula partial sums. methods fall general classiﬁcation convergence tests. tests quite valuable since general applied sequence terms series i.e. an. therefore advantage working almost always known analytically expressible sequence. however one seldom ever gets something nothing world. convergence tests prove exception rule exact two penalties return convenience allowing us work an. ﬁrst single universally applicable universally conclusive convergence test. every test least series simply cannot applied others unable reach deﬁnite conclusion i.e. mathematically shrugs shoulders. second penalty must pay use convergence tests drawback common almost mathematically called existence results. drawback that even convergence tests conclusively state series converge provide value sum i.e. limit partial sums state sum limit exists. therefore practical applications convergence tests must followed computational algorithms procedures allow sum convergent inﬁnite series approximated within acceptable accuracy. large number convergence tests. purposes however small subset commonly needed. shown table 1. example using tests show that n0 3n2 n n4 3n3 1 converges since 1 n2 n0 n n2 1 diverges since 1 x x2 1 dx diverges n1 n n 1 diverges since lim n n n 1 1 0 note that observed earlier none convergence tests table 1 provide formulae calculate limit convergent inﬁnite series. limit can general approximated taking partial sum enough terms series partial sum suﬃciently close ﬁnal limit. procedure implicitly assumes priori knowledge many terms enough give good approximation. knowledge however would impossible without information fast series converging. would information come form all convergence 13
convergence tests 1. lim nan 0 diverges. this test inconclusive lim nan 0. 2. absolute convergence test an converges converges. this test inconclusive an diverges. 3. comparison test bn a 0 bn either converge diverge. 4. alternating series test anan1 0 i.e. terms series alternate algebraic signs an1 an lim nan 0 converges. 5. integral test 0 fn function fx positive monotonically decreasing 1 fx either converge diverge. 6. ptest n1 1 np converges p 1 diverges p 1 . 7. order nptest 1 np p 1 n1 converges. 8. geometric series test n0 rn converges r 1 . diverges r 1 . 14
tests generally provide information given series converges. implies sequence partial sums eventually approaches limit convergence tests provide information fast limit approached. example consider two series n0 1 n e 2.7183 n1 1 n2 π2 6 1.6449 converge according one convergence tests following table partial sums n n n0 1 n n n1 1 n2 3 2.6667 1.3611 5 2.7167 1.4636 10 2.7183 1.5498 100 2.7183 1.6350 clearly ten times many terms partial sum one hundred terms second series provide good approximation value series tenterm partial sum ﬁrst. alluded above determination rate convergence inﬁnite series far academic interest try approximate value convergent inﬁ nite series computing partial sum. without realistic way determining many terms must computed order obtain good approximation computation would amount little mathematical shot dark. best would take far terms really necessary getting accurate result although paying far much computational cost it. worst however might stop sum prematurely end terribly inaccurate approximation. fortunately next section shall see idea order terms series fact estimate fairly well priori accurately given partial sum approximate ﬁnal series sum. 1.7 error estimates given general reasonable way approximate unknown sum inﬁnite series partial sum natural measure accuracy approximation would diﬀerence approximation actual sum. measure natural since series converges know sn n n0 s 15
sum series. therefore large n sn close to s error approximating sn simply en sn n0 n n0 nn1 . face it may seem terribly illuminating since general sn known en another unknown convergent series. however shall see many cases en estimated least order magnitude order magnitude estimate en needed determine many digits sn signiﬁcant. course obtaining estimate may trivial since en depends n. but reasonably large class series obtaining error estimates also really diﬃcult. consider inﬁnite series n1 1 np p 1 . following deﬁnition above error approximating series partial sum terms thus given en nn1 1 np 1 n 1p 1 n 2p 1 n 3p . look figure 8. note since rectangle ﬁgure base length one areas rectangles are respectively 1 n 1p 1 n 2p 1 n 3p . . . therefore total shaded area blocks figure 8 precisely equal error en. however clearly shaded rectangular area figure 8 also strictly smaller area curve ﬁgure. importance observation cannot compute sum error series exactly compute slightly larger integral. thus nn1 1 np n dx xp 1 p 1np1 or series en 1 p 1np1 . accuracy upper bound en checked case p 2 whose partial sums computed earlier example since case know exact value inﬁnite series i.e. n1 1 n2 π2 6 . 16
n n1 n2 n3 1xp 1n1p 1n2p 1n3p figure 8 estimating error partial sum results computing actual error comparing upper bound given integral shown following table n en 1 p 1np1 3 0.28382 0.33333 5 0.18132 0.20000 10 0.09517 0.10000 100 0.00995 0.01000 table demonstrates integral error bound fact gives excellent order magnitude estimate error n 5 . last result obtained speciﬁc series 1 np fairly straightforwardly extended series whose terms 1 np p 1 . happens 1 np then constant c an c np . then absolute value sum less equal sum absolute 17
values terms sum en nn1 nn1 an nn1 c np n c xp dx c p 1np1 en c p 1np1 . last result restated 1 np en 1 np1 . therefore series doubling number terms used partial sum approxi mation reduces error factor 1 2p1 . result generally provide tight bounds case coeﬃcients exactly 1 np nevertheless works acceptably well almost cases. 1.8 sequences functions discussion sequences series thus far implicitly using restrictive deﬁnition sequence necessary. speciﬁcally deﬁned sequence function deﬁned integers considered function unique in putoutput black box. examples realvalued functions. are however sequences realvalued ones. example se quences functions i.e. unique inputoutput black boxes valid inputs integers outputs numbers functions. example fnx xn n 0 with implicit understanding x0 1 deﬁnes sequence functions graphs ﬁrst members displayed figure 9. convergence sequences functions deﬁned reasonably similar manner convergence sequences constants since ﬁxed x sequence function values fact sequence constants. convergence sequences functions diﬀerent however values depend functions sequence itself value x well. thus fnx fx 18
f0x f1x f2x f3x f4x f5x 1 1 figure 9 sequence functions is sense incomplete statement values x holds true also speciﬁed. example xn 0 1 x 1 . thus completely specify behavior sequence would state xn 0 1 x 1 while xn 1 x 1 sequence converge either x 1 x 1 . behavior commonly referred pointwise convergence divergence. would ﬁnally note convergence sequences functions strong graphical interpretation convergence sequence constants could viewed terms tables values change point. sequences functions picture one sequence curves graphs which eventually begin lie top other ﬁnally become indistinguishable limit curve. inﬁnite series functions e.g. n0 fnx also deﬁned similarly inﬁnite series constants since again ﬁxed x inﬁnite series functions reduces series constants. again however sequence functions must explicitly recognized resulting sequence partial sums snx n n0 fnx 19
depends n x. therefore whether given inﬁnite series functions converges diverges generally aﬀected choice x. example sequence functions deﬁned above inﬁnite series n0 xn converges 1 x 1 by geometric series test therefore n0 xn 1 1 x 1 x 1 . note inﬁnite series converge x 1 even though sequence terms functions does furthermore even series functions converges rate converges hence number terms necessary good approximation generally depends value x evaluated. example explicit formula partial sum geometric series snx 1 xn1 1 x therefore fairly easily shown enx xn1 1 x . clearly latter formula enx depend n x. degree dependence however perhaps strikingly demonstrated following table displays error approximating sum series selected values x n . x e5x e10x 0.10 1.1 106 1.1 1011 0.25 0.00033 3.1 107 0.50 0.03125 0.00098 0.75 0.71191 0.16894 0.95 14.70183 11.37600 sequences functions kind convergence inﬁnite series functions convergence properties may vary value x commonly referred pointwise convergence. better convergence all pointwise convergence particularly desirable especially computational viewpoint since program used compute partial sums case would able adjust upper limit sum depending value x. reason programming convenience would far prefer that given desired accuracy criterion single number 20
took partial sum number terms would guaranteed within desired tolerance actual limit even taking number terms slightly ineﬃcient values x . is would like convergence behavior somewhat uniform across values x . behavior occurs naturally termed uniform convergence. mathematically deﬁned follows series n0 fnx converges uniformly if every ϵ 0 nϵ enx ϵ n nϵ x . notice english translation deﬁnition exactly outlined above. uniform convergence happens when given speciﬁed tolerance ϵ number terms nϵ taking partial sum least many terms guarantee result within speciﬁed tolerance actual value irrespective value x . unfortunately properties inﬁnite series functions whether series converges uniformly depends often not interval x values interest. behavior illustrated example series n0 xn which shown above enx xn1 1 x . therefore 1 2 x 1 2 enx xn1 1 x 1 2 n1 1 1 2 1 2n therefore interval series converges uniformly. however similar relation ship found interval 1 x 1 therefore series converge uniformly interval. discussion may seem bit confusing. so dont lose heart completely uniform convergence fairly delicate concept. also central concept series functions could easily encompass chapter itself. shall devote nearly much time deserved topic scratch surface terms properties associated uniform convergence. fortunately shall need use results mathematical theory associated uniform convergence series consider text general reasonably straightforward analyze. purposes one result associated uniform convergence tran scends others. covered following 21
theorem series functions n0 fnx converges uniformly limit function fx fnx continuous fx also continuous. note theorem tell whether particular given series converges uniformly not one may conclude determined series converge uniformly. thus example theorem allows us conclude sequence functions fnx xn converge continuous function 12 x 12 determined above sequence converged uniformly interval. actual determination whether particular series converges uniformly where generally diﬃcult even so determination convergence series constants. fortunately however purposes one extremely easytoapply test uniform convergence apply series must deal with. test socalled weierstrass mtest really another comparison test. theorem given sequence functions fnx sequence constants mn interval x b fnx mn n0 mn converges n0 fnx converges uniformly interval note probably reason call mtest karl theodore wilhelm weierstrass2 called sequence mn instead an. done latter would probably call weierstrass atest today furthermore note that weierstrass test suﬀers limitations convergence tests series apply even shows series converge uniformly provide information limit series is. apply theorem immediately problem representative type inﬁnite series shall deal rest text n1 1 n2 cosnx cosx 1 4 cos2x 1 9 cos3x . series functions form sequence terms satisfy conditions wierstauss mtest fnx 1 n2 cosnx 1 n2 x . therefore since n1 1 n2 converges 2see 22
conclude n1 1 n2 cosnx converges uniformly x therefore inﬁnite series represents continuous func tion. 23
problems 1. following sequences determine sequence converges diverges. sequence converges determine limit a. 2n1 3n2 b. n 12 5n2 2n 1 c. sinn n 1 d. cosn e. 2n 12 en 3n2 5n 10 f. n cos nπ 2 n 1 g. cosnπ n2 1 h. en n i. n sinnπ n 1 2. determine order big oh following sequences a. n3 2n2 1000 n7 600n6 n b. cosnπ n2 1 c. n n2 1 n n2 1 sinn 1 2π d. 10n3en n2 2n 12 cosn2π 3. consider inﬁnite series n0 n 122n 2n a. compute explicitely partial sums s3 s6 b. write equivalent series obtained replacing n k 2 i.e. shifting index. 4. determine whether following inﬁnite series diverges converges a. n0 en b. n0 n2 1 n 13 c. n0 n2 cosnπ n3 12 d. n0 n n 3 e. n0 en n cosnπ f. n2 1 n lnn 5. determine approximate upper bound error following inﬁnite series approximated twentyterm partial sum s20. a. n0 2n 1 3n4 n 1 b. n1 1 n5 c. n1 2n 12 n4 6. consider series n0 xn a. plot partial sums s1x s5x s10x s20x 2 x 2. b. conclude convergence partial sums interval c. what anything diﬀerent conclude convergence partial sums interval 1 2 x 1 2. 24
2 fourier series 2.1 introduction last chapter reviewed discussed concept inﬁnite series functions. series ﬁrst example students encounter taylor series frequently used approximate complicated functions terms simpler ones. example transcendental function ex exactly computable x 0. however taylor series n0 xn n compute desired degree accuracy approximate values complicated function terms simple powers x. chapter introduce almost without question commonly used inﬁnite series functions taylor series fourier series. deﬁnition fourier series inﬁnite series form fx a0 2 n1 cos nπx l bn sin nπx l 2.1.1 l positive number. note form is unfortunately quite universal. texts write leading coeﬃcient fourier series a0 rather a02. still texts use quantity t02 instead l. shall also try point appropriate time nothing wrong either alternative formulations simply lead feel slightly involved formulas. since fourier series inﬁnite series based earlier discussion several questions immediately come mind. among these shall consider following 1. kind functions fx write fourier series 2. convergence properties fourier series equivalently general conditions fourier series converge 3. coeﬃcients bn related function fx ﬁrst insight general class functions one write fourier series comes observation terms 2.1.1 periodic period 2l e.g. cos nπx 2l l cos nπx l 2nπ cos nπx l etc . hence clearly fourier series must also periodic period 2l i.e. fx 2l fx . 2.1.2 25
function whose general behavior described 2.1.2 shown figure 10. key course periodicity repetitive nature functions. furthermore repetition clearly evident inspection graph. lastly observe know given function fx periodic value l easily related graph since interval repetitions 2l. 2l l l 2l 3l figure 10 general periodic function last comments also dovetail quite nicely earlier ones inﬁnite series may oﬀer way decomposing complicated functions simple ones. along lines shall show fully course develops exactly fourier series provides mechanism complicated periodic functions broken sums sines cosines simplest periodic functions. 2.2 derivation fourier series coeﬃcients next turn question coeﬃcients fourier series related function fx. is unfortunately single technique produce formulas coeﬃcients inﬁnite series. may recall example taylor series gx n0 anx x0n repeated diﬀerentiation followed evaluation x x0 leads conclusion coeﬃcients deﬁned 1 n dng dxnx0 which immediately implies taylor series written functions inﬁnite number derivatives. show totally diﬀerent approach determines fourier series coeﬃcients and simultaneously provides important insights type functions fourier series actually written. starting point number integral formulas socalled orthogonality integrals. shall see clearly study develops integrals pivotal fourier analysis. precise formulas orthogonality integrals 26
l l cos nπx l cos mπx l dx 0 n l n l l sin nπx l sin mπx l dx 0 n l n l l sin nπx l cos mπx l dx 0 m n 2.2.3 n positive integers. is least surface nothing partic ularly extraordinary integrals. easily veriﬁed standard calculus techniques. one simply uses standard trigonometric identities cosa cosb cosa b cosa b 2 reduce integrand sum elementary functions ﬁnds antiderivative ﬁnally evaluates antiderivative end points using fact sinkπ 0 coskπ coskπ integer k. given orthogonality integrals however proceed reduce 2.1.1 follows. let denote ﬁxed arbitrary positive integer. this means may assume positive integer. need necessarily even odd large small etc. simply cannot assume steps valid require information fact positive integer. shall multiply sides 2.1.1 cos mπx l then using property integral sum sum respective integrals integrate series formally term term l l. performing steps leads equation l l fx cos mπx l dx a0 2 l l cos mπx l dx n1 l l cos nπx l cos mπx l dx bn l l sin nπx l cos mπx l dx . 2.2.4 mathematically say integration formal really analytic uncertainty validity interchanging operations integration inﬁnite summation. fact certain inﬁnite series none fourier series interchang ing operations fact shown produce totally incorrect results. reader rest assured however rigorously proven that fourier series interchange shown valid although proof claim far beyond scope discussion here. 27
returning 2.2.4 use elementary integration techniques show that since 0 ﬁrst integral right hand side identically zero. look rest expression closely. remember series notation implies terms inside brackets must evaluated every positive integer n values summed. wait positive integer then orthogonality integrals 2.2.3 every term right side identically zero except one. single nonzero term cosine integral arises one time inside summation value n equals m. somewhat graphically means entire righthand previous equation obeys a0 2 l l cos mπx l dx 0 n1 l l cos nπx l cos mπx l dx 0 mn bn l l sin nπx l cos mπx l dx 0 mn . therefore 2.2.4 simpliﬁes to l l fx cos mπx l dx l l cos2 mπx l dx or solving using value orthogonality integral cosines n 1 l l l fx cos mπx l dx 0 . now note since designated ﬁxed arbitrary last equation fact simply formula am. therefore equally well replace n sides expression rewrite as 1 l l l fx cos nπx l dx . 2.2.5 derive similar formula bn bn 1 l l l fx sin nπx l dx 2.2.6 multiplying sides 2.1.1 sine integrating. finally a0 determined observing average values cos nπx l sin nπx l zero whole number cycles periods. thus average value interval l l zero. hence average value every term summation sign zero thus average value entire righthand side 2.1.1 precisely a0 2 . 28
since average value lefthand side computed 1 2l l l fxdx clearly equating two expressions canceling factors two a0 1 l l l fxdx . 2.2.7 equivalently one likes brute force approach may simply integrate sides 2.1.1 l l show integrals summation sign identically zero arrive exactly result. would obviously nothing wrong approach generally prefer use physical insight wherever possible. might note 2.2.7 precisely would obtain letting n 0 2.2.5 even though 2.2.5 originally derived assuming n 0. exactly adopted seemingly rather awkward form leading coeﬃcient 2.1.1 since see writing constant term series a02 able use 2.2.5 represent ans series n positive. unfortunately however still generally compute a0 separately order avoid antiderivative zero denominator. however representation still leaves one less formula remember. example consider function fx 0 2 x 0 x 0 x 2 fx 4 fx function graphed figure 11. 5 4 3 2 1 0 1 2 3 4 5 0 0.5 1 1.5 2 figure 11 piecewise continuous function example straightforward computation using 2.2.5 2.2.6 yields a0 1 l l l fxdx 1 2 2 0 xdx 1 a0 2 1 2 29
1 l l l fx cos nπx l dx 1 2 2 0 x cos nπx 2 dx 1 2 2x nπ sin nπx 2 2 nπ 2 cos nπx 2 2 0 2 n2π2cosnπ 1 bn 1 l l l fx sin nπx l dx 1 2 2 0 x sin nπx 2 dx 1 2 2x nπ cos nπx 2 2 nπ 2 sin nπx 2 2 0 2 nπ cosnπ . thus combining these fx 1 2 n1 2 n2π2cosnπ 1 cos nπx 2 2 nπ cosnπ sin nπx 2 1 2 4 π2 cos πx 2 2 π sin πx 2 1 π sinπx 4 9π2 cos 3πx 2 2 3π sin 3πx 2 realize point result may fact quite meaningless yet proven series even converges fact appropriate place review exactly inﬁnite series notation represents mean convergence. deﬁnition write fourier series representation 2.1.1 mean that consider sequence partial sums given by snx a0 2 n n1 cos nπx l bn sin nπx l 2.2.8 then lim nsnx fx . but notation clearly implies partial sums functions i.e. graphs. thus fourier series converges sequence graphs given pre cisely graphs partial sums look more like fx larger n 30
is. demonstrate example above plotted figure 12 several partial sums series derived. graphs clearly seem larger values n look progressively like original fx therefore example series found appear converge fx. 2 1 0 1 2 0.5 0 0.5 1 1.5 2 2 1 0 1 2 0.5 0 0.5 1 1.5 2 2 1 0 1 2 0.5 0 0.5 1 1.5 2 2 1 0 1 2 0.5 0 0.5 1 1.5 2 2.5 figure 12 convergence partial sums fourier series always generate set graphs order conﬁrm convergence however could clearly become quite cumbersome. therefore part study consider general convergence properties fourier series classify functions fourier series guaranteed converge graphs even plotted. start developing bit complete characterization precisely kind functions makes sense try write fourier series i.e. even try compute bn given 2.2.5 2.2.6. observe computing bn requires evaluating deﬁnite integrals. therefore since cos nπx l sin nπx l continuous seems almost selfevident basic calculus considerations fx cannot too far away continuous function. initial impression basically correct although precise condition slightly involved proof far beyond scope discussion here. therefore simply state commonly used result functions convergent fourier series theorem function fx convergent fourier series coeﬃcients 31
bn given by 1 l l l fx cos nπx l dx n 0 bn 1 l l l fx sin nπx l dx n 0 . provided fx periodic period 2l fx f x least piecewise continuous l x l. piecewise continuous function is course one composed ﬁnite number continuous segments pieces ﬁnite interval. precise deﬁnition. function fx piecewise continuous a b exists ﬁnite number points x1 x2 . . . xn b f continuous open interval xj xj1 one sided limits fxj fxj1 exist j n 1. common examples functions continuous except jump discontinuities. function used last example one these satisﬁes conditions theorem since continuous everywhere except jumps x 2 diﬀerentiable everywhere except jumps sharp point x 0. func tions f x piecewise continuous often called piecewise smooth. second consideration question precisely fourier series converges jump discontinuity. proof answer proof existence theorem well beyond scope discussion. shown that jump fourier series takes essentially path least resistance converging average value i.e. jump snx fx fx 2 . fx fx refer right lefthand limits respectively e.g. fx means limϵ0 fx ϵ ϵ positive. thus far then introduced fourier series shown fourier coeﬃ cients bn computed one example displayed graphically conver gence resulting series. 32
problems 1. derive formula fourier sine coeﬃcients bn eqn. 2.2.6 using method similar used derive eqn. 2.2.5. 2. following functions ﬁnd fourier coeﬃcients fourier series sketch partial sums s2x s5x s10x a. fx 0 1 x 0 1 0 x 1 fx 2 fx b. fx 3 x 3 x 0 3 x 0 x 3 fx 6 fx c. fx 0 2 x 0 x 0 x 1 2 x 1 x 2 fx 4 fx d. fx 1 cosπx 1 x 1 3. a. show alternative fourier series representation fx a0 n1 cos nπx l bn sin nπx l leads formulas a0 1 2l l l fxdx 1 l l l fx cos nπx l dx n 0 bn 1 l l l fx sin nπx l dx n 0 formula a0 longer n 0 special case formula an. 33
b. show alternative fourier series representation ft a0 2 n1 cos 2nπt t0 bn sin 2nπt t0 note independent variable t l replaced t02 leads 2 t0 t02 t02 fx cos 2nπt t0 dt n 0 bn 2 t0 t02 t02 fx sin 2nπt t0 dt n 0 . note formula a0 n 0 special case formula an. 4. following ﬁnd point l x l fx discontinuity. find left righthand limits fx f x point discontinuity end points interval. without computing fourier coeﬃcients indicate values series converge points. a. fx x2 1 x 3 0 2 x 1 2x 3 x 2 b. fx 3 π2 x π 2x 2 π x π2 c. fx x2 2 x 0 0 0 x 1 4x 1 1 x 2 34
2.3 odd even functions saw previous section use fourier series represent appropriate periodic function combination basic trigonometric functions sine cosine. furthermore veriﬁed particular function satisﬁes necessary con ditions fourier series representation determining coeﬃcients i.e. bn using 2.2.5 2.2.6 straightforward although perhaps quite laborious exercise integration. section show sometimes signiﬁcantly reduce amount com putation necessary construct particular fourier series representation exploiting sym metries function fx. start observing inspection graphs sine cosine figure 13 shows functions possess certain natural symmetries. speciﬁcally cosine perfect reﬂection symmetric respect yaxis sine inverted antisymmetric one. thus unreasonable expect strong connec tion symmetries if any periodic function fx presence or absence sine cosine terms fourier series. show fact case. 4 2 0 2 4 1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1 4 2 0 2 4 1 0.8 0.6 0.4 0.2 0 0.2 0.4 0.6 0.8 1 figure 13 symmetries sine cosine symmetry antisymmetry described identical various powers x i.e. xn depending whether exponent n even odd. therefore symmetry antisymmetry exist commonly referred to respectively even odd. precise definition function fx even fx fx x. function fx odd fx fx x. would hasten caution functions symmetries i.e. neither even odd. the ﬁrst example last section one function. furthermore nontrivial function even odd. one dealing either even odd function several rules simplify calculations. among useful rules following 1. product two even functions even 35
2. product two odd functions even 3. product even odd function odd 4. derivative even function odd 5. derivative odd function even 6. gx even l l gxdx 2 l 0 gxdx 7. gx odd l l gxdx 0 . statements straightforward prove generally follow immediately deﬁnitions even odd. statements integrals reﬂect facts evident figure 14 even function sweeps equal areas sides axis odd function almost thing except odd function areas diﬀer algebraic sign cancel. figure 14 integrals even odd functions now applying properties fourier series leads observation fx even fx cos nπx l even fx sin nπx l odd however integral properties above follows immediately last statement fx even fourier series 2.2.5 2.2.6 previous section 2 l l 0 fx cos nπx l dx 36
bn 0 . thus even function fx a0 2 n1 cos nπx l similarly show fx odd fx n1 bn sin nπx l bn 2 l l 0 fx sin nπx l dx . must emphasize results really diﬀerent from produce information already contained basic fourier series coeﬃcient formulas 2.2.5 2.2.6. simply means reduce computational burden selected cases indicating ahead time coeﬃcients identically zero. one would obtain identical values long way i.e. using 2.2.5 2.2.6 directly much eﬀort increased chance algebraic error. thus example odd function shown 6 4 2 0 2 4 6 4 3 2 1 0 1 2 3 4 figure 15 fx x 3 x 3 immediately claim fx n1 bn sin nπx 3 bn 2 3 3 0 x sin nπx 3 dx 6 nπ cosnπ . bother computing an since know must identically zero. results odd even functions also used simplify computation number cases addition subtraction constant convert 37
otherwise unsymmetric function odd function. note addition subtraction constant alter evenness function. example function fx 4 x 3 x 3 fx 6 fx neither even odd fx 4 clearly odd. hence could write fx 4 n1 bn sin nπx 3 bn computed above. 38
problems 1. find fourier series following functions a. fx 1 2 x 0 0 0 x 2 fx 4 fx b. fx x fx 2 fx c. fx sinx d. fx 2 x 2 x 1 x 1 x 1 2 x 1 x 2 fx 4 fx 39
2.4 convergence properties fourier series previous sections seen fourier series written least formally periodic of period 2l function fx fx a0 2 n1 cos nπx l bn sin nπx l 1 l l l fx cos nπx l dx n 0 1 2 . . . bn 1 l l l fx sin nπx l dx n 1 2 . . . obviously given particular fx would little point bothering compute integrals without assurance resulting series would converge cited earlier theorems involving extremely straightforward test i.e. fx least piecewise smooth that satisﬁed would guarantee series constructed using coeﬃcients fact converge. however shall see later another equally important consideration frequently arises study fourier series. often occasions one must deal fourier series itself explicit identiﬁcation function fx represents. cases one would like able infer examining series i.e. coeﬃcients whether converges also does general characteristics function represents possess therefore motivated concerns shall investigate greater detail exact mechanisms drive convergence fourier series. study primary goal fully understand interrelationships properties function fx general behavior coeﬃcients bn. start recalling primary vehicle describing quantitative convergence properties inﬁnite series error remaining approximating series partial sum. case fourier series error given formula enx fx snx nn1 cos nπx l bn sin nπx l 2.4.9 snx given equation 2.2.8. already discussed length consideration general inﬁnite series functions formula also emphasizes error associated rate convergence series will general depend value x evaluating series. furthermore fourier series converge pointwise fx lim nenx 0 values x . lastly fourier series converge uniformly hence limit function fx con tinuous enx ϵn lim nϵn 0 . 2.4.10 40
look carefully 2.4.9. speciﬁcally note cos nπx l 1 sin nπx l 1 x. therefore take absolute values sides 2.4.9 use fact absolute value sum less equal sum absolute values have enx nn1 an bn . 2.4.11 is course guarantee series right converge but does clearly satisﬁed condition given 2.4.10 hence would uniformly convergent series. formally state observations in theorem fourier series fx converges uniformly therefore fx con tinuous n1 an bn converges . observe major part value theorem involves series constants whose convergence therefore easily tested usual tests e.g. integral test 1np test. fact based earlier discussion series constants immediately conclude practically useful corollary fourier series fx converge uniformly therefore fx continuous bn 1 np p 1 . before proceeding need emphasize one point use term continuous discussions fourier series. remember fourier series implicitly considers function fx periodic. thus fx given explicitly interval assumed continued periodically in fx 0 2 x 0 x 0 x 2 fx 4 fx 2.4.12 continuous context used fx continuous given interval also end points begins periodically repeat itself. thus function given continuous context fourier series theorems even though continuous 2 x 2 jump discontinuity x 2. several texts try avoid confusion discussing continuity speaking periodic continuation fx rather referring fx. 41
discussion theorem fairly well specify conditions coeﬃcients fourier series converge continuous function. however cant really stop yet since saw computed fourier series 2.4.12 ﬁrst section theorem cited indicates formally constructed fourier series also converge at least cases discontinuous functions. a discontinuous function however clearly uniformly convergent fourier series. moreover based preceding discussion appears virtually certain fourier series converges discontinuous function either n1 an n1 bn or both must diverge situation seemingly destined case either bn look like 1n. seems exactly occurred fourier series 2.4.12. one two examples discontinuous functions fourier series general theory make. thus still left unanswered question whether fourier series least one coeﬃcients 1n converge and so drives convergence since series right 2.4.11 converge case order understand answer last questions ﬁrst need introduce concept mean square measure. speciﬁcally given function fx integral 1 2l l lfx2dx 2.4.13 called mean square value fx interval l l. physically recognize interpreted essentially measure average energy represented function interval. however apply integral measure eﬀect average energy contained error fourier series approximation expand squared series 1 2l l l enx2 dx 1 2l l l nn1 cos nπx l bn sin nπx l 2 dx 1 2l l l nn1 a2 n cos2 nπx l b2 n sin2 nπx l mnn1 mn aman cos nπx l cos mπx l bmbn sin nπx l sin mπx l mnn1 ambn cos mπx l sin nπx l dx look happens interchange summations integration assuming interchanges mathematically valid. integrals involved become 42
orthogonality integrals 3 precisely terms double series orthogonality integrals integrate zero thus last equation simply reduces to 1 2l l l enx2 dx 1 2 nn1 a2 n b2 n 2.4.14 equation may look especially signiﬁcant look carefully actually embodies one basic properties fourier series importance relation understanding series shall investigate detail starting observation 2.4.14 implies lim n 1 2l l l enx2 dx 0 n1 a2 n b2 n converges. note key diﬀerence last situation i.e. convergence sequence squares earlier theorem uniform convergence speciﬁcally series squares converge even though n1 an bn diverges this again almost certainly arise either bn both 1n precisely case frequently encounter. clearly now lim n 1 2l l l enx2 dx 0 2.4.15 represents type convergence kind simply one types already discussed diﬀerent form fundamentally new last comments new convergence apparently occur even series converge uniformly therefore must diﬀerent uniform convergence. even interestingly new convergence occur even series converge pointwise fx i.e. even enx approach zero values x. reason perhaps surprising statement possible l l gx2 dx 0 even though gx 0 provided gx discontinuous bounded nonzero ﬁnite number points. this simply generalization notion area point zero. words 2.4.15 may occur even though snx converge fx everywhere almost everywhere lim nsnx fx except ﬁnite number points. points course must points discontinuity fx begin with. 43
since shown new convergence deﬁned 2.4.15 coincide either uniform pointwise convergence needs distinctive title. perhaps imaginatively normally called convergence meansquare simply mean square convergence. precise definition sequence functions snx said converge func tion fx meansquare sense almost everywhere lim n 1 2l l l snx fx2 dx 0 . 2.4.16 point view meansquare convergence important describes convergence appears diﬀerent uniform pointwise convergence also convergence apparently governs convergence fourier series dis continuous functions. incorporate insights recent discussion following theorem fourier series fx converges meansquare sense n1 a2 n b2 n converges. moreover theorem uniform convergence also immediately conclude practically applicable corollary fourier series fx converge meansquare sense bn 1 n . must emphasize strongly meansquare convergence nec essarily imply snx converge fx every point l l although certainly could case. meansquare convergence describes essentially convergence terms energy rather pointwise convergence. therefore say series converges meansquare sense fx eﬀect saying fx limit series diﬀer most function contains energy therefore physically uninteresting. recap development point thus far established hierarchy convergences increasingly stringent conditions. speciﬁcally uniform convergence pointwise convergence mean square convergence 44
direction arrows emphasizes stringent convergences automat ically imply less stringent ones vice versa. importance diﬀerence context fourier series that a. fourier series converges uniformly converges continuous function bn 1np p 1. b. fourier series converges uniformly converge discontinuous function. moreover case expect least one coeﬃcients bn or both decay faster 1n. c. diﬀerence pointwise meansquare convergence fourier series converges mean square need converge function fx used compute coeﬃcients bn close in sense diﬀering function whose energy zero function. shall close portion discussion ﬁnal observation that fx fourier series at least formally write f x n1 nπ l sin nπx l nπ l bn cos nπx l . now essentially repeating earlier discussions easily conclude series represent continuous function bn 1n3. similar argu ment repeated successively higher derivatives. leads us conclude that simply looking order coeﬃcients fourier series one determine highest continuous derivative resulting function. ﬁnal conclusion analysis incorporated following corollary fourier series fx converge least meansquare sense bn 1 np p 1 furthermore case f p2x continuous. last result completes ﬁrst part study convergence fourier series describes essentially much information infer function represented fourier series simply observing behavior coeﬃcients. shall look side question given function fx infer general behavior coeﬃcients fourier series even compute them speciﬁcally given continuity properties function predict order fourier series coeﬃcients answer question predictable relationship exists. relation ship expressed following theorem presented without proof 45
theorem k integer f kx highest order derivative fx continuous f k1x piecewise smooth continuous then bn fourier series fx 1nk2 least one or perhaps both 1nk3 . thus example fx continuous f x discontinuous piecewise smooth k 0 theorem expect bn least 1n2 and hence series converge uniformly least one 1n3. instance function 2.4.12 whose series computed ﬁrst section discontinuous piecewise smooth. therefore although perhaps quite obviously appropriate value k use problem theorem k 1 since integral fx continuous. value theorem predicts coeﬃcients 1n least 1n2. fact precisely case since direct computation example showed 1 n2 bn 1 n . principal value theorem provides convenient quick check accuracy computations fourier coeﬃcients. example tells us would inconsistent ﬁnd coeﬃcient 21 cosnπ n 1 n arising known continuous fx. this check unfortunately totally foolproof computed coeﬃcient 1n2 theorem could guarantee exact algebra correct arrived correct order. nevertheless theorem frequently cheaply indicate many cases algebra wrong last theorem also completes study relationships function represented fourier series coeﬃcients series. besides introducing new category meansquare convergence results two major consequences. first all given fourier series function allow us infer studying coeﬃcients series continuity basic function involved derivatives. secondly given function provide convenient quick though foolproof check accuracy computations fourier coeﬃcients. 46
problems 1. following fourier series determine whether series converge uni formly converge mean square diverge a. 1 3 n1 1 n3π3 sin nπx 3 b. 1 n1 1 nπ cosnπ cos nπx 2 c. 2 n1 n cosnπ n2 1 cosnx 1 n2π2 sinnx d. n1 n n 1π sinnπx 2. convergent fourier series problem 1 above determine highest derivative periodic extension fx continuous. 3. consider fourier series fx x 1 x 1 found earlier. diﬀerentiate series term term compare answer actual series f x. 47
2.5 interpretation fourier coeﬃcients thus far chapter introduced fourier series learned calculate coeﬃcients an bn seen coeﬃcients relate convergence properties series. section shall turn slightly diﬀerent question interpret fourier coeﬃcients i.e. really represent this shall need following trigonometric identity cosωx b sinωx cosωx φ cosωx δ a2 b2 φ tan1 b δ 1 ω tan1 b 1 ωφ using identity engineering texts refer φ phase angle δ delay. you also note that order use identity sine cosine must frequency need handle inverse tangent function delicately 0. identity easily veriﬁed aid standard identity cosine diﬀerence two angles cosu v cosu cosv sinu sinv apply identity nth term fourier series 2.1.1 cos nπx l bn sin nπx l cos nπx l φn cos nπ l x δn where a2 n b2 n φn tan1 bn δn l nπ tan1 bn using delay form identity write fourier series 2.1.1 equivalent representation fx a0 n1 cos nπ l x δn a0 a0 2 2.5.17 48
representing fourier series way clearly emphasizes nature fourier series involves synthesizing complicated periodic function combination pure si nusoidal terms musically sinusoids would correspond pure tones. furthermore clearly represents amplitude associated nth frequency component. more over since power energy pure sinusoid proportional square amplitude also aﬀect way energy complex signal distributed within diﬀerent frequencies. thus plot or a2 n function n or frequency n2l represents essentially ﬁngerprint signal terms amplitude energy. amplitude plot commonly referred spectrum signal. note complete characterization ﬁngerprint signal would also require displaying phase φn function n or frequency. sample spectrum for signal computed ﬁrst example ﬁrst section shown below. 1 2 3 4 5 6 7 n 0.5 1.0 figure 16 spectrum signal interpretation expanded computing average energy contained single period fourier series signal i.e. 1 2l l l fx2 dx integral computed similarly derivation 2.4.14 where squaring terms integrate zero orthogonality leaving 1 2l l l fx2 dx a2 0 n1 1 2a2 n a2 0 4 n1 a2 n b2 n 2 2.5.18 result frequently referred parsevals3 theorem several rather interesting interpretations. first all indicates powerenergy diﬀerent frequencies add independently give total energy. stated another way various diﬀerent frequencies series spill over interact other. changing amplitude or also shown phase single frequency eﬀect 3marcantoine parseval des chˆ enes see 49
terms series. property interesting applications communications since implies several independent signals could sent single physical channel unambiguously decoded far end provided properly chosen set diﬀerent frequencies used. fact one show fairly simply general concept orthogonality given 3 important sending signals since suﬃcient condition separation energy two signals l l fx gx2 dx l l fx2 dx l l gx2 dx is l l fxgxdx 0 second conclusion follows 2.5.18 necessary condition meansquare convergence fourier series function fx must ﬁnite energy interval l l. ﬁnal conclusion shall develop 2.5.18 describes how good ap proximation fx obtained using partial fourier series. speciﬁcally suppose wish approximate appropriate fx ﬁnite trigonometric sum i.e. fapproxx α0 2 n n1 αn cos nπx l βn sin nπx l 2.5.19 αn βn are moment set constants necessarily bn. ﬁnd accurate approximation given terms fourier series answer question write complete fourier series fx subtract trigonometric approximation term term would ﬁnd error approximating fx 2.5.19 is ex fx fapproxx a0 2 n1 cos nπx l bn sin nπx l α0 2 n n1 αn cos nπx l βn sin nπx l γ0 2 n1 γn cos nπx l ηn sin nπx l γn αn 0 n n n n ηn bn βn 1 n n bn n n 50
since error signal ex fourier series coeﬃcients γn ηn apply 2.5.18 compute total energy error l l ex2 dx l 2 γ2 0 n1 γ2 n η2 n now point indicated choose coeﬃcients trigonometric approximation. every term right last equation non negative. therefore mean square error i.e. energy error signal minimized making many terms possible equation equal zero. since ﬂexibility choice bn cannot aﬀect values γn ηn n n. implies minimize error approximation 2.5.19 choosing γn 0 0 n n ηn 0 1 n or αn 0 n n βn bn 1 n importance result which sometimes get lost algebra order minimize energy error trigonometric approximation must use precisely fourier coeﬃcients bn therefore purely trigonometric approximation cannot improved given partial sum fourier series without increasing number n frequencies considered. observation summarized following theorem. theorem fourier coeﬃcients bn optimum sense provide best mean square approximation given function ﬁnite number trigonometric functions. theorem represents last result shall derive section. means exhausted interpretations applied fourier coeﬃcients developed primary ones relate coeﬃcients amplitude phase power diﬀerent frequencies series. lastly shown fourier coeﬃcients provide mean square sense best possible choice coeﬃcients trigonometric expansion given function. 51
problems 1. plot amplitude phase function frequency fourier series found problems ﬁrst section chapter. 2. prove parsevals theorem equation 2.5.18. hint ﬁrst show 1 2l l l fx2 dx a2 0 4 n1 a2 n b2 n 2 use deﬁnition an. 52
2.6 complex form fourier series although reasons may apparent now often advantageous express basic concepts fourier series terms complex variables. alternative formulation clearly possible since relations eix cosx sinx cosx eix eix 2 sinx eix eix 2i allow conversion normal realvalued trigonometric forms complex ex ponential form. no apology made mathematicians use symbol rather engineers j stand 1. could convert 2.1.1 complex form replacing trigonometric functions equivalent complex exponential forms algebraically regroup terms. substitution clearly produce complex exponential terms form einπxl positive negative values n i.e. values plus minus inﬁnity therefore reduce 2.1.1 expression like fx n cneinπxl 2.6.20 cn probably complex constants. note n 0 term series must represent constant dc term original signal. already noted could determine values cn algebraically regrouping terms replacing trigono metric functions equivalent complex exponentials. easier way also happens theoretically pleasant recall derived coeﬃcients original realvalued fourier series. multiplied sides series equation one trigonometric functions integrated applied orthogonality integrals. try similar approach here general approach mind let k stand ﬁxed arbitrary integer multiply sides 2.6.20 eikπxl integrate l l. interchanging summation integration yields l l fxeikπxldx n cn l l eiknπxldx now direct computation show complex exponentials also obey orthog onality relation l l eiknπxldx 0 n k 2l n k 2.6.21 53
therefore equation simpliﬁes l l fxeikπxldx 2l ck or replacing k n sides cn 1 2l l l fxeinπxldx 2.6.22 formula cn express complex exponent terms ordinary trigonometric functions use fact integral sum sum integrals rewrite 2.6.22 cn 1 2l l l fx cos nπx l dx i l l fx sin nπx l dx 1 2 an ibn n 0 2.6.23 bn are course standard fourier coeﬃcients given 2.2.5 2.2.6. shows expected cn complex. furthermore easily relate φn appeared converted realvalued fourier series amplitudephase form. speciﬁcally see 2.6.23 that cn 1 2an argcn tan1 bn φn nπ l δn note also that provided fx real cn c n i.e. cn complex conjugate cn. thus cn 1 2an argcn φn. thus energy single frequency real series split equally two one positive one negative respective complex frequencies. above clear one could plot cn function n for n and except fact amplitude associated real frequency nπl equally divided two complex frequencies obtain identical information would obtained plotting realvalued amplitude spectrum described earlier. fact especially applications electrical engineering signal processing com plex amplitude spectrum customarily used instead realvalued amplitude spectrum. many reasons this including relative ease working complex numbers especially multiplications involved often powerful insights possible using complex plane representation. time however consider complex fourier series simply alternative realvalued representation although shall return later discuss fourier transforms. 54
problems 1. find complex fourier series following functions a. fx x 3 x 3 fx 6 fx b. fx 0 1 x 0 1 0 x 1 fx 2 fx 2. plot complex amplitude spectrum series found problem 1 above. 3. show use t0 period signal rather 2l formula complex fourier series coeﬃcients reduces cn 2 t0 t02 t02 fxe2inπxt0dx 4. using complex form fourier series prove following form parsevals theorem 1 2l l l fx2 dx n cn2 hint show fx2 fxfx n cneinπxl m c meimπxl n m cnc meinmπxl integrate. 55
2.7 fourier series ordinary diﬀerential equations thus far concentrated developing basic concepts fourier series themselves. many interesting questions series might pursue. ﬁrst one shall study fourier series ﬁt scheme ordinary diﬀerential equations. consider ordinary diﬀerential equation py qy ry fx 2.7.24 p q r may constants fx general periodic function i.e. fx 2l fx fx need simple sine cosine. for example fx might look like figure 17. wish ask question ordinary dif ferential equations methods seen prior fourier series variation parameters laplace transforms undetermined coeﬃcients wellsuited problem know variation parameters produce solution problem. but recall that besides usually least eﬃcient method produce non homogenous solutions when viable choice exists variation parameters produces solution terms integrals involve forcing function. but seen earlier computations fourier coeﬃcients evaluating integral involving piecewise smooth fx normally involves splitting integral several parts. example fx given figure 17 ex homogeneous solution 4 3 2 1 0 1 2 3 4 5 0.2 0 0.2 0.4 0.6 0.8 1 fx x x 2x x4 figure 17 typical periodic function variation parameters would require computing integrals like 4.5 0 exfxdx which due deﬁnition fx would expressed as 1 0 xexdx 2 1 2 xexdx 4.5 4 x 4exdx 56
would actually require three integrations. behavior could general make evaluation variation parameters integrals computational morass class prob lems. thus possible would like avoid variation parameters here. utility laplace transform class problems aﬀected fact interesting periodic nonsinusoidal functions jump discontinuities either function one derivatives. the fx graphed discontinuities f x sharp corners. furthermore wellknown study laplace transforms jump discontinuity fx produce esc type term transform x c location discontinuity. periodic function one discontinuity must inﬁnite number. therefore example transform square wave see fig. 18 1 2 3 4 5 1 figure 18 square wave would inﬁnite number diﬀerent esc type terms. speciﬁcally able show fx fs 1 n1 1nens but since using laplace transform solve ordinary diﬀerential equation 2.7.24 involves term form fs as2 bs c clear fs contains inﬁnite number diﬀerent esc type terms will least principle apply laplace inverse including heaviside shift inﬁnite number diﬀerent times. like variation parameters possible least theory normally becomes computationally cumbersome leads results often complicated provide insight real properties solution. thus conclude laplace transform also particularly suitable method class problems. neither variation parameters laplace transform seeming es pecially attractive method class problems turn last standard methods studied ordinary diﬀerential equation methods undetermined coeﬃcients. undetermined coeﬃcients however used forcing function fx meets 57
fairly stringent criteria. speciﬁcally periodic functions method ap plies sines cosines linear combinations these. functions shown figure 18 ﬁrst appear meet criterion. however know and similar fx written combination sines cosines speciﬁcally form fourier series. furthermore since ordinary diﬀerential equation linear know principle superposition holds. thus claim particular solution py qy ry fx a0 2 n1 cos nπx l bn sin nπx l ypx yp0x n1 ypnx py p0 qy p0 ryp0 a0 2 py pn qy pn rypn cos nπx l bn sin nπx l n 1 2 . . . obvious undetermined coeﬃcients applied equations cases general expression ypx obtained assuming ypnx αn cos nπx l βn sin nπx l n 1 2 . . . note special values p q r form might appropriate one and one value n. fact p q r positive prove particular solution always form ypx α0 2 n1 αn cos nπx l βn sin nπx l where yp0 a0 2r provided fourier series fx converges. thus conclude main value fourier series ordinary diﬀerential equations allows us decompose complicated periodic forcing functions linear com bination sines cosines thus permitting particular solution linear constant coeﬃcient equation obtained using undetermined coeﬃcients principle superposition. as shall shortly see role fourier series partial diﬀerential equations even crucial. 58
problem 1. use fourier series construct nonhomogeneous solution ordinary diﬀerential equations y 2y fx where fx x 0 x 1 2 x 1 x 2 fx 2 fx 59
2.8 fourier series digital data transmission emergence socalled digital signal processing major impact electronics especially communications. simplest form digital signal consists sequence ones zeros example 1 1 0 1 0 1 0 0 . you may recognize binary representation number 212. physically transmission lines 1 might represented unit voltage present 0 voltage. thus sample signal might transmitted time sequence shown figure 19. v 1 2 3 4 5 6 7 figure 19 transmitted digital signal major concern communications engineers much signal altered distorted degraded passes circuits transmission lines. simple model nevertheless illustrates two major eﬀects developed using elementary concepts fourier series ordinary diﬀerential equations analyze response circuit shown figure 20 applied digital test signal. circuit consists resistor capacitor inductor input voltage source et. output assumed occur measuring voltage across resistor which often called load. qt it denote respectively charge capacitor current circuit time t qt it related by it dt qt dq dt . 2.8.25 relations variables instantaneous voltages across resistor capacitor inductor given respectively vr ri vc qc vl ldi dt . 60
figure 20 simple circuit thus apply kirchhoﬀs law states sum voltage drops around circuit components must equal applied source voltage ldi dt ri 1 c q et ld2q dt2 rdq dt 1 c q et . 2.8.26 suppose circuit subjected et made periodic train 0s 1s shown figure 21. 5 4 3 2 1 0 1 2 3 4 5 0 0.2 0.4 0.6 0.8 1 et figure 21 periodic digital test signal principal interest closely voltage across r follows pattern equiv alently could another electrical device attached r reasonably infer original test signal based voltages observed across r determine this ﬁrst assume 61
look circuit transients eﬀectively dropped zero i.e. steadystate solution persists. input signal et represented fourier series et a0 2 n1 an cosnπt bn sinnπt 1 2 n1 1 cosnπ nπ sinnπt 2.8.27 bn determined usual formulae 1 1 et cosnπt dt bn 1 1 et sinnπt dt . now since et thus sum albeit inﬁnite one forcing terms since dif ferential equation 2.8.26 linear invoke principle superposition claim steadystate solution form qsst q0t n1 qnt qnt response single frequency forcing voltage et. thus ld2q0 dt2 rdq0 dt 1 c q0 1 2 ld2qn dt2 rdqn dt 1 c qn 1 cosnπ nπ sinnπt n 1 2 3 . . . . 2.8.28 in reality nontrivial extension usual principle superposition. forcing function ﬁnite sum whole structure inﬁnite series arises precisely properties hold ﬁnite sums also hold inﬁnite series. rigorous justiﬁcation approach point would require detailed analysis rates convergence series involved. fortunately case fourier series investigation almost always show method valid. provided r 0 undamped resonance therefore method undetermined coeﬃcients see qnt form q0t α0 2 qnt αn cosnπt βn sinnπt n 1 2 3 . . . 2.8.29 62
αn βn constants. substitution expressions 2.8.28 followed equating coeﬃcients like terms yields α0 c n2π2l 1c αn nπrβn 0 nπrαn n2π2l 1c βn 1 cosnπ nπ . 2.8.30 solution system two equations two unknowns 2.8.30 yields αn 1 cosnπ nπ nπr n2π2l 1 c 2 n2π2r2 βn 1 cosnπ nπ n2π2l 1c n2π2l 1 c 2 n2π2r2 . 2.8.31 thus qsst c 2 n1 1 cosnπnπr cosnπt n2π2l 1c sinnπt nπ n2π2l 1 c 2 n2π2r2 . 2.8.32 recall however really interested voltage across load resis tor r. but according basic equations steadystate current circuit time derivative steadystate charge capacitor i.e. it dqss dt and therefore ohms law steadystate voltage across resistor given vsst rdqss dt . diﬀerentiating qsst given 2.8.32 term term which mathematically precise would require consideration rates convergence series involved multiplying resistance yields vsst r n1 1 cosnπnπr sinnπt n2π2l 1c cosnπt n2π2l 1 c 2 n2π2r2 . 2.8.33 63
standard trigonometric identities4 used reduce vsst n1 1 cosnπ nπ sinnπt φn 2.8.34 nπr n2π2l 1 c 2 n2π2r2 φn tan1 n2π2l 1c nπr . 2.8.35 careful examination representation vsst yields valuable insights problems digital communications starting observation striking similarities series output vsst given 2.8.34 series input et given 2.8.27. however also two striking diﬀer ences 1 sine term vsst amplitude equal 1 cosnπ nπ 2.8.36 precisely times amplitude corresponding sine term i.e. fre quency input. 2 sine term output undergone phase shift due φn term relative corresponding input term. communications engineers would describe two eﬀects amplitude distortion phase or delay distortion respectively. shall see shortly exactly eﬀects distort signal ﬁrst need explicitly deﬁne term distortion illustrate deﬁnition simple examples. distortion shall mean change serves alter fundamental shape signal i.e. change original signal cannot recovered combination following techniques 4the identity is sin θ b cos θ a2 b2 a2 b2 sin θ b a2 b2 cos θ r sinθ φ r a2 b2 sin φ b a2 b2 cos φ a2 b2 . 64
1 simple ampliﬁcation i.e. change scale vertical axis or equivalently multiplication function constant5 2 insertion dc bias i.e. shift horizontal t axis or equivalently addition constant 3 simple time delay i.e. shift right left location vertical axis or equivalently replacing t c.6 thus example ﬁgures 22ac represent distortion original et since et recovered respectively ampliﬁcation factor gain 2 time shift 12 dc bias 14. figure 22d however represent distortion since way restore form et operations. 1 0 1 2 3 4 5 0.5 0 0.5 1 et 1 0 1 2 3 4 5 0.5 0 0.5 1 et 1 0 1 2 3 4 5 0.5 0 0.5 1 et 0 2 4 6 0.5 0 0.5 1 1.5 figure 22 undistorted distorted signals relatively easy prove signal composed one frequency fourier series term one component frequency term multiplied constant then remain undistorted every component must multiplied factor. similar conclusion also shown valid time delays signal composed 5the term ampliﬁcation used general sense includes cases ampliﬁed signal actually weaker original signal. latter cases commonly referred engineering ampliﬁcation gain less one. 6with periodic functions one must slightly careful ensure delays truly diﬀerent since delay integral number periods is physically undetectable. 65
sum terms i.e. eﬀect serves introduce diﬀerent delays diﬀerent terms distort resulting signal would out step marchers parade. ideas mind return 2.8.36. note value n repre sents factor amplitude frequency component input ampliﬁed. obvious 2.8.35 constant respect diﬀerent values n. thus restore amplitudes output original input values would require multiplication term diﬀerent amount .e. diﬀerent ampliﬁcation frequency. since discussed above amplifying output signal single factor conclude vsst distorted version et. since distortion arises due changes relative amplitudes diﬀerent frequencies aptly termed amplitude distortion. however distortion relative amplitudes source distortion vsst. also observe that since vsst 2.8.34 written vsst n1 1 cosnπ nπ sin nπ 1 nπφn 2.8.37 nth frequency vsst shifted relative corresponding frequency input. here little careful analysis required φn 2.8.34 positive term vsst advanced compared term et. equivalently negative value φn corresponds delay response. therefore nth frequency vsst delayed relative corresponding frequency et time interval equal tdn 1 nπφn 1 nπ tan1 n2π2l 1c nπr . 2.8.38 implies that unless tdn constant diﬀerent frequencies output signal delayed selectively i.e. diﬀerent amounts. but deﬁned above tdn clearly constant. thus output signal restored original form inserting additional diﬀerent delay frequency single delay frequencies. selective ampliﬁcation selective delay must distort signal hence conclude nonconstant φn also distort vsst. type distortion commented commonly referred phase delay distortion. in transmission physical media optical communications similar eﬀect observed. eﬀect commonly attributed nonconstant index refraction causes slightly diﬀerent transmission velocities diﬀerent frequencies slightly diﬀerent transmission path lengths diﬀerent frequencies. encountered context eﬀect often referred dispersion. we close part discussion noting mathematician would likely conclude vsst distorted noting large n coeﬃcients 2.8.33 2.8.34 obey 1 cosnπ nπ 1 n2 66
hence series vsst must converge uniformly by weirstrauss mtest. standard result study inﬁnite series guarantees vsst continuous function hence cannot sharp jumps characterize et. thus clearly shape vsst cannot et hence vsst distortion et. observe moment done. studied possible eﬀects circuit original input signal analyzing individual frequencies signal aﬀected. call frequency domain analysis method almost certainly best way understand output signal may look exactly like input. much little eﬀect distortion signal graphically demonstrated two carefully chosen numerical examples. consider ﬁrst case l 0.02 c 5.00 r 0.75 . 0 5 10 15 20 0 0.2 0.4 0.6 0.8 1 n 0 5 10 15 20 0 0.2 0.4 0.6 0.8 1 n tdn 2 1 0 1 2 0.6 0.4 0.2 0 0.2 0.4 0.6 vss figure 23 first sample output graphs tdn speciﬁc values l r c shown fig ures 23ab respectively. note ﬁgures tdn shown values integer n values although vsst computed values integer points used. curve shows noticeable attenuation higher fre quencies although n 20 attenuation never exceeds 50. engineers would refer 3 db point. curve tdn appears almost ﬂat n 1 suggesting almost 67
noticeable delay distortion occur example. computation shows maximum diﬀerence delays encountered diﬀerent frequencies is td1 td5 0.0185 less 1 one period. therefore neither amplitude delay distor tion seeming especially severe might expect output across r would badly distorted. is fact case. figure 23c shows vsst computed values l r c given taking suﬃcient number terms partial sum 2.8.33. note except dc shift 12 unit fairly recognizable replica original signal. note figure 23c since represents sum series time domain view phenomenon while discussed above figures 23a 23b frequency domain views. lastly consider example given by l 0.16 c 0.01 r 0.75 . 0 5 10 15 20 0 0.2 0.4 0.6 0.8 1 n 0 5 10 15 20 0.2 0 0.2 0.4 0.6 0.8 1 n tdn 1 0.5 0 0.5 1 0.2 0.1 0 0.1 0.2 vss figure 24 second sample output graphs an tdn values shown ﬁgures 24ab respectively. again ﬁgures plot noninteger values well integer values. observe case curves nowhere near constant corresponding curves previous example. curve shows frequencies n 6 n 12 severely 68
attenuated. furthermore maximum diﬀerence delays diﬀerent frequencies case approximately 0.40 factor twenty greater previous example. thus would expect output circuit noticeably distorted previous case. shown figure 24c is. fact time domain output represented figure 24c badly distorted would almost impossible determine original input signal observing output. course would make second circuit almost useless communicating original signal. contrast since output ﬁrst circuit slightly distorted original input circuit would reasonably appropriate transmitting particular signal. example demonstrates ability fourier series decompose complicated periodic time domain signals pure frequency domain sines cosines allows communications engineerdesigner analyze distortion introduced digital signal passes simple circuit compute distorted output signal circuit. one aspect power fourier series ability provide simple analysis response linear systems. 69
3 onedimensional wave equation 3.1 introduction one principal applications fourier series solution partial diﬀerential equations. equations fundamental physical sciences engineering describe tremendous variety diﬀerent important physical processes. chapter shall study application fourier series primarily one partial diﬀerential equation socalled wave equation. intent study two fold investigate mechanics solving problems fourier series also observe interpret physical insights contained solutions. part insights shall see wave equation shares dual timedomain frequencydomain nature observed earlier fourier series application fourier series solution ordinary diﬀerential equations. wave equation really mathematical model set equations describe idealized simpliﬁed version reality. mathematical models common applications generally arise one tries include important inter relationships forces etc. modeling physical processes start identifying primary forces relationships. then generally apply various conservation laws identiﬁed forces produce governing equations. finally equations solved must show solutions accurately reﬂect observed actual physical behavior consider model valid. proceed study wave equation endeavor point various aspects modeling process occur. 3.2 onedimensional wave equation physical prototype wave equation thin tightly stretched elastic string violin string guitar string. strings usually wound around type peg produce extremely strong internal tension. the magnitude tension readily appreciated anyone seen guitar string break played. ﬁrst step deriving mathematical model identify believe primary forces interactions or equivalently assume terms interactions neglected. model assume string extremely thin comparison length tightly stretched rest equilibrium sag lies perfectly horizontal. assume motion string purely onedimensional in vertical direction displacements equilibrium string angles string horizontal small. any readers skeptical assumptions look closely plucked guitar string displacement virtually imperceptible. fact reasonable scale figure 25 would meter along hori zontal axis millimeter vertical emphasize motion vary time position shall denote vertical displacement equilibrium symbol ux t. shall also assume original crosssectional area density string 70
figure 25 elastic string uniform constant changes crosssection density resulting stretching string motion negligible. the latter part assumption ensures consistency earlier assumption extremely small vertical displacements. therefore linear density mass per unit length string remain constant. shall use symbol ρ represent density. finally assume uniform tension shall denote τ throughout length string. now may feel seems like making lot assumptions. sense perhaps are. however observed above eﬀectively one acid test whether assumptions fact reasonable. test whether values mathematical solutions equations result model assumptions agree actually observed measured physical motion strings. values agree well reality accept model relatively accurate representation reality. if however model leads solutions agree observed phenomena would go back fundamental assumptions ask what missed ignored neglected apparently something important going captured. identiﬁed basic assumptions this similar problem next step set complete description interior forces work. this consider idealized segment located arbitrary position interior string figure 26. segment assumed small length denoted x located x x x. assuming basic background physics readers part proceed identify possible forces could act small segment. since assumed motion except vertical direction analyze forces x z direction concentrate vertical. figure 26 clearly seems indicate three possible forces acting 71
figure 26 small segment string segment tension force exerted left end neighboring segment string there another similar tension force exerted right end then possibly kind external commonly called body force. ﬁgure implies body force any eﬀectively acts center mass segment which since assumed string uniform density coincides geometrical center. typical body forces arise gravity which already eﬀectively neglected assuming string lies ﬂat equilibrium magnetic ﬁelds damping due air friction etc. model shall simply represent acceleration due body force gx 1 2x t. note representation emphasized fact force acts center mass also forces generally vary time position. some body forces fact may also vary displacement vertical velocity string i.e. form gx 1 2x t u u t . however since also assumed constant linear density eﬀective lengthening string mass small segment ρx resulting vertical force segment due body forces represented ρgx 1 2x tx . turn next tension forces exerted segment neighboring segments. represent forces must lie along tangents string two vectors equal magnitude tangent left right ends segment. the fact magnitudes equal accidental direct consequence earlier model assumptions internal tension uniform net horizontal motion angles string horizontal small. however string curvature angles right left ends small generally equal. therefore even though magnitudes two tension forces equal directions generally identical result net vertical force component due tension. normal rules decomposing vectors components give vertical 72
components tension left right hand ends respectively τ sinθl τ sinθr therefore sum τ sinθr τ sinθl represents net vertical tension segment. you understand since components vectors negative sign associated left hand component mandatory ensure consistency convention upward vertical direction positive. return assumptions displacements angles θl θr small. therefore mathematically θi sinθi tanθi l r express net vertical force due tension essentially equal τ tanθr τ tanθl . however fundamental result calculus slope tangent line curve also precisely tangent angle curve horizontal axis given value ﬁrst derivative equation curve point. therefore tanθr u xx x t tanθl u xx t . note must use partial derivatives here since ux t function two variables. note slope given partial respect x vertical velocity would given partial respect t. combining components yields net vertical force acting small segment string τ u xx x t u xx t ρgx 1 2x t x but net force zero then according newtons second law motion segment experience proportional vertical acceleration i.e. f ma. conservation law you may recall basic college physics courses newtons second law equivalent conservation momentum. since already know mass segment ρx since vertical acceleration computed center mass segment nothing second derivative position respect time i.e. 2u t2 x 1 2x t newtons second law reduces ρ2u t2 x 1 2x tx τ u xx x t u xx t ρgx 1 2x t x . 73
this course mathematics text written mathematician way succumb temptation divide x yielding ρ2u t2 x 1 2x t τ u xx x t u xx t x ρgx 1 2x t let x approach zero. after all isnt every mathematics book does thus ρ2u t2 x t τ lim x0 u xx x t u xx t x ρgx t . now wait limit look familiar. its nothing more basically deﬁnition partial derivative speciﬁcally partial derivative ux respect x i.e. exactly mean 2u x2x t . putting pieces together have ρ2u t2 x t τ 2u x2x t ρgx t writing shall simplify dividing ρ dropping explicit reference x u yield 2u t2 τ ρ 2u x2 gx t . 3.2.1 one ﬁnal simpliﬁcation shall use. since τ ρ positive constants ratio τρ therefore shall replace ratio single value c2. square deliberately used unequivocally emphasize positive character ratio. it also make algebra lot simpler later on thus ﬁnally onedimensional wave equation 2u t2 c22u x2 gx t . 3.2.2 one further minor simpliﬁcation shall mention. partial derivatives frequently represented subscripts instead ratio notation except cases subscripts would ambiguous. thus example many texts write ux instead u x etc. notation shall frequently use onedimensional wave equation would written utt c2uxx gx t . 3.2.3 74
problems 1. show uniform thin tightly stretched elastic string acted upon forces internal tension external air resistance proportional vertical velocity newtons second law leads partial diﬀerential equation form 2u t2 κd u t τ ρ 2u x2 κd positive constant proportionality. 2. show uniform thin tightly stretched elastic string acted upon forces internal tension external springlike restoring force proportional vertical displacement newtons second law leads partial diﬀerential equation form 2u t2 τ ρ 2u x2 κsu κs positive constant proportionality. 75
3.3 boundary conditions justcompleted derivation onedimensional wave equation includes however one crucial restriction. arose considering internal segment string really expresses physics occurring inside string. guitar strings dont stretch forever theyre usually two three feet long best ends physically ends denoted x 0 x l model represent locations physics process change i.e. locations internal physics longer validly describe occurring. mathematically ends beyond basic governing partial diﬀerential equation ceases hold true. in words precisely correct really written 3.2.3 utt c2uxx gx t 0 x l . therefore order completely specify model must also derive mathematical equations correctly express physics occurring ends. derivation mathematical conditions ends turns bit involved derivation equation interior physics primarily because instead single basic internal process studied fairly large number diﬀerent physical processes may occur ends. one particular however almost always occurs ends real guitar string ends rigidly attached immovable object e.g. bridge body guitar turnable peg neck figure 25. key feature string meets ﬁxed object string doesnt move i.e. displacement ends. since weve already decided represent displacement generic point ux t sensible expressions displacements left righthand ends strings u0 t ul t respectively. displacement ends quantities must course equal zero. thus mathematical formulation physics ﬁxed ends must u0 t 0 ul t 0 . 3.3.4 mathematically equations type 3.3.4 called boundary conditions. diﬀer familiar initial conditions encountered ordinary diﬀerential equations e.g. y 2y y 0 y0 0 y0 1 initial conditions ordinary diﬀerential equation speciﬁed value independent variable in case x 0. conditions 3.3.4 contrast speciﬁed two diﬀerent values x 0 x l values physically correspond boundaries region interest. shall see shortly number possible diﬀerent mathematical general boundary conditions exist wave equation. form 3.3.4 commonly referred ﬁxed end conditions dirichlet7 7johann peter gustav lejeune dirichlet see 76
conditions boundary conditions ﬁrst kind. alluded above could specify physics ends string simply making immobile. practical matter conditions never seen ends real guitars strings. however number intriguing physical models whose internal physics also described wave equation lead mathematical boundary conditions arise would unusual boundary physics guitar string. shall examine conditions now determine kind mathematical boundary conditions produce. one alternative set physics socalled free end condition. model end condition assumes string terminates small eﬀectively weightless ring slide down without friction drag vertical pole also located end string. shall assume moment condition occurs righthand end string. situation clearly righthand end string move therefore boundary condition ul t 0 would silly. and furthermore wrong determine correct boundary condition physics consider enlarged view righthand end adopt common approach physics two observer approach. method assume one observer stands outside string observing ring pole measuring forces there second measures forces inside end string. key approach realization since two observers fact observing phenomenon measurements must therefore identical. realization becomes eﬀective conservation principle derive boundary condition equation. figure 27 free end conditions consider outside observer case. sees ring sliding without friction along pole. without friction pole cannot exert drag ring. therefore pole exerts net vertical force ring since drag 77
ring would instantly transferred end string outside observer concludes zero vertical force string. there is course signiﬁcant horizontal force. beside point inside observer contrast knowledge exactly going outside end string measure tension force net angle righthand end string horizontal. measures net vertical force exerted end exactly earlier derivation figure 26 equal magnitude vertical component tension there i.e. τ sinθl . in figure 26 denoted angle righthand end interior segment θ2. since angle end string x l choose call θl instead. reconcile observations two observers net vertical force measured inside observer must equal measured outside observer i.e. zero. therefore τ sinθl 0 sinθl 0 θl 0 . words string must exact location righthand end horizontal i.e. parallel xaxis. however case slope must also zero. hence led equation u xl t 0 3.3.5 or subscript notation uxl t 0 boundary condition end model. similar argument shows similar frictionless ring pole placed lefthand end string result boundary condition u x0 t 0 3.3.6 ux0 t 0 . boundary conditions form commonly referred free end conditions neu mann conditions8 boundary conditions second kind. we cannot emphasize strongly point requirement place kind pole ring apparatus left end. could equally well left left end ﬁxed. physics left right ends totally independent. third type boundary physics also essentially never encountered real guitar strings leads mathematical condition uncommon models occurs modify slip ring arrangement previous example attaching small vertical spring exert force end string whenever spring either compressed stretched figure 27. case assume position spring 78
figure 28 mixed end conditions already adjusted equilibrium when string horizontal spring neither compressed stretched. consider inside outside observers view newest situation outside observer observe spring stretched wants return toward equilibrium position. therefore spring exert force ring ring onto string proportional amount stretched. outside observer measure net force exerted end string spring equal ksul t ks referred spring constant. note quantities involved forces negative sign explicitly ensures correct orientation spring force i.e. spring exerts force opposite direction displaced. inside observer again knowledge outside mechanism measure net force exerted end string terms vertical component tension sees end. therefore again using smallangle assumption compute net vertical force exerted end as τ sinθl τ tanθl τ u xl t . here positive sign necessary retain proper sense vector components in volved however since observers fact observing force calculated forces must yield result i.e. ksul t τ u xl t 8carl gottfried neumann see carl.html 79
or collecting terms side equality ksul t τ u xl t 0 3.3.7 ksul t τuxl t 0 . type boundary condition commonly referred mixed end robin con dition boundary condition third kind. conduct similar analysis lefthand end string would ﬁnd spring slipring arrangement end would lead mathematical boundary condition ksu0 t τ u x0 t 0 3.3.8 ksu0 t τux0 t 0 . important realize that particular string model may described wave equation one boundary conditions hold one boundary i.e. mutually exclusive. this clear physical principles however noted before conditions diﬀerent ends independent. is would totally permissible string ﬁxed left free right i.e. boundary conditions u0 t 0 uxl t 0 . equally well might attach spring slip ring left ﬁx righthand end yielding ksu0 t τux0 t 0 ul t 0 or apply appropriate combination. is however one fundamental unifying element three types conditions linear homogeneous. is special cases general form α1u0 t β1ux0 t 0 α2ul t β2uxl t 0 speciﬁc values αi βi. 80
problems 1. physically correct algebraic signs boundary conditions critical. show mathematically physically following boundary conditions a. ul t uxl t 0 b. u0 t ux0 t 0 physically realistic. 2. show uniform thin tightly stretched elastic string attached right hand boundary slipring pole frictionless friction proportional vertical velocity along pole boundary condition point becomes τ u x κd u t 0 κd positive constant proportionality. 81
3.4 initial conditions thus far general formulation onedimensional wave equation utt c2uxx gx t α1u0 t β1ux0 t 0 α2ul t β2uxl t 0 . equations however still suﬃcient uniquely specify either mathematically physically motion elastic string. reason really quite simple still important information missing. missing information string starts out i.e. initial conditions. after all diﬀerential equation two time derivatives. based experience ordinary diﬀerential equations expect model require two initial conditions one u another ut . physically equivalent saying that order unique solution need know initial position velocity. mathematically two conditions would written ux 0 fx utx 0 gx 3.4.9 fx gx may virtually two functions. thus ﬁnally complete speci ﬁcation onedimensional wave equation becomes utt c2uxx gx t α1u0 t β1ux0 t 0 α2ul t β2uxl t 0 ux 0 fx utx 0 gx 3.4.10 complete problem partial diﬀerential equation plus appropriate boundary initial conditions commonly referred boundary value problem. 3.5 introduction solution wave equation start investigation application fourier series techniques solution partial diﬀerential equations onedimensional wave equation derived previous section. chose begin equation two reasons 1 obviously represents real world model. 2 partial diﬀerential equations deﬁnition involve one independent vari able. clearly simplest ones involve exactly two. partial diﬀerential equa tions exactly two independent variables wave equation easiest observe interpret dual time domainfrequency domain approach problems. go solving partial diﬀerential equation weve never solved one before answer includes fundamental insight mathematics one great values mathematics power generalize i.e. solve new problems extend ing ideas methods already shown work other similar situations. 82
therefore shall try much possible apply study partial diﬀerential equations lessons learned study ordinary diﬀerential equations starting observation that written one dimensional wave equation 3.4.10 conforms idea linear nonhomogeneous problem since term gx t contain un known u derivatives. motivate general direction shall take recall ﬁrst confronting general second order linear nonhomogeneous ordinary diﬀerential equation pxy qxy rxy gx one jump in try immediately develop complete solution problem. instead ﬁrst study nature solutions simpler case homogeneous constant coeﬃcient equation py qy ry 0 . similar approach seems logical confronting ﬁrst partial diﬀerential equation. therefore shall start adding numerous assumptions already made assumption body forces 3.4.10 negligible. again reader consider whether assumption reasonable real guitar strings. thus summarize ﬁrst case solve is utt c2uxx 0 x l 0 α1u0 t β1ux0 t 0 0 α2ul t β2uxl t 0 0 ux 0 fx 0 x l utx 0 gx 0 x l 3.5.11 observe simplest yet reasonably general case could deal with. example also homogeneous initial conditions i.e. ux 0 0 utx 0 0 ux t 0 would satisfy problem. fact could show would solution call trivial case clearly practical interest. also emphasis speciﬁcally identiﬁed domains various equations valid. henceforth shall simply take domains implicitly given. homogenous problem 3.5.11 is however still somewhat general made particular restriction αi βi. could conceivably proceed general approach point keeping αi βi arbitrary. shall this. instead shall ﬁrst consider several special cases prescribe speciﬁc values constants try arrive complete solution particular problem. finally developed suﬃcient feel examples shall compare diﬀerent problems try identify common features arise irrespective choice αi βi try deduce general theory. in sense trying analogous separating rules game theory sport particular teams players functions equations may arise speciﬁc case. 83
problems 1. brieﬂy describe sentences physical model following boundary value problems a. utt 4uxx u0 t ux3 t 0 ux 0 2x 0 x 1 0 1 x 3 utx 0 0 b. utt uxx u0 t u1 t 0 ux 0 0 utx 0 1 c. utt 9uxx ux0 t ux2 t 0 ux 0 x utx 0 0 d. utt uxx u0 t 0 u3 t 2ux3 t 0 ux 0 2x 0 x 1 0 1 x 3 utx 0 10 84
3.6 fixed end condition string ﬁrst special case shall start one physically common. this discussed earlier occurs ends string rigidly attached immovable object i.e. ﬁxed end dirichlet boundary conditions. terms homogeneous problem 3.5.11 coincides α1 α2 1 β1 β2 0 ux t satisﬁes utt c2uxx u0 t 0 ul t 0 ux 0 fx utx 0 gx 3.6.12 now lets start reﬂecting experience second order linear ordinary diﬀerential equations. ﬁrst step problems identify linearly independent homogeneous solutions. case constant coeﬃcient equations iden tiﬁcation accomplished primarily assuming certain solution form y erx substituting form equation ﬁnally determining speciﬁc values r nontrivial solutions exist. necessary number linearly independent solutions corresponding values r constructed general solution formed taking linear combination linearly independent solutions i.e. multiplying linearly independent solution arbitrary constant summing together. point initial conditions considered order determine values constants solution. try somewhat analogous approach here. try similar procedure here ﬁrst step identify constitutes homogenous problem case partial diﬀerential equations. considering ﬁxedend problem 3.5.11 recalling solving linearly independent homogenous solutions ordinary diﬀerential equations initial conditions enter end problem then according normal concept homogenous homogeneous problem without initial conditions 3.6.12 appears utt c2uxx u0 t 0 ul t 0 3.6.13 problem clearly least trivial solution ux t 0. as linear homogeneous problem all 0 solved ay by cy 0. trivial solutions inherently uninteresting. important question 3.6.13 nontrivial solutions and so ﬁnd them many linearly independent after all nontrivial solutions determined arbitrary multiplicative constant anyway. wait observe homogeneous partial diﬀerential equation 3.6.13 possesses natural splitting lefthand side involves operation time behavior ux t righthand side involves operation spatial behavior ux t. suggests solutions might also splitting time behavior 85
spatial behavior. although means guaranteed shall ﬁrst look solutions form ux t xxtt . 3.6.14 this commonly called separation variables assumption. its important point emphasize form assumed solution erx assumed solution py qy ry 0. course guarantee homogeneous solutions partial diﬀerential equation need product form certainly priori assurance be. for example solutions y 2y 0 pure exponentials. one way tell whether 3.6.14 really yield valid nontrivial solutions substitute 3.6.13 see follows. precisely shall next. order substitute 3.6.14 3.6.13 ﬁrst need compute various indicated derivatives. but basic rules partial diﬀerentiation 2u t2 2 t2 xxtt xxt t and 2u x2 2 x2 xxtt xxtt . thus upon substitution partial diﬀerential equation 3.6.13 becomes xxt t c2xxtt or dividing sides product c2xt t c2tt xx xx . 3.6.15 consider implication last equation carefully. lefthand side contains terms depend time hence completely independent x. righthand side contains terms x independent time. sides equal. thus side equation independent time x therefore side must constant. denote constant λ. we shall see moment ulterior motive involving essentially facts yet developed here including explicit minus sign. really basis information developed time expect one algebraic sign another. thus equation becomes t c2tt xx xx λ t λc2tt 0 xx λxx 0 . 3.6.16 86
appreciate point necessarily single value λ approach works. after all generally two values r secondorder ordinary diﬀerential equation. now value λ seem good other. actually slight overstatement. physical arguments seems unlikely λ 3.6.16 could negative since general solution equation tt would include growing exponential time solution physically reasonable normal guitar strings. mathematically cannot yet discard possible values λ. actually fact cant say anything yet values λ really surprising since utilized information contained homogeneous problem 3.6.13. speciﬁcally yet used boundary conditions. product solution 3.6.14 satisfy entire homogenous problem must necessity satisfy partial diﬀerential equation boundary conditions 3.6.13 well. so substitute product 3.6.14 ﬁrst lefthand end condition yielding u0 t x0tt 0 . 3.6.17 observe x0 value function one point number i.e. constant. product 3.6.17 identically zero. therefore unless x0 0 tt must identically zero. tt identically zero ux t xxtt would also identically zero trivial therefore interest us already know trivial solution satisﬁes homogeneous problem. care nontrivial solutions thus retain hope ﬁnding nontrivial solutions must require x0 0 . similarly x l essentially identical argument must also have ul t xltt 0 xl 0 . now collect information far deduced xx tt deﬁned 3.6.14 have λc2t 0 x λx 0 x0 0 xl 0 . 3.6.18 observe equation tt still solution value λ since conditions tt diﬀerential equation. clearly longer true xx. all equation xx x λx 0 x0 0 xl 0 . 3.6.19 homogeneous two homogeneous boundary conditions. initial conditions would enough guarantee unique solution xx clearly 87
would xx 0. ﬁnd λ nontrivial solutions well well suppose moment consider situation λ negative. conveniently represent writing λ ζ2 ζ nonzero real number. substitution xx satisﬁes x ζ2x 0 general solution xx c1eζx c2eζx . determine constants c1 c2 boundary conditions satisﬁed. x 0 x0 c1 c2 0 c2 c1 xx c1 eζx eζx . condition x l becomes xl c1 eζl eζl 0 . however easily shown eζl eζl cannot zero l 0 ζ 0. therefore must c1 0 means xx 0 trivial solution x λx 0 x0 xl 0 λ 0. in view earlier comment wanting growing exponential solutions tt somewhat relief mathematics conﬁrms physical intuition key attribute consider model valid. since interested trivial solutions drop λ 0 mention problem. well λ negative maybe λ zero. but case diﬀerential equation xx becomes x 0 general solution xx d1 d2x . now try satisfy boundary conditions solution. x 0 x0 d1 0 xx d2x 88
x l xl d2l 0 . easily seen possible way happen d2 0 trivial solution. λ 0 must also discarded problem. we would note clear geometrically since diﬀerential equation boundary conditions λ 0 imply xx straight line passing xaxis x 0 x l xx 0 obviously solution conditions end rope. remaining hope ﬁnding nontrivial solutions form 3.6.14 rests possibility λ positive. case could write λ ξ2 ξ nonzero real number. xx would satisfy diﬀerential equation x ξ2x 0 whose general solution xx a1 sinξx a2 cosξx . must try pick a1 a2 boundary conditions satisﬁed. yields ﬁrst x 0 x0 a2 0 xx a1 sinξx then x l xl a1 sinξl 0 . 3.6.20 major diﬀerence equation cases λ 0 λ 0 neither two terms 3.6.20 automatically nonzero. could fact still set a1 0 get trivial solution longer necessary order satisfy boundary condition could equally well arrange sinξl zero simply picking ξl integer multiple π. latter case a1 could remain nonzero integer use ξl π works ξl 2π ξl 3π ξl 4π forth. fact time pick ξl nπ n 1 2 3 . . . get nonzero value a1 hence nontrivial solution. sake convenience shall denote diﬀerent values subscript ξn nπ l n 1 2 3 . . . . 3.6.21 thus diﬀerent value ξn get another nontrivial solution to x ξ2x 0 x0 xl 0 89
shall denote solution by xnx sinξnx sin nπx l n 1 2 3 . . . . 3.6.22 note dropped arbitrary constant solution xx. since already observed that solutions linear homogeneous problem xx determined arbitrary constant anyway. interested linearly independent solutions problem interestingly passed one step doubts whether splitting assumed 3.6.14 would work λ determination works inﬁnite number diﬀerent values λ given λn nπ l 2 n 1 2 3 . . . . 3.6.23 returning equation tt see value λn given 3.6.23 two linearly independent solutions represent tnt cos nπct l bn sin nπct l n 1 2 3 . . . . finally combining various pieces see value λ given 3.6.23the separation variables form 3.6.14 produces two nontrivial linearly independent solutions 3.6.13 given by unx t xnxtnt cos nπct l bn sin nπct l sin nπx l n 1 2 3 . . . . 3.6.24 at point shall pause momentarily reﬂect happened. linear homogeneous problem given 3.6.13 containing parameter λ. type problem normally would trivial solution. upon substitution however found nontrivial solutions fact exist certain speciﬁed values parameter. behavior qualitatively similar encountered study systems linear algebraic equations matrixvector equation ax λx x ndimensional vector constant matrix. equation nontrivial solutions λ satisﬁes deta λi 0 is λ eigenvalue x corresponding eigenvector. similarities call λ given 3.6.23 eigenvalues problem 3.6.19. similar vein nontrivial solutions xnx given 3.6.22 often referred eigenfunctions. 90
proceed. 3.6.24 seem identiﬁed linearly independent solutions 3.6.13. although proven this number would indicate it. therefore since basic problem linear able form general solution taking arbitrary linear combination linearly independent homogeneous solutions general solution y 2y 0 given linear combination ex xex i.e. yx c1ex c2xex . following logic general solution 3.6.13 given ux t a1 cos πct l b1 sin πct l sin πx l a2 cos 2πct l b2 sin 2πct l sin 2πx l a3 cos 3πct l b3 sin 3πct l sin 3πx l 3.6.25 ai bi 1 2 3 . . . arbitrary constants. note save great deal writing using instead shorthand ux t n1 cos nπct l bn sin nπct l sin nπx l . 3.6.26 but really represents general solution 3.6.13 must able proper choice constants ai bi satisfy initial conditions would wish apply speciﬁcally satisfy initial conditions given 3.6.13 ux 0 fx utx 0 gx . this fact satisfy conditions really one way ﬁnd out take general solution substitute equations. therefore let 0 3.6.26 supposed represent ux t times substitute expression ux 0 initial conditions. produces fx ux 0 n1 sin nπx l . 3.6.27 ﬁnd satisﬁed answer course yes provided fx least piecewise smooth 0 x l. physically must almost certainly case since diﬃcult conceive guitar string real world whose displacements least piecewise smooth. really its quite diﬃcult think continuous need extend fx oddly region l x 0. since 3.6.27 nothing fourier sine series extension coeﬃcients the an given by 2 l l 0 fx sin nπx l dx . 3.6.28 91
second initial condition must ﬁrst diﬀerentiate 3.6.26 obtain expression utx t utx t n1 nπc l sin nπct l nπc l bn cos nπct l sin nπx l let 0 substitute second initial condition get utx 0 n1 nπc l bn sin nπx l . 3.6.29 simply another fourier sine series except eﬀective coeﬃcient nπc l bn instead bn. therefore usual rule computing coeﬃcients simply imagine gx oddly extended nπc l bn 2 l l 0 gx sin nπx l dx bn 2 nπc l 0 gx sin nπx l dx . 3.6.30 clearly indicates bn also computed provided initial velocity gx least piecewise smooth physical considerations really needs be. therefore see satisfy initial condition homogeneous partial diﬀerential equation 3.6.13 solution form 3.6.26 hence seem fact found general solution. wait minute solution deﬁned 3.6.26 involves inﬁnite series inﬁnite series known converge. possible might ﬁnd problems solution converge fortunately here answer no easily shown that since sinnπctl cosnπctl always less equal one magnitude 3.6.26 always converge provided original fourier sine series 3.6.27 did. but original fourier sine known converge basic principles fourier series provided initial displacement velocity piecewise smooth problem. example consider problem utt uxx 0 x 2 0 u0 t 0 u2 t 0 ux 0 x 0 x 1 2 x 1 x 2 utx 0 0 problem describes elastic string length two ﬁxed ends displaced lifting center shape shown figure 29 held there 0 released. at instant release velocity thus zero even though acceleration not. 92
u x 2 1 figure 29 initial displacement ux 0 retrace steps 3.6.13 problem see eigenfunctions satisfy x λx 0 x0 x2 0 therefore xnx sin nπx 2 n 1 2 3 . . . . eigenvalues λn ξ2 n nπ 2 2 n 1 2 3 . . . thus general solution problem looks like ux t n1 cos nπt 2 bn sin nπt 2 sin nπx 2 . initial displacement becomes ux 0 n1 sin nπx 2 x 0 x 1 2 x 1 x 2 and therefore usual fourier formulas 2 2 2 0 fx sin nπx 2 dx 1 0 x sin nπx 2 dx 2 1 2 x sin nπx 2 dx 2 nπx cos nπx 2 2 nπ 2 sin nπx 2 1 0 2 nπ2 x cos nπx 2 2 nπ 2 sin nπx 2 2 1 2 2 nπ 2 sin nπ 2 . 93
similarly initial condition velocity utx 0 yields bn 0 therefore ux t n1 2 2 nπ 2 sin nπ 2 cos nπt 2 sin nπx 2 . 94
problems 1. solve utt uxx u0 t u3 t 0 ux 0 2x 0 x 12 2 2x 12 x 1 0 1 x 3 utx 0 0 sketch tenterm partial sum computed solution 0 1 2 4 . 2. solve utt uxx u0 t uπ t 0 ux 0 0 0 x π4 4x ππ π4 x π2 3π 4xπ π2 x 3π4 0 3π4 x π utx 0 0 3. solve utt uxx u0 t uπ t 0 ux 0 xπ x 0 x π utx 0 0 4. solve utt uxx u0 t u3 t 0 ux 0 0 utx 0 x 5. solve utt 9uxx u0 t uπ t 0 ux 0 sinx utx 0 1 6. solve utt 4uxx u0 t u5 t 0 utx 0 x 0 x 52 5 x 52 x 5 ux 0 0 95
7. dissipation heat very large solid slab thickness l whose faces held ﬁxed reference temperature 0o described partial diﬀerential equation ut kuxx u0 t ul t 0 ux 0 fx ux t denotes temperature location x time t. a. one initial condition required problem b. show method separation variables also works problem leads formally general solution ux t n1 bneknπl2t sin nπx l where bn 2 l l 0 fx sin nπx l dx . 96
3.7 free end conditions problem already indicated general approach study onedimensional wave equation 3.5.11 ﬁrst solve several special cases try deduce main as pects general theory speciﬁc cases. case would correspond diﬀerent speciﬁc boundary conditions i.e. values α β 3.5.11. previous section ﬁrst case developed solutions ﬁxed end condition dirichlet problem. saw separation variables assumption applied homogeneous problem without initial conditions leads inﬁnite number linearly independent solutions deter mined eigenvalues problem. furthermore formed general solution linear combination solutions found satisfying initial conditions reduced nothing ordinary fourier sine series. obvious question must pursue much behavior onedimensional wave equation share aspects purely example dirichlet boundary conditions speciﬁc answer this must course study cases. section consider second special case speciﬁcally α1 α2 0 β1 β2 1 ux t satisfying utt c2uxx ux0 t 0 uxl t 0 ux 0 fx utx 0 gx 3.7.31 physically problem could represent string frictionless slip rings located ends figure 30. mathematically mentioned derivation various bound ary conditions call problem free end condition problem or alternatively say neumann conditions ends. before reading further pause reﬂect physically qualitative properties might expect solutions problem would have. will course want apply problem much possible experience dirichlet conditions. after all little proﬁt reinventing wheel repeating mathematical steps know or know ahead time result be. since dirichlet case started homogeneous problem without initial conditions proceed similarly 3.7.31 start utt c2 uxx ux0 t 0 uxl t 0 3.7.32 clearly also occurred dirichlet case problem always least trivial solution important question becomes linearly independent non trivial solutions ﬁnd them well observe partial diﬀerential equation 3.7.32 changed dirichlet problem still possesses natural splitting time spatial operations. therefore seems quite reasonable use separation variables 97
figure 30 free end conditions problem form 3.6.14 here. moreover since partial diﬀerential equation unchanged only boundary conditions changed substituting product form partial diﬀerential equation must produce exactly separated ordinary diﬀerential equations i.e. t λc2tt 0 xx λxx 0 . keep mind point λ necessarily represent single value. fact based last example almost certainly expect inﬁnite number diﬀerent values work also. they all boundary conditions arent last case theres reason expect solutions. still seem unlikely physical arguments λ could negative since exponentially growing solutions tt seem physically reasonable before. but mathematically cant assume anything yet possible values λ problem. last point shouldnt really surprising since course havent yet considered boundary conditions problem boundary conditions crucial determining eigenvalues ﬁxed end problem. furthermore since boundary conditions problem 3.7.31 dirichlet case cant say x0 xl 0 must derive new set conditions appropriate case substituting separation variables form 3.6.14 directly boundary conditions 3.7.32. substitution yields ux0 t x0tt 0 which since x0 must constant requires x0 0 . 98
nontrivial solutions. essentially identical argument x l leads uxl t xltt 0 xl 0 . thus new set boundary conditions complete set separated equations λc2t 0 x λx 0 x0 0 xl 0 . 3.7.33 observe equations ﬁxed end case equation tt solvable value λ. still seems reasonable since yet applied initial conditions. furthermore equation xx x λx 0 x0 0 xl 0 . 3.7.34 homogeneous homogenous boundary conditions clearly still trivial solution. therefore fundamental question must answer changed dirichlet problem values λ nontrivial solutions 3.7.34 well unless were clairvoyant theres way answer basically repeat procedure before i.e. assuming sequence negative zero positive values λ seeing produce solutions. before start assuming λ could negative represent λ ζ2 ζ 0 . yields xx c1eζx c2eζx therefore boundary conditions reduce x0 ζc1 ζc2 0 xl ζc1eζl ζc2eζl 0 since ζ l nonzero shown satisﬁed c1 c2 0. thus before trivial solution λ 0 which coincides earlier expectation growing exponentials still occur solutions tt. λ 0 may dropped consideration problem. next must check whether λ zero. well case xx d1 d2x substitute solution free boundary conditions ﬁnd x0 d2 0 xl d2 0 . 99
wait requires d2 0 doesnt say anything d1 apparently value. therefore here unlike ﬁxed end case λ 0 produce trivial solution cannot dropped problem. fact according deﬁnitions zero clearly eigenvalue here. corresponding eigenfunction constant which since interested linearly independent solutions assign convenient value. purposes become clear later chose value 1 2. thus summary λ0 0 x0x 1 2 you might note conclusion obvious geometrical interpretation λ 0 condition requires straight line parallel xaxis x 0 x l. but course far really half answer λ 0 real goal identify linearly independent solutions ux t. therefore xx must corresponding tt. λ0 0 tt must clearly satisfy diﬀerential equation 0 0 t0t a0 b0t therefore corresponding product solution becomes u0x t x0xt0t 1 2a0 b0t . cant stop yet identify linearly independent homogeneous solutions ﬁxed end problem inﬁnite number diﬀerent eigenvalues. doesnt seem reasonable problem would one therefore must also investigate whether nontrivial solutions exist λ positive. thus write λ ξ2 ξ nonzero real number. gives general solution 3.7.34 xx a1 sinξx a2 cosξx must try pick a1 a2 zero boundary conditions satisﬁed. free boundary conditions leads x 0 x0 a1ξ 0 xx a2 cosξx since ξ assumed nonzero. then x l boundary condition reduces xl a2ξ sinξl 0 ξ 0 . here ﬁxed end case unlike situation λ negative its necessary set a2 equal zero and trivial solution order satisfy 100
boundary condition could keep a2 nonzero making sinξl zero using eigenvalues worked before i.e. ξn nπ l λn nπ l 2 n 1 2 3 . . . . 3.7.35 important diﬀerence compared ﬁxedend case even though eigenvalues changed review last steps shows eigenfunctions have. fact eigenfunctions xnx cosξnx cos nπx l n 1 2 3 . . . 3.7.36 where dropped arbitrary constant since care linearly independent solutions. geometrically eigenfunctions reﬂect fact that λ 0 equation 3.7.34 may viewed simply specifying sinusoidal functions zero slope ends interval before eigenvalue given 3.7.35 also determines corresponding diﬀerential equation tt whose solution involves two linearly independent solutions represented tnt cos nπct l bn sin nπct l n 1 2 3 . . . . combining information see positive eigenvalue produces two non trivial linearly independent solutions 3.7.32 given by unx t xnxtnt cos nπct l bn sin nπct l cos nπx l n 1 2 3 . . . . 3.7.37 point would seem identiﬁed linearly independent solu tions 3.7.32. although proven this. therefore form general solution taking arbitrary linear combination linearly independent homogeneous solutions including λ 0 yield ux t 1 2a0 b0t a1 cos πct l b1 sin πct l cos πx l a2 cos 2πct l b2 sin 2πct l cos 2πx l a3 cos 3πct l b3 sin 3πct l cos 3πx l 1 2a0 b0t n1 cos nπct l bn sin nπct l cos nπx l 3.7.38 101
ai bi 0 1 2 . . . still determined. moreover before really represents general solution 3.7.32 able determine constants ai bi satisfy set initial conditions would wish apply speciﬁcally initial conditions ux 0 fx utx 0 gx . see whether accomplish here must before substitute 3.7.38 supposed represent ux t times directly expressions ux 0 utx 0. this have ux 0 fx 1 2a0 n1 cos nπx l 0 x l utx 0 gx 1 2b0 n1 nπc l bn cos nπx l 0 x l . 3.7.39 ﬁnd bn satisfy these answer ﬁxed end case yes provided fx gx least piecewise smooth 0 x l which physical considerations clearly case. is however one change. problem must use even extensions rather odd ones since even function fourier cosine series which 3.7.39 represent. computing coeﬃcients usual way recognizing eﬀective coeﬃcient second series nπc l bn instead an yields 2 l l 0 fx cos nπx l dx n 0 1 2 . . . b0 2 l l 0 gxdx bn 2 nπc l 0 gx cos nπx l dx n 1 2 3 . . . 3.7.40 therefore found solution form 3.7.38 satisfy initial condition homogeneous partial diﬀerential equation 3.7.31 hence fact represent general solution. mathematically case series solution ﬁxed end case 3.7.38 really represents formal solution. is its valid unless inﬁnite series involved converge. fortunately though before could show series always converge provided initial condition fourier cosine series 3.7.39 did. cannot leave problem however without pointing one additional intriguing feature 3.7.38 see dirichlet solution presence 1 2a0b0t term. represent would meaning 1 2a0 1 l l 0 fxdx 0 102
interpret 1 2b0 1 l l 0 gxdx 0 we leave reader reﬂect questions illustrate general class problems consider example utt uxx 0 x 4 0 ux0 t 0 ux4 t 0 ux 0 0 0 x 1 x 1 1 x 2 3 x 2 x 3 0 3 x 4 utx 0 0 problem describes elastic string length four free ends whose middle half displaced triangular shape held there released 0. see figure 31. note c 1 example. x 4 0 1 figure 31 initial displacement fx retrace steps 3.7.32 problem see eigenfunctions satisfy x λx 0 x0 x4 0 therefore eigenvalues λ0 0 λn nπx 4 n 1 2 3 . . . corresponding eigenfunctions x0x 1 2 xnx cos nπx 4 n 1 2 3 . . . respectively. thus according steps outlined chapter general solution problem ux t 1 2a0 b0t n1 cos nπt 4 bn sin nπt 4 cos nπx 4 . 103
initial conditions become ux 0 1 2a0 n1 cos nπx 4 0 0 x 1 x 1 1 x 2 3 x 2 x 3 0 3 x 4 and utx 0 gx 1 2b0 n1 nπ l bn cos nπx 4 0 . therefore usual fourier formulas 1 2a0 1 4 2 4 4 0 fx cos nπx 4 dx 1 2 2 1 x 1 cos nπx 4 dx 1 2 3 2 3 x cos nπx 4 dx 1 2 4 nπ 2 2 cos nπ 2 cos nπ 4 cos 3nπ 4 bn 0 n 0 1 2 . . . . note value 1 2a0 its simply average height along string initial displacement i.e. center mass vertical direction string. furthermore external forces acting string average value cos nπt 4 zero implies since bn 0 could view solution vibration ﬁxed oﬀset center mass. if youre still sure interpret b0 general case try applying conservation momentum initial conditions 3.7.31. 104
problems 1. solve utt 25uxx ux0 t ux1 t 0 ux 0 0 0 x 14 x 14 14 x 34 12 34 x 1 utx 0 0 interpret solution physically. 2. solve utt uxx ux0 t ux2 t 0 ux 0 2x 0 x 12 2 2x 12 x 32 2x 4 32 x 2 utx 0 1 interpret solution physically. 105
3.8 mixed end conditions problem thus far solved two special cases onedimensional wave equation eﬀort develop feel general behavior solutions equation. cases separation variables assumption applied homogeneous parts problem produced inﬁnite number linearly independent solutions determined eigenvalues. eigenvalues however associated eigenfunctions problem dependent i.e. changed problem although neither case eigenvalues negative one two cases zero eigenvalue. furthermore cases general solution formed linear combination solutions reduced ordinary fourier series applied initial conditions. fundamental question still front us many properties remain unchanged study cases. therefore continue investigating third case utt c2uxx u0 t 0 uxl t 0 ux 0 fx utx 0 gx . 3.8.41 in terms standard form 3.5.11 newest case corresponds α1 β2 1 β1 α2 0 . physically equation could model string ﬁxed left end frictionless slip ring located right end figure 32. mathematically may either refer mixed end condition problem say problem dirichlet conditions left neumann conditions right. point problem pause continuing consider qualitative physical mathematical properties might expect ﬁnd solution. will course want apply much possible experience two earlier problems. examples started considering homogeneous parts problem without initial conditions. seems reasonable start utt c2uxx u0 t 0 uxl t 0 3.8.42 given 3.8.42 important question two previous cases linearly independent nontrivial solutions ﬁnd them answer new case obvious priori already solved earlier cases least able skip many intermediate details. example partial diﬀerential equation still changed therefore separation variables form 3.6.14 and does produce exactly two sepa rated ordinary diﬀerential equations. furthermore already separated lefthand 106
figure 32 mixed end condition problem boundary condition ﬁrst example righthand condition second one. thus proceed immediately write separated problems λc2t 0 x λx 0 x0 0 xl 0 . 3.8.43 before equation tt produce two linearly independent solutions value λ. by now long dont apply initial conditions expect behavior. therefore earlier cases fundamental question remains values λ x λx 0 x0 0 xl 0 3.8.44 nontrivial solutions procedure answering question also remains same i.e. considering sequence negative zero positive values λ seeing produce solutions. current example λ negative represented λ ζ2 ζ 0 general solution xx xx c1eζx c2eζx therefore boundary conditions reduce x0 c1 c2 0 xl ζc1eζl ζc2eζl 0 107
shown satisﬁed c1 c2 0. negative values λ still produce trivial solutions therefore growing exponentials tt still mathematically and physically impossible. λ zero general solution ordinary diﬀerential equation still xx d1 d2x and substituted boundary conditions yields x0 d1 0 xl d2 0 . were back problem trivial solutions λ 0 therefore zero eigenvalue here. geometrically conclusion intuitive since problem λ 0 viewed deﬁning straight line passes xaxis x 0 horizontal x l. lastly must consider whether positive values λ generate nontrivial solutions. this seem highly likely since previous cases. following usual convention letting λ ξ2 ξ 0 gives xx a1 sinξx a2 cosξx . boundary conditions case however produce slightly diﬀerent result seen previously try determine a1 a2. speciﬁcally x0 a2 0 xx a1 sinξx xl a1ξ cosξl 0 ξ 0 . here ﬁxed free end examples important point dont choose a1 0 get trivial solution. however order avoid trivial solution change condition selecting ξ sinξl 0 cosξl 0 . new condition course true ξl π 2 3π 2 5π 2 . . . or equivalently ξn 2n 1π 2l n 1 2 3 . . . λn 2n 1π 2l 2 n 1 2 3 . . . . 3.8.45 108
deviation albeit minor earlier cases studied positive eigenvalues pure ﬁxed end free end cases. neither eigenfunctions xnx sinξnx sin 2n 1πx 2l n 1 2 3 . . . 3.8.46 note continued practice dropping arbitrary constant one linearly independent solution exists. also use 2n 1 simply convenient way generating odd integers. could equally correctly written xnx sin nπx 2l n 1 3 5 . . . etc. latter representation however generally used since becomes cum bersome. now before corresponding eigenvalue given 3.8.45 two linearly independent solutions tt represented case tnt cos 2n 1πct 2l bn sin 2n 1πct 2l n 1 2 3 . . . therefore also two linearly independent solutions 3.8.42 given by unx t xnxtnt cos 2n1πct 2l bn sin 2n1πct 2l sin 2n1πx 2l n 1 2 3 . . . . again since identiﬁed linearly independent solutions form general solution usual way ux t a1 cos πct 2l b1 sin πct 2l sin πx 2l a2 cos 3πct 2l b2 sin 3πct 2l sin 3πx 2l a3 cos 5πct 2l b3 sin 5πct 2l sin 5πx 2l n1 cos 2n 1πct 2l bn sin 2n 1πct 2l sin 2n 1πx 2l . 3.8.47 continuing before attempt determine constants ai bi 1 2 3 . . . 109
substituting 3.8.47 initial conditions. yields ux 0 fx n1 sin 2n 1πx 2l 0 x l utx 0 gx n1 2n 1πc 2l bn sin 2n 1πx 2l 0 x l . 3.8.48 again 3.8.48 poses identical question faced earlier examples ﬁnd bn satisfy these unlike earlier problems studied however answer clear. one diﬃculty series 3.8.48 fourier sine series period 4l 2l. look denominators carefully. therefore even fx gx least piecewise smooth 0 x l extend oddly l x 0 dont enough values use fourier sine series formulas directly. were missing values fx l x 2l 2l x l actually since were using sine series values l x 2l really matter. now well one choice still try use odd even fourier series concepts. all general fourier sine series period 2l form n1 bn sin nπx 2l b1 sin πx 2l b2 sin 2πx 2l b3 sin 3πx 2l b4 sin 4πx 2l b5 sin 5πx 2l . then seem need order satisfy ﬁrst 3.8.48 ﬁgure extend fx interval l x 2l way b2 b4 0 a1 b1 a2 b3 a3 b5 . . . . could actually this but things turn out approach simply delays inevitable better way ﬁnd coeﬃcients 3.8.48 return fundamental principles ask important earlier examples initial conditions reduce elementary fourier series straightforward answer would seem could immediately compute coeﬃcients. thats superﬁcial. could compute coeﬃcients answer orthogonality sines cosines i.e. integrals like 2l 2l sin nπx 2l sin mπx 2l dx 0 n allowed use derive appropriate formulas. therefore try various perhaps artiﬁcial extensions 3.8.48 ask instead perhaps functions series 110
i.e. eigenfunctions also orthogonal interval 0 x l. means considering integral l 0 sin 2n 1πx 2l sin 2m 1πx 2l dx . using standard trigonometric identities l 0 sin 2n 1πx 2l sin 2m 1πx 2l dx 1 2 l 0 cos 2n 1 2m 1πx 2l dx l 0 cos 2n 1 2m 1πx 2l dx 1 2 l 0 cos n mπx l dx l 0 cos m n 1πx l dx 1 2 l nmπ sin nmπx l l 0 l mn1π sin mn1πx l l 0 n 1 2 xl 0 l mn1π sin mn1πx l l 0 n 0 n 1 2l n since n integers note must assume m n 1 order compute second antiderivative did. thus eigenfunctions orthogonal. later shall show result predictable based solely type diﬀerential equation boundary conditions satisﬁed xnx view fortuitous occurrence. know eigenfunctions orthogonal able proceed derivation original fourier series coeﬃcient formulas. is multiply sides 3.8.48 sin 2m1πx 2l denotes ﬁxed integer integrate 0 l. this taking sin 2m1πx 2l inside summation sign series interchanging order integration summation would l 0 fx sin 2m 1πx 2l dx n1 l 0 sin 2n 1πx 2l sin 2m 1πx 2l dx 111
but course summation notation indicates term right evaluated every integer value n starting 1 summed. yet orthogonality evaluation result zero every value n except one value n. thus sum reduces single term speciﬁcally term obtained n equation becomes l 0 fx sin 2m 1πx 2l dx l 2 or 2 l l 0 fx sin 2m 1πx 2l dx using computed using formula expect solution 3.8.47 satisfy ﬁrst initial condition 3.8.48. similar reasoning second condition 3.8.48 leads 2 l l 0 fx sin 2n 1πx 2l dx bn 4 2n 1πc l 0 gx sin 2n 1πx 2l dx 3.8.49 solved problem. actually last statement bit premature. using 3.8.49 compute coeﬃcients really socalled formal solution. solution computed formally based assumptions certain mathematical steps really valid. sure really solution two minor points addressed. these minor points worried mathematiciann many years. first all above blandly interchanged summation integration inﬁnite series. yet many examples exist show valid arbitrary series. fact generally speaking series needs uniformly convergent interchange permitted although work series converge uniformly. speciﬁcally rather elegant theory called generalized functions shown that fourier fourier like series a term leave purposely vague interchange valid. the proof result far beyond scope course. so rest course need worry concern. second problem deals whether actually represent arbitrary fx gx i.e. arbitrary initial conditions using functions sin 2n1πx 2l . words guarantees either equation 3.8.48 really represents valid equality answer mathematically obvious basic problem that unless dealing standard fourier series have point theory assure us fx gx acually expanded series. but noted above try look 3.8.48 elementary fourier series terms sines πx 2l odd numbered terms seems though half functions need. 112
really enough not course conclusions might draw including 3.8.49 would meaningless. perhaps analogy may help clarify concern. go back look 3.8.48 again. trying may viewed trying construct build up fx gx linear combinations using functions sin 2m1πx 2l building blocks components. isnt exactly one tries linear algebra write arbitrary vector v terms set vectors i.e. try write v a1v1 a2v2 anvn recall linear algebra conditions representation may possible. example consider ordinary threedimensional space vectors v1 1 1 0 v2 1 1 1 . easily shown orthogonal usual dot product r3. suppose try write vector 312 terms them. would 3 1 2 a11 1 0 a21 1 1. taking dot products sides using orthogonality leads a1 3 1 2 1 1 0 1 1 0 1 1 0 2 a2 1 1 1 1 1 1 1 1 1 1 1 1 4 3 then analysis correct have 3 1 2 2 1 1 0 4 3 1 1 1 10 3 2 3 4 3 happened computed coeﬃcients correctly didnt we. answer simple enough. vectors orthogonal used try write vector r3 r3 vector space dimension three. thus basis r3 requires three vectors two. in linear algebra terminology basis complete. thus original equality invalid even though able compute coeﬃcients it. but course r3 obvious fuss well go back look 3.8.48 says fx n1 sin 2n 1πx 2l . trying write function vector fx terms orthogonal functions vectors sin 2n1πx l . apparently analogy valid dimension vector space inﬁnite hence its longer obvious enough functions 113
vectors basis it fact weve argued looks like left half already. well turns enough basis complete. reason in terestingly enough lies type diﬀerential equation boundary conditions generated eigenfunctions. show that long include eigenfunctions appropriate right kind diﬀerential equation boundary conditions assured complete basis. we shall comment discuss sturm9liouville10 problems. point simply proceed assumption basis complete. one could argue really right. come immediate halt rigorously proved wild assertions. we commonly applied mathematics one ﬁrst attempt justify rigorously every intermediate step. instead one works problems assumption operations appear reasonable later justiﬁed. done see whether best case ones method work. for one cant ﬁnd least formal solution best case worry whether steps valid shall close case example. consider utt 4 uxx 0 x 3 0 u0 t 0 ux3 t 0 ux 0 0 utx 0 1 note c2 4. general solution problem according earlier development ux t n1 cos 2n 1πt 3 bn sin 2n 1πt 3 sin 2n 1πx 6 initial condition equations reduce ux 0 n1 sin 2n 1πx 6 0 utx 0 n1 2n 1π 3 bn sin 2n 1πx 6 1 . therefore 0 bn 3 2n 1π 2 3 3 0 sin 2n 1πx 6 dx 12 2n 12π2 9jacques charles franc .ois sturm see 10joseph liouville see 114
ux t n1 12 2n 12π2 sin 2n 1πt 3 sin 2n 1πx 6 . summary then example also found separation variables led so lution although eigenvalues eigenfunctions diﬀered earlier examples. furthermore initial condition reduced series which elementary fourier series closely akin one eigenfunctions involved orthogonal. fact seems may problems changing basis depending particular problem. perhaps loose way speaking separation variables may nothing picking orthogonal basis functions natural particular boundary value problem hand. 115
problems 1. solve utt uxx u0 t 0 ux2 t 0 ux 0 x 0 x 1 1 1 x 2 utx 0 0 2. solve utt 4uxx ux0 t 0 u1 t 0 ux 0 1 0 x 12 2 2x 12 x 1 utx 0 0 3. solve utt 9uxx ux0 t u2 t 0 ux 0 0 utx 0 2 x 0 x 2 4. show normal fourier series reduces fx n1 b2n1 sin 2n1πx 2l b1 sin πx 2l b3 sin 3πx 2l b5 sin 5πx 2l provided a. fx odd b. fx periodic period 4l c. fx l fl x 0 x l 116
3.9 generalizations method separation variables recall goal chapter develop reasonable feel basic principles rules game apply partial diﬀerential equations studying special cases onedimensional wave equation. point as reader hopefully beginning suspect may reached point diminishing returns terms additional general information extract study special cases. fact already appear reinvented wheel number times. speciﬁcally i cases characterized usual physics physically unrealistic negative eigenvalues λ 0 occurred although prove case completely solving appropriate ordinary diﬀer ential equation boundary conditions. ii every case inﬁnite number eigenvalues as sociated eigenfunctions. iii every case eigenfunctions orthogonal interval interest i.e. satisﬁed l 0 xnxxmxdx 0 whenever xnx xmx eigenfunctions two diﬀerent eigen values although mixed end conditions case prove fact brute force integration iv every example worked eigenfunctions appeared complete sense that least numerically formal solutions seemed able satisfy whatever initial conditions although mathematically prove mixed end conditions case. least aspects problems seeming fairly predictable stop solving special cases 3.5.11 time. instead well turn investigating degree patterns fact reﬂect general theory underlies cases i.e. aspects behavior able predict aspects determine individually case. for example cases studied far indicate expect general theory predict speciﬁc eigenvalues eigenfunctions advance since changed diﬀerent problem. shouldnt completely surprising either. situation occurs ordinary diﬀerential equations know ay by cy 0 least solutions form erx determine actual values r problembyproblem basis. start observation cases exercises weve already studied suggest at least uniform media separation variables works whenever basic 117
partial diﬀerential equation boundary conditions homogeneous. substantiate apparent rule return general form homogeneous onedimensional wave equation without initial conditions ρxutt x τxu x qxu 0 x l 0 α1u0 t β1ux0 t 0 α2ul t β2uxl t 0 . 3.9.50 this form general cases weve studied still linear homogeneous. added qxu term partial diﬀerential equation addressed earlier derivation wave equation could physically represent series springs distributed length original string. coeﬃcients course connote variable density andor tension. fundamental consideration here special cases identify much possible linearly independent solutions problem. observe partial diﬀerential equation 3.9.50 signiﬁcantly involved ones solved thus far still possesses natural splitting observed simplest case utt c2uxx lefthand side diﬀerentiates time behavior righthand side diﬀerentiates spatial behavior. so along look linearly independent solutions form ux t xxtt . standard diﬀerentiation rules applied product form substituted diﬀer ential equation yield ρxxt dx τxdx dx qxxt or upon simpliﬁcation τxx qxx ρxx λ again may introduce separation constant since lefthand side clearly independent x righthand side independent t. note one slight diﬀerence formulation one used examples. speciﬁcally c2 term appears constant coeﬃcient case absorbed λ here. brief calculation shows always separate boundary conditions 3.9.50 manner examples e.g. α1u0 t β1ux0 t α1x0tt β1x0tt tt α1x0 β1x0 0 α1x0 β1x0 0 118
similarly righthand end. combining results shows original problem separates λt 0 τxx λρx qxx 0 α1x0 β1x0 0 α2xl β2xl 0 . 3.9.51 here true special cases problem xx secondorder ordinary diﬀerential equation two auxiliary conditions involving unspeciﬁed constant λ. normally therefore expect unique and hence trivial solution xx. problem thus before determine values λ nontrivial solutions xx also exist. of course tt also solution ordinary diﬀerential equation. equation still constant coeﬃcient without boundary initial conditions immediately write general solution know value values λ nontrivial solutions xx found. reasons hopefully become clearer later choose view problem xx special case another general diﬀerential equation pxyx λwx qxyx 0 0 x l α1y0 β1y0 0 α2yl β2yl 0 . for wave equation appropriate correspondence px τx wx ρx yx xx. physical considerations related interpretations motivate ad ditional restrictions coeﬃcients problem. example in terpretation terms onedimensional wave equation expect px wx normally positive qx nonnegative. diﬀerential equation attendant boundary conditions certain restrictions constitutes known sturm liouville problem. since its linear homogeneous know trivial solution yx 0 always satisﬁes problem nontrivial solutions if any determined arbi trary multiplicative constants. so again focus nontrivial solutions arise general properties proceeding further must emphasize nothing magical sturmliouville problem. its really template match candidate problems. example every special case solved thus far chapter involved ordinary diﬀerential equation x λx 0 fact special case sturmliouville diﬀerential equation corresponding px 1 wx 1 qx 0 . importance sturmliouville theory identify particular problem solving choice px qx wx plus αi βi reduces sturmliouville form speciﬁc problem shall see number 119
conclusions immediately draw. analogously ordinary diﬀerential equations recognize y 3y 2y 0 special case constant coeﬃcient equation ay by cy 0 immediately conclude two linearly independent solutions exist erx solution values r satisfy r2 3r 2 0 . conclusions save us work since future know completely behavior expect. is unfortunately one result study sturmliouville pro vide. simply general formula expresses solution sturmliouville ordinary diﬀerential equation even explicit formulas px qx wx given. your earlier study ordinary diﬀerential equations already made fairly clear. really quite lucky so far encountered constant coeﬃcient equations one instances closed form solution possible. nevertheless truly amazing much information general character solutions problem able extract without formula solutions without knowing speciﬁc coeﬃcient functions speciﬁc values αi βi without knowing actual formula solution. shall develop results next sec tion. point simply recognize study sturmliouville problem important us special case arise every time try separation variables homogeneous wave equation 3.9.50 heat equation laplaces equation which mention later. therefore results derive solutions sturmliouville problem general hold prospect greatly reducing workload apply method separation variables speciﬁc cases. 3.10 sturmliouville theory deﬁnition regular sturmliouville problem consists describing existence behavior nontrivial solutions pxyx λwx qxyx 0 0 x l α1y0 β1y0 0 α2yl β2yl 0 3.10.52 where px wx qx continuous 0 x l px 0 0 x l. wx 0 0 x l. α2 β2 0 1 2 . 120
development wave equation earlier discussions already indicated diﬀerential equations boundary conditions general type interest. however words origin restrictions coeﬃcients order. already noted actual wave equation px wx qx represent physical quantities tension density etc. since discontinuous changes tension density etc. inside string likely continuity restrictions certainly seem physically reasonable. moreover mathematically sturmliouville diﬀerential equation written pxyx pxyx λwx qxyx 0 assumptions continuity seen important existence wellbehaved solutions. however continuity coeﬃcients alone ensure wellbehaved solutions. additional requirement px 0 0 x l precludes existence singular points interval avoid nasty mathematical problems. furthermore physically wave equation px represents tension. tension vanish somewhere inside string i. e. px 0 point 0 x l physical eﬀect would string cut two point. lastly negative tension point would correspond inward push string collapsing it destroying originally assumed model. therefore physical grounds restriction px 0 inside string seems eminently reasonable. for strings px 0 either end seems equally disturbing although media problems situation fact occur. situation p0 0 well several cases lead called singular sturmliouville problems. many qualitative properties solutions singular case however similar shall derive solutions regular sturmliouville problem. singular sturmliouville problems also require modiﬁcations types boundary conditions acceptable singular point. example p0 0 mathematically physically boundary conditions α1y0 β1y0 0 longer really appropriate. also interpret restrictions wx physically. example wave equa tion wx represents density. thus requiring wx 0 inside string clearly physi cally realistic. in idealized situations models however conditions w0 0 wl 0 could possible. example imagine string gets progressively thin ner near end ﬁnally vanishes precisely end ﬁrst major result regular sturmliouville problem prove earlier conjecture λ 0 almost never arises. order this however need restrictions algebraic signs beyond prescribed above. speciﬁcally need qx 0 0 x l αiβi 0 1 2 . 3.10.53 as already noted that wave equation qx term interpreted spring distributed length string. therefore order satisfy action 121
normal springs qx must also positive interval or equal zero mechanism. restriction signs αi βi agrees signs arise wave equation normal physics. assuming necessary restrictions satisﬁed let yx denote nontrivial solution 3.10.52 without regard might constructed. multiply sides diﬀerential equation yx integrate 0 l. noting right hand side remains identically zero yields order pxyxyx λwx qxy2x 0 l 0 pxyxyxdx l 0 λwx qxy2xdx 0 . simplify ﬁrst integral integrating pxyxyx term parts moving terms containing λ across equal sign. performing calculations dropping explicit reference x inside integrals simpliﬁes last equation to λ l 0 wy2dx l 0 py2dx l 0 qy2dx pxyxyx l 0 3.10.54 analyze algebraic signs terms equation. since px qx assumed positive clearly ﬁrst two integrals right nonnegative. third term written fully plylyl p0y0y0 . however according ﬁrst boundary condition 3.10.52 α1y0 β1y0 0 α1y0 β1y0 . α1β1 0 see 3.10.53 clearly α1 β1 cannot diﬀerent algebraic signs. by convention zero may either sign. but case satisfy equality y0 y0 cannot diﬀerent signs either. furthermore assumption px positive interval including end points. therefore p0y0y0 0 . similarly show boundary condition x l implies plylyl 0 pxyxyx l 0 0 . thus every term right 3.10.54 nonnegative. but wx 0 yx 0 must also l 0 wy2dx 0 122
therefore shown that terms contained 3.10.54 λ l 0 wy2dx 0 l 0 py2dx 0 l 0 qy2dx 0 pxyxyx l 0 0 so clearly λ cannot negative i.e. λ 0. proves ﬁrst conjecture i.e. occurrence negative values λ impossible wave equation normal physics. weve pointed before seemed physically obvious since otherwise would exponentially growing solutions tt. reassuring terms validity mathematical model mathematical formulation via sturmliouville theory reaches conclusion. however were done yet theres still information extract last equation. suppose λ 0. lefthand side zero. every term right nonnegative possible every one terms zero i.e. l 0 py2dx l 0 qy2dx pxyxyx l 0 0 . px 0. thus l 0 py2dx 0 y2 0 yx must constant which cannot zero yx nontrivial solution. note immediately guarantees third term pyyl 0 identically zero well . however nonzero constant solution entire problem must satisfy sturmliouville boundary conditions. substituting constant conditions immediately im plies α1 α2 0. therefore original boundary conditions must been ux0 t 0 uxl t 0 . lastly yx nonzero constant λ zero basic sturmliouville diﬀerential equation 3.10.52 reduces to qxyx 0 qx 0 . result veriﬁes conjecture appearance zero eigenvalue unlikely. also allows us decide inspection whether λ 0 eigenvalue sturm liouville problem. furthermore test zero eigenvalue interpreted either mathematically physically. example uniform string test either stated mathematical condition zero eigenvalue exist qx 0 α1 α2 0 interpreted physical statement homogeneous wave equation homogeneous boundary conditions non trivial rigid body motion solutions ends string free internal springs. 123
again see mathematical restrictions make physical sense. may seem quite amazing degree mathematics physics reinforce other. also quite reassuring feel conﬁdent mathematics physics disjoint really working hand hand models appear captured essence physics involved. mathematically sum results far theorem 2.1 regular sturmliouville problem pxyx λwx qxyx 0 0 x l α1y0 β1y0 0 α2yl β2yl 0 where px wx qx continuous 0 x l px 0 qx 0 0 x l αiβi 0 1 2 wx 0 0 x l nontrivial solutions λ 0. furthermore solution λ 0 occurs a qx 0 b yx const 0 satisﬁes boundary conditions. theorem is course somewhat negative tells us certain choices λ produce eigenvalues but unfortunately ones will. fact even assure us eigenvalues. although al ways found examples. or so many. we always found inﬁnite number. again surprise us for already discussed val ues actual eigenvalues eigenfunctions depend totally speciﬁc problem one solving must determined solving problem detail. short cuts however sturmliouville theory still yield insights number eigenvalues qualitative properties associated eigenfunctions. following theorem whose proof far beyond scope course one general result sturmliouville provide eigenvalues eigenfunctions general case theorem 2.2 given regular sturmliouville problem exist inﬁnite number eigenvalues λn eigenfunctions ynx problem. eigenvalues λn as n ynx one zero interval 0 x l predecessor. proven here theorem easy believe based examples. ﬁrst problem 3.6.12 eigenvalues λn nπl2 corresponding eigenfunctions 124
which happened call xnx there sinnπxl n 1 zeros interval 0 x l. second problem 3.7.31 positive eigenvalues eigenfunctions equal cosnπxl n zeros interval 0 x l. theorem 2.2 guarantees solutions sturmliouville problem act general way. next turn conjecture orthogonality observed behavior earlier examples appeared separation variables producing orthogonal set functions basis natural given problem. show that fact eﬀectively case. suppose λn λm denote two diﬀerent eigenvalues pxyx λwx qxyx 0 0 x l α1y0 β1y0 0 α2yl β2yl 0 where px wx qx continuous 0 x l px 0 wx 0 0 x l. note using sturmliouville restrictions. deﬁnition associated eigenvalue nontrivial solution shall denote ynx ymx respectively. thus have since solution pxy nx λnwx qxynx 0 pxy mx λmwx qxymx 0 also α1yn0 β1y n0 0 α2ynl β2y nl 0 α1ym0 β1y m0 0 α2yml β2y ml 0 3.10.55 multiply upper equation 3.10.55 ymx lower ynx subtract pxy nxymx pxy mxynx λn λmwxynxymx 0 . integrating 0 l dropping explicit reference dependence x yields l 0 py nym py myn dx λn λm l 0 wynymdx 0 3.10.56 note following identity veriﬁed dxpy nym y myn py ny py nym py my n py myn py nym py myn 125
since ﬁrst third terms cancel. thus ﬁrst integral 3.10.56 exact antiderivative so l 0 py nym py myn dx py nym yny m l 0 ply nlyml ynly ml p0y n0ym0 y n0y m0 . 3.10.57 but since ynx ymx solutions boundary conditions well diﬀerential equation must also α2ynl β2y nl 0 α2yml β2y ml 0 . wait simply two homogeneous algebraic equations two unknowns α2 β2. furthermore assumed α2 β2 zero otherwise wouldnt boundary condition original problem x l. therefore cramers rule determinant coeﬃcients must zero i.e. ynl y nl yml y ml ynly ml y nlyml 0 . in ordinary diﬀerential equation determinant called wronskian. but except minus sign determinant exactly ﬁrst term brackets right hand side 3.10.57. therefore since minus zero still zero y nym yny 0 x l. similarly boundary condition x 0 implies y nym yny 0 x 0. thus l 0 ympxy nx ynpxy mx dx py nym yny m l 0 0 i.e. ﬁrst integral 3.10.56 must vanish. hence λn λm l 0 wynymdx 0. wait again said λn λm diﬀerent. thus λn λm 0 therefore l 0 wxynxymxdx 0 . 3.10.58 126
two solutions ynx ymx orthogonal they wx term integral. relationship expressed 3.10.58 is fact type orthogonality. its called orthogonality respect weighting function case wx. show shortly orthogonality provides beneﬁts terms able compute coeﬃcients observed simpler fourier series orthogonality. sturmliouville problems wx 1 wx term 3.10.58 fact absolutely necessary since problems other integral l 0 ynxymxdx 3.10.59 provide correct relationships. although shall develop aspect theory point could show quite higher mathematical level essentially deﬁning dot product functions f g l 0 wxfxgxdx . 3.10.60 here mathematically restriction wx 0 0 x l becomes pivotal know dot product one vital property must hold f f 0 unless f 0. 3.10.60 really sense analogous dot product then deﬁnition f f l 0 wxfx2dx see wx 0 0 x l necessary guarantee integral positive unless fx identically zero. summarize result theorem 2.3 ynx ymx solutions regular sturmliouville problem 3.10.52 corresponding two diﬀerent values λ l 0 wxynxymxdx 0. note interestingly enough result need assume either qx 0 αiβi 0 l 2. thus orthogonality holds even face negative eigenvalues. also note observed earlier speciﬁc examples onedimensional wave equation dealt diﬀerential equation x λx 0 which terms sturmliouville form equivalent yx xx px wx 1 qx 0 . 127
explains fourier orthogonality integral always worked examples. ac cording theorem however 3.10.59 apparently proper integral cases wx constant. one ﬁnal aspect sturmliouville problem wish address here. last item not means exhaust study quite fascinating equation. however shall covered high points points primary practical importance solving partial diﬀerential equations separation variableseigenvalue eigenfunction approach. last item addresses question completeness. seen example cases often wish able write reasonable function e.g. initial conditions inﬁnite series using eigenfunctions basis. example occurs looking linearly independent solutions ρxutt x τxu x qxu 0 x l 0 α1u0 t β1ux0 t 0 α2ul t β2uxl t 0 ux 0 fx utx 0 gx 3.10.61 assuming ux t xxtt leads to seen regular sturmliouville problem xx orthogonal eigenfunctions xnx n 1 2 . . . l 0 wxxnx xmxdx 0 n tt must satisfy equation n λntn 0 . assuming positive values λ occur tnt cos λnt bn sin λnt 3.10.62 therefore general solution 3.10.61 formally written ux t n1 cos λnt bn sin λnt xnx . 3.10.63 trying ﬁt initial conditions 3.10.61 solution leads to ux 0 n1 anxnx fx utx 0 n1 λnbnxnx gx . 3.10.64 128
then since xnx orthogonal could usual way multiply sides ﬁrst equation 3.10.64 wxxmx denotes ﬁxed integer integrate yield l 0 wxfxxmxdx n1 l 0 wxxnxxmxdx 0 mn so l 0 wxfxxmxdx l 0 wx xmx2 dx l 0 wxfxxnxdx l 0 wx xnx2 dx . 3.10.65 formally compute an a similar set steps would produce formulas bn. but mathematically question remains whether ensure 3.10.64 really valid set equalities. seen example simply using orthogonality compute coeﬃcients constitute validation original equality. commented inﬁnite number functions may guarantee enough represent arbitrary ux t. also seen either initial conditions 3.10.61 may discontinuous least terms periodic extensions. righthand sides 3.10.64 may discontinuous. functions series lefthand side continuous. sense series converge fx gx guarantee always ﬁnd formal solution like 3.10.62 computing coeﬃcients using 3.10.65 valid answer yes provided xnx include linearly independent eigenfunc tions associated sturmliouville problem comes separation variables. shall prove statement such proof far outside scope course present appropriate theorem. theorem 2.4 fx least piecewise smooth 0 x l xnx n 1 2 . . . full set eigenfunctions regular sturmliouville problem 3.10.52 series fx n1 anxnx determined l 0 wxfxxnxdx l 0 wx xnx2 dx converges pointwise fx point f continuous mean average value f discontinuous. since expect encounter functions level least piecewise smooth shall discuss occurs functions less piecewise smooth. 129
theorem concludes discussion sturmliouville problem. closing would emphasize exhausted results known fascinating problem. however covered high points relevant problems shall cover. 130
problems 1. following problems determine given equation sturmliouville form and so identify values appropriate function px qx wx values αi βi a. x 1y λx 1y y 0 y1 0 y2 0 b. x2 1u 3λu 0 u0 0 u12 0 c. y λxy 0 y0 0 y3 2y3 0 d. y xy λy 0 y0 0 y1 0 2. following similar steps used class show eigenfunctions singular sturmliouville problem pxy λwxy qxy 0 x b ya ya ﬁnite α2yb β2yb 0 px wx qx continuous px 0 x b pa 0 wx 0 x b qx 0 x b corresponding diﬀerent eigenvalues orthogonal respect weighting func tion wx. 131
3.11 frequency domain interpretation wave equation analysis sturmliouville problem essence completely described mechanics solving onedimensional wave equation. now hopefully beginning feel least somewhat comfortable carrying steps necessary gener ate series solution onedimensional constantcoeﬃcient wave equation. however mathematics stopping grasp merely mechanics steps generate solutions rarely advisable. dont mean mechanics unimportant. quite contrary mastery computations problem misses fundamental purpose mathematics provide insights beyond numbers may emerge various solutions. section following one therefore seek go beyond mechanics separation variables onedimensional wave equation bring light more fundamental insights nature waves. shall see insights actually embodied onedimensional string model obscured rather imposing notation inﬁnite series produced separation variables. start were back beginning special case ﬁrst wave equation solved string ﬁxed ends initial displacement initial velocity. know solution problem ux t n1 cos nπct l sin nπx l where 2 l l 0 ux 0 sin nπx l dx eigenvalues eigenfunctions λn nπ l 2 xnx sin nπx l n 1 2 3 . . . respectively. observe however simple interchange order functions within terms within series appear allows us rewrite solution ux t n1 sin nπx l cos nπct l a1 sin πx l cos πct l a2 sin 2πx l cos 2πct l a3 sin 3πx l cos 3πct l a1x cos πct l a2x cos 2πct l a3x cos 3πct l 3.11.66 anx sin nπx l . 132
what any new information viewpoint alternative formulation provide well rewritten form clearly still represents sum superposition diﬀerent terms. however recall ω cosωt represents single temporal vibration i.e. pure tone one would hear tuning fork electronic tuner. therefore may view separate term expression representing single individual tone frequency fn c 2π λn nc 2l spatiallyvarying amplitude deﬁned anx . furthermore since λn nλ1 frequencies may viewed multiples square root lowest eigenvalue. moreover amplitude functions really multiples eigenfunctions. one vibration corresponding n 2 shown figure 33. 1 0 0.2 0.8 0.6 0 0.5 0.4 1 x 1 0.5 figure 33 a2x cos 2πct l individual frequencies contribute solution 3.11.66 f1 c 2l f2 2c 2l f3 3c 2l . . . must represent natural frequencies string arise absence external stimulus provided initial conditions. similar natural frequency ω ksm already seen arise solution mechanical system md2u dt2 ksu 0 . analogous natural frequencies also found electrical circuits. one signiﬁcant diﬀerence frequency structure model mechanical electrical models commonly studied ordinary diﬀerential equations. mechanical system electrical circuit single natural frequency. vibrating string contrast inﬁnite number deﬁnite structure. theyre multiples f1 socalled fundamental frequency f1 c 2l 1 2l τ ρ 133
τ denotes internal tension string ρ density per unit length. moreover last equation clearly implies exactly three ways raise fundamental frequency guitar string 1 shorten i.e. decrease l 2 tighten i.e. increase τ 3 make thinner i.e. decrease ρ. never looked closely stringed instrument before would good time strings give bass notes happens sound press one strings somewhere along neck increase tension tightening one pegs dont observations agree precisely analysis higher frequencies f2 f3 . . . called overtones harmonics. presence principal contributor customary rich full sound quality stringed musical instrument compared sound tuning fork example. degree particular harmonic present particular solution depends course value ai turn depends directly initial conditions. explains slight variation perceived sound guitar string plucked center opposed right next bridge. now lets return amplitudes associated vibrations. shapes deﬁned anx course scaled multiples eigenfunctions called modes vibration string. ﬁrst plotted below. 0 0.2 0.4 0.6 0.8 1 1 0.5 0 0.5 1 a1x x 0 0.2 0.4 0.6 0.8 1 1 0.5 0 0.5 1 a2x x 0 0.2 0.4 0.6 0.8 1 1 0.5 0 0.5 1 a3x x 0 0.2 0.4 0.6 0.8 1 1 0.5 0 0.5 1 a4x x figure 34 various modes vibration 134
keep mind that addition diﬀerent shapes modes also vibrate diﬀerent rates higher mode faster rate vibration. physicists frequently refer vibrating modes standing waves since basic shape never changes except continually changing scale introduced cos nπct l term. the actual maximum displacements noted determined initial conditions eﬀect values ai. concept modes occurs theory musical instruments also communications. one simplest radio antennas basically vertical wire. although shall prove here response antenna radio signal satisﬁes wave equation vibrating string. therefore electrical analog ﬁxed ends antennas natural frequency determined lowest eigenvalue shorter antenna higher fundamental frequency. fact helps explain broadcast stations require general long antennas mobile car radios cellular phones use short ones car radios operating much higher frequencies. furthermore ﬁxed end condition antenna fundamental mode sinπxl. exactly halfcycle sine wave explains antennas commonly called halfwave antennas communications engineers. there also antenna design leads mixed ﬁxedfree boundary conditions. hopefully obvious reasons antennas referred quarterwave antennas. would close part discussion one last observation. discussing solution properties terms happens particular individual frequencies. words fourier analysis terminology considering solution frequency domain. analysis entirely appropriate natural musical instruments since primary sensor evaluating human ear and alluded before nature human ear try decompose sounds component frequencies match library stored human brain. would also recall frequency domain one two domains fourier analysis tells us may view signals. other course time domain. therefore shall next turn attention may time domain interpretations solutions wave equation. 135
problem 1. find three lowest natural frequencies sketch associated modes equation utt c2uxx u0 t uxl t 0 plot frequency domain natural frequencies string. 136
3.12 dalembert solution wave equation previous section developed frequency domain analysis solution onedimensional wave equation ﬁxedend conditions observed solution could interpreted superposition standing waves string unique shape frequency. furthermore frequencies waves constant multiples square roots eigenvalues shape multiple corresponding eigenfunction. interpretation solution valuable since explain many simple observations one make stringed musical instruments mention transmission lines longwire high frequency radio antennas. section shall show solution simultaneous dual timedomain interpretation interpretation explains observed wave phenomena would miss considering frequency domain alone. analysis starts solution last section solution ﬁxed end zero initial velocity initially displaced string ux t n1 cos nπct l sin nπx l 3.12.67 where 2 l l 0 ux 0 sin nπx l dx . using standard trigonometric identities reduces ux t n1 2 sin nπ l x ct sin nπ l x ct 1 2 n1 sin nπ l x ct 1 2 n1 sin nπ l x ct . observe ﬁrst two series actually function single com bined argument x ct whereas second function combined argument x ct. therefore alternatively represent solution 3.12.67 ux t fx ct fx ct 3.12.68 fx 1 2 n1 sin nπx l 1 2ux 0 . truly intriguing inﬁnite series solution fact equal sum exactly two functions appears halfsize copy initial displacement. actually last statement quite accurate. precise close examination series shows fx odd periodic extension period 2l half size copy initial condition. always case exactly interpret x ct x ct dependency 137
shown 3.12.68 actually special case general result. this shouldnt necessarily surprising. all 3.12.68 represents solution one special case ends ﬁxed initial velocity zero. general result applies at least initially wave equation unbounded in words one without boundaries region initial conditions i.e. solution utt c2uxx x ux 0 fx utx 0 gx 3.12.69 states solution problem always written form ux t fx ct gx ct 3.12.70 suitable and yet determined functions f g . form com monly known dalembert solution11. satisﬁes partial diﬀerential equation shown straightforward substitution. key step showing recogni tion function fx ct depends single combined value x ct. thus providing f suitably diﬀerentiable basic rules partial diﬀerentiation yield x fx ct dx ct fx ct xx ct f x ct f denotes ordinary derivative function sense cosine derivative function sine. conversely take partial derivative respect t obtain tfx ct cf x ct . second partial derivatives computed similarly. so easily show 2 t2 fx ct c2f x ct c2 2 x2 fx ct or words fx ct always solution partial diﬀerential equation 3.12.69. virtually identical set calculations veriﬁes holds true func tion single combined variable x ct. then since basic partial diﬀerential equation linear homogenous principal superposition homogeneous solutions implies combination 3.12.70 also solves equation. quite obvious f g dalembert solution always determined form also satisﬁes initial conditions prob lem regardless fx gx may be. on hand initial conditions really represent two equations solved. therefore since two unknown functions f g available use shouldnt seem surprising either. 11jean le rond dalembert see 138
shall return momentarily look relationship speciﬁc initial conditions functions f g dalembert solution. ﬁrst well investigate extremely interesting physical interpretation solution. recall that general expression fx a simply represents basic shape fx shifted right amount a. therefore plotting fx ct several diﬀerent values t easily see figure 35 fxct corresponds basic shape wave fx moving right velocity c. 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 0 0.1 0.2 ct0 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 0 0.1 0.2 ct1 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 0 0.1 0.2 0.3 ct2 figure 35 moving function similarly gx ct shown imply leftward moving wave. therefore looking back dalembert form 3.12.70 incorporating last observations see essence of fundamental insight oﬀered dalembert solution solution onedimensional wave equation visualized sum two traveling waves one f moving right velocity c g moving left speed. furthermore case zero initial velocity 3.12.68 seems imply fx gx. think words carefully. described solution evolves time almost manner considering sequence successive snapshots words dalembert solution really time domain interpretation solution wave equation. and earlier frequency domain interpretation instances clearly proper interpretation use. example frequency domain interpretation useful explaining way musical instrument sounds time domain view best explains sound propagates instrument listeners ear. shall actually prove proper choice f g dalembert solu tion 3.12.70 satisfy arbitrary initial conditions general wave equation 3.12.69. 139
shall show always reduces earlier solution 3.12.68 initial velocity zero i.e. gx 0. noted above general dalembert form always satisﬁes partial diﬀerential equa tion 3.12.69. substituting form initial position velocity with gx 0 equations thus leads ux 0 fx gx fx utx 0 cf x cgx 0 . 3.12.71 dividing second equation c integrating yields fx gx k 3.12.72 k arbitrary constant. ﬁrst 3.12.71 3.12.72 system two equations two unknowns solving system produces fx 1 2fx k gx 1 2fx k 3.12.73 fx ct 1 2fx ct k gx ct 1 2fx ct k . finally substituting last two expressions fxct gxct dalembert solution 3.12.70 yields ux t fx ct gx ct 1 2 fx ct fx ct . 3.12.74 observe arbitrary constant k canceled. value totally irrelevant may therefore consider zero ignore discussions dalembert solution. k 0 according 3.12.73 fx gx. thus least case zero initial velocity wave equation unbounded region initial representation 3.12.68 3.12.74 identical. as alluded above detailed calculations show general dalembert solution 3.12.70 satisfy 3.12.69 even gx 0 case g longer identical f . graphical construction dalembert solution inﬁnite domain par ticular zero initial velocity case consists fairly straightforward sequence steps 1. sketch initial displacement fx. 2. sketch function fx 1 2fx. 3. determine time solution ux t determined. 4. shift curves fx right left respectively amount ct. 140
5. add two shifted curves give ux t. figure 36 shows steps process applied 0.5 1.5 solution problem utt c2uxx x ux 0 1 x 1 x 1 0 otherwise utx 0 0 . step 1 1 1 1 x fx step 2 1 1 12 x fx step 3 ct12 step 4 12 x fx12 12 x fx12 step 5 ct12 12 x step 6 12 x fx32 12 x fx32 step 7 ct32 12 x figure 36 constructing dalembert solution unbounded region wait initial separation variables solution problem boundary conditions reconcile result dalembert solution region boundaries next section addresses intriguing question. 3.13 eﬀect boundary conditions noted derivation above general dalembert solution 3.12.70 one fundamental limitation describes properties solution one dimensional waveequation unbounded region. yet commented earlier separation variables solution reﬂected real strings ﬁnite length. happens dalembert solution boundary conditions introduced answer shall show that one new wrinkle solution remains valid. complete analysis however general boundary conditions beyond scope study shall instead look special cases starting dirichlet problem 141
already solved separation variables utt c2uxx u0 t 0 ul t 0 ux 0 fx utx 0 0 3.13.75 dalembert solution form 3.12.70 clearly still satisﬁes partial diﬀerential equation functions f g . however obvious conditions guarantee also satisfy boundary conditions. furthermore since initial conditions now strictly speaking valid 0 x l also need question evaluate terms dalembert solution values x ct x ct lie outside range. ignoring last mathematical nicety moment plow see happens. ﬁrst observe since initial velocity condition zero then weve discussed general dalembert solution 3.12.70 reduces case 3.12.68 i.e. ux t fx ct fx ct 3.13.76 initial displacement condition implying fx 1 2fx 0 x l . initial conditions thus accounted for consider eﬀect lefthand boundary condition. substituting x 0 3.13.76 yields u0 t fct fct 0 fct fct 0 . equation must hold positive values t implies f odd function i.e. fx fx . similarly righthand end 3.13.76 reduces ul t fl ct fl ct 0 fl ct fl ct 0 . however logic therefore equivalent fl x fl x . two alternative ways interpret this. simply say f odd xl. however also use fact f odd about zero give fl x fl x fx l therefore substituting l x x sides fl l x fl x l fx 2l fx 142
i.e. f must periodic period 2l. combined earlier observation f odd implies really need values fx 0 x l order completely deﬁne f removes earlier concerns evaluate solution x ct x ct outside interval. furthermore ﬁnal set restrictions 3.13.76 satisﬁes every part original problem words solution simply consider fx odd periodic of period 2l extension halfheight copy original displacement. equivalently taking liberty view fx odd periodic extension original displacement see represent complete solution original boundary value problem ux t 1 2 fx ct fx ct . thus wave equation ﬁxed ends initial velocity general dalembert solution reduces sum two waves one moving right left velocity c perfect halfheight copy odd periodic extension initial displacement. this course exactly interpretation discovered applying trigonometric identities separation variables solution. important point rediscovered fact directly partial diﬀerential equation without ever referring to even needing know separation variables solution. considering example dalembert solution string ﬁxed length would note last case even though mathematically dalembert solution exists x fx viewed periodic extension 1 2fx physically real solution exists 0 x l you might wish imagine opaque curtains covering rest line parts dalembert solution seen one enters either stage left stage right. anything outside region really purely mathematical construct. really concern us provided obtain correct answer physical region 0 x l. steps involved graphical construction dalembert solution region boundaries almost identical described unbounded region exception second step described earlier must replaced 2. sketch function fx extension appropriate symmetry period 1 2fx. figure 37 construct according discussion dalembert solution 1.5 problem utt uxx 0 x 4 0 u0 t 0 u4 t 0 ux 0 x 0 x 1 2 x 1 x 2 0 otherwise utx 0 0 143
step 1 4 2 1 x ux0fx step 2 8 4 4 8 12 x fx step 3 t1.5 step 4 4 4 8 12 x fx12 4 4 8 12 x fx12 step 5 12 x 2 4 figure 37 dalembert solution boundary conditions one last important insight comes problem. consider figure 38 plots result dalembert solution problem two diﬀerent times. look carefully happened initial shape expected started moving right unit velocity since c2 1 problem. ran righthand boundary 3 reﬂected mathematically course happened portion leftward moving wave originated outside interval 0 x l entered region. remember physical reality inside interval. physically would observe fact reﬂection. shape reﬂected its upside down. looked another way either phase changed 180o reﬂected oddly. another fundamental insight waves ﬁxed boundaries produce odd reﬂections phase reversals. insight would never come separation variables solution frequency domain analysis. comes time domain analysis implicit dalembert solution. a similar analysis show free boundaries produce even reﬂections. 144
t2 4 12 x t6 4 12 x figure 38 boundary reﬂections via dalembert solution 145
problem 1. sketch dalembert solutions 0 1 2.5 4 to a. utt uxx u0 t u3 t 0 ux 0 2x 0 x 12 1 12 x 32 4 2x 32 x 2 0 2 x 3 utx 0 0 b. utt uxx u0 t ux3 t 0 ux 0 2x 0 x 12 1 12 x 32 4 2x 32 x 2 0 2 x 3 utx 0 0 c. utt 4uxx ux0 t u1 t 0 ux 0 1 0 x 12 2 2x 12 x 1 utx 0 0 146
4 twodimensional wave equation 4.1 introduction remarked close previous chapter eﬀectively reached point diminishing returns regard study onedimensional wave equation. outside solving additional special cases perhaps considering nonuniform media seen major fundamental results separation variables method eigen valueeigenfunction structure sturmliouville theory complementary time fre quency domain interpretations. therefore time come move on. direction choose proceed increase number spatial dimensions still focusing primarily partial diﬀerential equations whose solutions wave interpretations. hence natural class problems study next involving two spatial dimensions say x y. study these one primary intents last chapter discern basic principles rules game opposed speciﬁc problemdependent players. furthermore shall try develop mechanics calculating solutions problems also look beyond mechanics physical principles embodied solutions. twodimensional analog vibrating string membrane thin tightly stretched elastic sheet would found drum. derived partial diﬀerential equation vibrating string assume membrane uniform density uniform internal tension produces motions eﬀectively vertical direction external forces displacements angles small. governing partial diﬀerential equation which shall derive vertical displacement u point becomes 2u t2 c22u 2 operator commonly called laplacian. appearance lapla cian comes ﬁrst signiﬁcant diﬀerence onedimensional problem. one representation i.e. 2u x2 second derivative one dimension form laplacian two dimensions changes depending geometry coordi nate system adopted. example rectangular coordinates twodimensional wave equation would become 2u t2 c2 2u x2 2u y2 cylindrical polar coordinates would 2u t2 c2 1 r r ru r 1 r2 2u θ2 . shall ignore moment minor detail cylindrical coordinates actually appropriate wish model drum consider rectangular form two 147
dimensional wave equation. terms subscript notation partial derivatives becomes utt c2 uxx uyy . now course recognize equation alone constitute com plete problem i.e. provide enough information determine unique solution. speciﬁcally diﬀerential equations involve second derivatives respect x t. therefore would expect need two additional side conditions variables could reasonably expect unique solutions. furthermore based experience vibrating string natural additional conditions expect two boundary conditions x y two initial conditions t. moreover expect exact boundary conditions vary problem problem initial con ditions always involve precisely displacement ux y 0 velocity utx y 0. 4.2 rigid edge problem begin study twodimensional wave equation rectangular coordinates began study vibrating string special case albeit one would seem physically likely. case occurs edges membrane rigidly attached immovable rectangular frame length l width w creating is eﬀect rectangular drum. as noted above quite normal circular drum. however shall see later certain mathematical aspects far simpler treat circular case. therefore much appropriate pedagogical starting point. rigid frame course implies zero displacement along edges i.e. homogeneous dirichlet boundary conditions. addition generic initial conditions complete statement problem becomes utt c2 uxx uyy u0 y t ul y t 0 ux 0 t ux w t 0 ux y 0 fx y utx y 0 gx y 4.2.1 solve problem shall try done along build much possible earlier insights. appropriate place start reﬂecting experience onedimensional wave equation utt c2uxx . key problem obviously separation variables motivated observation partial diﬀerential equation possessed natural separation operations operations x reduced homogeneous problem two ordinary diﬀerential equations. similar approach then would seem advisable here ﬁrst step would seem identify appropriate homogeneous problem twodimensional case. straightforward extrapolation comparison 4.2.1 148
onedimensional problem indicates reasonable choice utt c2 uxx uyy u0 y t ul y t 0 ux 0 t ux w t 0 4.2.2 clearly onedimensional wave equation problem least trivial so lution ux y t 0. thus important question nontrivial solutions ﬁnd them many linearly independent begin answer questions observation partial diﬀerential equation 4.2.2 possesses natural splitting operations t x y. observation coupled experience onedimensional problem suggests twodimensional homogeneous solutions might also admit analogous splitting. thus shall assume homogeneous solutions form ux y t xxy ytt . 4.2.3 before realize assumed solution form. clearly priori guarantees work. hand certainly seems like reasonable choice furthermore point really seems like only game town. whether actually produce necessary solutions depends happens substitute problem see happens. shall start substituting product solution partial diﬀerential equation. so taking appropriate derivatives obtain xxy yt t c2 xxy ytt xxy ytt or dividing sides product c2xy t t c2tt xx xx y y λ . 4.2.4 last equation course simply invoked similar argument one used onedimensional case. speciﬁcally lefthand side depends time therefore independent x y righthand side independent time yet sides equal. therefore must equal constant. chose represent constant symbol λ include minus sign based experience onedimensional problems sturmliouville theory. thus arrive two separated problems t λc2tt 0 xx xx y y λ however unlike onedimensional case were ﬁnished yet since still havent un coupled behavior xx y xx xx y y λ 149
uncoupling would seem necessary continue approach. achieving uncoupling really diﬃcult. simply move either term last equation righthand side equality. example moving terms depending y right hand side yields xx xx y y y λ µ . introducing new separation constant µ involves essentially repetition argument onedimensional case except lefthand side independent righthand side independent x. since already used λ must represent new separation constant another symbol. choice letter µ totally arbitrary however choose minus sign order to much possible follow onedimensional case. new constant dropping explicit dependencies minimal amount algebra arrive fully separated ordinary diﬀerential equations λc2t 0 x µx 0 λ µy 0 point realize really cant say anything values λ µ. furthermore even think λ µ representing single value. reasonably expect inﬁnite number them. cant yet specify either λ µ point shouldnt surprising since havent yet separated boundary conditions. therefore really want product solution 4.2.3 satisfy entire homogenous problem 4.2.2 must substitute form boundary conditions well. lefthand boundary obtain u0 y t x0y ytt 0 . condition familiar ground since x0 still value function one point i.e. number. therefore either x0 0 ytt 0. but onedimensional case second alternative would yield trivial solutions thus must require x0 0 . similarly boundary conditions imply ul y t xly ytt 0 xl 0 ux 0 t xxy 0tt 0 y 0 0 ux w t xxy wtt 0 y w 0 . now collecting information far deduced xx y tt deﬁned 4.2.2 4.2.3 have λc2t 0 x µx 0 λ µy 0 x0 0 0 0 xl 0 w 0 4.2.5 150
equation tt solvable value λ case one dimensional problem since conditions tt diﬀerential equa tion. contrast xx y satisfy sturmliouville problems fortunately however sturmliouville problems already solved before. the second problems one small subtlety combined quantity λ µ ac tually plays role eigenvalue equation y. thus one might easily argue replace combined quantity another single symbol. shall that however since prefer keep number diﬀerent symbols minimum. note problems xx y essentially independent eigenfunctions determined without reference other. example problem xx immediately leads µn nπ l 2 xnx sin nπx l n 1 2 3 . . . . 4.2.6 we may omit arbitrary constants since eigenvalue corresponds single independent eigenfunction. solution y almost direct although properly representing requires bit precise notation. reason that noted above one may perfectly well take view combined quantity λ µ plays role eigenvalue second eigenvalue problem. therefore might seem reasonable write eigenvalues eigenfunctions y λ µm mπ w 2 ymy sin mπy w 1 2 3 . . . . 4.2.7 note must use diﬀerent index variable used xnx order empha size basic independence problems xx y. however representation really adequately convey one important point 4.2.5 µ 4.2.7 may arbitrarily selected must correspond one eigenvalues problem xx. could denote writing λ µnm mπ w 2 m n 1 2 3 . . . . representation clearly implies values λ fact depend n. probably cleanest way denote λnm nπ l 2 mπ w 2 ν2 nm m n 1 2 3 . . . 4.2.8 introduce symbol ν solely avoid write square roots later on. formula eﬀectively validates earlier decision write separation constant λ since clear representation values n λ never negative. 151
xx y determined must return solve equation 4.2.5 tt. equation second order value λnm given generate two linearly independent solutions represent tnmt anm cos νnmct bnm sin νnmct m n 1 2 3 . . . . for notational consistency t b depend subscripts νnm. finally combining various pieces see value λnm given 4.2.8 produces two nontrivial linearly independent solutions 4.2.2 given by unmx y t anm cos νnmct bnm sin νnmct sin nπx l sin mπy w m n 1 2 3 . . . . 4.2.9 actually pairs n may produce eigenvalue diﬀerent eigenfunctions. therefore since expect general solution linear combination linearly independent homogenous solutions appropriate representation general solution would ux y t m1 n1 anm cos νnmct bnm sin νnmct sin nπx l sin mπy w . 4.2.10 assuming logic point correct series also solve 4.2.1 constants anm bnm properly selected satisfy initial conditions. we fact select them really one way ﬁnd out let 0 expression supposed represent ux y t times substitute resulting quantity initial conditions. let 0 4.2.10 have ux y 0 m1 n1 anm sin nπx l sin mπy w fx y . 4.2.11 similarly diﬀerentiate respect let 0 utx y 0 m1 n1 νnmcbnm sin nπx l sin mπy w gx y . 4.2.12 ﬁnd anm bnm equations satisﬁed formally answer yes provided fx y gx y least piecewise smooth region 0 x l 0 w. case either appeal fourier series results apply orthogonality property directly. example write 4.2.11 ux y 0 m1 n1 anm sin nπx l bmx sin mπy w fx y 152
viewed ﬁxed value x standard fourier sine series y coeﬃcient bmx. thus usual formula gives bmx n1 anm sin nπx l 2 w w 0 fx y sin mπy w dy . fourier sine series x second application basic fourier coeﬃcient formula yields anm 2 l l 0 bmx sin nπx l dx 2 l l 0 2 w w 0 fx y sin mπy w dy sin nπx l dx 4 lw l 0 w 0 fx y sin nπx l sin mπy w dydx . 4.2.13 you convince identical formula would result multiply sides 4.2.11 sin kπx l sin lπy w integrate rectangle separate double integral summation product iterated integrals apply orthogonality. virtually identical set steps leads bnm 4 νnmclw l 0 w 0 gx y sin nπx l sin mπy w dydx . 4.2.14 since diﬃcult conceive membrane real world whose initial displacements velocities least piecewise smooth basic fourier series results imply integrals series exist. therefore solved problem example this consider problem utt 4 uxx uyy u0 y t u2 y t 0 ux 0 t ux 1 t 0 ux y 0 0 utx y 0 1 review previous development shows problem corresponds case c 2 l 2 w 1 fx y 0. therefore according 4.2.11 4.2.12 4.2.13 solution ux y t m1 n1 bnm sin 2νnmt sin nπx 2 sin mπy 153
νnm nπ 2 2 mπ2 bnm 1 νnm 2 0 1 0 sin nπx 2 sin mπy dydx 1 νnm 2 0 sin nπx 2 dx 1 0 sin mπy dy 2 mnπ2νnm 1 cosnπ1 cosmπ . development series solutions appropriate combinations boundary conditions closely parallels development onedimensional string left exercises. 4.3 frequency domain analysis previous section developed basic mechanics constructing solutions two dimensional rectangular wave equation problems. however also case vibrating string much vibrating membrane problem mere mechanics impressive may be writing doubly inﬁnite series solutions. equally important physical principles incorporated solutions. section following one show solutions rectangular membrane analyzed manner similar solutions vibrating string although striking diﬀerences resulting properties. starting point analysis membrane rigidly ﬁxed edges zero initial velocity. as true vibrating string ﬁxed ends simply easiest case treat algebraically. cases diﬀer signiﬁcantly terms qualitative behavior. according methods described above solution problem ux y t m1 n1 anm cos νnmct sin nπx l sin mπy w m1 n1 anm sin nπx l sin mπy w cos νnmct a11 sin πx l sin πy w cos ν11ct a12 sin πx l sin 2πy w cos ν12ct a21 sin 2πx l sin πy w cos ν21ct . 4.3.15 we chosen order terms summation corresponding value n written together. fairly common convention generally results terms approximately order grouped together. 154
case vibrating string solution clearly sum diﬀerent shapes modes anmx y anm sin nπx l sin mπy w m n 1 2 3 . . . vibrating separately natural frequency given fnm νnmc 2π m n 1 2 3 . . . . however unlike modes vibrating string modes depend two independent variables x y i.e. shapes represent surfaces curves. hence plotting visualizing bit diﬃcult case string. two alternative methods displaying modes commonly used. one eﬀectively project onto plane done painting. draw contour lines done maps. figures 39 40 use techniques respectively display ﬁrst modes rectangle ﬁxed edges. mn11 mn12 mn13 mn14 mn21 mn22 mn23 mn24 mn31 mn32 mn33 mn34 mn41 mn42 mn43 mn44 figure 39 modes vibrating rectangle rectangular membrane vibrating string however diﬀer ways 155
20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 figure 40 contour lines modes vibrating rectangle nature modes. structure natural frequencies membrane fnm νnmc 2π c 2π nπ l 2 mπ w 2 c 2 n l 2 w 2 m n 1 2 3 . . . also diﬀers signiﬁcantly structure string fn nc 2l n 1 2 3 . . . two principal reasons. ﬁrst obviously frequencies membrane depend two subscripts one. thus listing requires table rather simply listing order. leads situation best illustrated following table shows ﬁrst frequencies rectangle length three width two with c 2π nm 1 2 3 4 1 1.888 3.312 4.827 6.370 2 2.618 3.776 5.157 6.623 3 3.512 4.443 5.564 7.025 4 4.474 5.236 6.305 7.551 5 5.467 6.106 7.044 8.179 6 6.477 7.025 7.854 8.886 156
carefully study table moment. note wished list frequencies strictly increasing order certainly unreasonable idea jump around table signiﬁcant amount since f11 f21 f12 f31 f22 f32 f41 f13 f23 f42 . . . . moreover also unlike frequency structure vibrating string frequency vibrating membrane may correspond two distinct eigenfunctions. example table created note f34 f62 signiﬁcant diﬀerence membrane string respect structure natural frequencies arises fact square root sum sum square roots. therefore simple calculation table show natural frequencies membrane fnm constant multiples fundamental frequency f11. therefore spectrum rectangular drum lose even picket fence appearance characterized string ordinary fourier series. figure 41 displays table frequencies. note ﬁgure plotted amplitudes identical situation would almost never occur practice. dont dwell purpose ﬁgure solely emphasize uneven spacing frequencies rectangular membrane. picture clearly indicates that issue sounds better drum violin would left individual listener question sound alike. 0 1 2 3 4 5 6 7 8 9 0 0.2 0.4 0.6 0.8 1 figure 41 spectrum rectangular drum close section simply noting demonstrated kind frequency domain arguments apply normal fourier series also applied two and implication three dimensional wave equation solutions least rectangular geometries analysis leads additional insights workings real world. 157
4.4 time domain analysis last section analyzed solution twodimensional wave equation terms component frequencies. recall previously saw solution vibrating string also could interpreted terms traveling waves via dalembert solution. show although shall skip algebra traveling wave solutions also exist twodimensional wave equation. speciﬁcally suppose k1 k2 constants k2 1 k2 2 1 . then f suitably diﬀerentiable function standard rules partial derivatives show 2 t2 fk1x k2y ct c2f k1x k2y ct 2 x2 fk1x k2y ct k2 1f k1x k2y ct 2 y2fk1x k2y ct k2 2f k1x k2y ct therefore ux y t fk1x k2y ct solution utt c2 uxx uyy . interpret solution. well recall chose ux y t denote vertical displacement twodimensional surface. therefore ux y t fk1x k2y ct must represent vertical displacement constant height along curve k1x k2y ct constant. but course ﬁxed time t equation k1x k2y ct constant simply deﬁnes straight line normal vector k k1 k2 j i.e. ﬁxed time displacement described fk1x k2y ct would look like set parallel ridges. equally interesting interpretation picture displacement change time. fairly easily shown k1x k2y ct equal particular value time t time 1 k1x k2y ct equal value line parallel ﬁrst located exactly c units away direction k. therefore whatever feature originally located ﬁrst line fact propagated velocity c direction k. see figure 42. thus whatever features may happen described fk1x k2y ct move parallel lines like ranks marchers parade. obvious 158
0 0.5 1 1.5 2 0 0.5 1 1.5 2 0 1 2 3 4 0 0.5 1 1.5 2 0 0.5 1 1.5 2 0 1 2 3 4 0 0.5 1 1.5 2 0 0.5 1 1.5 2 0 1 2 3 4 figure 42 traveling plane wave reasons motion commonly called plane wave. solutions exist twodimensional wave equation actually surprising. its really analog dalembert solution onedimensional string one fundamental diﬀerence. onedimensional string exactly two traveling waves one moving left right. contrast twodimensional wave equation inﬁnite number simultaneous independent plane wave solutions since inﬁnite number diﬀerent possible choices k. figure 43 displays solution consisting superposition two wave traveling diﬀerent opposite directions. one waves consists two crests moving generally left right. second consists single crest moving less toward reader. shall pursue line investigation further. actual mechanics decomposing arbitrary twodimensional initial displacement sum plane waves far involved onedimensional dalembert decomposition two halfheight copies initial displacement beyond scope notes. simply content observed rectangular membrane enjoys dual frequency domain timedomain nature discovered vibrating string. furthermore string nature dominates particular problem depends problem phenomenon considered. frequency domain view answers drums violins sound alike. time domain view explains sound either instrument propagates across room listeners ear. 4.5 wave equation circular regions turn physically realistic mathematically challenging problem describing vibrations circular membrane i.e. standard drum. natural 159
0 0.5 1 1.5 2 0 0.5 1 1.5 2 0 2 4 6 0 0.5 1 1.5 2 0 0.5 1 1.5 2 0 2 4 0 0.5 1 1.5 2 0 0.5 1 1.5 2 0 2 4 figure 43 two plane waves traveling directions ˆ k k coordinate system course formulate problem polar or cylindrical coordinates. furthermore already noted coordinate system two dimensional wave equation becomes 2u t2 c2 1 r r ru r 1 r2 2u θ2 . 4.5.16 qualitatively problem retains similarities rectangular membrane notably still second order respect t r θ. however one new striking dissimilarity. original twodimensional wave equation written terms laplacian represented uniform membrane constant coeﬃcients. 4.5.16 however direct consequence expressing laplacian cylindrical coordinates variable coeﬃcients partial diﬀerential equation. although shall prove so appearance variable coeﬃcients shown isolated occurrence expected outcome expressing laplacian rectangular coordinates. appearance variable coeﬃcients particularly auspicious all experience ordinary diﬀerential equations shown moving constant coeﬃcient problems variable coeﬃcient ones caused signiﬁcant increase computational diﬃculty since solutions longer simple exponentials usually kind general inﬁnite series. this diﬃculty however still far less encountered nonlinear diﬀerential equations. but ﬁrst things ﬁrst even talk solving problem must address fact diﬀerential equation itself represents incomplete problem. moreover based familiar arguments seems convert 4.5.16 complete problem require deﬁning two boundary conditions r two boundary conditions θ two initial conditions t. specifying latter i.e. appropriate initial conditions problem challenge. before 160
simply initial position velocity i.e. ur θ 0 fr θ utr θ 0 gr θ however deducing appropriate boundary conditions problem quite straightforward. all circular drum one physical boundary outer edge r l normal drum course outer edge ﬁrmly clamped frame drum clearly implies boundary condition ul θ t 0 appropriate. boundary r two boundary conditions θ come from order understand missing boundary conditions developed need ﬁrst appreciate one fundamental basic fact. fact problem one physical boundary really three mathematical boundaries places which coincide physical ending medium nevertheless cannot crossed cross boundary r l without leaving drum. we shall see precisely boundaries shortly. physically bound aries artiﬁces particular coordinate system involved. nevertheless diﬀerential equation sees completely real. furthermore shall see mathematical boundaries fact introduce precisely conditions necessary ensure 4.5.16 unique solution. ﬁrst mathematical boundary occurs r 0 for convention radius cannot take negative values cylindrical coordinates. situation might math ematically lead negative radius taken care keeping radius positive changing angle θ 180o. in nautical terms example one would never hear another ship sighted bearing twenty range minus ten miles. since r 0 boundary conditions appropriate there answer question fairly easy deduce least circular drum. fundamental requirement similar situations mathematical boundaries may introduce physically unrealistic solutions problem. produce boundary condition r 0 well noted earlier partial diﬀerential equation 4.5.16 becomes singular origin. moreover wellknown least study ordinary diﬀerential equations singular points diﬀerential equation may cause corresponding singularities solutions themselves. however singular solution would imply inﬁnitely large vertical displacement center drum would clearly physically unrealistic. equally unacceptable would solution whose gradient became singular center. therefore must pose boundary condition ensures solutions occur mathematically solution 4.5.16. one condition u0 θ t u r 0 θ t ﬁnite . this condition actually involves bit overkill. simply requiring u0 θ t ﬁnite turn suﬃcient produce correct solutions cases shall 161
consider. thus second boundary condition r may turn developing appropriate boundary conditions angle. mathematical boundaries θ also arise convention related coordi nate system. speciﬁcally polar coordinates values θ restricted lie 360o range e.g. π π. mathematical calculation would produce angle outside range taken care increasing decreasing argument suﬃcient multiple 2π bring back acceptable range. for example θ 760o reset 40o. restriction eﬀect introduces boundaries θ π θ π. actually could equally well place boundaries 0 2π. would make diﬀerence condition shall develop. but mathematically ensure solutions 4.5.16 recognize this one way impose periodicity restriction ur θ t ur θ 2π t . 4.5.17 this condition simply says points r θ r θ 2π physically identical solutions must also identical. wait didnt say needed two boundary conditions θ well did. periodicity condition actually lead two conditions θ although last fact probably obvious reason evaluate 4.5.17 θ π yielding ur π t ur π t take partial derivative condition respect θ evaluate derivative also θ π yield second condition uθr π t uθr π t . thus fact two boundary conditions θ although shall choose still use periodicity condition since slightly simpler write understand. since appear developed requisite number boundary conditions therefore enough information ensure existence unique solution 4.5.16. simply combine various pieces following complete problem circular drum 2u t2 c2 1 r r ru r 1 r2 2u θ2 u0 θ t u r 0 θ t ﬁnite ul θ t 0 ur θ t ur θ 2π t ur θ 0 fr θ utr θ 0 gr θ 4.5.18 turns out still fairly diﬃcult conceptual computational problem. there fore next section shall restrict investigating solution slightly simpler special case it. 162
4.6 symmetric vibrations circular drum notes shall solve detail one special case wave equation cir cular regions. case nevertheless exhibits important aspects problem especially aspects new diﬀerent compared properties solutions rectangular regions. special case shall study symmetric vibrations i.e. independent angle. such vibrations arise due symmetry if example drum struck precisely center. boundary value problem de scribes vibrations may found simply dropping anglerelated dependence 4.5.18 setting derivatives respect angle equal zero. yields 2u t2 c21 r r ru r u0 t u r 0 t ﬁnite ul t 0 ur 0 fr utr 0 gr 4.6.19 like simple vibrating string problem involves precisely two independent vari ables r t. furthermore like string partial diﬀerential equation possess natural splitting time operations left spatial operations right. therefore using bynow familiar logic separation variables shall assume homogenous solutions i.e. solutions 2u t2 c21 r r ru r u0 t u r 0 t ﬁnite ul t 0 4.6.20 form ur t rrtt . substituting product partial diﬀerential equation dividing product c2rrtt yields t c2tt rrr rrr λ . here before introduce separation constant λ standard argument left hand side independent r right hand side independent t. the minus sign included conform earlier experience. last equation written two equivalent equations λc2t 0 rr λrr 0 163
chosen suppress explicit dependence variables. point diﬀerential equations are theory solvable value λ. but course also seen before yet applied separation assumption full homogeneous problem i.e. must still separate boundary conditions. terms separated form condition origin becomes u0 t r0tt ﬁnite u r 0 t r0tt also ﬁnite clearly implies r0 r0 ﬁnite stated compactly r0 r0 ﬁnite. boundary condition outer rim drum identical dirichlet condi tions string separates similarly ul t rltt 0 rl 0 . thus complete homogenous problem separation form solutions becomes λc2t 0 rr λrr 0 r0 r0 ﬁnite rl 0 4.6.21 now next step fairly obvious. must determine eigenvalues eigenfunctions rr λrr 0 r0 r0 ﬁnite rl 0 4.6.22 look details solving problem general observations make. first all one important aspect problem radically diﬀerent earlier cases ordinary diﬀerential equation constant coeﬃcient. as noted earlier direct consequence representation laplacian nonrectangular coordinates. equally important however fact that even added twist problem still sturmliouville although slightly diﬀerent type encountered before. the correspondence standard sturmliouville form pxyx λwx qxyx 0 0 x l x r yr rr pr r wr rand qr 0. is however regular problem sense deﬁned previous chapter violates condition px positive entire interval including end points example called singular sturmliouville problem. nevertheless solutions shown possess essentially properties found solutions 164
regular problem. thus example negative eigenvalues. furthermore zero eigenvalue nonzero constant cannot satisfy boundary condition r l. moreover inﬁnite number positive eigenvalues λn ξ2 n n . lastly eigenfunction one zero interval 0 r l predecessor. conclusions made even know form general solution variable coeﬃcient ordinary diﬀerential equation involved however still left problem calculating speciﬁc eigenvalues eigenfunctions 4.6.22 λ 0. conceptually could reasonably diﬃcult given variable coeﬃcient nature diﬀerential equation. according theory ordinary diﬀerential equations would assume general power series solution i.e. rr n0 anrnp substitute form diﬀerential equation determine values p recurrence relation coeﬃcients. fortunately carry steps here ordinary diﬀerential equation appears problem already extensively studied. equation shall rewrite equivalent form r2r rr λr2r 0 4.6.23 called bessels12 equation order zero. appendix a. general solution problem involves two linearly independent functions as general solution second order linear ordinary diﬀerential equation. solutions normally represented symbols j0ξr y0ξr called ordinary bessel functions ﬁrst second kinds order zero respectively. calculated using inﬁnite series converge values r standard computer systems include programs compute values. graphs shown figure 44. purposes shall treat functions somewhat like black boxes every time encounter 4.6.23 shall simply proceed write general solution linear combination j0ξr y0ξr full assurance functions could calculated whenever required even though may point fully appreciate exact mechanisms calculations would made. this approach may seem bit troubling perhaps be least initially. however might reﬂect that fair degree really little write solution y ξ2y 0 yx a1 cosξx a2 sinξx for all really immediately know sine cosine except . graphs look like . values obtained either using calculator or computer tables 12friedrich wilhelm bessel see 165
. satisfy certain identities example really know chip inside pocket calculator arrives value hit sin key probably dont its really important. crucial values accurate use calculations shall adopt similar view toward bessel functions. 0.8 0.6 0.4 0 0.2 x 20 15 10 5 0 1 0.2 0.4 y0 j0 figure 44 ordinary bessel functions j0r y0r since noted above represent general solution diﬀerential equation 4.6.22 with λ ξ2 rr a1j0ξr a2y0ξr . 4.6.24 must determine exact eigenvalues applying boundary conditions solution. referring figure 44 see y0ξr singular origin therefore cannot satisfy boundary condition r 0 thus cannot appear solution i.e. r0 r0 ﬁnite a2 0 . note could make similar conclusion case include origin part physical region e.g. build donutshaped drum. region would second real physical boundary at inner radius hence would needed ﬁniteness condition anyway. finally remaining boundary condition 4.6.22 yields rl 0 a1j0ξl 0 j0ξl 0 . 4.6.25 sturmliouville theory general shape graph j0r figure 44 assure us inﬁnite number diﬀerent values ξ satisfy condition. fact every axis crossing j0r ﬁgure generate another solution 4.6.25 way every axis crossing curve sinx generated another integer multiple 166
π another solution sinξl 0 vibrating string problem. really diﬀerence solving 4.6.25 determining eigenvalues one dimensional string problem that unfortunately convenient general formula such nπ describes roots j0r 0 therefore convenient formula representing solutions j0ξl 0. listed e.g. ξ1l 2.4048 ξ2l 5.5201 ξ3l 8.6537 ξ1 2.4048 l ξ2 5.5201 l ξ3 8.6537 l 4.6.26 again choose view notational inconvenience computational drawback therefore conclude found eigenfunctions problem rnr j0ξnr n 1 2 . . . 4.6.27 ξn deﬁned above. furthermore shall need use shortly sturm liouville theory also assures eigenfunctions orthogonal respect weighting function wr r i.e. l 0 rj0ξnrj0ξmrdr 0 n . 4.6.28 lastly eigenfunctions guaranteed complete sense use expand piecewise smooth function deﬁned 0 r l fourierlike series. course eigenfunctions hand still completely identiﬁed linearly independent solutions homogeneous partial diﬀerential equation. need also solve tt. isnt really diﬃcult since know according development 4.6.21 tt must satisfy equation n ξ2 nc2tn 0 . 4.6.29 therefore before two linearly independent time solutions represent tnt cos ξnct bn sin ξnct . 4.6.30 thus usual way general solution 4.6.20 formally written ur t n1 an cos ξnct bn sin ξnct j0ξnr . 4.6.31 course calling general solution mean satisﬁes homogeneous partial diﬀerential equation boundary conditions also should correct choice bn able satisfy appropriate initial conditions. trying ﬁt initial conditions original boundary value problem 4.5.18 solution requires substituting 0 4.6.31 partial derivative respect 167
t leads to ur 0 n1 anj0ξnr fr utr 0 n1 ξncbnj0ξnr gr . 4.6.32 two equations really represent general problem. diﬀerence eﬀective coeﬃcient second initial condition an ξncbn. thus shall focus determining coeﬃcients ﬁrst initial condition assurance whatever approach ﬁnd coeﬃcients problem could immediately applied ﬁnd coeﬃcients initial velocity condition. course sturmliouville theory ﬁnding formula coeﬃcients 4.6.32 could formidable task. theory assures us since j0ξnr orthogonal apply general sturmliouville coeﬃcient formula given 3.10.57 chapter 3 directly. equivalently could usual way multiply sides 4.6.32 rj0ξmr denotes ﬁxed integer integrate yielding l 0 rfrj0ξmrdr n1 l 0 rj0ξnrj0ξmrdr 0 mn apply orthogonality property. approaches lead formula l 0 rfrj0ξnrdr l 0 r j0ξnr2 dr . 4.6.33 formally compute an a similar set steps shown produce following formula bn bn l 0 rgrj0ξnrdr ξnc l 0 r j0ξnr2 dr . 4.6.34 appreciate and almost certainly do integrals generally elementary i.e. antiderivatives found standard calculus texts. cases antiderivatives may even known therefore values various coeﬃcients would done numerically e.g. simpsons rule. nevertheless mathematically results sturmliouville theory ensure us 4.6.32 really valid set equalities that using coeﬃcients computed 4.6.334.6.34 satisfy initial condition provided j0ξnr include linearly independent eigenfunctions associated sturmliouville problem comes separation variables. eﬀectively completes solution 4.5.18. 168
close section brief example illustrating method 2u t2 1 r r ru r 0 r 4 0 u0 t u r 0 t ﬁnite u4 t 0 ur 0 1 r2 16 utr 0 0 represents circular drum radius four tension density c2 1. initial displacement initial velocity. according development eigenfunctions rnr j0ξnr n 1 2 . . . eigenvalues determined j04ξn 0 n 1 2 . . . thus values ξ1 0.6012 ξ2 1.3800 ξ3 2.1634 ξ4 2.9479 ξ5 3.7327 ξ6 4.5178 . general solution since c2 1 ur t n1 an cos ξnt bn sin ξnt j0ξnr ξn given. initial conditions reduce ur 0 n1 anj0ξnr 1 r2 16 utr 0 n1 ξnbnj0ξnr 0 bn 0 or ur t n1 cos ξnt j0ξnr where according 4.6.33 4 0 r1 r216j0ξnrdr 4 0 r j0ξnr2 dr . 169
numerical integration could show a1 4 0 r1 r216j0.6012rdr 4 0 r j0.6012r2 dr 1.1080 a2 4 0 r1 r216j01.3800rdr 4 0 r j01.3800r2 dr 0.1398 a3 4 0 r1 r216j02.1634rdr 4 0 r j02.1634r2 dr 0.0455 a4 4 0 r1 r216j02.9479rdr 4 0 r j02.9479r2 dr 0.0210 a5 4 0 r1 r216j03.7327rdr 4 0 r j03.7327r2 dr 0.0116 a6 4 0 r1 r216j04.5178rdr 4 0 r j04.5178r2 dr 0.0072 . . . . 4.7 frequncy domain analysis circular drum previous section showed vibrations circular membrane ﬁxed outer edge nonzero initial displacement initial velocity given ur t n1 cos ξnct j0ξnr ξn deﬁned j0ξnl 0 n 1 2 . . . . case vibrating string rectangular membrane may rewrite solution ur t a1j0ξ1r cos ξ1ct a2j0ξ2r cos ξ2ct a3j0ξ3r cos ξ3ct 170
i.e. sum standing vibrations vibrating diﬀerent frequency fn ξnc 2π n 1 2 . . . . various modes vibration are before simply eigenfunctions problem j0ξnr n 1 2 . . . . ﬁrst four plotted figure 45. this ﬁgure actually plots full cross section membrane negative points polar coordinates really mean change angle 180o. shape similar modes ﬁxed end vibrating string although peaks one mode height. 1.2 1 1.2 n1 1.2 1 1.2 n2 1.2 1 1.2 n3 1.2 1 1.2 n4 figure 45 modes circular membrane natural frequencies circular membrane determined except scale factor c2π zeros bessel function. already discussed length zeros uniformly spaced along horizontal axis. therefore spectrum circular membrane like rectangular membrane lack regular picket fence structure spectrum vibrating string. however degree circular membranes frequencies evenly spaced pronounced case rectangular membrane. illustrated figure 46 plots frequencies circular membrane. one must look fairly closely ﬁgure see uneven spacing. nevertheless evenly spaced therefore would sound better clearly matter personal taste unmistakable conclusion circular drum sound like violin 4.8 time domain analysis circular membrane complete analysis structure traveling waves cylindrical coordinates beyond scope discussion here. nevertheless claim interpretation possible circular traveling waves fact exist. proof this one need drop small stone small ﬂat pond water observe resulting motion. 171
0 2 4 6 8 10 12 14 16 0 0.2 0.4 0.6 0.8 1 3.1153 3.1336 3.1378 3.1394 figure 46 spectrum circular membrane 172
problems 1. shown small free vertical vibrations uniform beam e.g. bridge girder governed fourth order partial diﬀerential equation 2u t2 c24u x4 0 c2 constant involving elasticity moment inertia density cross sectional area beam. beam freely supported ends e.g. sitting piling boundary conditions problem become u0 t ul t 0 uxx0 t uxxl t 0 show separation variables works problem and case beam initially rest i.e. utx 0 0 produces general solution form n1 cos n2π2ct l2 sin nπx l 2. solve twodimensional rectangular wave equation utt uxx uyy u0 y t u1 y t 0 ux 0 t ux 1 t 0 ux y 0 .01xy1 x1 y utx y 0 0 3. solve twodimensional rectangular wave equation utt 16 uxx uyy u0 y t u3 y t 0 ux 0 t ux 2 t 0 ux y 0 y2 y sin 2πx 3 utx y 0 0 173
4. find eigenvalues form eigenfunctions for utt 9 uxx uyy u0 y t u4 y t 0 uyx 0 t ux 1 t 0 calculate actual values four lowest natural frequencies. 5. one quirks twodimensional wave equation rectangular coordinates that unlike onedimensional problem two diﬀerent values n may yield natural frequency therefore single natural frequency may two or more independent modes shapes associated it. example l 2 w 1 eigenvalues eigenfunctions are λnm n 2 2 m2 π2 unm sinmπy sin nπx 2 . show following eigenvalues fact equal λ41 λ22 λ61 λ23 λ62 λ43 λ72 λ14 6. show square membrane certain natural frequencies may four independent modes shapes associated them. 174
problems 1. show separation variables ur θ t rrθθtt applied wave equation circular region radius a 2u t2 c2 1 r r ru r 1 r2 2u θ2 ua θ t 0 u0 θ t u r 0 θ t ﬁnite ur θ t ur θ 2π t leads λc2t 0 r rr λr2 µr 0 θ µθ 0 r0 r0 ﬁnite θθ θθ 2π ra 0 2. explain mathematical physical signiﬁcance condition ur θ t ur θ 2π t. 3. find three lowest natural frequencies utt 6 r r ru r u4 t 0 u0 t u r 0 t ﬁnite ur 0 fr utr 0 0 need appendix solve following two problems. 4. using recurrence formulas table values j0x j1x ﬁnd a. j 1x terms j0x j1x b. j22.0 c. j 31.0 175
5. write terms jnx ynx general solution a. x2y xy 4x2y 0 b. x2y xy 9x2 4y 0 c. 4x2y 4xy x2 1y 0 6. solve following problems. nonzero coeﬃcients may left terms deﬁnite integrals known functions. a. utt 1 r r ru r u2 t 0 u0 t u r 0 t ﬁnite ur 0 sinπr utr 0 0 b. utt 4 r r ru r u1 t 0 u0 t u r 0 t ﬁnite ur 0 1 r2 utr 0 0 c. utt c21 r r ru r u2 t 0 u0 t u r 0 t ﬁnite ur 0 0 utr 0 1 7. solve following problem. nonzero coeﬃcients may left terms deﬁnite integrals known functions. physically interpret boundary conditions relate 176
properties solution utt c21 r r ru r url t 0 u0 t u r 0 t ﬁnite ur 0 fr utr 0 0 177
5 introduction fourier transform 5.1 periodic aperiodic functions thus far study focused developing understanding properties uses fourier series and immediate relatives. almost beginning seen power fourier series tool decomposing constructing general usually complicated periodic functions terms pure sines cosines seen series provide signiﬁcant insights physical world. physical world however full interesting aperiodic functions well periodic ones. fact cynic might well argue only aperiodic functions real. truly periodic function must continue unabated time clearly requires inﬁnite energy therefore existence functions impossible. for example astronomer attest even sun actually running down losing energy every day pragmatist course would likely counter so what mathematical physics based modeling approximations neglecting small terms etc. long model aperiodic function suﬃciently closely periodic one or vice versa whats harm moreover takes million years tell diﬀerence whos going notice degree fourier series results weve studied far agree observed physical behavior argues strongly fact made unwarranted approx imations. therefore since aperiodic functions real fact life coming sections shall study properties relate fourier sense periodic functions. start considering might represent arbitrary aperiodic function. shall denote function ht. we choose call ft two reasons. first wish reinforce notion functions use letter f. secondly later going want use letter f stand diﬀerent physical quantity. since ht periodic cannot represent terms fourier series least immediately. start shall take approach generally produces valid results applied mathematics combine bit common sense appeal physical reasonableness. speciﬁcally introduce new function denoted ht properties 1 ht periodic period 2l 2 l large 3 ht ht identical l l. idea course l suﬃciently large say million years practical purposes ht ht indistinguishable. moreover reason says larger l becomes closer mathematical representations become. suggests that mathematically consider limit ht l . after all mathematics text limit ht ht become physically identical therefore 178
reason limit mathematical representation weve developed ht also represent ht. but course this must ﬁrst representation ht you appreciate that practical problems would also ﬁrst need strong understanding time scales dominant physical processes since terms large small really meaning relative sense. example hour would clearly large time analysis single radar pulse million years might actually short examine astronomical phenomenon 5.2 representation aperiodic functions deriving representation ht diﬃcult. all weve assumed periodic therefore simply express fourier series exactly shall do albeit one slight twist well use complex form series. the reason primarily convention. almost texts develop complexvalued representation aperiodic functions. rest assured completely identical somewhat cumbersome representation could derived real form fourier series. complex form fourier series already shown is ht n cnejnπtl 5.2.1 in deference convention signal processing texts use j stand for1 coeﬃcients cn 1 2l l l htejnπtldt 1 2l l l htejnπtldt 5.2.2 represent complex amplitudes various component frequencies make ht. note use assumption ht ht identical l x l. use 5.2.2 substitute cn 5.2.1 naturally changing dummy variable integration t have ht n 1 2l l l huejnπuldu ejnπtl 5.2.3 modify representation slightly introducing change variables ω π l ωn nπ l . 5.2.4 this change really involves nothing particularly new ωn simply nth radian frequency ω diﬀerence two adjacent frequencies representation ht. new variables series representation becomes ht 1 2π n l l huejωnudu ejωntω 5.2.5 179
wait series form 1 2π n fωnω 5.2.6 fωn l l huejωnudu ejωnt xn yxn b yx x figure 47 approximation deﬁnite integral form 5.2.6 look least bit familiar. its close although shall see quite identical riemann13 sum used virtually every calculus text part introduction deﬁnite integral. riemann sum interval interest say x b ﬁrst divided n equal segments length x b an. area arbitrary continuous function yx approximated area set rectangles base length x height equal height curve value x denoted xn inside nth rectangle figure 47. riemann sum total area rectangles clearly approximates area curve. moreover x becomes progressively smaller sum become closer closer actual area. thus since deﬁnition area curve deﬁnite integral figure 47 lim x0 n n1 yxnx b yxdx 5.2.7 13georg friedrich bernhard riemann see 180
choose view last formula simply formal operational procedure i.e. arrive deﬁnite integral start appropriate riemann sum then limit replace summation integral sign proper limits drop subscript xn ﬁnally replace x dx. of course signiﬁcant nontrivial mathematical results precede formal procedure. one hardest proving limit even exists shall concern repeating proofs results here must still recognize crucial importance formal procedure deduce 5.2.7. returning attempt derive representation ht recall expect representations ht ht become identical limit l . however according 5.2.4 see l equivalent ω 0 seems perfectly reasonable write ht lim l ht lim l 1 2π n l l huejωnudu ejωntω 1 2π lim ω0 n fωnω 5.2.8 but since last term 5.2.8 looks like lefthand side 5.2.7 able apply formal procedure weve described compute limits actually argument glosses great deal thin theory. interval integration associ ated 5.2.8 ω whereas 5.2.7 involves interval ﬁnite length. thus integral improper 5.2.8 represents nontrivial extension riemann sum somewhat way inﬁnite series nontrivial extension usual sum. similar concerns text shall dwell one. prefer moment assume extension work see whether results seem either useful interesting both. useful results emerge approach whether valid becomes essence moot. if however formal procedure produce apparently valuable results rigorously verify validity various steps were taking. veriﬁcation however far beyond level text. assume 5.2.8 treated limit normal riemann sum 5.2.7 remains mechanics. noted earlier comments ﬁrst step mechanics replace summation sign 5.2.8 integral. furthermore since values ωn summation ranges values to values become limits integral. second formal step 5.2.7 drop subscripts become integrand. present case however integrand tobe fωn l l huejωnudu ejωnt therefore since really taking limit l tends drop subscript on ωn also replace upper lower limits inner integral 181
respectively. applying argument inner integral involves step beyond replacements 5.2.7 represents another part derivation properly requires later theoretical analysis. ﬁnal step formal procedure must apply 5.2.8 replace ω dω. completed steps described 5.2.8 becomes ht 1 2π huejωudu ejωtdω 5.2.9 5.2.9 reached goal ﬁnding representation general aperiodic function. were quite done reason say 5.2.9 completely valid expression. is are however equivalent forms commonly seen. one alternative forms deﬁned change variables ω 2πf 5.2.10 ht hue2πjfudu e2πjftd f 5.2.11 form shall study rest text. 5.3 fourier transform inverse transform closed previous section derived 5.2.11 fourier representation general aperiodic function ht. consider implications identity especially similarities diﬀerences fourier series representation periodic function. therefore continuing here may ﬁrst want review previous section concentrating especially particular parts 5.2.11 arose fourier series ht course derivation. one immediate striking diﬀerence representation aperiodic func tion given 5.2.11 fourier series representation periodic function complex form given 5.2.1 simply representing aperiodic function appar ently requires integral instead series. consistent observation made derivation previous section diﬀerence neighboring radian frequencies series representation ht πl. limiting process letting l go inﬁnity forces diﬀerence approach zero eﬀect squeezing neighboring fre quencies ht closer closer together ﬁnally become essence continuous smear extending across entire frequency spectrum. sums course even inﬁnite se ries inadequate combine values something respect continuously varying argument precisely deﬁnite integral appears basic calculus courses. addition deliberately placed brackets inside integral 5.2.11. brackets eﬀectively separate integral two parts allow us choose easily replace single identity two separate formulae 182
hf hte2πjft dt ht hfe2πjft f 5.3.12 note separated form also removes need use dummy variable integration u ﬁrst integral. furthermore since dummy variable integral result must be denoted function variable f. choosing express identity 5.2.11 two parts seem either unusual surprising since virtually thing routinely fourier series. example dont normally express complex fourier series 5.2.1 5.2.2 rather combined form 5.2.3 ﬁrst formula 5.3.12 twopart representation aperiodic functions com monly called fourier transform or fourier integral transform ht. moreover review derivation previous section show integral formula essentially direct descendent via limiting process l fourier series coeﬃcient 5.2.2. therefore shall interpret similarly i.e. describing sense amplitudes continuum component frequencies make aperiodic func tion ht i.e. hf represents complex amplitude pure tone frequency f found ht. clearly then terms terminology thus far hf represents frequency domain analysis time domain signal ht. second formula 5.3.12 commonly referred inverse fourier transform hf. since interpreting fourier transform analog case aperiodic functions fourier series coeﬃcients periodic functions naturally view inverse transform describing reconstruct assemble ape riodic time domain signal ht frequency domain knowledge amplitudes component frequencies. words second 5.3.12 performs identical function aperiodic functions fourier series 5.2.1 periodic ones. calling ﬁrst 5.3.12 fourier transform may bring mind another transform undoubtedly seen laplace14 transform lht 0 htestdt . several similarities fourier laplace transforms shall investigate later detail. moment would simply note fourier laplace transforms involve integral depending another variable laplace transform f fourier. we must also note one signiﬁcant diﬀerence two transforms eulers identity implies complex exponential real imaginary parts fourier transform generally non zero imaginary part even timedomain function ht real laplace transform realvalued function real long is. 14pierresimon laplace see 183
fourier inverse fourier transforms suﬃciently important applications that like laplace transform generally given symbol usually script capital letter. shall follow convention write fht hf hte2πjft dt 5.3.13 f 1hf ht hfe2πjft f 5.3.14 recognize speciﬁc aperiodic function ht uniquely identiﬁed fourier transform periodic function uniquely identiﬁed fourier series coef ﬁcients. therefore particular function 5.3.13 5.3.14 in fact must viewed inseparable pair knowledge either member serves uniquely determine other. the statement true laplace transforms precisely tables transform useful ﬁnding inverse transforms fact pairing metaphor central concept fourier transforms many texts use special notation ht hf symbolize it. fourier transform is however unlike laplace transform one important respect. diﬀerence arises because due limits integration fourier laplace transforms improper integrals. therefore according standard arguments calculus whether transforms particular functions actually even exist depends asymptotic behavior integrands limits. however e2πjft term inside fourier transform decay zero continually oscillates. therefore fourier transform integral deﬁned exist timedomain functions ht decay zero strongly enough force convergence integral. in contrast rapid decay inﬁnity est guarantees convergence laplace transform rapidly growing functions proving exact conditions function fourier transform beyond scope text. therefore simply state two common tests either satisﬁed suﬃcient ensure fourier transform exists least usual calculus sense. the reason last qualiﬁcation that later on shall extend notion fourier transform functions are view usual calculus wellbehaved. tests contained following theorem 5.1 fourier integral transform function ht hf hte2πjft dt exist either ht dt ht2 dt 184
functions satisfy second two tests commonly referred square integrable. light earlier discussions fourier series shall interpret square integrable functions ﬁnite energy time. theorem implies functions inﬁnite energy e.g. truly periodic ones probably fourier transforms least again sense usual calculus. close section one last insight fourier transform. observe fundamental identity 5.2.11 involves double integral. therefore may least formally interchange order integration rewrite identity ht hu e2πjfue2πjft f du. something formula look familiar speciﬁcally seems say value general aperiodic function speciﬁc point may obtained integral product all values of function function e2πjfue2πjft f e2πjftu f . note call last integral function simply contains variables u variable integration. one function already encountered conjuction laplace transform property able punch out single value remaining parts integrand. function is course delta function unit impulse normally symbolized δt u. thus formally appear e2πjftu f δt u . 5.3.15 interpretation raises questions example 5.3.15 embody generaliza tion concept orthogonality aperiodic functions question intriguing. considering it questions wish develop feel mechanics fourier transform computing transforms speciﬁc functions. 5.4 examples fourier transforms graphical repre sentation previous section discussed formulas fourier transform. shall apply compute transform speciﬁc function ht 1 t 1 2 0 otherwise function commonly called square pulse figure 48. note square wave periodic one single isolated pulse since single pulse clearly contains ﬁnite energy assured earlier result fourier transform exists. deﬁnition 5.3.13 transform 185
ht 1 1 1 figure 48 square pulse hf hte2πjft dt 12 12 1 e2πjft dt e2πjft 2πjf 12 12 ejπf ejπf 2πjf sinπf πf where simpliﬁed result using eulers identity replace two comples expo nentials. graph hf shown figure 49. curve represents one classic fourier transforms many texts introduce special function name sinc describe it. speciﬁcally sincx sinx x therefore example hf sincπf. however name choose call function really secondary. important focus moment properties think frequency domain description square pulse i.e. according earlier discussion viewed displaying amplitudes various frequencies necessary reconstruct ht. one property clearly hf seems exist values f. therefore conclude constructing square pulse time domain requires energy frequencies except 1 2 . . . secondly f approaches transform decays like 1f or terms order notation chapter 1 hf 1 f reﬂection asymptotic behavior surprising since ht piece wise continuous know amplitudes fourier series piecewise continuous periodic functions decay like 1n behavior similar. lastly shape graph seems indicate large amplitudes hence bulk energy 186
signal occur lower frequencies insight parallels experience fourier series. f hf 1 2 3 1 2 3 1 figure 49 fourier transform square pulse realize graphing hf square pulse quite straightforward since sinc function real values. however based earlier discussion general properties fourier transform also appreciate last example probably somewhat atypical. speciﬁcally due presence complex exponential fourier transform integrand already concluded functions purely real fourier transform. shall consider example graphically best convey frequency domain information fourier transform complexvalued. choose timedomain function ht et 0 0 otherwise 5.4.16 figure 50. according deﬁnition 5.3.13 fourier transform hf hte2πjftdt 0 ete2πjftdt e12πjft 1 2πjf 0 1 1 2πjf 1 2πjf 1 2πf2 1 1 2πf2 j 2πf 1 2πf2 discussion led us expect unlike square pulse example transform true complexvalued function nonzero real imaginary parts given respectively 187
ht 1 2 1 figure 50 function ht given 5.4.16 ℜhf 1 1 2πf2 ℑhf 2πf 1 2πf2 . therefore graphing transform provides bit challenge graphing sinc function. various authors use diﬀerent approaches here. try threedimensional perspective f along one axis real part hf along second imaginary part along third. graph type transform shown figure 51a. general however threedimensional plots widely used largely diﬃculties many people visualizing them. instead authors choose simply display two diﬀerent twodimensional graphs. even here however complete uniformity. certain texts display ℜhf versus f one plot ℑhf versus f other shown figure 51b. however authors favor approach either rather plot magnitude hf hf ℜhf2 ℑhf2 hfhf versus f one plot argument hf θ hf arctan ℑhf ℜhf versus f other. plots commonly referred amplitude spectrum phase spectrum respectively. view approach similar fourier series produces physically illuminating description since fairly directly addresses ﬁrst question usually arises analysis signal energy signal apportioned diﬀerent component frequencies in saying this actually using fact direct parallel fourier transforms parsevals theorem basic fourier series. parsevals theorem discussed chapter 2 states total energy periodic signal proportional sum squares amplitudes frequency. example magnitude 188
argument hf are respectively hf hfhf 1 1 2πf2 θ hf arctan 2πf 1 arctan 2πf . graphs two quantities plotted figure 51c. again expected see larger amplitudes occur lower frequencies. furthermore square pulse hf o1f result seems reasonable light fourier series experience since ht discontinuous 0. 4 2 0 2 4 0 0.5 1 f ℜ hf 4 2 0 2 4 0.5 0 0.5 f ℑ hf 4 2 0 2 4 0 0.5 1 f hf 4 2 0 2 4 2 1 0 1 2 f θhf figure 51 alternative graphical descriptions fourier transform two examples introduced basic considerations involved inter preting fourier transforms. shall continue use ideas turn study mechanics computing transforms detail. 5.5 special computational cases fourier transform computing fourier transforms involves computing fourier series evaluating inte grals often tedious timeconsuming process. tables help function looking tables. cases including developing tables ﬁrst place one choice compute transform direct integration. fortunately 189
however fourier transforms fourier series certain special cases exploit particular properties ht order simplify even totally avoid fully compute integrals. section consider important cases time domain function either even odd zero 0. case even odd timedomain function simpliﬁcation possible eulers identity fact integral sum sum integrals. allow us respectively replace complex exponential 5.3.13 trigonometric terms simplify result hf ht cos2πftdt j ht sin2πftdt . note formula implies among things purely realvalued time domain functions ht ℜhf ht cos2πftdt ℑhf ht sin2πftdt . formulas unfortunately correct ht complex. but since cos2πft even sin2πft odd ht real either even odd simplify integrals bn fourier series. speciﬁcally essentially repeating derivation chapter 2 show ht real even hf 2 0 ht cos2πftdt 5.5.17 furthermore hf real even ht real odd hf 2j 0 ht sin2πftdt 5.5.18 moreover hf purely imaginary odd imaginary part. thus example since function ht et clearly even figure 52 according 5.5.17 could equally well compute fourier transform hf 2 0 et cos2πftdt 2 1 2πf2 note however contrast two earlier examples ht continuous ht not therefore apparently consequence this hf o1f 2. 190
ht 1 2 1 2 1 figure 52 function et would add results even odd functions actually also used generate frequently called respectively fourier cosine sine transforms. these transforms analogous fourier cosine sine series periodic functions. shall not however discuss properties sine cosine transforms further content knowledge that appropriate cases results may produce faster simpler calculation fourier transform. results also imply fact sinc function real even predictable consequence transform real even time domain function square pulse. second simpliﬁcation fourier transform occurs ht socalled causal function ht 0 0. the terminology causal arises phys ical applications assume whatever process system studied starts 0 with perhaps initial conditions given. therefore system would ex hibit nonzero response behavior negative values would essence anticipating stimuli occur 0 i.e. acting without valid cause. observe causal function fourier transform 5.3.13 reduces hf 0 hte2πjft dt . compare formula laplace transform ht lht 0 htestdt see two identical except fourier transform replaces laplace transform variable s quantity 2πjf i.e. causal functions hf lht s2πjf . 5.5.19 thus ht causal function whose laplace transform known e.g. table transforms could write fourier transform simply replacing laplace table 2πjf. approach would worked one earlier examples since 191
according standard laplace tables let 1 1 therefore fourier transform function shown figure 50 found simply letting 2πjf i.e. hf 1 2πjf 1 1 1 2πjf . actually one needs quite careful applying result speciﬁcally may produce erroneous results unless function ht causal also fourier transform usual sense i.e ht satisﬁes conditions theorem 5.1 e.g. ﬁnite energy. attempting apply method functions satisfy one conditions may produce incorrect fourier transforms. one causal function inﬁnite energy procedure compute correct transform ht 1 0 t 0 otherwise shall show correctly compute transform function directly diﬀerent approach later section. tricks exploit special properties ht order simplify computing fourier transforms principal ones. special formulas calculating fourier series coeﬃcients even odd functions however results weve presented section really provide new information could obtained computing integrals standard fourier transform formula. generate usually computationally eﬃcient ways determine information certain special cases. 5.6 relations transform inverse transform thus far deﬁned fourier inverse fourier integral transforms computed transforms example functions. now hopefully also appreciate computing transforms directly deﬁnition i.e. actually integration 5.3.13 generally preferred approach after all computes derivatives directly limit deﬁnition practice fourier transforms computed laplace transforms using tables formulas rules exploit much possible already computed transforms. section shall determine certain relationships transform inverse used simplify fourier transform computations. relationships depend part adopting view perhaps already alluded to although strongly. central idea view transforms are nature functions themselves. diﬀerence transform inputs outputs numbers functions. thus light discussion 192
ﬁrst chapter could also view fourier transform black box inputs time domain functions outputs frequency domain ones i.e. ht f hf functions equally well speciﬁed either analytically i.e. formula graph ically therefore may equally well think fourier transform black box whose inputs outputs graphs. example could view relationship square pulse fourier transform either 1 t 12 0 otherwise f sinπf πf ht 1 1 1 f f hf 1 2 3 1 2 3 1 stated another way transform pair completely deﬁned displaying re spective graphs time input frequency output domain functions giving formulas. of course based earlier discussion might prefer use multiple graphs either input output or both complex. view mind shall perform little formal algebra. according transform formulas timedomain signal ht recovered frequencydomain transform hf according formula ht hfe2πjftd f if moment treat variables f simply symbols without physical interpretation algebraically manipulate them. speciﬁcally replace f f t sides obtain identity hf hte2πjftdt 5.6.20 look carefully righthand side identity compare 5.3.13 integral really nothing than deﬁnition fourier transform timedomain function whose graph ht i.e. function whose graph shape function hf function f. for example terms timedomain square pulse transform would ht sinπt πt . 193
interpreting lefthand side 5.6.20 slightly diﬃcult diﬃculty due solely presence minus sign. little elementary curve sketching convince that f value curve hf value curve ht f. words curve hf function f mirror image with respect vertical axis curve ht i.e. simply take curve ht swap positive negative ends horizontal axis relabel horizontal axis f figure 53. rather intriguing result summarized ht f hf figure 53 relationship ht hf formula f ht hf 5.6.21 or terms black boxes ht f hf result implies among things every fourier transform actually gener ates two transform pairs two entries table transforms. example square pulse generates 1 t 12 0 otherwise sinπf πf sinπt πt 1 f 12 0 otherwise of course since ht even function swapping positive negative axes produce hf produces perceptible eﬀect. terms black box model could equivalently portray result 5.6.21 speciﬁc functions ht 1 1 1 f f hf 1 2 3 1 2 3 1 194
f hf 1 2 3 1 2 3 1 f ht 1 1 1 we would add result counterpart laplace transform theory. one interesting result may arrived manipulating fourier inverse formula ht f 1 hf hfe2πjftd f algebraically interchange variables time replacing f f t ﬁnd hf hte2πjftdt or equivalently hf hte2πjftdt . 5.6.22 compare integral deﬁnition fourier transform 5.3.13 see nothing calculation fourier transform ht i.e. function whose graph as function t mirror image graph hf function f. ﬁrst may seem surprising nevertheless fundamental practical impact. impact simply that 5.6.22 one needs design piece computer hardware software calculate fourier transform. identical device used compute inverse transform provided one must enter data reverse order. potential economic savings observation immense. 5.7 general properties fourier transform linearity shift ing scaling weve already alluded to one seldom computes transforms directly deﬁnition i.e. actually integrating 5.3.13. generally preferred approach use tables already computed transforms some may computed using rules discussed previous section. however creating using tables involves certain tradeoﬀs complete table longer costly becomes harder often becomes ﬁnd particular formula. therefore rules properties cut length tables without compromising eﬀectiveness potentially valuable. section shall develop common rules used simplify computation fourier transforms given information transforms related functions. ﬁrst rules direct straightforward consequence facts fourier transform deﬁned terms integral integral operation certain properties. speciﬁcally given two diﬀerent time domain functions say ht 195
gt constants b then according deﬁnition transform 5.3.13 fact integral sum sum integrals f aht bgt aht bgte2πjftdt hte2πjftdt b gte2πjftdt af ht bf gt or equivalently f aht bgt ahf bgf 5.7.23 stated slightly diﬀerently fourier transform linear operation as common operations calculus derivative integral. second rule shall consider involves functions form hat ht function whose transform denoted hf already know. in following development shall assume 0. invite reader ponder change 0. however order able better interpret result shall ﬁrst review graphical terms meaning expression hat. speciﬁcally terms inputoutput model functions hat function whose output value given input say t0 value original function ht input at0 t0. thus input value 1 function hat produces output value ha. interpretation depicted figure 54 cases 1 1. of course 1 graphs ht hat identical. note 1 graph hat spread version graph ht 1 compressed i.e. replacing create hat eﬀectively rescales horizontal t axis. think physical time variable interpret 1 eﬀectively slowing down time 1 speeds up. viewed another way event occurs 1 function ht occurs 1 function hat 1 1 1. realistic physical model situation variable speed tape recorder 1 corresponds playing tape faster speed recorded at 1 playing slower. 1 ht 1 hat 1 1 hat 1 figure 54 relationship ht hat model mind consider algebra computing fourier transform 196
hat. start beginning deﬁnition f hat hate2πjftdt change variables inside integral replacing ta f hat hte2πjftadta 1 hte2πjfatdt note procedure normally also changes limits integral inﬁnite case positive change unnoticeable here. careful look last integral shows is deﬁnition 5.3.13 nothing fourier transform original unscaled function ht except integral evaluated fa f. equivalently state f hat 1 ah f 5.7.24 note eﬀect right hand side equality. nothing scaled version hf two important distinctions relative interpretation hat scaled version ht. first all vertical scale hfa diﬀerent hf factor 1a. secondly horizontal axis also scaled reciprocal a. words 1 frequency domain maximum amplitudes reduced amplitude spectrum spread out. conversely 1 maximum amplitudes increased pushed toward lower frequencies. behavior shown figure 55. ﬁgure clearly supports common sense tells us 1 eﬀect time scaling fourier transform push energy ht higher frequencies. words try play tape recorder fast forward hear mickey mouse sound. opposite course happens 1. dont forget however 5.7.24 derived assuming positive consider result would change negative instead. example already shown ht et hf f et 2 1 2πf2 therefore according 5.7.24 f e2t 1 2h f 2 1 2 2 1 2πf22 1 1 πf2 197
f hf 1 1 1 f hf 1 1 1 1 f hf 1 1 1 1 figure 55 relationship hf 1 ah f third rule simplifying computation fourier transforms involves functions form ht b nothing identical copies function ht shifted right assuming b positive amount b figure 56. physically shifts generate exactly signal later time. determine eﬀect time shift fourier transform start deﬁnition 5.3.13 change variables replacing t b inside integral. leads f ht b ht be2πjftdt ht b be2πjftbdt b e2πjfb hte2πjftdt . but course deﬁnition last integral formula nothing transform unshifted function. therefore summarize rule f ht b e2πjfbhf 5.7.25 case rule transform scaled function rule also clear interpretation terms amplitude phase spectra transform frequency domain. speciﬁcally e2πjfbhf e2πjfb hf hf since e2πjfb 1 θ e2πjfbhf θ e2πjfb θhf 2πfb θhf . words pure time shift introduces absolutely change amplitude spectrum aﬀects phase. since time shift really alter signal for example tape sound tomorrow today assuming play speed result seems perfectly reasonable. 198
b ht b htb figure 56 relationship ht ht b example consider function gt e1t 1 0 otherwise quick sketch convince function nothing function figure 50 shifted right one unit i.e. gt ht 1. therefore taking b 1 f gt e2πjf1hf e2πjf 1 1 2πjf may also observe also combine 5.7.24 5.7.25 single rule follows f hat b f hat ba e2πjfbaf hat e2πjfba h f 5.7.26 rules properties presented thus far means exhaustive. however suﬃcient compute transforms many interesting commonly occurring functions. equally importantly may used begin generate tables fourier transforms much way tables laplace transforms produced and later extend tables. transform tables signiﬁcantly streamline speed transform computations especially used conjunction linearity shifting etc. 5.8 fourier transform derivatives integrals previous section introduced rules could use simplify computation fourier transform shifted rescaled versions given timedomain function. section shall develop rules apply somewhat diﬀerent situation function wish transform derivative integral function whose transform know. recall speciﬁc relationship exists laplace transforms function derivative speciﬁcally l ht sl ht h0 . 199
furthermore relationship allows us convert linear constant coeﬃcient ordinary diﬀerential equations algebraic ones include eﬀects initial conditions. unfortunately also recall resulting algebraic equation terms transform variable original timedomain one. therefore must still invert solutions algebraic equation order recover solution diﬀerential equation. given earlier comments similarities fourier laplace transforms surprised similar perhaps identical relation occurs fourier transform. following discussion before shall denote particular timedomain function whose transform know ht transform hf. addition assume fourier transform ht also exists furthermore ht 0 . this last restriction actually less assumption almost virtually guaranteed consequence ht function satisﬁes theorem 5.1. assumptions start deﬁnition transform integrate parts one time yield f ht hte2πjftdt hte2πjft ht 2πjfe2πjft dt hte2πjft 2πjf hte2πjftdt since assumption ht vanishes ﬁrst term last equation drops out. moreover integral second term is deﬁnition nothing fourier transform original undiﬀerentiated timedomain function. therefore shown f ht 2πjfhf 5.8.27 note that hopefully unexpectedly similar result laplace transform derivative. major diﬀerence outside replacement laplace variable s fourier term 2πjf initial value data appears fourier transform derivative. reﬂection however lack initial data fourier transform seem surprising. all laplace transform integrates values 0. therefore line basic theory ordinary diﬀerential equations initial data would seem mandatory transforming derivative. contrary fourier transform need initial data since interval integration assumes values function known for time. later shall see 5.8.27 used solution diﬀerential equations. moment however consider another rule may invoked required simplify computations. thus consider following example could compute fourier transform function gt et 0 t et 0 200
ht 1 1 1 1 f hf 1 1 1 figure 57 fourier transform computed using derivative rule figure 57 directly and simplify computation using fact gt odd function note gt dt et . since already computed fourier transform et thus use earlier computation plus last rule state f gt f dt et 2πjff et 2πjf 2 1 2πf2 4πjf 1 2πf2 again already observed timedomain function gt odd. therefore expected based earlier discussion transform purely imaginary imaginary part odd function. furthermore transform o1f f apparently consequence jump gt 0. finally note apply 5.8.27 repeatedly commonly corresponding rule laplace transforms show f dnh dtn 2πjfnhf 5.8.28 derivatives also ﬁgure prominently next fourier transform rule consider diﬀerent perspective. speciﬁcally transform hf function f. there fore seems reasonable see valuable information derivative hf respect f. determine this start deﬁnition transform 201
apply usual liebnitz rule diﬀerentiating integral sign f hf d f hte2πjftdt f hte2πjft dt ht 2πjte2πjft dt 2πj tht e2πjftdt last integral simply deﬁnition fourier transform quantity tht. therefore dividing sides 2πj factor f tht 1 2πj f hf 5.8.29 or applied repeatedly f tnht 1n 2πjn dn f n hf 5.8.30 where course valid tnht must also satisfy conditions theorem 5.1. use rule demonstrated following example consider gt tet now according last result since already computed fourier transform et gf f tet 1 2πj f f et 1 2πj f 2 1 2πf2 8πjf 1 2πf22 again note oddness gt evidences gf purely imaginary odd imaginary part fact hf o1f 3 relates fact ht ht continuous. last rule shall consider section basically reverse derivative rule. speciﬁcally fundamental theorem integral calculus ht dt hsds therefore 5.8.27 hf f ht 2πjf f hsds 202
equivalently f hsds hf 2πjf 5.8.31 provided htdt 0 this last restriction necessary order satisfy earlier requirement function whose derivative transforming must vanish . section completes ﬁrst part introduction deﬁnition interpretation computation fourier transform. could perhaps continue derive rules ones covered far represent principal ones needed compute transform normal functions i.e. satisfy one two tests theorem 5.1. unfortunately many realworld applications involve signals easily modeled functions satisfy theorem. the periodic functions one obvious example. therefore rather continue try ﬁnd rules normal functions shall turn instead next section extending deﬁnition computation fourier transform broader class physically interesting functions. 5.9 fourier transform impulse function im plications already brieﬂy mentioned impulse delta function assume already introduced laplace transforms. function which really function sense usual calculus normally symbolized δt represents instantaneous force occurs inelastic collision billiard balls. of course truly instantaneous forces exist truly periodic ones. are however forces act short periods time relative time scales rest particular physical process appear eﬀectively instantaneous process. however instantaneous forces create minor mathematical dilemma. reason force instantaneous must zero times instant application. but mathematically force also produces noticeable eﬀect unless accomplishes physical work work related integral magnitude force duration application. therefore since usual calculus sense area point ﬁnite height zero force ﬁnite amplitude instantaneous duration physical work. hence physical eﬀect instantaneous force must modeled inﬁnite amplitude instant application. functions clearly cannot treated normal calculus. physically model instantaneous force completely determined know occurs value integral. this integral colloquially refer area instantaneous function course eﬀectively determines amount work force perform. thus simplest instantaneous force consider would seem one acting 0 unit area this course becomes call δt unit impulse. texts usually generate δt limit set even rectangular pulses unit area continually smaller duration and hence proportionally growing amplitudes. limit δt thus inﬁnite amplitude 0 zero elsewhere. graph ically usually portrayed boldface vertical arrow spike unit height located 203
0 figure 58. line discussion basic properties usually stated δt 0 0 δt t0 dt 1 p2 p2 1p pt p 0 δt 1 figure 58 graphical interpretations δt course instantaneous forces need occur 0. therefore must also able represent instantaneous force unit area located t0 0. notationally natural way i.e. symbol δt t0. terms notation show delta function also satisﬁes third critical equality commonly called sifting property. sifting property states that continuous function ht htδt t0 dt ht0 . note mentally view δt t0 inﬁnitely strong pulse unit area located t0 shall graphically portray simply shifted copy δt i.e. bold spike unit height t0. furthermore since sifting property implies cδt t0 dt c c constant shall therefore graph instantaneous force area work c occurs t0 spike height c located t0. moreover since area unit impulse change easily adopt view location coincides point overall argument e.g. t0 zero state following equalities δt0 t δt t0 δt δt alternatively according last equation may say figure 58 implies delta function even. finally since figure 58 seems clearly allow us consider impulse real nonnegative valued function shall also accept δt δt 204
continuing must reemphasize that inﬁnite height delta function function sense ordinary calculus. moreover properties intuitively clear. for example show later δ2t 1 2δt result seems face value inconsistent graphical interpretation time scaling normal functions portrayed figure 54. therefore mathe matics involving delta function must really justiﬁed arguments beyond ordinary calculus e.g. sifting property cannot really justiﬁed terms nor mal integrals. justiﬁcations exist form core rather elegant section mathematics called distribution generalized function theory. theory is however well beyond level. therefore much following development formal rather rigorous. fortunately formally derive results using little sifting property concept fourier transform pair. start considering fourier transform shifted delta function. assuming transform actually exists compute formally deﬁnition transform sifting property f δt t0 δt t0e2πjftdt e2πjft0 furthermore evaluate transform t0 0 second transform pair f δt 1 figure 59. according symmetry property transform i.e. property f ht hf immediately still another transform pair δt 1 f hf figure 59 transform pair f δt 1 f 1 δf δf where apply earlier view delta function even. pair immediately leads fairly powerful insight nature transform. physical model constant function time domain direct current dc battery never runs down. 205
such battery is course unrealizable since must store inﬁnite energy. recall fourier transform supposedly identiﬁes amplitudes component frequencies timedomain signal therefore seems quite reasonable transform constant function delta function located origin frequency domain. real height inﬁnite because course battery apparently inﬁnite energy area as denoted height spike corresponds exactly amplitude time domain signal. location spike also seems appropriate since deﬁnition direct current oscillate. symmetry property transform fact unit impulse uniquely characterized location zero argument also allow us compute another new pair f e2πjf0t δf f0 δf f0 . particular pair however illuminating itself since complexvalued timedomain signals somewhat unrealistic. however use compute several relevant transforms. ﬁrst uses transform along identity cost ejt ejt 2 linearity transform follows f cos2πf0t f 1 2e2πjf0t 1 2e2πjf0t 1 2f e2πjf0t 1 2f e2πjf0t 1 2δf f0 1 2δf f0 result yields still another fairly valuable insight general nature transform. cosine represents single pure frequency f0 time domain which periodic also contains inﬁnite amount energy. transform consists exactly two frequency domain impulses located f0 i.e. precisely frequency and negative time domain oscillation. impulses apparently similar interpretation impulse origin direct current battery i.e. connote inﬁnite energy time domain exists real frequency f0. furthermore impulses area exactly half amplitude real timedomain periodic function. behavior previously encountered complex fourier series also divides amplitudes real signal evenly positive negative frequencies frequency domain. using essentially steps used above also show f sin2πf0t j 2δf f0 j 2δf f0 206
cos2 pi f0 t 1 1 1f0 1f0 f hf 1 f0 f0 figure 60 transform pair f cos2πf0t sin2 pi f0 t 1 1 1f0 1f0 f imhf 1 1 f0 f0 figure 61 transform pair f sin2πf0t and except fact transform purely imaginary apparently direct conse quence oddness sine function make similar conclusions made transform cosine figure 60. transform complex exponential provides least one additional interesting insight. already seen general periodic function ht time domain always represented complex fourier series form ht n cne2jnπf0t where terms earlier notation f0 12l. therefore provided assume linearity fourier transform applies sum convergent inﬁnite series well sum two terms a nontrivial assumption have formally f ht n cnδf nf0 . words transform general periodic function inﬁnite train impulses located multiples fundamental frequency plus course constant dc term. impulse area equal i.e. drawn graphically spike amplitude of value corresponding complex fourier series coeﬃcient. thus except inclusion arrowheads signify impulses graph hf transform would look identical amplitude spectrum original fourier series developed chapter 2. 207
2 1 2 1 ht 1 f 2 2 .5 hf figure 62 transform pair periodic function close section one last insight impulses. somewhat contrast ones thus far one highlights one important diﬀerence normal functions impulses eﬀect time scaling. consider δ2t . according earlier discussions still clearly impulse located 0. still unit i.e. unit area impulse interesting enough assume impulses obey basic rules derived thus far fourier transforms not speciﬁcally according 5.7.24 2 f δ2t 1 2f δt f 2 1 2 1 f 2 1 2 1 2f δt f 1 2δt thus must conclude exactly one following true . δ2t 1 2δt . timedomain impulses transforms cannot considered inseparable pairs . impulses satisfy rules fourier transforms. since last two choices unattractive must basically accept ﬁrst one i.e. deﬁne δ2t 1 2δt actually really painful choice make. furthermore interpretation consistent natural view δ2t limit p 0 square pulses given p2t pt deﬁned figure 58. completes introduction relationships impulses fourier trans forms. insights developed physically quite satisfying. moreover results seem imply one meaningfully talk trans forms least timedomain functions represent inﬁnite amount energy shall next try extend notion transform other nonperiodic functions inﬁnite energy. 208
5.10 extensions fourier transform previous sections developed fourier transform properties normal functions i.e. satisfy one two tests theorem 5.1 im pulses periodic functions. fact extend transform periodic functions strongly implies that willing accept impulses singularities transform domain may extend transform even include inﬁnite energy functions. fact case although rigorous showing would require concepts distribution theory well beyond level text. therefore similar properties shall proceed rather formal manner. basic approach however fairly intuitive shall start trying approximate functions functions close sense also fourier transforms normal sense. throughout following discussion assume two restrictions hold true 1. ht bounded i.e. ht constant values t 2. ht contains periodic components restrictions proceed consider transforms two special cases functions satisfy theorem 5.1. ﬁrst case i lim l 1 2l l l htdt 0 terms electric circuits case functions interpreted physically longterm average dc level. shall deﬁne fourier transform functions hf lim α0 eαtht e2πjftdt 5.10.32 provided limit exists. if limit exist would assume function transform. observe carefully eﬀect done here. physically claim seems reasonable thing do. first all since assumed function ht bounded multiplying decaying exponential eαt produces integrand clearly satisﬁes conditions theorem 5.1 α 0. thus transform hteαt perfectly wellbehaved function α 0 therefore able formally take limit α 0 fairly easily. however small values α i.e. weak decay ht hteαt reasonably close other least fairly long period time. furthermore smaller α becomes closer ht hteαt become. therefore seems plausible that α decreases transforms also become closer and limit α 0 become identical. we would note however dealing improper integrals include fourier transform dealing inﬁnite series plausible results always correct witness zenos paradox. therefore strongly advise reader 209
may need extend results beyond examples presented ﬁrst consult text contains complete discussion generalized functions transforms. computing transform case type function illustrated following example sgnt 1 0 0 0 1 0 figure 63. function really represents nothing algebraic sign 1 1 ht 1 1 f 2 2 .5 hf figure 63 transform pair function sgnt t clearly satisfy conditions theorem 5.1 satisfy conditions case i. therefore using fact sgnt obviously odd may compute transform according procedure follows f sgnt lim α0 f eαtsgnt lim α0 eαtsgnt e2πjftdt 2j lim α0 0 eαt sin2πftdt 2j lim α0 2πf α2 2πf2 j πf note that hopefully unexpectedly transform singular origin. functions satisfy restriction case i satisfy restric tions section called case ii type functions. speciﬁcally case ii functions occur case ii lim l 1 2l l l htdt 0 in line earlier discussion shall interpret case ii functions which electrical sense longterm average dc level. note that based earlier discussion impulses clearly expect transforms functions computed 210
impulse origin frequency domain. treat case ii functions standard mathematical ploy reducing problem weve already solved. speciﬁcally ht case ii function write ht gt graphically gt simply ht shifted amount a i.e. its ht dc level removed. already discussed fourier transform linear operation therefore f ht f a f gt aδf f gt however gt constructed clearly case type function since longterm dc level ht contained a. therefore dont already know transform gt compute procedure ﬁnished describing case functions. thus transform ht completely determined. example consider function commonly called unit step figure 64. ut 1 0 t 0 0 its quite easy show function longterm average dc level 12 1 1 ht 1 f 2 1 1 2 1 hf figure 64 transform pair unit step function therefore based immediately previous discussion shall represent ut 1 2 1 2 sgnt now also according discussion transform example computed one f ut f 1 2 1 2 sgnt 1 2f 1 1 2f sgnt 1 2δf j 2πf note presence expected impulse origin height equal dc level time domain. pattern expected. note also even though 211
time domain function example causal laplace transformable would obtain erroneous result missing impulse origin simply replaced laplace transform 2πjf. cautioned earlier converting laplace transforms functions satisfy conditions theorem 5.1. last example completes discussion transform functions inﬁnite energy. undoubtedly rules properties associated comput ing transform could study hope appreciate interest transform primarily formula evaluated. contrary interest transform transform primarily utility tool use solve physical problems. therefore next chapter turn investigating various problems fourier transform may applied consider insights physical processes dual timedomainfrequencydomain view world provide. 212
problems 1. compute deﬁnition using properties even odd functions appropriate fourier transform following functions. case plot ht amplitude spectrum phase angle graphs. a. ht eαt α 0. plot α 1 α .05 b. ht 1 0 t 1 0 otherwise c. ht tet 0 0 otherwise d. ht 1 t2 1 1 0 otherwise e. ht 1 t2 1 1 0 otherwise f. ht aeαt cos2πt α 0 . plot α 1 α .05 g. ht 1 t 1 0 1 0 t 1 2 t 1 2 0 otherwise h. ht ateαt α 0 i. ht 1 1 0 otherwise 2. find directly deﬁnition inverse following fourier transforms plot ht amplitude phase graphs a. hf 1 f 22 1 f 1 0 otherwise b. hf f e2f f . 213
problems 1. compute fourier transform following functions using tables shifting scaling etc. appropriate. case plot ht amplitude spectrum phase angle graphs. a. ht 2 1 5 0 otherwise b. ht 0 t 2 4 t 2 4 0 otherwise c. ht sin 3 d. ht 2 3 0 otherwise e. ht 1 4 t2 f. ht sin23t 6t2 g. ht et 0 2 0 otherwise 2. find using tables shifting andor scaling etc. inverse following fourier transforms plot ht amplitude phase graphs a. hf 1 1 f 2 f b. hf e3jπfe2f f c. hf 2 3 f 3 0 otherwise 214
6 applications fourier transform 6.1 introduction fourier transform become widely used tool mathematics physics en gineering especially solution diﬀerential equations design signal processing systems. already encountered properties made popular linearity manner relates transform derivative transform original function ability describe behavior systems frequencybyfrequency basis. chapter shall brieﬂy examine commonly found applications fourier transform. intent neither allinclusive conduct indepth engineering analyses. contrary simply want show context classic applications fourier transform used helps provide unique powerful insights behavior physical systems. however must develop one property transform. property involves operation called convolution may transforms single important one relation applications consider. one sense behavior fourier transform relative convolution may viewed simply one rule like previous chapter. however convolution central applications chose present earlier instead waited consider chapter. there actually also second operation called correlation perhaps almost important applications convolution. shall brieﬂy note properties relation fourier transforms end chapter. 6.2 convolution fourier transforms deﬁne convolution two functions say ht gt gt ht gτht τdτ . 6.2.1 order write gt ht actually irrelevant since fairly easily shown gt τhτdτ gτht τdτ therefore gt ht ht gt i.e. convolution commutative operation. is however crucial recognize variable τ 6.2.1 dummy variable variable not therefore convolution two functions function i.e. ht gt graph number already encountered term convolution study laplace transforms. so parts deﬁnition seem familiar. 6.2.1 identical deﬁnition convolution laplace transform functions since lower 215
limit laplace convolution diﬀerent lower limit 6.2.1. however quite easily show functions causal i.e. ht 0 gt 0 0 fourier laplace forms convolution become identical. therefore since laplace transform treats functions deals eﬀectively values 0 may view laplace convolution simply special case fourier form diﬀerent deﬁnition. computing convolution two functions deﬁnition given 6.2.1 reasonably straightforward provided one ﬁrst carefully considers aspects inte gral. speciﬁcally since expressed integral convolution involves area curve. however noted before free variable integration therefore cases particular curve integrated limits integration vary t. moreover since integral respect τ t one must understand graphs gτ ht τ interpreted functions τ. graphical interpretation gτ simple graph gt except horizontal axis relabeled τ. however minus sign ht τ viewed function τ simply shifted copy ht. rather ht τ represents ht ﬁrst shifted original origin becomes located τ t shape original curve reversed figure 65. words ht τ look time run backwards. convolution gt ht becomes area product backwards curve curve gτ. ht τ htτ figure 65 relation ht function ht τ function τ already discussed convolution two functions function. terms justcompleted graphical description occurs amount curve ht shifted reversed depends variable t particular pictures used describe convolution change t. eﬃcient computation convolutions therefore requires reasonably structured procedure generating proper pictures. would suggest following sequence steps 1. draw gτ hτ function τ. 2. reverse graph hτ give hτ. shift curve arbitrary amount label point reversed curve corresponds original origin t. this graph ht τ. 216
3. lay graph htτ top gτ point labeled located near τ on graph gτ. 4. slowly slide graph ht τ right. point area product two graphs convolution value t. demonstrate procedure consider following example gt 1 0 1 0 otherwise ht et 0 t 0 0 figure 66 notice apply second step think reversed ex ponential leading edge τ t. therefore initially when 0 nonzero values common square pulse product and integrand convolution would zero case i. eventually however leading edge reversed exponential moves right begins intersect pulse causing nonzero integrand origin point τ case ii. finally leading edge reversed exponential move right leading edge pulse nonzero integrand origin τ 1 case iii. computationally express three cases case 0 gt ht 0 case ii 0 1 gt ht 0 1 etτ dτ etτ 0 1 et case iii 1 t gt ht 1 0 1 etτ dτ etτ 1 0 et1 et lastly combine three cases single graph figure 67 represents convolution two functions. now hopefully ﬁrmer understanding mechanics computing convolu tions consider fourier transform convolution relates fourier trans forms two individual functions produced convolution. start assuming gt ht transforms denote respectively gf hf. then previous discussions write formal deﬁnition trans form convolution i.e. f gt ht gτht τdτ e2πjftdt . 217
3 3 gt 1 3 3 1 ht τ 3 3 gτ 1 τ 3 3 1 htτ τ 3 3 1 htτ case 0 τ 3 3 1 htτ case ii 0 1 τ 3 3 1 htτ case iii 1 figure 66 graphical description convolution however since double integral may least formally interchange order integration write f gt ht gτht τe2πjftdt dτ note discussions fourier transform call interchange formal really rigorously justiﬁed extending basic calculus result improper integrals these. but integral written way may move gτ outside new inner integral since depend variable integration t there. yields f gt ht gτ ht τe2πjftdt dτ 218
1 0 1 2 3 4 5 6 0.2 0 0.2 0.4 0.6 0.8 1 1.2 gtht figure 67 graph gt ht example now deﬁnition remaining inner integral fourier transform respect ht τ i.e. f gt ht gτf ht τ dτ . therefore shifting property transform replace f ht τ f gt ht gτe2πjfτhf dτ . however form hf depend τ therefore may take completely outside integral i.e. f gt ht hf gτe2πjfτdτ . this remaining integral is deﬁnition nothing transform gt transform simply reduces f gt ht hfgf summarize shown f gt ht gfhf 6.2.2 or words convolution time domain corresponds multiplication respective transforms frequency domain. result commonly referred convolution theorem. as may recall basically property holds laplace transform laplace convolution except course interpret laplace transform domain terms frequencies. view earlier comments similarities laplace fourier transforms identity laplace fourier convolutions causal functions surprising. demonstrate convolution theorem consider following example ht 1 1 2 1 2 0 otherwise 219
show fairly straightforwardly using figure 68 ht ht 1 t 1 0 1 t 0 1 0 otherwise 3 3 ht 1 τ 3 3 hτ 1 τ 3 3 hτ 1 τ 3 1 t12 t12 htτ τ 3 t12 htτ case i 1 τ 3 t12 htτ case ii 1 0 τ 3 t12 htτ case iii 0 1 τ 3 t12 htτ case iv 1 figure 68 graphical description second convolution since resulting convolution even function compute fourier transform 220
directly follows f ht ht 2 1 0 1 t cos2πftdt 1 t 2πf sin2πft 1 2πf2 cos2πft 1 0 1 cos2πf 2πf2 sin2πf πf2 sinπf πf sinπf πf result course exactly convolution theorem predicts. note use trigonometric identity sin2x 1 cos2x 2 . therefore one hand might say convolution theorem like transform rule really provide information could obtain direct computation. hand needed f ht ht using convolution theorem would saved us signiﬁcant computational eﬀort. but shall see subsequent sections importance convolution theorem solely computational tool. convolution theorem also provides fundamental insights behavior physical systems insights would extremely diﬃcult ﬁnd without it. would close section one thought. one powerful aspects fourier transform analysis that similarity transform inverse time domain properties produce related frequency domain properties. also case convolution theorem fairly straightforwardly show fourier transform product two functions produces convolution transforms frequency domain i.e. f gt ht gf hf gφhf φ dφ . 6.2.3 shall also see applications result later sections. 221
problems 1. compute using deﬁnition convolution htgt following cases. then case compute fourier transform convolution verify result agrees convolution theorem a. ht 2 0 2 0 otherwise gt et 0 0 otherwise b. ht gt 2 2 2 0 otherwise c. ht et gt cos2πt 222
6.3 linear shiftinvariant systems one primary uses fourier transform analysis several authors e.g. gaskill refer linear shiftinvariant systems. socalled ﬁlters arise signal processing one especially important instance systems. linear shiftinvariant systems are shall see fundamental much engineering design. however proceeding further ﬁrst need specify exactly mean term linear shiftinvariant system. speak system mean mathematical physical process essentially described inputoutput relationship. many control engineering texts use terms plant system interchangeably. words system basically entity described black box model xt s yt sxt input system output systems arise time subject physical object kind force control mechanism often described diﬀerential equations yt represents solution xt applied force. examples systems deﬁned ordinary diﬀerential equations yt 3yt 2yt xt yt2 yt x2t . note diagram indicates input output system occur domain in case time domain. contrasts fourier transform box used earlier inputs outputs diﬀerent time frequency respectively domains. furthermore inputs outputs xt yt respectively system need limited scalars may vectors well. systems may either linear nonlinear. linear system one obeys principal superposition i.e. one response output system sum inputs sum responses outputs corresponding individual inputs. terms black box model linear system must behave follows 223
x1t s y1t x2t s y2t ax1t bx2t s ay1t by2t x1t x2t represent arbitrary inputs b arbitrary scalars. lastly system shiftinvariant delaying input arbitrary amount produces identical response undelayed input except response delayed amount input or terms black box model xt s yt xt t0 s yt t0 shiftinvariant behavior common perhaps take granted. but anything else would make chaos almost engineering design example could possibly build automobiles turning steering wheel clockwise produced right turns mondays left turns tuesdays eﬀect wednesdays etc. regards preceding discussion realize far systems concerned linearity shift invariance independent considerations. example system may linear shiftinvariant e.g. system represented y ty xt neither linear shift invariant e.g. system represented y tey xt . light comments surprised ﬁnd linear shift invariant systems expressed terms diﬀerential equations equations linear constant coeﬃcient. engineering design focuses least initially linear shiftinvariant systems. whether realworld systems least approximated them 224
whether ones explicit solutions generally possible arguable result same. linear shiftinvariant systems simply fundamental real applications. purposes study mathematical importance linear shiftinvariant systems show least formally system linear shiftinvariant output convolution input other system speciﬁc function i.e. terms black box model linear shiftinvariant system must obey relation xt s yt sxt xuht udu ht function depends solely particular system considered xt. formally showing systems whose outputs convolutions must linear shift independent extremely straightforward. linearity follows immediately basic calculus principles since integral sum sum integrals. showing con volution also shiftinvariant requires simple change variables integral i.e. yt t0 xuht t0 udu xu t0ht udu sxt t0 convert ﬁrst second integrals replacing u ﬁrst integral u t0 everywhere. proving linear shiftinvariant systems must satisfy convolution relationship bit involved. start considering arbitrary interval say t0 t1. next divide interval subintervals equal length denoting endpoints subintervals u0 u1 u2 . . . un u0 t0 un t1. also let u ui1 ui. then linearity given functions xt wt system must obey n i1 xuiwt uiu s n i1 xuiswt uiu swt denotes response output system input wt. the output must form ui constant values. therefore xui numbers i.e. coeﬃcients input linear combination wt ui. hence since system linear output must sum corresponding individual responses. however input output precisely form riemann sum approximation deﬁnite integral. therefore applying arguments used deriving original fourier transform identity previous chapter claim u 0 system inputoutput relationship must become 225
t1 t0 xuwt udu s t1 t0 xuswt udu now special case t0 and t1 a case really requires delicate consideration limiting processes formally write xuwt udu s xuswt udu appreciate derived last relationship using information except system linear assuming nothing xt wt. shall proceed continuing assume xt arbitrary limiting attention special wt impulse i.e. wt δt. case inputoutput black box relationship immediately written xuδt udu s xusδt udu or using sifting property delta function xt s xusδt udu also assumed system shiftinvariant. therefore denote response system basic impulse δt ht i.e. ht sδt equivalently δt s ht then shift invariance sδt u ht u. therefore summary shown that arbitrary linear shift invariant system xt s xuht udu since δt impulse ht deﬁned commonly referred impulse response system. therefore may restate conclusion discussion output linear shiftinvariant system convolution input system impulse response. last result expresses fundamental behavior linear shiftinvariant systems time domain. but seen many times before one central ideas fourier 226
analysis phenomena exist simultaneously time frequency domain. therefore completeness also interpret inputoutput relationship linear shiftinvariant systems frequency domain. ﬁnding interpretation simple convolution theorem 6.2.2 tells us that transform domain xf s xfhf representation course hf simply denotes fourier transform systems impulse response i.e. hf hte2πjftdt . transform commonly also called transfer function frequency response system. fact linear shiftinvariant systems output product frequency domain turns immense value analyzing responses systems general inputs time domain would require computing convolutions and hopefully appreciate examples already presented mechanics performing computations subsequent visualization result fairly involved. contrast consequence convolution theorem analyzing systems general behavior frequency domain simply requires multiplying two curves together. usually straightforward result generally quite easy picture. example consider figure 69. ﬁgure clearly implies output system consideration signal low frequencies original input unaﬀected high frequencies lost. whether output good bad design issue point discussion here. important point us frequency domaintransfer function formulation system response lets us quickly capture essence particular system aﬀects input signals. use frequency domaintransfer function formulation also greatly simplify analysis construction complicated systems allowing modular building block approach. speciﬁcally suppose system consisting two serial subsystems de noted s1 s2. subsystems impulse response transfer function denote natural way e.g. h2f would transfer function subsystem s2. frequency domain inputoutput diagram system becomes xf s1 h1fxf s2 h2fh1fxf 227
figure 69 graphical description system output transform domain diagram clearly implies resultant output would single system whose transfer function h1fh2f. even importantly implies replace single complicated system system made number simpler components provided product transfer functions components simpler systems identical transfer function complicated system. conclusion farreaching implications terms design complexity cost preceding discussion indicates impulse response transfer function system pivotal describing behavior system designing systems behave certain desired ways. of course since impulse response transfer function fourier transform pair knowledge either uniquely determines uniquely completely deﬁnes system itself. point though procedures ﬁnding either one clear. therefore shall consider question next. 6.4 determining systems impulse response transfer func tion given fundamental roles impulse response transfer function deﬁning behavior given system determining becomes crucial part system design 228
analysis. actually noted above determining either one is concept suﬃcient. therefore consider various ways either may found special emphasis techniques apply model mathematical systems also real world. theoretically ﬁnding impulse response system quite straightforward. all impulse response is deﬁnition system behavior resulting impulse force 0. hit system impulse see happens mathematically quite easily done least systems modeled constant coeﬃcient ordinary diﬀerential equations. need replace forcing function systems diﬀerential equation δt zero initial conditions solve e.g. laplace transforms. example consider rc circuit shown figure 70. time charge capacitor qt applied external voltage et related figure 70 sample rc circuit r dq dt 1 c q et therefore take input applied external voltage output voltage measured across capacitor represent system black box et s voutt 1 c qt charge capacitor produced unit impulse voltage 0 simply solution r dq dt 1 c q δt q0 0 now noted earlier constant coeﬃcients impulsive forcing terms generally dictate solving laplace transforms. instance taking transform sides leads rs 1 c l qt 1 229
using tables appropriate l qt 1 r 1 1 rc qt 1 r etrc 0 0 otherwise converting charge voltage capacitor shows that deﬁnition impulse response circuit ht 1 c qt 1 rc etrc 0 0 otherwise 6.4.4 impulse response available could ﬁnd systems transfer function fourier transforming ht. note ht clearly causal ﬁnite energy. you view fairly natural consequence fact real physical system. therefore already laplace transform ht ﬁnd fourier transform one rules previous chapter i.e. simply replacing variable laplace transform fourier term 2πjf hf 1 c qf 1 c 1 r 1 2πjf 1 rc 1 1 2πjfrc 6.4.5 ht rc 2rc 1 f hf 1rc 1rc f θhf 1rc 1rc pi2 pi2 figure 71 example impulse response transfer function both impulse response transfer function system plotted figure 71. actually since know relate fourier transform derivative fourier transform original function could equally well attacked problem reverse order. is could ﬁrst determined transfer function fourier transforming diﬀerential equation. would yield 2πjfr 1 c qf 1 qf 1 2πjfr 1 c linearity transform could divide c obtain transform output capacitor voltage. would course yield hf found 230
above. finally inverse fourier transforming transfer function would produce impulse response ht. mathematically approach subjecting system impulse direct simple clearly applicable system described constant coeﬃcient ordinary diﬀerential equation. general worst expect encounter would nasty algebra part inverting laplace transform computing fourier transform inverse transform integrals. however subjecting real system impulse may good idea real impulses easy generate cant truly instantaneous still must extremely short duration order adequately approximate true impulse. furthermore short duration real impulses must also high am plitudes. unfortunately high amplitudes even short duration easily damage real system especially sensitive electronics involved. therefore less stressful testing method impulse loading seems preferable real systems. method use one easiest forcing functions generate one harmless simple sinusoid e.g. ac current. of course sinusoids periodic truly periodic functions. however common sense argues that provided subject system force periodic suﬃcient length time natural transients die out steadystate response system essentially response truly periodic force. response system sinusoidal input shed light response system impulse ﬁnd out select forcing function complex exponential e2πjf0t f0 ﬁxed frequency. terms black box model properties impulse response produces following inputoutput situation e2πjf0t s ht τe2πjf0τ dτ now make change variables u τ output integral remembering τ variable integration ht τe2πjf0τ dτ hue2πjf0tu du e2πjf0t hue2πjf0u du last integral nothing than deﬁnition fourier transform ht evaluated frequency f0 thus terms black box model e2πjf0t s e2πjf0thf0 i.e. output simply input sinusoid multiplied value transfer function frequency. words directly measure systems transfer function 231
measuring systems response periodic and almost certainly nondamaging forcing function more precisely hf0 ratio amplitude output amplitude input θ hf0 diﬀerence phase output input. course strictly speaking measurement tells us transfer function single forcing frequency f0. simply need repeat test appropriate range frequencies order ﬁnd transfer function range. testing method helps give transfer function commonlyused name frequency response. moreover obtained transfer function method ﬁnd impulse response mathematically taking inverse transform thus completely avoiding load system impulse. systems described constant coeﬃcient ordinary diﬀerential equations also implement last approach mathematically. simply replace forcing function diﬀerential equation complex exponential solve undetermined coeﬃcients steadystate solution. example circuit shown figure 70 input signal et e2πjf0t diﬀerential equation becomes r dq dt 1 c q e2πjf0t undetermined coeﬃcients steadystate particular solution problem qt 1 2πjf0r 1 c e2πjf0t c 2πjf0rc 1 e2πjf0t therefore response across capacitor voutt 1 c qt 1 2πjf0rc 1 e2πjf0t thus hf0 voutt e2πjf0t 1 2πjf0rc 1 hf 1 1 2πjfrc course exactly transfer function found earlier could determine impulse response system if already know it simply computing inverse fourier transform hf. one small practical problem development. complexvalued func tions really mathematical artiﬁces producible real world. use actual input signal answer quite simple use cos2πf0t able verify 232
basic trigonometric identities ht cos2πf0t hτ cos2πf0t τ dτ cos2πf0t hτ cos2πf0τ dτ sin2πf0t hτ sin2πf0τ dτ cos2πf0t ℜe hf0 sin2πf0t ℑm hf0 thus real imaginary parts hf0 respectively found directly real input signal measuring amplitude output two times one phase input 90o phase input. would close section one last point. results presented provide basic methods computing impulse response transfer function given system. would reemphasize ﬁnding mathematical exercise. transfer function fundamental analyzing behavior systems since earlier analysis linear shiftinvariant systems showed convolution theorem expresses output systems product transform input transfer function. example consider transfer function calculated section figure 71. fairly clearly hf 1 f 1rc hf 0 f system pass low frequency components input relatively unaﬀected severely attenuate high frequencies. furthermore θ hf f con stant diﬀerent frequency components input signal delayed diﬀerent amounts become out step. fact xf 1 f xfhf 1 f 2 words system apparently suﬃciently attenuate high frequencies necessary produce discontinuities that given discontinuous input signal output continuous discontinuous derivative. therefore expect system generally distort input signals. may conﬁrm analysis considering sample input function e.g. one shown together corresponding system output fig ure 72. this output computed directly convolution input 6.4.4. computation presented because except scaling terms identical ﬁrst example chapter. expected output clearly related input also clean copy distorted one. the similarities analysis one conducted lrc circuit periodic input chapter 2 coincidental 233
figure 72 example input output rc circuit 6.5 applications convolution signal processing filters indicated close last section importance fourier transform convolution transfer function etc. value tools analyzing physical systems. one important application areas used socalled signal processing. signal processing start assuming informationcarrying signal initially transmitted physical medium. for example either antenna may transmit radio signal atmosphere transducer may transmit sonar signal water. medium commonly referred channel. passing medium signal received location. this location usually always somewhere transmitting location. however physical eﬀects encountered passing channel perhaps presence one signal channel time received signal generally distorted otherwise aﬀected version original transmitted signal in somewhat way output capacitor last example distorted version input pulse. schematically may represent process xt c yt transmitted signal channel received signal signal processing tries design circuit etc. which pass received signal yt it undo eﬀects introduced channel resulting output reasonably close original transmitted signal i.e. yt s pt xt received signal processing output of course ideally would like pt equal xt. simplest model situation occurs channel linear shift invariant. channel transfer function denote hcf therefore transform received signal would 234
f hcfxf theoretically true need design processing system transfer function reciprocal channel i.e. hsf 1 hcf pf hsfy f 1 hcf hcfxf xf unfortunately several reasons touch here usually cant reach ideal real problems. one reason many cases cant even completely specify eﬀects channel makes diﬃcult remove them however may nevertheless often reverse primary corrupting eﬀects channel recover eﬀectively fully usable version transmitted signal. perhaps basic type signal processing involves ﬁlters. name implies ﬁlters let things reject others. simplest ﬁlters designed let certain frequencies block or attenuate remaining ones. ﬁlters fall three general categories almost selfdeﬁning . lowpass filters allow low frequencies block high ones . highpass filters allow high frequencies block low ones . bandpass filters block low high frequencies pass intermediate range figure 73 transfer functions ideal filters 235
figure 73 displays transfer functions ideal cases these. call ideal cases because none fact physically achievable. reason essentially case represent systems causal. example transfer function ideal lowpass ﬁlter hlpf 1 f f0 0 otherwise unfortunately means impulse response ﬁlter hlpt sin2πf0t πt and already indicated causal system. hand causal systems reasonably performance realizable. example rc system described figure 70 easily build quite inexpensive and according figure 71 reasonably able pass low frequencies block high ones provided select r c appropriately. high pass bandpass ﬁlters similarly realized simply combining appropriatelyvalued resistors capacitors inductors measuring output voltage appropriate device. magnitudes the transfer functions circuits shown figure 74. similar ﬁlters common virtually home electronics. example band pass ﬁlter consisting lrc circuit variable capacitor found tuning control virtually every radio. varying capacitor adjusts frequency center passband eﬀectively selects frequencies received signals frequency. et vc c r ht 1 hf f 1 et vr c r ht hf f 1 et vr c r l ht hf f 1 lc 12π lc 12π figure 74 real filters impulse responses transfer functions. top show rc ﬁlter lopassmiddle rc ﬁlter highpass bottom lrc ﬁlter bandpass juncture shall close discussion ﬁlters and reasonable degree linear shiftinvariant systems well. hope least reasonably well set importance engineering systems design pivotal role transfer function plays use. however done fourier transform yet examine instead applications. 236
6.6 applications convolution amplitude modulation fre quency division multiplexing previously alluded to one attractive feature fourier transform man ner nearsymmetry transform inverse transform operations implies almost every timedomain property near mirrorimage frequency domain. one instance earliermentioned converse convolution theo rem 3 repeat f gt ht gf hf investigate one common applications result. commercial radio broadcasts originally utilized call amplitude modulation. the ﬁrst commercial radio stations also limited using fairly narrow band broad cast frequencies low mhz range. modulation used virtually early stations band frequencies generally still referred band. amplitude modulation actual information signal e.g. talk show music consti tutes generally referred baseband signal denote mt. commonly refer range frequencies comprise baseband i.e. present fourier transform mf bandwidth denote b. in radio baseband bandwidth generally limited relatively narrow range low less 5 khz frequencies. amplitude modulation consists electronically multiplying baseband signal high frequency sinusoid called carrier signal shall represent cos2πf0t produce broadcast signal denote st figure 75. the carrier frequency f0 typically mhz range. time domain then re sulting broadcast signal may viewed rapid oscillation within envelope deﬁned slowly varying baseband signal. term amplitude modulation aptly describes process because ﬁgure shows amplitude carrier modiﬁed modulated according amplitude baseband signal. figure 75 amplitude modulation time domain view 237
frequency domain interpretation amplitude modulation turns quite intriguing. speciﬁcally according 6.2.3 transform domain sf f st f mt cos2πf0t mf f cos2πf0t mf 1 2δf f0 1 2δf f0 mf φ 1 2δφ f0 1 2δφ f0 dφ 1 2 mf φδφ f0 dφ 1 2 mf φδφ f0 dφ sifting property delta function simpliﬁes two last integrals here shows transform broadcast signal simply sf 1 2mf f0 1 2mf f0 6.6.6 figure 76 amplitude modulation frequency domain view graphical interpretation result frequency domain straightforward elegant. speciﬁcally 6.6.6 say sf consists precisely two halfsize copies transform original baseband signal. one copy centered f0 f0 figure 76. for graphical convenience chosen represent mf ﬁgure real even function. following discussion uses fact conclusions remain valid minor modiﬁcations case complex mf. figure 76 also helps explain another commonly occurring variant amplitude modula tion. ﬁgure even symmetry mf part spectrum sf f0 b f0 clearly totally redundant part f0 f0 b. these two parts referred to respectively lower upper sidebands trans mitted signal. lower sideband contains information also available upper sideband bother transmit both answer dont need to. could 238
simply bandpass ﬁlter sf transmitting it figure 77 ﬁlter selects upper sideband. note figure 77 plot positive frequencies since due symmetry spectrum negative frequencies would mirror image. method transmission usually referred single sideband modulation. in terminology then transmitting full signal figure 76 may properly referred double sideband modulation although often simply called amplitude modu lation. single sideband modulation attractive since transmit information double sideband signal half spectrum. practical matter how ever additional circuitry costs associated single sideband modulation frequently make uneconomical alternative double sideband modulation even though eﬃciently utilizes available frequencies. amplitude modulation one additional impor figure 77 single sideband modulation frequency domain view tant feature. speciﬁcally suppose two diﬀerent informationcarrying signals bandwidth b denoted respectively m0t m1t wish transmit simultaneously. we shall assume wish transmit trans mitter although identical argument would apply use diﬀerent transmitters. amplitude modulate signals diﬀerent carrier frequencies denoted f0 f1 respectively produce modulated output signals s0t s1t. add modulated signals form single output signal st. assume f0 f1 chosen f0 b less f1 b then based discussion time frequency domain representations process must shown fig ure 78. look carefully ﬁgure two output signals s0f s1f occupy totally distinct nonoverlapping regions frequency spectrum. therefore wish recover either one alone need bandpass ﬁlter total received signal eliminate unwanted part figure 79. method transmission often referred frequency division multiplexing secret behind commercial broadcasting. all one atmosphere commercial stations must share it. use diﬀerent carrier frequencies signals propagate together yet listener select station wish listen simply using tuner radio receiver. tuner nothing more noted earlier band pass ﬁlter movable band shall leave extremely brief introduction amplitude modulation although 239
topic uninteresting certainly fully covered interesting aspects. section earlier one ﬁlters fact barely scratched surface central focus signal processing transmission information subsequent recovery received signal. stopping point simply text signal processing intended give brief appreciation ﬂavor central role fourier transform plays it. hope done. figure 78 frequency division multiplexing time frequency domain views 240
figure 79 recovering frequency division multiplexed signal 241
6.7 dalembert solution revisited turn application fourier transform totally diﬀerent context signal processing study wave propagation. actually also apply several ideas shall discuss image processing loosely thought signal processing photographic images order improve quality. biggest single diﬀerence application earlier discussion signal processing going computing transforms respect diﬀerent variable spatial one x rather time t. mathematically changing symbol variable consider makes eﬀectively diﬀerence. example function denoted fx could equally well deﬁne spatial fourier transform rewriting fundamental fourier transform relationships 13 14 chapter 5 fk f fx fxe2πjkxdx 6.7.7 fx f 1 fk fke2πjkxdk 6.7.8 formally requires using symbol k instead f x instead t. we also choose write overhead tilde continually remind spatial rather temporal fourier transform. serving reminder tilde extraneous. moreover proper change symbols rules fourier transforms still valid e.g. f f dx 2πjk fk etc. physically course using spatial fourier transform require several inter pretations change. example discussion often terms wavelengths rather frequencies. one additional important factor comes play spatial fourier transform applied study wave phenomena. addition necessary because saw earlier chapters text wave phenomena involve functions two independent variables described partial diﬀerential equations. therefore consider brieﬂy interpret transform function two variables respect one them i.e. interpret f ux t actually diﬃcult provided carefully keep track variables involved remember spatial fourier transform respect x t. speciﬁcally deﬁnition may write f ux t ukt ux te2πjkx dx 242
note also slightly modiﬁed notation writing transform variable k subscript. emphasize that problems type k generally treated parameter rather independent variable. in words transforming problem corresponds eﬀect looking solution evolves respect time frequencybyfrequency basis. result spatial transform partial derivatives determined manner investigated time transform ordinary time derivatives i.e. starting deﬁnition integrating parts. diﬀerence must carefully distinguish transforms partials respect variable transforming partials variables i.e. transform partial respect x the transform variable behaves like f u xx t u xx te2πjkx dx ux te2πjkx x ux t x e2πjkx dx ux t2πjk e2πjkx dx 2πjk ux te2πjkx dx 2πjk ukt 6.7.9 transform partial respect which transform variable becomes f u t x t u t x te2πjkx dx dt ux te2πjkx dx d dt ukt uk dt 6.7.10 course fourier time transform results extended higher derivatives e.g. f 2u x2 x t 2πjk2 ukt 6.7.11 f 2u t2 x t d2 uk dt2 6.7.12 preliminaries way consider onedimensional wave equation region without boundaries initial displacement initial velocity 243
i.e. problem 2u t2 c22u x2 x 0 ux 0 fx u t x 0 0 course already seen problem solved dalemberts principle. however nevertheless instructive also apply spatial fourier transform here. note terms time variable problem character initial value problem. therefore taking fourier transform respect time would sensible anyway since initial conditions transform variable real meaning context fourier transforms. transform respect time could apply would laplace transform thats diﬀerent issue simply spatially fourier transform sides partial diﬀerential equation f 2u t2 x t f c22u x2 x t 6.7.13 use 6.7.11 6.7.12 d2 uk dt2 2πck2 uk 6.7.14 ordinary diﬀerential equation ukt. since secondorder however still need two initial conditions in transform domain order ﬁnd unique solution. come from answer obvious. two initial conditions original spatial domain transform them do obtain f ux 0 uk0 fxe2πjkx dx fk f u t x 0 d uk dt 0 0 respectively. combining results single problem statement d2 uk dt2 2πck2 uk uk0 fk uk dt 0 0 solution easily seen ukt fk cos2πckt 244
but course solution transform domain. ﬁnd solution original physical spatial domain must invert ux t f 1 ukt ukte2πjkx dk fk cos2πckte2πjkxdk apply eulers formula cosine term ux t fk 1 2 e2πjckt e2πjkct e2πjkx dk 1 2 fke2πjckte2πjkx dk 1 2 fke2πjkcte2πjkx dk 1 2 fke2πjkxct dk 1 2 fke2πjkxct dk 6.7.15 however according formulation inverse transform 6.7.8 fx fke2πjkx dk comparing last equation two integrals previous one see three identical except two integrals comprising ux t evaluated x x ct x ct respectively i.e. ux t 1 2fx ct 1 2fx ct this course dalemberts principle. interesting development manner spatial fourier transform permits us derive principle better understand arises. knowing formula arises generally far preferable simply stating formula arose magic. but shall see next section utility spatial fourier transform limited derivation. 6.8 dispersive waves addition providing derivation dalemberts principle spatial fourier transform also allows us study wave propagation problems dalemberts principle simply even apply. example consider problem 2u t2 c22u x2 u ux 0 fx u t x 0 0 6.8.16 245
shall dwell physics introduce additional u term here. however real physical models transmission lines equation model. intent solely demonstrate information spatial fourier transform provide solutions problem solutions may diﬀer pure onedimensional wave equation. fairly easily shown neither functions fx ct gx ct satisfy partial diﬀerential equation 6.8.16. therefore problem dalembert type solution feasible. however partial diﬀerential equation linear constant coeﬃcient transform methods still are. therefore shall simply proceed directly transform partial diﬀerential equations boundary conditions. similar manner derivation 6.7.136.7.13 associated initial conditions ﬁnd d2 uk dt2 1 2πck2 uk uk0 fk uk dt 0 0 solution may written following form ukt fk cos2πckνkt 6.8.17 deﬁne νk 1 1 2πck2 6.8.18 the reason introducing νk likely transparent point. however shall see simpliﬁes following analysis somewhat. again previous problem recognize 6.8.17 represents solution transform domain must inverted. exception addition νk term process identical previous inversion shown lead ux t 1 2 fke2πjkxνkct dk 1 2 fke2πjkxνkct dk however cannot simplify result dalembert problem. diﬃculty νk term makes neither integral function simple expression like x ct. of course since k variable integration cannot take νk outside integral however still obtain insights. speciﬁcally consider second integral 1 2 fke2πjkxνkct dk since integral arose inverse transform must view analog fourier series i.e. attempt reconstruct physical function combining 246
components spatial frequencies or equivalently wavelengths. one particular components described 1 2 fke2πjkxνkct ﬁxed value k. expression clearly represents wave single wavelength amplitude 1 2 fk moving right velocity cνk c. a similar analysis conclusion made integral except course wave moves left. νk depends k. words frequency components made initial displacement fx propagate diﬀerent speeds. therefore time evolves become progressively out step disperse. for reason problem commonly called dispersive wave equation. thus apparently still propagation case time passes dispersion cause increasing distortion shape propagating disturbance relative original displacement. contrast dalembert inverse 6.7.15 components move velocity c therefore remain in step causing original displacement lose shape propagates. figure 80 displays graphs solution dispersive wave equation computed numerically evaluating integrals involved. important point ﬁgure computed integrals substantiate intuitive expectation expectation came directly fourier transform analysis. figure 80 solutions dispersive wave equation diﬀerent times section close brief introduction spatial fourier transform. discussion signal processing discussion intended either in depth allinclusive. contrary intention provide brief ﬂavor 247
another instance fourier methods provide powerful insights workings physical world. earlier introduction signal processing hope short sections accomplished spatial fourier transform. 6.9 correlation one timedomain operation commonly associated fourier transform. socalled correlation deﬁned xt yt xτyt τ dτ 6.9.19 form correlation looks quite close convolution except correlation uses term t τ integrand vice tτ convolution. hand uses interpretations correlation convolution widely diﬀerent. correlation often used measure similarity signals especially one contaminated noise. forms convolution correlation look quite close fourier transform correlation produces similar though identical result convolution theorem. sake completeness shall present result here. start usual formal deﬁnition f xt yt xτyt τ dτ e2πjft dt convolution theorem twodimensional integral may least formally interchange order integration yielding xτyt τe2πjft dt dτ then inner integral depend τ may move xτ outside inner integral yielding xτ yt τe2πjft dt dτ since inner integral corresponds fourier transform yt shifted τ may use shift rule write xτy fe2πjfτ dτ since f depend variable integration may take completely outside integral write f xτe2πjfτ dτ however deﬁnition remaining integral simply fourier transform xt evaluated f i.e. 248
f xt yt fxf 6.9.20 its fairly important note that unlike convolution correlation generally commutative i.e. general xt yt yt xt reﬂected 6.9.20 because general fxf xfy f dealing transforms correlations thus quite bit delicate dealing transforms convolutions. regrettably shall pursue point. beyond presenting result 6.9.20 shall delve uses correla tion. nevertheless represent one operation which proper context may better understood frequency domain via fourier transform time domain. 6.10 summary chapter brieﬂy covered classic applications fourier trans form signal processing modulation wave propagation. noted several times coverage abbreviated designed show transform produces insights might diﬃcult arrive means. key point ability transform fourier series provide two views physical phenomenon one time domain frequency domain. depending aspect particular phenomenon primary interest usually determine domain appropriate study problem in. 249
problems 1. consider linear shiftinvariant system represented following circuit ldi dt ri et vout ri a. directly determine e.g. using laplace transform impulse response system. sketch response. b. 1 find transfer function system computing fourier transform impulse response determined part a. above. b. 2 show alternative method ﬁnding transfer function i.e. response system forcing function e2πjf0t produces result part 1 c. sketch amplitude phase spectra transfer function computed part b. 2. repeat problem 1 circuit except output taken voltage across inductor i.e. ldi dt ri et vout ldi dt 250
7 appendix bessels equation 7.1 bessels equation deﬁnition bessels equation order n commonly called bessels equation ordinary diﬀerential equation x2y xy x2 n2y 0 . 7.1.1 full details solution equation found standard ordinary diﬀerential texts boyce diprima. shall repeat detail here summarize highlights. start observing cursory visual inspection bessels equation reveals two primary attributes 1. variable coeﬃcients 2. singular point x 0. standard ordinary diﬀerential terminology origin regular singular point bessels equation. singularity standard solution technique the socalled method frobenius15 assume solution form general power series i.e. k0 akxkp 7.1.2 a0 assumed nonzero remainder an well value p determined. notice exponent written ﬁrst term series a0xp. therefore real loss generality assumption a0 0 since p determined beforehand. power series is course assumed solution form. whether solutions form actually exist and so exactly many depends develops one substitutes series diﬀerential equation. last comment quite relevant equation since may recall one quirk regular singular point problems one linearly independent solution always exist form 7.1.2 second one need not. contrast behavior solutions neighborhood ordinary points two linearly independent solutions always found terms taylor series. noted above shall present full solution bessels equation here refer interested reader standard ordinary diﬀerential equations text full details. simply assert conclusion substituting 7.1.2 diﬀerential equation one linearly independent power series solution exists solution expressed k0 1k kn k x 2 2kn . 15ferdinand georg frobenius see 251
function represented series conventionally referred ordinary bessel function first kind order n commonly simply bessel function order n. virtually universally represented symbol jnx i.e. jnx k0 1k kn k x 2 2kn . 7.1.3 course like second order linear equation bessels equation requires two linearly independent solutions order construct general solution. however according discussion completed solution independent jnx cannot written pure power series therefore must determined procedure method froebenius. fortunately several techniques available determin ing second linearly independent solution ordinary diﬀerential equation ﬁrst one known. one general method reduction order. second option tailored speciﬁc type problem involves careful diﬀerentiation series solution respect parameter p. either methods could applied generate solution linearly independent jnx. before omit details actual construction second linearly inde pendent solution bessels equation. again found standard texts. continuing however one slight word caution order. recall thing unique set linearly independent solutions secondorder diﬀerential equations. example could represent solution y y 0 equally well either yx c1ex c2ex yx d1ex d2 coshx . therefore occasionally texts papers may use diﬀerent formulas symbols ones shall present. however choice notation extremely common. probably frequently used choice second solution bessels equation function almost universally denoted ynx rather imposing deﬁnition ynx 1 π n1 k0 n k 1 k x 2 2kn 2 π ln x 2 jnx 1 π k0 ψk 1 ψn k 1 1n kn k x 2 2kn 7.1.4 ψ1 γ ψk γ k1 i1 1 252
γ constant commonly called eulers16 constant. like π value eulers constant cannot determined exactly approximately γ 0.57721566 . . . caveat notation almost quite universal ﬁnally arrive conclusion general solution 7.1.1 may represented yx c1jnx c2ynx . 7.2 properties bessel functions bessel functions occur wide variety applications. frequent appearance studied great detail well hundred years. studies uncovered many quite interesting properties relationships important shall brieﬂy consider. more comprehensive listings may found standard reference texts abramowitz stegun spanier oldham. least initially qualitative quantitative properties bessel functions obscured inﬁnite series involved deﬁnitions 7.1.3 7.1.4. however one sense series bad might be. series shown converge values x by ratio test alternating term series i.e. change sign every term. furthermore large k denominators grow approxi mately k2. therefore reasonable values x series converge fairly rapidly and terms alternate errors approximating series partial sums reasonably easy monitor control. hence especially given power modern computers computing graphing jnx ynx becomes fairly straightforward. figures 81 display graphs jnx top ynx bottom n 0 1 2 3 . several qualitative properties bessel functions may directly inferred graphs. types bessel functions oscillate appear resemble decaying trigono metric functions large values x. inﬁnite number axis crossings roots zeros property share sine cosine. this property important purposes since eigenvalues particular problem always related axis crossings. graphs however also highlight one major qualitative distinction behavior jnx contrasted ynx. ynx appear singular origin j0x approaches unity origin jnx appear go zero there. these last two statements immediately veriﬁed series representation 7.1.3. another intriguing important property bessel functions also derived directly series representations although algebra involved nontrivial. property socalled recurrence formulas establish certain relationships bessel 16leonhard euler see historymathematicianseuler.html 253
6 1 0.8 4 0 0.4 2 x 8 0 0.2 0.6 0.2 10 0.4 6 4 2 0 1 0 1 2 3 x 4 5 10 8 figure 81 bessel functions jn a yn b. functions diﬀerent orders derivatives. important njnx xj nx xjn1x nynx xy nx xyn1x njnx xj nx xjn1x nynx xy nx xyn1x . equations combined yield others perhaps useful comes simply adding regrouping terms yield jn1x 2n x jnx jn1x yn1x 2n x ynx yn1x . value last two formulas repeated application leads conclusion determining value jnx or ynx particular value x n requires table values or computer program accurately gives j0x j1x or y0x y1x point. example evaluating last recurrence relation involving 254
ordinary bessel functions n 2 n 1 yields j3x 4 xj2x j1x j2x 2 xj1x j0x simpliﬁed by eliminating j2x j3x 8 x2 1 j1x 4 xj0x . unfortunately point view realistic computations least one minor point glossed over. noted above series involved bessel functions converge values x. however presence x2kn term series cause converge slowly value x increases. practical matter then series useful primarily x small. therefore mathematicians de voted signiﬁcant eﬀorts develop simple expressions commonly called asymptotic formulas give approximate values various bessel functions values become accurate larger x is. perhaps widely used approximations jnx 2 πx cos x nπ 2 π 4 x 7.2.5 ynx 2 πx sin x nπ 2 π 4 x . 7.2.6 note two asymptotics conﬁrm earlier observation bessel func tions behave somewhat like decaying sinusoids. they however important equal decaying sinusoids since 7.2.5 7.2.6 approximate formulas. k j0x j1x j2x j3x 1 2.4048 3.8317 5.1356 6.3802 2 5.5201 7.0156 8.4172 9.7610 3 8.6537 10.1735 11.6198 13.0152 4 11.7915 13.3237 14.7960 16.2235 5 14.9309 16.4706 17.9598 19.4094 table 1 roots ordinary bessel functions first kind jn closing section one ﬁnal important point must make qualitative behavior bessel functions. earlier discussion asymptotic for mulas 7.2.57.2.6 indicated bessel functions like sine cosine 255
k y0x y1x y2x y3x 1 0.8936 2.1971 3.3842 4.5270 2 3.9577 5.4297 6.7938 8.0976 3 7.0861 8.5960 10.0235 11.3965 4 10.2223 11.7492 13.2100 14.6231 5 13.3611 14.8974 16.3790 17.8185 table 2 roots ordinary bessel functions second kind yn inﬁnite number distinct zeros. nevertheless fundamental diﬀerence struc ture zeros opposed structure zeros trigonometric functions. diﬀerence zeros bessel functions evenly spaced along axis convenient formula integer multiples π exactly describes them. way express list table. example table 1 lists ﬁrst zeros ﬁrst three jnx functions table 7.2 lists ﬁrst zeros ﬁrst three ynx functions. in practical applications zeros ynx generally used much zeros jnx. however situation quite bleak discussion might ﬁrst seem. asymptotic formulas imply large x kth zero large k jnx approach n 2 k 1 4 π kth zero ynx approach n 2 k 3 4 π. therefore one decides much accuracy need know roots bessel function simply use tables asymptotic value agrees table value desired accuracy. 7.3 variants bessels equation bessel functions appear frequently applications especially solution partial diﬀerential equations cylindrical coordinates. addition and sometimes solution partial diﬀerential equations several variants bessels equation frequently occur. perhaps common variants x2y xy ξ2x2 n2y 0 . 7.3.7 equation change variables ξx including application chain rule e.g. dy dx dy ds ds dx ξ dy ds reduces diﬀerential equation s2d2y ds2 sdy ds s2 n2y 0 clearly bessels equation order n independent variable instead x. therefore general solution 7.3.7 represented yx c1jnξx c2ynξx . 256
it may help view last result analogous fact general solution y 0 yx c1 sinx c2 cosx general solution y ξ2y 0 yx c1 sinξx c2 cosξx . impressive variant bessels equation general form x2y a 2bxp xy c dx2q ba p 1xp b2x2p 0 a b c d p q constants reduced bessels equation fairly tedious change variables. shall omit details here simply present resulting general solution yx xαeβxp c1jν λxq c2yν λxq α 1 a 2 β b p λ q ν 1 a2 4c 2q . one last fairly common variant bessels equation shall mention socalled modiﬁed bessels equation x2y xy x2 n2y 0 . 7.3.8 equation actually viewed nothing special case 7.3.7 occurs ξ j 1. solution could therefore written yx c1jnjx c2ynjx . however would awkward writing solution y y 0 yx c1 sinjx c2 cosjx instead yx d1ex d2ex . therefore conventional way represent solution 7.3.8 introducing two additional bessel functions denoted inx knx commonly referred modiﬁed bessel functions ﬁrst second kinds respectively. general solution 7.3.8 represented yx c1inx c2knx . similarly solution x2y xy ξ2x2 n2y 0 represented yx c1inξx c2knξx . shall encounter modiﬁed bessel functions text simply refer inter ested reader abramowitz stegun spanier oldham complete discussion. 257
problems use bessel functions the big ugly equation ﬁnd general solution ode below. 1. x2y 3 xy 3 4 x4 0 2. x2y x 2 x2 y 4 9 x2 x x2 0 3. x2y 5 xy 9 4 x2 0 4. x2y x 2 x3 y 14 x2 2 x2 x4 0 answers 1. yx j1x2x b y1x2x 2. yx ex a j23x b y23x 3. yx x3 a j02x b y02x 4. yx ex22 j121x b y121x 258
references abramowitz m. stegun i. handbook mathematical functions dover pub. co. new york 1965. boyce w. e. diprima r. c. elementary diﬀerential equations boundary value problems fifth edition john wiley sons new york 1992. spanier j. oldham k. b. atlas functions hemisphere pub. co. new york 1987. 259
index amplitude 49 amplitude distortion 64 amplitude spectrum 188 argument 188 asymptotic formulas 255 bessel function 252 bessel functions 165 253 bessels equation 251 bessels equation 165 257 best mean square approximation 51 boundary conditions 76 causal function 191 circular membrane 159 completeness 128 complex exponential form 53 convergence tests 13 dalembert solution 138 143 delay 48 delta function 185 digital signal processing 60 dirichlet 76 141 dispersion 66 eigenfunctions 90 101 eigenvalues 90 101 eulers constant 253 even function 35 first kind 252 ﬁrst kind 77 ﬁxed end 76 85 fourier v fourier coeﬃcients 28 fourier integral transform 183 fourier series 25 fourier transform 183 free end 77 78 frequency domain 154 170 frequency domain 159 fundamental frequency 133 harmonics 134 improper integral 7 impulse function 203 inﬁnite series 11 initial conditions 82 inverse fourier transform 183 lhopitals rule 7 laplace transform 183 laplacian 147 limit 5 linearity 195 magnitude 188 mean square measure 42 meansquare convergence 44 membrane 147 method frobenius 251 mixed end 80 106 modes 134 155 170 modiﬁed bessel functions 257 modiﬁed bessels equation 257 natural frequencies 133 171 natural frequency 155 neumann 78 97 odd function 35 order notation 8 10 orthogonality 125 153 167 orthogonality integrals 26 parsevals theorem 49 phase angle 48 phase distortion 64 phase spectrum 188 piecewise continuous 32 piecewise smooth 32 plane wave 159 pointwise convergence 19 20 polar coordinates 147 power series 251 recurrence formulas 253 reduction order 252 regular sturmliouville 121 riemann sum 180 rigid edge 148 robin 80 scaling 196 second kind 78 252 separation variables 86 148 sequences 4 shifting 198 260
sifting property 204 sinc function 186 singular sturmliouville 164 spectrum 49 171 square pulse 185 sturmliouville 119 151 third kind 80 time domain 158 172 time domain 159 transform derivative 200 transform integral 202 uniform convergence 21 41 unit impulse 185 unit step 211 weierstrass 22 weierstrass mtest 22 weirstrauss mtest 67 zenos paradox 1 261
mathematics machine learning additional exercises marc peter deisenroth a. aldo faisal cheng soon ong last update 20200516 living document provide additional exercises including solutions mathematics chapters book mathematics machine learning published cambridge university press 2020. possible solutions shown blue. may unique optimal. ﬁnd mistakes please raise github issue chapter 2 1. find solutions inhomogeneous system linear equations ax b a 1 2 3 0 1 2 b 1 0 1 . determine general solution inhomogeneous system linear equations good start compute reduced row echelon form augmented system ab 1 2 1 3 0 0 1 2 1 3r1 r1 1 2 1 0 6 3 0 4 2 1 3r2 1 6 2 3r2 1 0 0 0 1 1 2 0 0 0 last row augmented system see 0x1 0x2 0 always true. rows obtain x1 0 x2 1 2 x 0 1 2 unique solution system linear equations ax b. b 1 2 3 0 2 2 b 1 1 . general solution consists particular solution inhomogeneous system solutions homogeneous system ax 0. eﬃcient way determine general solution via reduced row echelon form rref augmented system ab 1 2 3 1 0 2 2 1 r2 1 2 1 0 1 0 0 1 1 1 2 1
. rref read particular solution not unique using pivot columns xp 0 1 2 0 r3 . here set x1 righthand side augmented rref ﬁrst row x2 righthand side augmented rref second row. since xp r3 otherwise matrixvector multiplication ax b would deﬁned third coordinate x3 0. . next determine solutions homogeneous system linear equations ax 0. lefthand side augmented rref immediately read solutions λ 1 1 1 λ r used minus1 trick. . putting everything together obtain set solutions system ax b x r3 x 0 1 2 0 λ 1 1 1 λ r . 2. compute matrix products ab possible a 1 2 3 0 1 2 b 4 1 2 0 2 1 matrix multiplication deﬁned since r23 b r23. matrix product deﬁned neighboring dimensions columns rows b would need match. here 2 3. b 1 2 3 0 1 2 b 4 1 2 0 2 1 ab 14 2 2 2 for example 14 1 4 2 2 3 2. 3. find intersection l1 l2 l1 l2 aﬃne spaces subspaces oﬀset 0 deﬁned l1 1 0 1 z p1 span 3 2 1 z u1 l2 10 6 2 z p2 span 1 1 1 5 4 1 z u2 . x l1 x p1 αb1 2
α r. deﬁned b1 basis vector u1. similarly x l2 x p2 β1c1 β2c2 β1 β2 r u2 spanc1 c2. therefore x l1 l2 conditions must hold arrive x l1 l2 α β1 β2 r αb1 β1c1 β2c2 p2 p1 leads inhomogeneous system linear equations aλ b λ α β1 β2and 3 1 5 2 1 4 1 1 1 b p2 p1. 9 6 3 bring augmented system ab reduced row echelon form using gaussian elimination 1 0 1 3 0 1 2 0 0 0 0 0 read particular solution α 3 ξ p1 3b1 10 6 2 p2. ﬁnd general solution need look intersection direction spaces u1 u2. corresponding rref obtain identical submatrix 1 0 1 0 1 2 0 0 0 reduced row echelon form augmented system. obtain β1 2β2 u1 u2 span 3 2 1 . arrive ﬁnal solution l1 l2 10 6 2 span 3 2 1 l1 i.e. l1 l2. chapter 3 1. consider r3 deﬁned x r3 x y xay 4 2 1 0 4 1 1 1 5 . an inner product show is symmetric i.e. x y y x. choose x 1 1 0and 1 2 0. x y 16 y x 14 16. general see directly symmetric. similarly symmetric a would need check positive deﬁnite e.g. via eigenvalues a. 3
chapter 4 1. compute determinants following matrices a 1 0 3 0 9 3 7 10 3 17 4 0 11 0 1 6 0 8 0 3 5 1 6 1 8 1 0 3 0 9 3 7 10 3 17 4 0 11 0 1 6 0 8 0 3 5 1 6 1 8 1 0 3 0 9 18 10 28 0 41 4 0 11 0 1 6 0 8 0 3 5 1 6 1 8 added 3 times last row second row. now develop determinant fourth column deta 1145 1 0 3 9 18 10 28 41 4 0 11 1 6 0 8 3 2nd col 10 1 3 9 4 11 1 6 8 3 1033 18 288 594 8 36 4010 use sarrus rule. b b 2 0 4 5 1 1 1 1 9 2 0 0 0 0 2 3 2 0 4 5 1 1 1 1 9 2 0 0 0 0 2 3 col 2 2 4 5 9 0 0 0 2 3 2 2 4 5 1 1 1 0 2 3 9 4 5 2 3 2 2 2 5 1 1 3 2 4 1 1 912 10 22 2 5 32 4 18 26 6 18. could seen second 3 3matrix development 2nd column rank deﬁcient the third row ﬁrst row minus twice second row results determinant 0. 2. consider endomorphism φ r3 r3 transformation matrix 4 0 2 1 3 2 1 2 1 λ r 4
a compute characteristic polynomial determine eigenvalues. pλ deta λi 4 λ 0 2 1 3 λ 2 1 2 1 λ 1st row 4 λ 3 λ 2 2 1 λ 2 1 3 λ 1 2 4 λ 3 λ1 λ 4 22 3 λ 4 λ3 λ1 λ 44 λ 4 23 λ λ3 6λ2 11λ 6. now need ﬁnd eigenvalues i.e. roots pλ λ3 6λ2 11λ 6 0 λ3 6λ2 11λ 6 0 λ 1λ 2λ 3 0 therefore eigenvalues 1 2 3. b compute bases eigenspaces. use gaussian eliminatin determine e1 kera i 3 0 2 1 2 2 1 2 2 3r2 r2 0 6 4 1 2 2 0 0 0 1 6 1 3r2 swap r1 1 0 2 3 0 1 2 3 0 0 0 therefore e1 span 2 2 3 . use gaussian elimination determine e2 kera 2i 2 0 2 1 1 2 1 2 3 2r2 r2 0 2 2 1 1 2 0 1 1 2r3 r3 move r1 move r2 1 0 1 0 1 1 0 0 0 obtain e2 span 1 1 1 finally e3 kera 3i compute via gaussian elimination 1 0 2 1 0 2 1 2 4 r1 r1 1 0 2 0 0 0 0 2 2 swap r3 1 2 1 0 2 0 1 1 0 0 0 e3 span 2 1 1 5
c determine transformation matrix b b1ab diagonal matrix provide diagonal matrix. desired matrix b consists eigenvectors as columns matrix given 2 1 2 2 1 1 3 1 1 . corresponding diagonal matrix b1ab 1 0 0 0 2 0 0 0 3 . note diagonal matrix eigenvalues diagonal. compute b1ab get answer. 3. consider matrix 3 4 4 5 9 5 7 4 8 aim ﬁnd matrix r33 2 a square root a. a find invertible matrix p diagonal matrix p dp 1. characteristic polynomial pλ λ3 14λ2 49λ 36. obvious root polynomial 1 factorize pλ λ1λ4λ9 gives us eigenvalues 1 4 9. use gaussian elimination compute eigenspace e1 kera1i get e1 span 1 0 1 . similarly get e4 span 0 1 1 e9 span 1 2 1 . deﬁne invertible matrix p diagonal matrix p 1 0 1 0 1 2 1 1 1 1 0 0 0 4 0 0 0 9 p dp 1. b let r33 let us assume 2 a. let us consider n p 1mp . show n 2 d. prove n commutes d i.e. nd dn. exploiting associativity matrix multiplication obtain n 2 p 1mp p 1mp p 1mp p 1mp p 1m 2p p 1ap and therefore nd nn 2 n 3 n 2n dn. c explain n thus necessarily diagonal. hint note diagonal values distinct. intuitively diagonal product nd multiplies columns n dn multiplies rows n. nd dn diﬀerent values diagonal n diagonal. let us prove result formally. 6
let us denote nij coeﬃcient matrix n row column j let di denote ith coeﬃcient diagonal d. note example j ranged 1 2 3 result extends matrices arbitrary size. let j 1 2 3. coeﬃcient nd row column j equal nijdj dn equal dinij. matrix equality nd dn yields i j 1 2 3 nijdj nijdi i.e. i j 1 2 3 nijdj di 0. 1 general product null least one factors null. values diagonal diﬀerent 1 equivalent i j 1 2 3 i j nij 0 ensures n diagonal. note two values diagonal equal n would necessarily diagonal would inﬁnitely many candidates n thus many m. d say ns possible values compute matrix m whose square equal a. many diﬀerent matrices there write n n diagn1 n2 n3 n 2 requires n2 1 1 n2 2 4 n2 3 9. diagonal values positive exactly two distinct square roots one. therefore 8 possible values n gather following set diagn1 n2 n3 n1 1 1 n2 2 2 n3 3 3 . now let us set n diag1 2 3 compute product p np 1. first gaussian elimination gives us p 1 1 2 3 1 1 1 0 1 1 1 1 ﬁnd one square root p np 1 0 1 1 1 3 1 2 1 3 . check 2 indeed equals a. choose amongst 8 diﬀerent possible values n ﬁnd new square root a. hence equally many diﬀerent matrices m. 4. let rmn. show that . aaand aa identical nonzero eigenvalues. . q eigenvector aathen aq eigenvector aa. . p eigenvector aa ap eigenvector aa. . start showing λ 0 eigenvalue aathen also nonzero eigenvalue aa. let λ 0 eigenvalue aaand q corresponding eigenvector i.e. aaq λq. aaaq aaaq aλq λaq. need show aq 0 conclude λ eigenvalue aa. assume aq 0. would follow aaq 0 contradicts aaq λq 0 since q eigenvector aawith associated eigenvalue λ. therefore q 0 implies aq 0. therefore λ eigenvalue aa aq corresponding eigenvector. 7
. let us consider case λ 0 eigenvalue aa. want show λ also eigenvalue aa. let λ 0 eigenvalue aa p corresponding eigenvector i.e. aap λp. aaap aaap aλp λap. similar above need show ap 0 draw conclusions. assume ap 0. 0 ap aap λp λ 0. contradicts assumption p eigenvector aa. therefore ap 0. therefore λ 0 eigenvalue aa corresponding eigenvector ap. chapter 5 1. consider f ax r32 x r2. compute partial derivative f a. . start determining dimension partial derivative. knowing dimensions x follows f r3. therefore f a r332. . look every element f f1 f2 f3and determine corresponding partial derivatives. deﬁnition fi 2 x j1 aijxj 1 2 3. therefore fi aij xj fi akj 0 k i. gives f1 a11 x1 f1 a12 x2 f1 a21 0 f1 a22 0 f1 a31 0 f1 a32 0 f2 a11 0 f2 a12 0 f2 a21 x1 f2 a22 x2 f2 a31 0 f2 a32 0 f3 a11 0 f3 a12 0 f3 a21 0 f3 a22 0 f3 a31 x1 f3 a32 x2 18 entries need construct 3 3 2 partial derivative done following way where store partial derivatives f fi j k fi ajk . 8
above see f 1 f a1 x1 0 0 0 x1 0 0 0 x1 r33 f 2 f a2 x2 0 0 0 x2 0 0 0 x2 r33 expect compute partial derivative vector f r3 respect column vector ai r3 matrix a. . alternative approach vectorize a compute partial derivatives reassemble afterwards. here deﬁne vector a1 a2 . . . a6 a11 a21 a31 a12 a22 a32 r6 consists stacked columns a. using vector obtain elements f f1 a1x1 a4x2 f2 a2x1 a5x2 f3 a3x1 a6x2 . partial derivative f r3 respect r6 results 3 6 matrix f a x1 0 0 x2 0 0 0 x1 0 0 x2 0 0 0 x1 0 0 x2 r36. get desired partial derivative f 1 x1 0 0 0 x1 0 0 0 x1 f 2 x2 0 0 0 x2 0 0 0 x2 . chapter 6 chapter 7 9
